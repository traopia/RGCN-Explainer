{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UNDERSTANDING GNN EXPLAINER\n",
    "\n",
    "\n",
    "GNN Explainer is a method used to explain the predictions made by Graph Neural Networks (GNNs) that was introduced in [https://arxiv.org/pdf/1903.03894.pdf]. \n",
    "GNNs are a type of neural network used for processing graph-structured data, and they have shown impressive performance in various domains such as social network analysis, chemistry, and recommendation systems. However, understanding how GNNs make predictions can be challenging due to their complex structure and the lack of transparency in their decision-making process.\n",
    "\n",
    "GNN Explainer provides an intuitive explanation of the predictions made by GNNs by highlighting the most relevant nodes and edges in the input graph. It works by using a secondary GNN model, called the explainer, which is trained to predict the importance of each node and edge in the input graph. The explainer model can then generate an explanation graph, which is a simplified version of the original input graph, highlighting the most important nodes and edges based on their importance scores.\n",
    "\n",
    "The explanation graph generated by GNN Explainer can help users understand why a particular prediction was made by the GNN model and can also aid in identifying potential biases or errors in the GNN model's predictions.\n",
    "\n",
    "## Scope of this tutorial\n",
    "\n",
    "In this tutorial we are going to explore how GNN Explainer works by breaking it down in pieces, as to make it more intuitevely understandable.\n",
    "\n",
    "We will work on the Cora dataset to show all the steps involved, and the model that we will explain is a small GCN model.\n",
    "\n",
    "\n",
    "## Steps\n",
    "The Explain module of GNNExplainer comprises of two main classes: ExplainModule (nn.Module where the Explainer model is defined) and Explainer (Where the Explainer model is called for training)\n",
    "\n",
    "### Explainer\n",
    "Given a node index for which we want to explain the model's node classification, the masked adjacency matrix - whose weights represent the relation importance - are returned.\n",
    "\n",
    "#### Steps\n",
    "1. Extract the neighborhood of the given node: this is done\n",
    "2. Get the original label of the node\n",
    "3. Get the predicted label of the node (argmax from the softmax layer of the network)\n",
    "4. Call the ExplainModule for the training of the Explainer Model\n",
    "\n",
    "\n",
    "\n",
    "## ExplainModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we import the necessary libraries\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "#for the model \n",
    "from torch_geometric.nn import GCNConv\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Explain Module\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "#to import the data\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.transforms import NormalizeFeatures\n",
    "from torch_geometric.utils import to_networkx"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of graphs: 1\n",
      "Number of features: 1433\n",
      "Number of classes: 7\n",
      "dataset under study\n",
      "Number of nodes: 2708\n",
      "Number of edges: 10556\n"
     ]
    }
   ],
   "source": [
    "#Here we import the Cora dataset. We use the Cora dataset because it is a small dataset and it is easy to understand.\n",
    "#We import the dataset from the Pytorch Geometric library as it is already preprocessed - and the goal is then to further expand the GNN implementation to other datasets starting from the PYG implementation.\n",
    "\n",
    "#import the dataset and print some information about it\n",
    "dataset = Planetoid(root='data/Planetoid', name='Cora', transform=NormalizeFeatures())\n",
    "print(f'Number of graphs: {len(dataset)}')\n",
    "print(f'Number of features: {dataset.num_features}')\n",
    "print(f'Number of classes: {dataset.num_classes}')\n",
    "data = dataset[0]\n",
    "print('dataset under study')\n",
    "print(f'Number of nodes: {data.num_nodes}')\n",
    "print(f'Number of edges: {data.num_edges}')\n",
    "\n",
    "#for convention to how things are named we rename the data\n",
    "label = data.y\n",
    "feat = data.x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we want to visualize the graph, we need to convert the data into a networkx graph. We can then use the networkx library to visualize the graph. Each node color represents a different class in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "G = to_networkx(data, to_undirected=True)\n",
    "#nx.draw(G, with_labels=False, node_color=data.y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another cool thing about importing our graph into a networkx object is that we can easily extract its adjacency matrix - which we will need to dig into the GNN Explainer method.\n",
    "\n",
    "We can easily see from the Adjacency matrix that the graph is directed as adj is not symmetric, but indeed as it should be is a square matrix.\n",
    "\n",
    "Just a reminder: what is an Adjacency matrix of a graph?\n",
    "\n",
    "It is a matrix where each row correspond to a node, and the row/column combination represent the connectivity between two nodes. \n",
    "In particular for node 1, we can see that it has a connection towards node 2, because it is a non zero element of the node 1 row.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connectivity for node 1: \n",
      " we can see that it is connected to node 2 [0 0 1 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "adj = nx.adjacency_matrix(G).todense()\n",
    "print('Connectivity for node 1: \\n we can see that it is connected to node 2',adj[1][0:10])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Extract the neighborhood of the given node\n",
    "- We are interested in getting a neighborhood not just in terms of actual neighbors but also in terms of potentian neighbors - namely node which are not directly connected but they would be connected if we consider a different number of hops.\n",
    "\n",
    "- For instance in a simple graph a-b-c: a is connected to c if we consider 2 hops.\n",
    "\n",
    "- We want to have a function that gets that for all our graph - so we rely on the adjacency matrix and we modify it according to the number of hops that we want to take into consideration.\n",
    "\n",
    "- We add self loops as well in terms of connections\n",
    "\n",
    "#from graph_utils import neighborhoods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2 hops Connectivity for node 1: \n",
      " we can see that it is connected to node 2 [0 1 1 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "#line 147 in utils/graph_utils.py\n",
    "def neighborhoods(adj, n_hops):\n",
    "    \"\"\"Returns the n_hops degree adjacency matrix adj.\"\"\"\n",
    "\n",
    "    adj = torch.tensor(adj, dtype=torch.float)\n",
    "    hop_adj = power_adj = adj\n",
    "    for i in range(n_hops - 1):\n",
    "        power_adj = power_adj @ adj\n",
    "        prev_hop_adj = hop_adj\n",
    "        hop_adj = hop_adj + power_adj\n",
    "        #print(type(hop_adj))\n",
    "        hop_adj = (hop_adj > 0).float()\n",
    "        #print(hop_adj)\n",
    "    return hop_adj.cpu().numpy().astype(int)\n",
    "n_hops = 2\n",
    "adj_hop = neighborhoods(adj, n_hops)\n",
    "print(f' {n_hops} hops Connectivity for node 1: \\n we can see that it is connected to node 2',adj_hop[1,:])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract neighborhood\n",
    "We are simplifying the function as we are working with 1 graph only (indeed we are working on node mode)\n",
    "\n",
    "Extract neighborhood which returns the new index node, the adjacency matrix of the neighboor, the features of the neighboor, the label and the actual neighbors.\n",
    "\n",
    "\n",
    "- So the neighbors are indeed the neighbros as defined to be the neighbors according to the number of hops used\n",
    "- the sub adj: is the adjacency matrix of the neighbors (adjacency where select only row and cols of the neighbor ) -- thus its size depends on how many neighbors the node has\n",
    "- new index usually just zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ww/33zq_rh50tx94n81lb4thx0w0000gn/T/ipykernel_91515/2699400499.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  adj = torch.tensor(adj, dtype=torch.float)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0,\n",
       " tensor([[0., 1., 0., 0., 0., 1., 0., 1.],\n",
       "         [1., 0., 0., 0., 1., 0., 1., 0.],\n",
       "         [0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "         [0., 1., 0., 0., 0., 1., 1., 0.],\n",
       "         [1., 0., 1., 0., 1., 0., 0., 1.],\n",
       "         [0., 1., 0., 0., 1., 0., 0., 0.],\n",
       "         [1., 0., 0., 1., 0., 1., 0., 0.]]),\n",
       " tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         ...,\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0667, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]),\n",
       " tensor([3, 3, 3, 3, 3, 3, 3, 3]),\n",
       " array([   0,  633,  926, 1166, 1701, 1862, 1866, 2582]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#line 506 in explain.py\n",
    "\n",
    "def extract_neighborhood(node_idx,adj,feat,label,n_hops):\n",
    "    \"\"\"Returns the neighborhood of a given ndoe.\"\"\"\n",
    "    neighbors_adj_row = neighborhoods(adj,n_hops)[node_idx, :] #take row of the node in the new adj matrix\n",
    "    # index of the query node in the new adj\n",
    "    node_idx_new = sum(neighbors_adj_row[:node_idx]) #sum of all the nodes before the query node (since they are 1 or 0) - it becomes count of nodes before the query node\n",
    "    neighbors = np.nonzero(neighbors_adj_row)[0] #return the indices of the nodes that are connected to the query node (and thus are non zero)\n",
    "    sub_adj = adj[neighbors][:, neighbors]\n",
    "    sub_feat = feat[neighbors]\n",
    "    sub_label = label[neighbors]\n",
    "    return node_idx_new, sub_adj, sub_feat, sub_label, neighbors\n",
    "\n",
    "node_idx_new, sub_adj, sub_feat, sub_label, neighbors = extract_neighborhood(0,adj,feat,label,n_hops)\n",
    "node_idx_new, sub_adj, sub_feat, sub_label, neighbors"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can visualize the neighbors for a given node and for a given number of hops!\n",
    "\n",
    "Due to the message passing idea of GNN, this is the structure which is going to be the most influential for the node classification task (namely: number of hops more and less corresponds to the number of layers in our model - IS IT CORRECT???)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "def visualize_hop_neighbor_subgraph(node_idx, data, neighbors, n_hops,adj,feat,label):\n",
    "    \"\"\"Visualizes the n-hop neighborhood of a given node.\"\"\"\n",
    "    node_idx_new, sub_adj, sub_feat, sub_label, neighbors = extract_neighborhood(node_idx,adj,feat,label,n_hops)\n",
    "    subdata = data.subgraph(torch.tensor(neighbors))\n",
    "    subindex = subdata.edge_index\n",
    "    Gsub = G = to_networkx(subdata, to_undirected=False)\n",
    "    labeldict = {}\n",
    "    for i,j in zip(range(len(neighbors)),neighbors):\n",
    "        labeldict[i] = j\n",
    "    nx.draw(Gsub, labels = labeldict, node_color = data.y[neighbors], cmap=\"Set2\")\n",
    "    return subdata, subindex\n",
    "\n",
    "\n",
    "node_idx = 1\n",
    "subdata_Data, subindex = visualize_hop_neighbor_subgraph(node_idx, data, neighbors, 2,adj,feat,label)\n",
    "subdata = subdata_Data.x\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Get the original label of the node - we already have it \n",
    "3. Get the predicted label of the node:\n",
    "\n",
    "A. Here is where we have to start thinking about the model that we want to explain.\n",
    "In this next section we will thus implement a very straight - forward GCN model adapted to how the models in GNN explainer have been designed. \n",
    "\n",
    "B. Another option implies taking a very straightforward model and then modify the structure of the explainer such that we can actually use that model\n",
    "\n",
    "\n",
    "For the sake of simplicity we first work on B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
       "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
       "        [0., 0., 0.,  ..., 0., 1., 0.]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_adjacency(data):\n",
    "    adj = torch.zeros(data.num_nodes, data.num_nodes)\n",
    "    for edge in data.edge_index.t():\n",
    "        adj[edge[0]][edge[1]] = 1\n",
    "    return adj\n",
    "\n",
    "adj = get_adjacency(data)\n",
    "#torch.save(adj, 'cora_chk/adj_cora')\n",
    "adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DEFINE THE MODEL\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(1234567)\n",
    "        self.conv1 = GCNConv(input_dim, 16)\n",
    "        self.conv2 = GCNConv(16, output_dim)\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        edge_index = adj.nonzero().t()\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return F.log_softmax(x, dim=1), adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|         | 23/400 [00:03<00:58,  6.43it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 58\u001b[0m\n\u001b[1;32m     54\u001b[0m       \u001b[39mreturn\u001b[39;00m test_acc\n\u001b[1;32m     57\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m tqdm(\u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, \u001b[39m401\u001b[39m)):\n\u001b[0;32m---> 58\u001b[0m     loss \u001b[39m=\u001b[39m train()\n\u001b[1;32m     60\u001b[0m test_acc \u001b[39m=\u001b[39m test()\n\u001b[1;32m     61\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mTest Accuracy: \u001b[39m\u001b[39m{\u001b[39;00mtest_acc\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m) \n",
      "Cell \u001b[0;32mIn[16], line 39\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m model\u001b[39m.\u001b[39mtrain()\n\u001b[1;32m     38\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()  \u001b[39m# Clear gradients.\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m out, adj \u001b[39m=\u001b[39m model(data\u001b[39m.\u001b[39mx, get_adjacency(data))  \u001b[39m# Perform a single forward pass.\u001b[39;00m\n\u001b[1;32m     40\u001b[0m loss \u001b[39m=\u001b[39m criterion(out[data\u001b[39m.\u001b[39mtrain_mask], data\u001b[39m.\u001b[39my[data\u001b[39m.\u001b[39mtrain_mask])  \u001b[39m# Compute the loss solely based on the training nodes.\u001b[39;00m\n\u001b[1;32m     41\u001b[0m loss\u001b[39m.\u001b[39mbackward()  \u001b[39m# Derive gradients.\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[13], line 4\u001b[0m, in \u001b[0;36mget_adjacency\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m      2\u001b[0m adj \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros(data\u001b[39m.\u001b[39mnum_nodes, data\u001b[39m.\u001b[39mnum_nodes)\n\u001b[1;32m      3\u001b[0m \u001b[39mfor\u001b[39;00m edge \u001b[39min\u001b[39;00m data\u001b[39m.\u001b[39medge_index\u001b[39m.\u001b[39mt():\n\u001b[0;32m----> 4\u001b[0m     adj[edge[\u001b[39m0\u001b[39m]][edge[\u001b[39m1\u001b[39;49m]] \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[39mreturn\u001b[39;00m adj\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#TRAIN THE MODEL\n",
    "\n",
    "node_idx = 0\n",
    "\n",
    "#DEFINE THE MODEL\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(1234567)\n",
    "        self.conv1 = GCNConv(input_dim, 16)\n",
    "        self.conv2 = GCNConv(16, output_dim)\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        edge_index = adj.nonzero().t()\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return F.log_softmax(x, dim=1), adj\n",
    "    \n",
    "\n",
    "    \n",
    "input_dim = dataset.num_features\n",
    "output_dim = dataset.num_classes\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = Net(input_dim, output_dim).to(device)\n",
    "data = data.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "def train():\n",
    "      model.train()\n",
    "      optimizer.zero_grad()  # Clear gradients.\n",
    "      out, adj = model(data.x, get_adjacency(data))  # Perform a single forward pass.\n",
    "      loss = criterion(out[data.train_mask], data.y[data.train_mask])  # Compute the loss solely based on the training nodes.\n",
    "      loss.backward()  # Derive gradients.\n",
    "      optimizer.step()  # Update parameters based on gradients.\n",
    "      return loss\n",
    "\n",
    "def test():\n",
    "      model.eval()\n",
    "      out, adj = model(data.x, get_adjacency(data))\n",
    "      pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "      test_correct = pred[data.test_mask] == data.y[data.test_mask]  # Check against ground-truth labels.\n",
    "      test_incorrect = pred[data.test_mask] != data.y[data.test_mask] # Check against ground-truth labels.\n",
    "      \n",
    "      test_acc = int(test_correct.sum()) / int(data.test_mask.sum())  # Derive ratio of correct predictions.\n",
    "      torch.save(out, 'cora_chk/prediction_cora')\n",
    "      return test_acc\n",
    "\n",
    "\n",
    "for epoch in tqdm(range(1, 401)):\n",
    "    loss = train()\n",
    "\n",
    "test_acc = test()\n",
    "print(f'Test Accuracy: {test_acc:.4f}') \n",
    "\n",
    "\n",
    "model.eval()\n",
    "pred, adj = model(data.x, get_adjacency(data))\n",
    "print(pred)\n",
    "torch.save(model, 'cora_chk/model_cora')\n",
    "# torch.save(model.state_dict(),'cora_chk/model_cora_dict' )\n",
    "# cg_data = {\n",
    "#       \"adj\": data[\"adj\"],\n",
    "#       \"feat\": data[\"feat\"],\n",
    "#       \"label\": data[\"labels\"],\n",
    "#       \"pred\": pred.cpu().detach().numpy(),\n",
    "#       \"train_idx\": data.train_mask,\n",
    "# }\n",
    "#save_checkpoint(model, optimizer, num_epochs=-1, isbest=False, cg_dict=None)\n",
    "\n",
    "pred, adj = model(data.x, get_adjacency(data))\n",
    "pred_label = pred.argmax(dim=1)\n",
    "\n",
    "print(pred_label[data.test_mask])\n",
    "print(data.y[data.test_mask])\n",
    "#we will only be interested in the predicted labels for the neighbirhood of the query node\n",
    "print(f'Predicted labels for node index, {node_idx} with hops {n_hops}: \\n ',pred_label[neighbors], \n",
    "      f'\\n Actual labels for node index, {node_idx} with hops {n_hops}: \\n ',data.y[neighbors])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 2,\n",
       "        2, 2, 2, 1, 2, 2, 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 5, 2, 2, 1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 3, 4, 4, 4, 4, 1, 1, 3, 1, 0, 3, 0,\n",
       "        2, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 5, 5, 5, 5,\n",
       "        5, 5, 2, 2, 2, 2, 1, 6, 6, 3, 0, 0, 5, 0, 5, 0, 3, 5, 3, 0, 0, 6, 0, 6,\n",
       "        3, 3, 1, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "        3, 3, 5, 5, 5, 5, 5, 5, 5, 5, 2, 2, 2, 4, 4, 4, 0, 3, 3, 2, 5, 5, 5, 5,\n",
       "        6, 5, 5, 5, 5, 0, 4, 4, 4, 0, 0, 5, 0, 0, 6, 6, 6, 6, 6, 6, 0, 0, 0, 0,\n",
       "        3, 0, 0, 0, 3, 3, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "        3, 3, 3, 3, 3, 5, 5, 5, 5, 3, 5, 5, 5, 5, 5, 5, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        6, 6, 5, 6, 6, 3, 5, 5, 5, 0, 5, 0, 4, 4, 3, 3, 3, 2, 2, 1, 3, 3, 3, 3,\n",
       "        3, 3, 5, 3, 3, 4, 4, 3, 3, 3, 3, 3, 3, 3, 0, 3, 3, 6, 3, 6, 0, 5, 0, 0,\n",
       "        4, 0, 6, 5, 5, 0, 1, 3, 3, 5, 6, 5, 3, 3, 4, 3, 3, 3, 3, 3, 4, 3, 3, 4,\n",
       "        3, 1, 1, 0, 1, 0, 6, 0, 0, 0, 0, 0, 0, 0, 5, 0, 5, 5, 5, 3, 3, 3, 3, 3,\n",
       "        0, 0, 0, 2, 0, 0, 0, 3, 3, 3, 3, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 0, 1, 3,\n",
       "        1, 1, 1, 1, 1, 0, 0, 0, 5, 5, 5, 5, 3, 5, 1, 1, 3, 6, 6, 5, 6, 2, 3, 3,\n",
       "        0, 3, 3, 3, 4, 4, 4, 4, 3, 3, 3, 4, 3, 3, 4, 0, 6, 0, 6, 6, 0, 0, 3, 3,\n",
       "        3, 3, 3, 1, 1, 1, 3, 3, 3, 3, 5, 6, 3, 4, 6, 0, 0, 6, 6, 6, 6, 6, 3, 3,\n",
       "        6, 6, 5, 2, 1, 2, 1, 0, 0, 6, 6, 2, 3, 3, 5, 0, 0, 0, 0, 0, 5, 5, 0, 3,\n",
       "        5, 0, 6, 3, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 1, 6, 1, 0,\n",
       "        3, 3, 3, 3, 3, 6, 1, 0, 2, 2, 4, 4, 4, 4, 4, 5, 6, 3, 3, 0, 0, 0, 0, 5,\n",
       "        4, 4, 4, 4, 4, 3, 3, 3, 3, 3, 0, 3, 4, 4, 4, 1, 1, 3, 1, 1, 5, 1, 3, 4,\n",
       "        4, 4, 4, 4, 4, 4, 0, 0, 0, 5, 5, 5, 5, 5, 0, 5, 3, 0, 6, 2, 0, 5, 3, 3,\n",
       "        5, 5, 5, 5, 5, 4, 4, 0, 4, 0, 4, 0, 3, 4, 4, 4, 1, 3, 3, 3, 3, 3, 4, 2,\n",
       "        3, 3, 3, 0, 0, 2, 3, 3, 3, 3, 1, 1, 3, 0, 1, 4, 1, 1, 1, 1, 1, 1, 0, 1,\n",
       "        0, 0, 2, 4, 4, 4, 3, 3, 3, 4, 0, 3, 3, 3, 3, 0, 3, 3, 4, 4, 4, 4, 4, 4,\n",
       "        0, 4, 3, 2, 0, 3, 4, 5, 0, 2, 2, 3, 3, 3, 3, 3, 2, 3, 5, 5, 4, 1, 4, 4,\n",
       "        4, 3, 4, 4, 0, 4, 4, 4, 5, 2, 2, 2, 2, 4, 6, 6, 6, 6, 3, 4, 4, 4, 1, 3,\n",
       "        0, 3, 3, 5, 0, 2, 3, 3, 3, 3, 3, 2, 4, 4, 0, 0, 3, 2, 6, 6, 0, 3, 3, 3,\n",
       "        5, 1, 3, 4, 4, 2, 4, 4, 4, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2,\n",
       "        2, 0, 6, 6, 5, 6, 6, 3, 2, 6, 3, 4, 4, 4, 2, 6, 6, 0, 0, 3, 0, 4, 4, 3,\n",
       "        2, 3, 1, 6, 6, 5, 3, 4, 3, 5, 3, 1, 1, 3, 4, 5, 2, 3, 3, 3, 4, 5, 4, 0,\n",
       "        3, 3, 0, 2, 1, 1, 5, 2, 3, 3, 5, 0, 2, 3, 2, 2, 5, 5, 4, 3, 4, 3, 2, 2,\n",
       "        4, 2, 4, 5, 5, 3, 2, 3, 1, 0, 3, 3, 4, 5, 4, 3, 3, 3, 3, 3, 0, 1, 2, 4,\n",
       "        4, 4, 3, 3, 3, 5, 2, 3, 2, 2, 2, 3, 2, 2, 0, 4, 4, 3, 3, 3, 3, 3, 3, 3,\n",
       "        3, 3, 3, 0, 0, 3, 0, 3, 0, 2, 3, 4, 1, 2, 5, 4, 3, 3, 3, 1, 5, 3, 4, 3,\n",
       "        2, 2, 1, 3, 3, 3, 3, 3, 6, 3, 3, 3, 6, 3, 3, 3, 2, 3, 2, 4, 2, 4, 2, 2,\n",
       "        1, 5, 6, 4, 3, 3, 3, 2, 5, 3, 3, 4, 3, 3, 3, 3, 3, 4, 6, 0, 3, 2, 2, 2,\n",
       "        5, 4, 4, 4, 4, 6, 3, 2, 2, 0, 2, 2, 2, 2, 2, 3, 4, 4, 4, 3, 3, 4, 4, 3,\n",
       "        3, 3, 4, 4, 4, 4, 4, 4, 3, 4, 4, 4, 4, 4, 4, 4, 4, 2, 3, 3, 3, 2, 6, 2,\n",
       "        3, 3, 4, 4, 3, 3, 3, 3, 3, 3, 0, 3, 3, 3, 3, 3])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = torch.load('cora_chk/prediction_cora')\n",
    "pred_label = pred.argmax(dim=1)\n",
    "pred_label[data.test_mask]\n",
    "data.y[data.test_mask]\n",
    "#print(data.y[data.test_mask])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mislabeled [141, 147, 161, 162, 167, 168, 172, 184, 191, 192, 224, 225, 240, 250, 252, 254, 255, 256, 260, 267, 272, 277, 278, 280, 285, 287, 291, 292, 293, 300, 311, 319, 320, 323, 332, 342, 354, 356, 365, 376, 377, 380, 383, 388, 397, 398, 401, 404, 417, 433, 435, 436, 438, 446, 454, 461, 462, 465, 467, 471, 473, 474, 476, 477, 478, 480, 490, 496, 498, 502, 503, 506, 512, 516, 517, 518, 520, 522, 530, 531, 548, 553, 554, 555, 571, 573, 574, 576, 578, 581, 590, 591, 592, 597, 600, 603, 606, 611, 614, 617, 626, 628, 630, 641, 642, 644, 645, 659, 670, 674, 675, 678, 680, 695, 696, 699, 702, 711, 717, 724, 728, 733, 738, 743, 749, 753, 755, 756, 760, 767, 768, 772, 776, 780, 791, 794, 798, 803, 818, 822, 827, 832, 842, 843, 849, 852, 854, 863, 865, 868, 872, 873, 876, 880, 896, 903, 906, 913, 917, 930, 931, 934, 939, 940, 943, 945, 948, 949, 952, 953, 968, 973, 974, 979, 989, 992, 993, 995]\n",
      "correct [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 142, 143, 144, 145, 146, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 163, 164, 165, 166, 169, 170, 171, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 185, 186, 187, 188, 189, 190, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 241, 242, 243, 244, 245, 246, 247, 248, 249, 251, 253, 257, 258, 259, 261, 262, 263, 264, 265, 266, 268, 269, 270, 271, 273, 274, 275, 276, 279, 281, 282, 283, 284, 286, 288, 289, 290, 294, 295, 296, 297, 298, 299, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 312, 313, 314, 315, 316, 317, 318, 321, 322, 324, 325, 326, 327, 328, 329, 330, 331, 333, 334, 335, 336, 337, 338, 339, 340, 341, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 355, 357, 358, 359, 360, 361, 362, 363, 364, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 378, 379, 381, 382, 384, 385, 386, 387, 389, 390, 391, 392, 393, 394, 395, 396, 399, 400, 402, 403, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 434, 437, 439, 440, 441, 442, 443, 444, 445, 447, 448, 449, 450, 451, 452, 453, 455, 456, 457, 458, 459, 460, 463, 464, 466, 468, 469, 470, 472, 475, 479, 481, 482, 483, 484, 485, 486, 487, 488, 489, 491, 492, 493, 494, 495, 497, 499, 500, 501, 504, 505, 507, 508, 509, 510, 511, 513, 514, 515, 519, 521, 523, 524, 525, 526, 527, 528, 529, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 549, 550, 551, 552, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 572, 575, 577, 579, 580, 582, 583, 584, 585, 586, 587, 588, 589, 593, 594, 595, 596, 598, 599, 601, 602, 604, 605, 607, 608, 609, 610, 612, 613, 615, 616, 618, 619, 620, 621, 622, 623, 624, 625, 627, 629, 631, 632, 633, 634, 635, 636, 637, 638, 639, 640, 643, 646, 647, 648, 649, 650, 651, 652, 653, 654, 655, 656, 657, 658, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 671, 672, 673, 676, 677, 679, 681, 682, 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694, 697, 698, 700, 701, 703, 704, 705, 706, 707, 708, 709, 710, 712, 713, 714, 715, 716, 718, 719, 720, 721, 722, 723, 725, 726, 727, 729, 730, 731, 732, 734, 735, 736, 737, 739, 740, 741, 742, 744, 745, 746, 747, 748, 750, 751, 752, 754, 757, 758, 759, 761, 762, 763, 764, 765, 766, 769, 770, 771, 773, 774, 775, 777, 778, 779, 781, 782, 783, 784, 785, 786, 787, 788, 789, 790, 792, 793, 795, 796, 797, 799, 800, 801, 802, 804, 805, 806, 807, 808, 809, 810, 811, 812, 813, 814, 815, 816, 817, 819, 820, 821, 823, 824, 825, 826, 828, 829, 830, 831, 833, 834, 835, 836, 837, 838, 839, 840, 841, 844, 845, 846, 847, 848, 850, 851, 853, 855, 856, 857, 858, 859, 860, 861, 862, 864, 866, 867, 869, 870, 871, 874, 875, 877, 878, 879, 881, 882, 883, 884, 885, 886, 887, 888, 889, 890, 891, 892, 893, 894, 895, 897, 898, 899, 900, 901, 902, 904, 905, 907, 908, 909, 910, 911, 912, 914, 915, 916, 918, 919, 920, 921, 922, 923, 924, 925, 926, 927, 928, 929, 932, 933, 935, 936, 937, 938, 941, 942, 944, 946, 947, 950, 951, 954, 955, 956, 957, 958, 959, 960, 961, 962, 963, 964, 965, 966, 967, 969, 970, 971, 972, 975, 976, 977, 978, 980, 981, 982, 983, 984, 985, 986, 987, 988, 990, 991, 994, 996, 997, 998, 999]\n"
     ]
    }
   ],
   "source": [
    "mislabeled = []\n",
    "correct = []\n",
    "for i in range(1000):\n",
    "    if torch.load('cora_chk/prediction_cora').argmax(dim=1)[i] != data.y[i]:\n",
    "        #print(i, pred_label[i], data.y[i])\n",
    "        mislabeled.append(i)\n",
    "    else:\n",
    "        correct.append(i)\n",
    "print('mislabeled', mislabeled)    \n",
    "print('correct',correct)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1708\n",
      "1709\n",
      "1710\n",
      "1711\n",
      "1712\n",
      "1713\n",
      "1714\n",
      "1715\n",
      "1716\n",
      "1717\n",
      "1718\n",
      "1719\n",
      "1720\n",
      "1721\n",
      "1722\n",
      "1723\n",
      "1724\n",
      "1725\n",
      "1726\n",
      "1727\n",
      "1728\n",
      "1729\n",
      "1730\n",
      "1731\n",
      "1732\n",
      "1733\n",
      "1734\n",
      "1735\n",
      "1736\n",
      "1737\n",
      "1738\n",
      "1739\n",
      "1740\n",
      "1741\n",
      "1742\n",
      "1743\n",
      "1744\n",
      "1745\n",
      "1746\n",
      "1747\n",
      "1748\n",
      "1749\n",
      "1750\n",
      "1751\n",
      "1752\n",
      "1753\n",
      "1754\n",
      "1755\n",
      "1756\n",
      "1757\n",
      "1758\n",
      "1759\n",
      "1760\n",
      "1761\n",
      "1762\n",
      "1763\n",
      "1764\n",
      "1765\n",
      "1766\n",
      "1767\n",
      "1768\n",
      "1769\n",
      "1770\n",
      "1771\n",
      "1772\n",
      "1773\n",
      "1774\n",
      "1775\n",
      "1776\n",
      "1777\n",
      "1778\n",
      "1779\n",
      "1780\n",
      "1781\n",
      "1782\n",
      "1783\n",
      "1784\n",
      "1785\n",
      "1786\n",
      "1787\n",
      "1788\n",
      "1789\n",
      "1790\n",
      "1791\n",
      "1792\n",
      "1793\n",
      "1794\n",
      "1795\n",
      "1796\n",
      "1797\n",
      "1798\n",
      "1799\n",
      "1800\n",
      "1801\n",
      "1802\n",
      "1803\n",
      "1804\n",
      "1805\n",
      "1806\n",
      "1807\n",
      "1808\n",
      "1809\n",
      "1810\n",
      "1811\n",
      "1812\n",
      "1813\n",
      "1814\n",
      "1815\n",
      "1816\n",
      "1817\n",
      "1818\n",
      "1819\n",
      "1820\n",
      "1821\n",
      "1822\n",
      "1823\n",
      "1824\n",
      "1825\n",
      "1826\n",
      "1827\n",
      "1828\n",
      "1829\n",
      "1830\n",
      "1831\n",
      "1832\n",
      "1833\n",
      "1834\n",
      "1835\n",
      "1836\n",
      "1837\n",
      "1838\n",
      "1839\n",
      "1840\n",
      "1841\n",
      "1842\n",
      "1843\n",
      "1844\n",
      "1845\n",
      "1846\n",
      "1847\n",
      "1848\n",
      "1849\n",
      "1850\n",
      "1851\n",
      "1852\n",
      "1853\n",
      "1854\n",
      "1855\n",
      "1856\n",
      "1857\n",
      "1858\n",
      "1859\n",
      "1860\n",
      "1861\n",
      "1862\n",
      "1863\n",
      "1864\n",
      "1865\n",
      "1866\n",
      "1867\n",
      "1868\n",
      "1869\n",
      "1870\n",
      "1871\n",
      "1872\n",
      "1873\n",
      "1874\n",
      "1875\n",
      "1876\n",
      "1877\n",
      "1878\n",
      "1879\n",
      "1880\n",
      "1881\n",
      "1882\n",
      "1883\n",
      "1884\n",
      "1885\n",
      "1886\n",
      "1887\n",
      "1888\n",
      "1889\n",
      "1890\n",
      "1891\n",
      "1892\n",
      "1893\n",
      "1894\n",
      "1895\n",
      "1896\n",
      "1897\n",
      "1898\n",
      "1899\n",
      "1900\n",
      "1901\n",
      "1902\n",
      "1903\n",
      "1904\n",
      "1905\n",
      "1906\n",
      "1907\n",
      "1908\n",
      "1909\n",
      "1910\n",
      "1911\n",
      "1912\n",
      "1913\n",
      "1914\n",
      "1915\n",
      "1916\n",
      "1917\n",
      "1918\n",
      "1919\n",
      "1920\n",
      "1921\n",
      "1922\n",
      "1923\n",
      "1924\n",
      "1925\n",
      "1926\n",
      "1927\n",
      "1928\n",
      "1929\n",
      "1930\n",
      "1931\n",
      "1932\n",
      "1933\n",
      "1934\n",
      "1935\n",
      "1936\n",
      "1937\n",
      "1938\n",
      "1939\n",
      "1940\n",
      "1941\n",
      "1942\n",
      "1943\n",
      "1944\n",
      "1945\n",
      "1946\n",
      "1947\n",
      "1948\n",
      "1949\n",
      "1950\n",
      "1951\n",
      "1952\n",
      "1953\n",
      "1954\n",
      "1955\n",
      "1956\n",
      "1957\n",
      "1958\n",
      "1959\n",
      "1960\n",
      "1961\n",
      "1962\n",
      "1963\n",
      "1964\n",
      "1965\n",
      "1966\n",
      "1967\n",
      "1968\n",
      "1969\n",
      "1970\n",
      "1971\n",
      "1972\n",
      "1973\n",
      "1974\n",
      "1975\n",
      "1976\n",
      "1977\n",
      "1978\n",
      "1979\n",
      "1980\n",
      "1981\n",
      "1982\n",
      "1983\n",
      "1984\n",
      "1985\n",
      "1986\n",
      "1987\n",
      "1988\n",
      "1989\n",
      "1990\n",
      "1991\n",
      "1992\n",
      "1993\n",
      "1994\n",
      "1995\n",
      "1996\n",
      "1997\n",
      "1998\n",
      "1999\n",
      "2000\n",
      "2001\n",
      "2002\n",
      "2003\n",
      "2004\n",
      "2005\n",
      "2006\n",
      "2007\n",
      "2008\n",
      "2009\n",
      "2010\n",
      "2011\n",
      "2012\n",
      "2013\n",
      "2014\n",
      "2015\n",
      "2016\n",
      "2017\n",
      "2018\n",
      "2019\n",
      "2020\n",
      "2021\n",
      "2022\n",
      "2023\n",
      "2024\n",
      "2025\n",
      "2026\n",
      "2027\n",
      "2028\n",
      "2029\n",
      "2030\n",
      "2031\n",
      "2032\n",
      "2033\n",
      "2034\n",
      "2035\n",
      "2036\n",
      "2037\n",
      "2038\n",
      "2039\n",
      "2040\n",
      "2041\n",
      "2042\n",
      "2043\n",
      "2044\n",
      "2045\n",
      "2046\n",
      "2047\n",
      "2048\n",
      "2049\n",
      "2050\n",
      "2051\n",
      "2052\n",
      "2053\n",
      "2054\n",
      "2055\n",
      "2056\n",
      "2057\n",
      "2058\n",
      "2059\n",
      "2060\n",
      "2061\n",
      "2062\n",
      "2063\n",
      "2064\n",
      "2065\n",
      "2066\n",
      "2067\n",
      "2068\n",
      "2069\n",
      "2070\n",
      "2071\n",
      "2072\n",
      "2073\n",
      "2074\n",
      "2075\n",
      "2076\n",
      "2077\n",
      "2078\n",
      "2079\n",
      "2080\n",
      "2081\n",
      "2082\n",
      "2083\n",
      "2084\n",
      "2085\n",
      "2086\n",
      "2087\n",
      "2088\n",
      "2089\n",
      "2090\n",
      "2091\n",
      "2092\n",
      "2093\n",
      "2094\n",
      "2095\n",
      "2096\n",
      "2097\n",
      "2098\n",
      "2099\n",
      "2100\n",
      "2101\n",
      "2102\n",
      "2103\n",
      "2104\n",
      "2105\n",
      "2106\n",
      "2107\n",
      "2108\n",
      "2109\n",
      "2110\n",
      "2111\n",
      "2112\n",
      "2113\n",
      "2114\n",
      "2115\n",
      "2116\n",
      "2117\n",
      "2118\n",
      "2119\n",
      "2120\n",
      "2121\n",
      "2122\n",
      "2123\n",
      "2124\n",
      "2125\n",
      "2126\n",
      "2127\n",
      "2128\n",
      "2129\n",
      "2130\n",
      "2131\n",
      "2132\n",
      "2133\n",
      "2134\n",
      "2135\n",
      "2136\n",
      "2137\n",
      "2138\n",
      "2139\n",
      "2140\n",
      "2141\n",
      "2142\n",
      "2143\n",
      "2144\n",
      "2145\n",
      "2146\n",
      "2147\n",
      "2148\n",
      "2149\n",
      "2150\n",
      "2151\n",
      "2152\n",
      "2153\n",
      "2154\n",
      "2155\n",
      "2156\n",
      "2157\n",
      "2158\n",
      "2159\n",
      "2160\n",
      "2161\n",
      "2162\n",
      "2163\n",
      "2164\n",
      "2165\n",
      "2166\n",
      "2167\n",
      "2168\n",
      "2169\n",
      "2170\n",
      "2171\n",
      "2172\n",
      "2173\n",
      "2174\n",
      "2175\n",
      "2176\n",
      "2177\n",
      "2178\n",
      "2179\n",
      "2180\n",
      "2181\n",
      "2182\n",
      "2183\n",
      "2184\n",
      "2185\n",
      "2186\n",
      "2187\n",
      "2188\n",
      "2189\n",
      "2190\n",
      "2191\n",
      "2192\n",
      "2193\n",
      "2194\n",
      "2195\n",
      "2196\n",
      "2197\n",
      "2198\n",
      "2199\n",
      "2200\n",
      "2201\n",
      "2202\n",
      "2203\n",
      "2204\n",
      "2205\n",
      "2206\n",
      "2207\n",
      "2208\n",
      "2209\n",
      "2210\n",
      "2211\n",
      "2212\n",
      "2213\n",
      "2214\n",
      "2215\n",
      "2216\n",
      "2217\n",
      "2218\n",
      "2219\n",
      "2220\n",
      "2221\n",
      "2222\n",
      "2223\n",
      "2224\n",
      "2225\n",
      "2226\n",
      "2227\n",
      "2228\n",
      "2229\n",
      "2230\n",
      "2231\n",
      "2232\n",
      "2233\n",
      "2234\n",
      "2235\n",
      "2236\n",
      "2237\n",
      "2238\n",
      "2239\n",
      "2240\n",
      "2241\n",
      "2242\n",
      "2243\n",
      "2244\n",
      "2245\n",
      "2246\n",
      "2247\n",
      "2248\n",
      "2249\n",
      "2250\n",
      "2251\n",
      "2252\n",
      "2253\n",
      "2254\n",
      "2255\n",
      "2256\n",
      "2257\n",
      "2258\n",
      "2259\n",
      "2260\n",
      "2261\n",
      "2262\n",
      "2263\n",
      "2264\n",
      "2265\n",
      "2266\n",
      "2267\n",
      "2268\n",
      "2269\n",
      "2270\n",
      "2271\n",
      "2272\n",
      "2273\n",
      "2274\n",
      "2275\n",
      "2276\n",
      "2277\n",
      "2278\n",
      "2279\n",
      "2280\n",
      "2281\n",
      "2282\n",
      "2283\n",
      "2284\n",
      "2285\n",
      "2286\n",
      "2287\n",
      "2288\n",
      "2289\n",
      "2290\n",
      "2291\n",
      "2292\n",
      "2293\n",
      "2294\n",
      "2295\n",
      "2296\n",
      "2297\n",
      "2298\n",
      "2299\n",
      "2300\n",
      "2301\n",
      "2302\n",
      "2303\n",
      "2304\n",
      "2305\n",
      "2306\n",
      "2307\n",
      "2308\n",
      "2309\n",
      "2310\n",
      "2311\n",
      "2312\n",
      "2313\n",
      "2314\n",
      "2315\n",
      "2316\n",
      "2317\n",
      "2318\n",
      "2319\n",
      "2320\n",
      "2321\n",
      "2322\n",
      "2323\n",
      "2324\n",
      "2325\n",
      "2326\n",
      "2327\n",
      "2328\n",
      "2329\n",
      "2330\n",
      "2331\n",
      "2332\n",
      "2333\n",
      "2334\n",
      "2335\n",
      "2336\n",
      "2337\n",
      "2338\n",
      "2339\n",
      "2340\n",
      "2341\n",
      "2342\n",
      "2343\n",
      "2344\n",
      "2345\n",
      "2346\n",
      "2347\n",
      "2348\n",
      "2349\n",
      "2350\n",
      "2351\n",
      "2352\n",
      "2353\n",
      "2354\n",
      "2355\n",
      "2356\n",
      "2357\n",
      "2358\n",
      "2359\n",
      "2360\n",
      "2361\n",
      "2362\n",
      "2363\n",
      "2364\n",
      "2365\n",
      "2366\n",
      "2367\n",
      "2368\n",
      "2369\n",
      "2370\n",
      "2371\n",
      "2372\n",
      "2373\n",
      "2374\n",
      "2375\n",
      "2376\n",
      "2377\n",
      "2378\n",
      "2379\n",
      "2380\n",
      "2381\n",
      "2382\n",
      "2383\n",
      "2384\n",
      "2385\n",
      "2386\n",
      "2387\n",
      "2388\n",
      "2389\n",
      "2390\n",
      "2391\n",
      "2392\n",
      "2393\n",
      "2394\n",
      "2395\n",
      "2396\n",
      "2397\n",
      "2398\n",
      "2399\n",
      "2400\n",
      "2401\n",
      "2402\n",
      "2403\n",
      "2404\n",
      "2405\n",
      "2406\n",
      "2407\n",
      "2408\n",
      "2409\n",
      "2410\n",
      "2411\n",
      "2412\n",
      "2413\n",
      "2414\n",
      "2415\n",
      "2416\n",
      "2417\n",
      "2418\n",
      "2419\n",
      "2420\n",
      "2421\n",
      "2422\n",
      "2423\n",
      "2424\n",
      "2425\n",
      "2426\n",
      "2427\n",
      "2428\n",
      "2429\n",
      "2430\n",
      "2431\n",
      "2432\n",
      "2433\n",
      "2434\n",
      "2435\n",
      "2436\n",
      "2437\n",
      "2438\n",
      "2439\n",
      "2440\n",
      "2441\n",
      "2442\n",
      "2443\n",
      "2444\n",
      "2445\n",
      "2446\n",
      "2447\n",
      "2448\n",
      "2449\n",
      "2450\n",
      "2451\n",
      "2452\n",
      "2453\n",
      "2454\n",
      "2455\n",
      "2456\n",
      "2457\n",
      "2458\n",
      "2459\n",
      "2460\n",
      "2461\n",
      "2462\n",
      "2463\n",
      "2464\n",
      "2465\n",
      "2466\n",
      "2467\n",
      "2468\n",
      "2469\n",
      "2470\n",
      "2471\n",
      "2472\n",
      "2473\n",
      "2474\n",
      "2475\n",
      "2476\n",
      "2477\n",
      "2478\n",
      "2479\n",
      "2480\n",
      "2481\n",
      "2482\n",
      "2483\n",
      "2484\n",
      "2485\n",
      "2486\n",
      "2487\n",
      "2488\n",
      "2489\n",
      "2490\n",
      "2491\n",
      "2492\n",
      "2493\n",
      "2494\n",
      "2495\n",
      "2496\n",
      "2497\n",
      "2498\n",
      "2499\n",
      "2500\n",
      "2501\n",
      "2502\n",
      "2503\n",
      "2504\n",
      "2505\n",
      "2506\n",
      "2507\n",
      "2508\n",
      "2509\n",
      "2510\n",
      "2511\n",
      "2512\n",
      "2513\n",
      "2514\n",
      "2515\n",
      "2516\n",
      "2517\n",
      "2518\n",
      "2519\n",
      "2520\n",
      "2521\n",
      "2522\n",
      "2523\n",
      "2524\n",
      "2525\n",
      "2526\n",
      "2527\n",
      "2528\n",
      "2529\n",
      "2530\n",
      "2531\n",
      "2532\n",
      "2533\n",
      "2534\n",
      "2535\n",
      "2536\n",
      "2537\n",
      "2538\n",
      "2539\n",
      "2540\n",
      "2541\n",
      "2542\n",
      "2543\n",
      "2544\n",
      "2545\n",
      "2546\n",
      "2547\n",
      "2548\n",
      "2549\n",
      "2550\n",
      "2551\n",
      "2552\n",
      "2553\n",
      "2554\n",
      "2555\n",
      "2556\n",
      "2557\n",
      "2558\n",
      "2559\n",
      "2560\n",
      "2561\n",
      "2562\n",
      "2563\n",
      "2564\n",
      "2565\n",
      "2566\n",
      "2567\n",
      "2568\n",
      "2569\n",
      "2570\n",
      "2571\n",
      "2572\n",
      "2573\n",
      "2574\n",
      "2575\n",
      "2576\n",
      "2577\n",
      "2578\n",
      "2579\n",
      "2580\n",
      "2581\n",
      "2582\n",
      "2583\n",
      "2584\n",
      "2585\n",
      "2586\n",
      "2587\n",
      "2588\n",
      "2589\n",
      "2590\n",
      "2591\n",
      "2592\n",
      "2593\n",
      "2594\n",
      "2595\n",
      "2596\n",
      "2597\n",
      "2598\n",
      "2599\n",
      "2600\n",
      "2601\n",
      "2602\n",
      "2603\n",
      "2604\n",
      "2605\n",
      "2606\n",
      "2607\n",
      "2608\n",
      "2609\n",
      "2610\n",
      "2611\n",
      "2612\n",
      "2613\n",
      "2614\n",
      "2615\n",
      "2616\n",
      "2617\n",
      "2618\n",
      "2619\n",
      "2620\n",
      "2621\n",
      "2622\n",
      "2623\n",
      "2624\n",
      "2625\n",
      "2626\n",
      "2627\n",
      "2628\n",
      "2629\n",
      "2630\n",
      "2631\n",
      "2632\n",
      "2633\n",
      "2634\n",
      "2635\n",
      "2636\n",
      "2637\n",
      "2638\n",
      "2639\n",
      "2640\n",
      "2641\n",
      "2642\n",
      "2643\n",
      "2644\n",
      "2645\n",
      "2646\n",
      "2647\n",
      "2648\n",
      "2649\n",
      "2650\n",
      "2651\n",
      "2652\n",
      "2653\n",
      "2654\n",
      "2655\n",
      "2656\n",
      "2657\n",
      "2658\n",
      "2659\n",
      "2660\n",
      "2661\n",
      "2662\n",
      "2663\n",
      "2664\n",
      "2665\n",
      "2666\n",
      "2667\n",
      "2668\n",
      "2669\n",
      "2670\n",
      "2671\n",
      "2672\n",
      "2673\n",
      "2674\n",
      "2675\n",
      "2676\n",
      "2677\n",
      "2678\n",
      "2679\n",
      "2680\n",
      "2681\n",
      "2682\n",
      "2683\n",
      "2684\n",
      "2685\n",
      "2686\n",
      "2687\n",
      "2688\n",
      "2689\n",
      "2690\n",
      "2691\n",
      "2692\n",
      "2693\n",
      "2694\n",
      "2695\n",
      "2696\n",
      "2697\n",
      "2698\n",
      "2699\n",
      "2700\n",
      "2701\n",
      "2702\n",
      "2703\n",
      "2704\n",
      "2705\n",
      "2706\n",
      "2707\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = 0\n",
    "for i,j in zip(data.test_mask, range(len(data.test_mask))):\n",
    "    if i==True:\n",
    "        print(j)\n",
    "        a +=1\n",
    "a        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1849,\n",
       " 1855,\n",
       " 1869,\n",
       " 1870,\n",
       " 1875,\n",
       " 1876,\n",
       " 1880,\n",
       " 1892,\n",
       " 1899,\n",
       " 1900,\n",
       " 1932,\n",
       " 1933,\n",
       " 1948,\n",
       " 1958,\n",
       " 1960,\n",
       " 1962,\n",
       " 1963,\n",
       " 1964,\n",
       " 1968,\n",
       " 1975,\n",
       " 1980,\n",
       " 1985,\n",
       " 1986,\n",
       " 1988,\n",
       " 1993,\n",
       " 1995,\n",
       " 1999,\n",
       " 2000,\n",
       " 2001,\n",
       " 2008,\n",
       " 2019,\n",
       " 2027,\n",
       " 2028,\n",
       " 2031,\n",
       " 2040,\n",
       " 2050,\n",
       " 2062,\n",
       " 2064,\n",
       " 2073,\n",
       " 2084,\n",
       " 2085,\n",
       " 2088,\n",
       " 2091,\n",
       " 2096,\n",
       " 2105,\n",
       " 2106,\n",
       " 2109,\n",
       " 2112,\n",
       " 2125,\n",
       " 2141,\n",
       " 2143,\n",
       " 2144,\n",
       " 2146,\n",
       " 2154,\n",
       " 2162,\n",
       " 2169,\n",
       " 2170,\n",
       " 2173,\n",
       " 2175,\n",
       " 2179,\n",
       " 2181,\n",
       " 2182,\n",
       " 2184,\n",
       " 2185,\n",
       " 2186,\n",
       " 2188,\n",
       " 2198,\n",
       " 2204,\n",
       " 2206,\n",
       " 2210,\n",
       " 2211,\n",
       " 2214,\n",
       " 2220,\n",
       " 2224,\n",
       " 2225,\n",
       " 2226,\n",
       " 2228,\n",
       " 2230,\n",
       " 2238,\n",
       " 2239,\n",
       " 2256,\n",
       " 2261,\n",
       " 2262,\n",
       " 2263,\n",
       " 2279,\n",
       " 2281,\n",
       " 2282,\n",
       " 2284,\n",
       " 2286,\n",
       " 2289,\n",
       " 2298,\n",
       " 2299,\n",
       " 2300,\n",
       " 2305,\n",
       " 2308,\n",
       " 2311,\n",
       " 2314,\n",
       " 2319,\n",
       " 2322,\n",
       " 2325,\n",
       " 2334,\n",
       " 2336,\n",
       " 2338,\n",
       " 2349,\n",
       " 2350,\n",
       " 2352,\n",
       " 2353,\n",
       " 2367,\n",
       " 2378,\n",
       " 2382,\n",
       " 2383,\n",
       " 2386,\n",
       " 2388,\n",
       " 2403,\n",
       " 2404,\n",
       " 2407,\n",
       " 2410,\n",
       " 2419,\n",
       " 2425,\n",
       " 2432,\n",
       " 2436,\n",
       " 2441,\n",
       " 2446,\n",
       " 2451,\n",
       " 2457,\n",
       " 2461,\n",
       " 2463,\n",
       " 2464,\n",
       " 2468,\n",
       " 2475,\n",
       " 2476,\n",
       " 2480,\n",
       " 2484,\n",
       " 2488,\n",
       " 2499,\n",
       " 2502,\n",
       " 2506,\n",
       " 2511,\n",
       " 2526,\n",
       " 2530,\n",
       " 2535,\n",
       " 2540,\n",
       " 2550,\n",
       " 2551,\n",
       " 2557,\n",
       " 2560,\n",
       " 2562,\n",
       " 2571,\n",
       " 2573,\n",
       " 2576,\n",
       " 2580,\n",
       " 2581,\n",
       " 2584,\n",
       " 2588,\n",
       " 2604,\n",
       " 2611,\n",
       " 2614,\n",
       " 2621,\n",
       " 2625,\n",
       " 2638,\n",
       " 2639,\n",
       " 2642,\n",
       " 2647,\n",
       " 2648,\n",
       " 2651,\n",
       " 2653,\n",
       " 2656,\n",
       " 2657,\n",
       " 2660,\n",
       " 2661,\n",
       " 2676,\n",
       " 2681,\n",
       " 2682,\n",
       " 2687,\n",
       " 2697,\n",
       " 2700,\n",
       " 2701,\n",
       " 2703]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = []\n",
    "for i in mislabeled:\n",
    "    m.append(i+1708)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-4.4391, -4.3505, -4.8059,  ..., -3.6707, -5.1621, -5.9049],\n",
       "        [-4.5603, -5.7525, -5.7548,  ..., -0.0281, -6.1082, -5.8389],\n",
       "        [-3.6160, -4.4984, -4.7485,  ..., -0.1217, -5.3765, -5.3285],\n",
       "        ...,\n",
       "        [-0.9638, -0.9840, -6.8011,  ..., -5.7509, -2.1900, -3.4285],\n",
       "        [-3.9921, -3.6853, -4.1449,  ..., -2.7006, -4.9017, -5.7876],\n",
       "        [-3.9293, -3.4936, -3.8329,  ..., -2.8517, -4.5302, -5.4696]],\n",
       "       grad_fn=<LogSoftmaxBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adj = torch.load('cora_chk/adj_cora')\n",
    "m = torch.load('cora_chk/masked_adj')\n",
    "model = torch.load('cora_chk/model_cora')\n",
    "\n",
    "\n",
    "ypred, adj = model(data.x, adj)\n",
    "ypred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# def create_filename(save_dir, isbest=False, num_epochs=-1):\n",
    "#     \"\"\"\n",
    "#     Args:\n",
    "#         args        :  the arguments parsed in the parser\n",
    "#         isbest      :  whether the saved model is the best-performing one\n",
    "#         num_epochs  :  epoch number of the model (when isbest=False)\n",
    "#     \"\"\"\n",
    "#     filename = os.path.join(save_dir, 'cora')\n",
    "#     os.makedirs(filename, exist_ok=True)\n",
    "\n",
    "#     if isbest:\n",
    "#         filename = os.path.join(filename, \"best\")\n",
    "#     elif num_epochs > 0:\n",
    "#         filename = os.path.join(filename, str(num_epochs))\n",
    "\n",
    "#     return filename + \".pth.tar\"\n",
    "\n",
    "\n",
    "# def save_checkpoint(model, optimizer, num_epochs=-1, isbest=False, cg_dict=None):\n",
    "#     \"\"\"Save pytorch model checkpoint.\n",
    "\n",
    "#     Args:\n",
    "#         - model         : The PyTorch model to save.\n",
    "#         - optimizer     : The optimizer used to train the model.\n",
    "#         - args          : A dict of meta-data about the model.\n",
    "#         - num_epochs    : Number of training epochs.\n",
    "#         - isbest        : True if the model has the highest accuracy so far.\n",
    "#         - cg_dict       : A dictionary of the sampled computation graphs.\n",
    "#     \"\"\"\n",
    "#     filename = create_filename('cora_chk', isbest, num_epochs=num_epochs)\n",
    "#     torch.save(\n",
    "#         {\n",
    "#             \"epoch\": num_epochs,\n",
    "#             \"model_type\": 'exp',\n",
    "#             \"optimizer\": optimizer,\n",
    "#             \"model_state\": model.state_dict(),\n",
    "#             \"optimizer_state\": optimizer.state_dict(),\n",
    "#             \"cg\": cg_dict,\n",
    "#         },\n",
    "#         filename,\n",
    "#     )\n",
    "\n",
    "\n",
    "# def load_ckpt( isbest=False):\n",
    "#     '''Load a pre-trained pytorch model from checkpoint.\n",
    "#     '''\n",
    "#     print(\"loading model\")\n",
    "#     filename = create_filename('cora_chk', isbest=False)\n",
    "#     print(filename)\n",
    "#     if os.path.isfile(filename):\n",
    "#         print(\"=> loading checkpoint '{}'\".format(filename))\n",
    "#         ckpt = torch.load(filename)\n",
    "#     else:\n",
    "#         print(\"Checkpoint does not exist!\")\n",
    "#         print(\"Checked path -- {}\".format(filename))\n",
    "#         print(\"Make sure you have provided the correct path!\")\n",
    "#         print(\"You may have forgotten to train a model for this dataset.\")\n",
    "#         print()\n",
    "#         print(\"To train one of the paper's models, run the following\")\n",
    "#         print(\">> python train.py --dataset=DATASET_NAME\")\n",
    "#         print()\n",
    "#         raise Exception(\"File not found.\")\n",
    "#     return ckpt\n",
    "\n",
    "# load_ckpt( isbest=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ckpt = load_ckpt(   isbest=False)\n",
    "# cg_dict = ckpt[\"cg\"] # get computation graph\n",
    "# input_dim = cg_dict[\"feat\"].shape[2] \n",
    "# num_classes = cg_dict[\"pred\"].shape[2]\n",
    "# print(\"Loaded model from {}\".format('cora_chk'))\n",
    "# print(\"input dim: \", input_dim, \"; num classes: \", num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we have the predicted labels for each node stored in pred.\n",
    "4. Call the ExplainModule into an obejct : we will have to dig in the ExplainModule class before going forward with the explanation of the Explainer module"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explain Module\n",
    "inputs: adj, x, model, label \n",
    "\n",
    "1. get mask and mask bias -> construct edge_mask (line 656) - in a similar way the feature masks are initialized\n",
    "\n",
    "->Mask are a torch.nn.parameter.Parameter object randomly initialized with dimension equal to the number of nodes in the neighborhood x number of nodes in the neighborhood (according to different initialization strategies: normal and constant are defined)\n",
    "\n",
    "Feature mask built in a similar way : the size of the nn.Parameter is the size of the last dimension of x (and thus the number of features for the node)\n",
    "Diagonal mask is a  tensor of 1s with 0s on the diagonal \n",
    "\n",
    "2. Forward function ( line 703 )\n",
    "Masked adjacency : (_masked_asdj) is the non linear activated mask (sym_mask: it is symmetric because we add its transpose and then divide by 2) multiplied by the adjacency matrix and the diagonal mask (in this way the diagonal elements are 0s out) -> remember that the adjacency matrix we see here is the one of the neighborhood already so the shapes match \n",
    "??? So the whole idea about masking is that we perturbate the adjacency matrix  - so that the values inside the matrix are not 0s and 1s but can get different values (according to the initialization)\n",
    "Then in the forward:\n",
    "Ypred and adj_att are the model prediction on the node given the masked adjacency matrix (we take the softmax of the node prediction - so the result is a vector of probabilities - and we will be able to inquire how the probability of the correct class change given that we did perturb the graph)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mask functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Num nodes: is how many nodes in the neighborhood (computed as the sum of 1s over the row of the node of interest - as there is a 1 only if the node is connected to other nodes)\n",
    "\n",
    "num_nodes = len(neighbors)\n",
    "diag_mask = torch.ones(num_nodes, num_nodes) - torch.eye(num_nodes) #create a diag mask of 1 and 0s on the diagonal\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct mask\n",
    "Edge and feature masks: initialization of masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "def construct_edge_mask( num_nodes, init_strategy=\"normal\", const_val=1.0):\n",
    "    \"\"\"\n",
    "    Construct edge mask\n",
    "    input;\n",
    "        num_nodes: number of nodes in the neighborhood\n",
    "        init_strategy: initialization strategy for the mask\n",
    "        const_val: constant value for the mask\n",
    "    output:\n",
    "        mask: edge mask    \n",
    "\n",
    "    \"\"\"\n",
    "    torch.manual_seed(42)\n",
    "    mask = nn.Parameter(torch.FloatTensor(num_nodes, num_nodes))  #initialize the mask\n",
    "    if init_strategy == \"normal\":\n",
    "        std = nn.init.calculate_gain(\"relu\") * math.sqrt(\n",
    "            2.0 / (num_nodes + num_nodes)\n",
    "        )\n",
    "        with torch.no_grad():\n",
    "            mask.normal_(1.0, std)\n",
    "    elif init_strategy == \"const\":\n",
    "        nn.init.constant_(mask, const_val)\n",
    "    print(mask.grad)    \n",
    "    return mask\n",
    "\n",
    "edge_mask = construct_edge_mask(num_nodes, init_strategy=\"normal\", const_val=1.0)\n",
    "#edge mask is a matrix of size num_nodes x num_nodes initialized with random values from a normal distribution with mean 1 and std calculated using the formula given "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_dim = data.num_features\n",
    "def construct_feat_mask( feat_dim, init_strategy=\"normal\"):\n",
    "    \"\"\"\n",
    "    Construct feature mask\n",
    "    input:\n",
    "        feat_dim: dimension of the feature\n",
    "        init_strategy: initialization strategy\n",
    "    output:\n",
    "        mask: feature mask    \n",
    "    \"\"\"\n",
    "    torch.manual_seed(42)\n",
    "    mask = nn.Parameter(torch.FloatTensor(feat_dim))\n",
    "    if init_strategy == \"normal\":\n",
    "        std = 0.1\n",
    "        with torch.no_grad():\n",
    "            mask.normal_(1.0, std)\n",
    "    elif init_strategy == \"constant\":\n",
    "        with torch.no_grad():\n",
    "            nn.init.constant_(mask, 0.0)\n",
    "            # mask[0] = 2\n",
    "    return mask\n",
    "\n",
    "feat_mask = construct_feat_mask(feat_dim, init_strategy=\"normal\")\n",
    "\n",
    "#feat mask is a vector of size feat_dim initialized with random values from a normal distribution with mean 1 and std 0.1 (if normal strategy is chosen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original sub_adj: \n",
      " tensor([[0., 1., 0., 0., 0., 1., 0., 1.],\n",
      "        [1., 0., 0., 0., 1., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 1., 0., 0., 0., 1., 1., 0.],\n",
      "        [1., 0., 1., 0., 1., 0., 0., 1.],\n",
      "        [0., 1., 0., 0., 1., 0., 0., 0.],\n",
      "        [1., 0., 0., 1., 0., 1., 0., 0.]]) \n",
      " masked_adj: \n",
      " tensor([[0.0000, 0.7511, 0.0000, 0.0000, 0.0000, 0.5748, 0.0000, 0.5617],\n",
      "        [0.7511, 0.0000, 0.0000, 0.0000, 0.6457, 0.0000, 0.6555, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7299, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7995],\n",
      "        [0.0000, 0.6457, 0.0000, 0.0000, 0.0000, 0.6385, 0.7203, 0.0000],\n",
      "        [0.5748, 0.0000, 0.7299, 0.0000, 0.6385, 0.0000, 0.0000, 0.7764],\n",
      "        [0.0000, 0.6555, 0.0000, 0.0000, 0.7203, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5617, 0.0000, 0.0000, 0.7995, 0.0000, 0.7764, 0.0000, 0.0000]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ww/33zq_rh50tx94n81lb4thx0w0000gn/T/ipykernel_91515/1389454049.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  adj = torch.tensor(adj)\n",
      "/var/folders/ww/33zq_rh50tx94n81lb4thx0w0000gn/T/ipykernel_91515/1389454049.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  print('original sub_adj: \\n', torch.tensor(sub_adj), '\\n masked_adj: \\n', masked_adj)\n"
     ]
    }
   ],
   "source": [
    "def _masked_adj(mask,adj, diag_mask):\n",
    "    \"\"\" Masked adjacency matrix \n",
    "    input: edge_mask, sub_adj, diag_mask\n",
    "    output: masked_adj\n",
    "    \"\"\"\n",
    "    sym_mask = mask\n",
    "    sym_mask = torch.sigmoid(mask)\n",
    "    \n",
    "    sym_mask = (sym_mask + sym_mask.t()) / 2\n",
    "    adj = torch.tensor(adj)\n",
    "    masked_adj = adj * sym_mask\n",
    "\n",
    "    return masked_adj * diag_mask\n",
    "\n",
    "masked_adj = _masked_adj(edge_mask,sub_adj, diag_mask)\n",
    "masked_adj\n",
    "print('original sub_adj: \\n', torch.tensor(sub_adj), '\\n masked_adj: \\n', masked_adj)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The whole idea of masking the adjacency matrix is to add perturbations in the node connections - they are not connected with the same weights anymore."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward\n",
    "The method takes several arguments, but the most important one is node_idx, which appears to be the index of a node in a graph. The purpose of this method is to generate a prediction for that node, based on its features and the features of its neighbors, in the form of a softmax output.\n",
    "\n",
    "The method first checks whether the adjacency matrix should be constrained (i.e. symmetrical) or unconstrained, and generates a masked adjacency matrix accordingly. If the mask_features flag is set, it also masks the input features. If the marginalize flag is set, it generates a random noise vector and scales it according to the feature mask.\n",
    "\n",
    "The method then passes the masked input features and masked adjacency matrix to a model, which generates a prediction for the target node. If the graph_mode flag is set, it applies a softmax to the entire output, whereas if it is not set, it extracts the prediction for the target node and applies a softmax to that.\n",
    "\n",
    "Finally, the method returns the softmax output and the adjacency matrix attention, which may be used for visualization or analysis purposes."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are just exploring how the prediction works if we only consider the subadjacency and the subgraph \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok so now we have to use the simplified version of this fancy more to make some training ---> then we want to try and explain this trained boy with the GNN Explainer simplified method. \n",
    "\n",
    "\n",
    "Let's see if everything will work out well :)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward loop! and how the loss works\n",
    "\n",
    "\n",
    "#### LOSS\n",
    "The loss is - as for any ML task - our objective function that we want to minimize during training.\n",
    "\n",
    "#### many y!!\n",
    "Some notation to understand better what's going on.\n",
    "- y : ground thruth label value\n",
    "- y_hat : prediction of the label value made by the original model (that has access to the full graph)\n",
    "- y_e : prediction of the label value made on the sub_graph - (this gets update according to how the masked adjacency matrix gets updated)\n",
    "\n",
    "\n",
    "#### Losses\n",
    "- PRED LOSS: encourage y_e to be close to y_hat\n",
    "- MASK EDGE SIZE LOSS: encourage the mask to have the least amount of edges\n",
    "- MASK FEATURE SIZE LOSS: // of features\n",
    "- MASK EDGE ENTROPY LOSS: encourage the model to have a balanced level of uncertainty\n",
    "- MASK FEATURE ENTROPY LOSS\n",
    "- LAPLACIAN LOSS: encourage the smoothness of the mask over the graph "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'subdata' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 77\u001b[0m\n\u001b[1;32m     74\u001b[0m model \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mload(\u001b[39m'\u001b[39m\u001b[39mcora_chk/model_cora\u001b[39m\u001b[39m'\u001b[39m) \u001b[39m#load the original model\u001b[39;00m\n\u001b[1;32m     76\u001b[0m \u001b[39m#We are using the original model to get the prediction of the query node\u001b[39;00m\n\u001b[0;32m---> 77\u001b[0m pred, adj \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mforward(torch\u001b[39m.\u001b[39mTensor(subdata), torch\u001b[39m.\u001b[39mTensor(sub_adj))\n\u001b[1;32m     78\u001b[0m node_pred \u001b[39m=\u001b[39m pred[node_idx_new, :] \u001b[39m#get the prediction of the query node\u001b[39;00m\n\u001b[1;32m     79\u001b[0m pred \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mSoftmax(dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)(node_pred) \u001b[39m#apply softmax to it\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'subdata' is not defined"
     ]
    }
   ],
   "source": [
    "#FUNCTIONS THAT WE USE IN THE EXPLAIN.PY FILE\n",
    "\n",
    "def loss_fc(edge_mask, feat_mask, masked_adj,adj, pred, pred_label,label, node_idx, epoch, print=False):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        pred: y_e :  prediction made by current model\n",
    "        pred_label: y_hat : the label predicted by the original model.\n",
    "    \"\"\"\n",
    "    #PRED LOSS\n",
    "    pred_label_node = pred_label[node_idx] #pred label is the prediction made by the original model\n",
    "    gt_label_node = label[node_idx]\n",
    "\n",
    "    logit = pred[gt_label_node] #pred is the prediction made by the current model\n",
    "\n",
    "    pred_loss = -torch.log(logit) #this is basically taking the cross entropy loss\n",
    "\n",
    "    # MASK SIZE EDGE LOSS\n",
    "    \n",
    "    mask = edge_mask\n",
    "\n",
    "\n",
    "    mask = torch.sigmoid(mask)\n",
    "\n",
    "    size_loss = 0.005 * torch.sum(mask)\n",
    "\n",
    "    \n",
    "    #MASK SIZE FEATURE LOSS\n",
    "    feat_mask = (torch.sigmoid(feat_mask))\n",
    "    feat_size_loss = 1.0 * torch.mean(feat_mask)\n",
    "\n",
    "    # EDGE MASK ENTROPY LOSS\n",
    "    mask_ent = -mask * torch.log(mask) - (1 - mask) * torch.log(1 - mask)\n",
    "    mask_ent_loss = 1.0 * torch.mean(mask_ent)\n",
    "    \n",
    "    # FEATURE MASK ENTROPY LOSS\n",
    "    feat_mask_ent = - feat_mask * torch.log(feat_mask) - (1 - feat_mask) * torch.log(1 - feat_mask)\n",
    "\n",
    "    feat_mask_ent_loss = 0.1  * torch.mean(feat_mask_ent)\n",
    "\n",
    "    # LAPLACIAN LOSS\n",
    "    D = torch.diag(torch.sum(masked_adj, 0))\n",
    "    m_adj = masked_adj \n",
    "    L = D - m_adj\n",
    "\n",
    "    pred_label_t = torch.tensor(pred_label, dtype=torch.float)\n",
    "\n",
    "\n",
    "    lap_loss = ( 1.0\n",
    "        * (pred_label_t @ L @ pred_label_t)\n",
    "        / torch.Tensor(adj).numel())\n",
    "\n",
    "\n",
    "    loss = pred_loss + size_loss  + mask_ent_loss + feat_size_loss + lap_loss\n",
    "    if print== True:\n",
    "        print(\"optimization/size_loss\", size_loss, epoch)\n",
    "        print(\"optimization/feat_size_loss\", feat_size_loss, epoch)\n",
    "        print(\"optimization/mask_ent_loss\", mask_ent_loss, epoch)\n",
    "        print(\n",
    "            \"optimization/feat_mask_ent_loss\", mask_ent_loss, epoch\n",
    "        )\n",
    "\n",
    "        print(\"optimization/pred_loss\", pred_loss, epoch)\n",
    "        print(\"optimization/lap_loss\", lap_loss, epoch)\n",
    "        print(\"optimization/overall_loss\", loss, epoch)\n",
    "    return loss\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#SEE how the loss function is used in the explain.py file\n",
    "pred_label = torch.load('cora_chk/prediction_cora') #load prediction of original model\n",
    "pred_label = torch.argmax(pred_label[neighbors], dim=1) #get the prediction of the query node\n",
    "adj = torch.load('cora_chk/adj_cora') #load the adjacency matrix of the original model\n",
    "model = torch.load('cora_chk/model_cora') #load the original model\n",
    "\n",
    "#We are using the original model to get the prediction of the query node\n",
    "pred, adj = model.forward(torch.Tensor(subdata), torch.Tensor(sub_adj))\n",
    "node_pred = pred[node_idx_new, :] #get the prediction of the query node\n",
    "pred = nn.Softmax(dim=0)(node_pred) #apply softmax to it\n",
    "loss_fc( edge_mask, feat_mask, masked_adj,adj, pred, pred_label,data.y, node_idx, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 0., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 0., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 0., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 0., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 0., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 0., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 0., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 0.]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def construct_diag_mask(neighbors):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        adj: adjacency matrix of the graph\n",
    "    \"\"\"\n",
    "    num_nodes = len(neighbors)\n",
    "    diag_mask = torch.ones(num_nodes, num_nodes) - torch.eye(num_nodes) \n",
    "\n",
    "    return diag_mask\n",
    "\n",
    "construct_diag_mask(neighbors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7236, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def mask_density(edge_mask):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        mask: edge mask\n",
    "    \"\"\"\n",
    "    mask = torch.sigmoid(edge_mask)\n",
    "    return torch.sum(mask) / torch.Tensor(mask).numel()\n",
    "\n",
    "mask_density(edge_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_adj_grad(adj, masked_adj, node_idx, pred_label,pred_label_node, x, epoch, label=None):\n",
    "    \"\"\" \n",
    "    Computes the gradient of the adjacency matrix with respect to the loss\n",
    "    \n",
    "    \"\"\"\n",
    "    log_adj = False\n",
    "\n",
    "    predicted_label = pred_label\n",
    "    # adj_grad = torch.abs(self.adj_feat_grad(node_idx, predicted_label)[0])[self.graph_idx]\n",
    "    adj_grad, x_grad = adj_feat_grad(node_idx, pred_label_node, model , adj,x)\n",
    "    print('adj_grad: ', adj_grad)\n",
    "    adj_grad = torch.abs(adj_grad)\n",
    "    x_grad = x_grad[node_idx][:, np.newaxis]\n",
    "        # x_grad = torch.sum(x_grad[self.graph_idx], 0, keepdim=True).t()\n",
    "    adj_grad = (adj_grad + adj_grad.t()) / 2\n",
    "    adj_grad = (adj_grad * adj).squeeze()\n",
    "\n",
    "\n",
    "    masked_adj = masked_adj[0].cpu().detach().numpy()\n",
    "    return masked_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adj_feat_grad(node_idx, pred_label_node, model , adj,x, ):\n",
    "    \"\"\"\n",
    "    Compute the gradient of the prediction w.r.t. the adjacency matrix\n",
    "    and the node features.\n",
    "    \"\"\"\n",
    "    model.zero_grad()\n",
    "    adj.requires_grad = True\n",
    "    x.requires_grad = True\n",
    "    print('adj',adj)\n",
    "    if adj.grad is not None:\n",
    "        adj.grad.zero_() # zero out the gradient\n",
    "        x.grad.zero_() # zero out the gradient\n",
    "\n",
    "    x, adj = x, adj\n",
    "    ypred, _ = model(x, adj)\n",
    "\n",
    "    logit = nn.Softmax(dim=0)(ypred[ node_idx, :])\n",
    "    logit = logit[pred_label_node]\n",
    "    loss = -torch.log(logit)\n",
    "    loss.backward()\n",
    "    return adj.grad, x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adj tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.]], requires_grad=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None,\n",
       " tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00],\n",
       "         [6.5486e-04, 2.6171e-03, 1.5889e-02,  ..., 8.0785e-03, 2.8308e-03,\n",
       "          5.5990e-03],\n",
       "         [1.3034e-04, 3.9236e-05, 4.1168e-03,  ..., 1.8827e-03, 5.9122e-05,\n",
       "          1.2444e-03],\n",
       "         ...,\n",
       "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00],\n",
       "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00],\n",
       "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00]]))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_label = torch.load('cora_chk/prediction_cora')\n",
    "pred_label = torch.argmax(pred_label[neighbors], dim=1)\n",
    "\n",
    "pred_label_node = pred_label[node_idx]\n",
    "adj = torch.load('cora_chk/adj_cora')\n",
    "x = data.x\n",
    "adj_feat_grad(node_idx, pred_label_node, model , adj,x, )\n",
    "#log_adj_grad(adj, masked_adj, node_idx, pred_label,pred_label_node, x, 1, label=None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explain\n",
    "In order to make our life easier for what concerns training we wrap up all the functions that we defined so far in a class - Explain.\n",
    "\n",
    "In addition to the existing funtion we define a forward, that basically just perform a forward using the already trained model using the data of the neighborhood of interest only (namely subdata and a sub-adjacency matrix - in the paper those are called as G_c and A_c)\n",
    "\n",
    "The goal that we want to achieve with training via backpropagation on the predefined loss is to find G_s and A_s (as referenced in the paper) such that we can maximize the MUTUAL information.\n",
    "\n",
    "--> Explanation for prediction y is thus a subgraph GS that minimizes uncertainty of  when the GNN computation is limited to GS. \n",
    "\n",
    "We also define \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Explain(nn.Module):\n",
    "    def __init__(self, model, data, node_idx, n_hops):\n",
    "        super(Explain, self).__init__()\n",
    "        #Those are the parameters of the original data and model\n",
    "        self.model = model\n",
    "        self.data = data\n",
    "        self.node_idx = node_idx\n",
    "        self.n_hops = n_hops\n",
    "        self.adj = get_adjacency(data)\n",
    "        self.label = torch.Tensor(data.y)\n",
    "        self.feat = torch.Tensor(data.x)\n",
    "        self.feat_dim = data.num_features\n",
    "        self.epoch = 1\n",
    "        self.x = data.x\n",
    "\n",
    "        self.pred_label = torch.load('cora_chk/prediction_cora')\n",
    "\n",
    "        self.node_idx_new, self.sub_adj, self.sub_feat, self.sub_label, self.neighbors = extract_neighborhood(self.node_idx, self.adj, self.feat, self.label, self.n_hops)\n",
    "        self.num_nodes = len(self.neighbors)\n",
    "        self.diag_mask = construct_diag_mask(self.neighbors)\n",
    "        self.subdata = torch.Tensor(data.subgraph(torch.tensor(self.neighbors)).x)\n",
    "        self.edge_mask = construct_edge_mask(self.num_nodes)\n",
    "        self.feat_mask = construct_feat_mask(self.feat_dim, init_strategy=\"normal\")\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    def _masked_adj(self):\n",
    "        \"\"\" Masked adjacency matrix \n",
    "        input: edge_mask, sub_adj, diag_mask\n",
    "        output: masked_adj\n",
    "        \"\"\"\n",
    "        sym_mask = self.edge_mask\n",
    "        sym_mask = torch.sigmoid(sym_mask)\n",
    "        \n",
    "        sym_mask = (sym_mask + sym_mask.t()) / 2\n",
    "        adj = torch.tensor(self.sub_adj)\n",
    "        masked_adj = adj * sym_mask\n",
    "\n",
    "        return masked_adj * self.diag_mask\n",
    "    \n",
    "        \n",
    "        \n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            ypred: prediction of the query node made by the current model (on the subgraph)\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        self.masked_adj = self._masked_adj()\n",
    "        print(self.edge_mask)\n",
    "        feat_mask = (torch.sigmoid(self.feat_mask))\n",
    "        x = self.sub_feat * feat_mask\n",
    "        #ypred, adj_att = model(self.subdata, masked_adj)\n",
    "        ypred, adj_att = self.model(x, self.masked_adj)\n",
    "        node_pred = ypred[self.node_idx_new, :]\n",
    "        res = nn.Softmax(dim=0)(node_pred)\n",
    "        return res, adj_att, self.sub_adj\n",
    "    \n",
    "    def criterion(self, epoch):\n",
    "        \"\"\"\n",
    "        Computes the loss of the current model\n",
    "        \"\"\"\n",
    "        #prediction of explanation model\n",
    "        pred, adj_e, sub_adj = self.forward()\n",
    "\n",
    "        #prediction of original model\n",
    "        pred_label = torch.argmax(self.pred_label[self.neighbors], dim=1)\n",
    "\n",
    "\n",
    "        loss_val = loss_fc(self.edge_mask, self.feat_mask, self.masked_adj,self.adj, pred, pred_label, self.label,self.node_idx, self.epoch)\n",
    "\n",
    "        return loss_val \n",
    "    \n",
    "    def mask_density(self):\n",
    "        \"\"\"\n",
    "        Computes the density of the edge mask\n",
    "        \"\"\"\n",
    "        mask_sum = torch.sum(self.masked_adj)\n",
    "        adj_sum = torch.sum(self.adj)\n",
    "        return mask_sum / adj_sum\n",
    "    \n",
    "    def return_stuff(self):\n",
    "        pred_label = torch.argmax(self.pred_label[self.neighbors], dim=1)\n",
    "        return pred_label[self.node_idx], self.label[self.node_idx], self.neighbors, self.sub_label, self.sub_feat, self.n_hops\n",
    "    \n",
    "    def log_adj_grad(self, node_idx, pred_label, epoch, label=None):\n",
    "        \"\"\" \n",
    "        Computes the gradient of the adjacency matrix with respect to the loss\n",
    "        \n",
    "        \"\"\"\n",
    "        log_adj = False\n",
    "\n",
    "        predicted_label = pred_label\n",
    "        # adj_grad = torch.abs(self.adj_feat_grad(node_idx, predicted_label)[0])[self.graph_idx]\n",
    "        adj_grad, x_grad = self.adj_feat_grad(node_idx, predicted_label)\n",
    "        adj_grad = torch.abs(adj_grad)\n",
    "        x_grad = x_grad[node_idx][:, np.newaxis]\n",
    "            # x_grad = torch.sum(x_grad[self.graph_idx], 0, keepdim=True).t()\n",
    "        adj_grad = (adj_grad + adj_grad.t()) / 2\n",
    "        adj_grad = (adj_grad * self.adj).squeeze()\n",
    "\n",
    "\n",
    "        masked_adj = self.masked_adj[0].cpu().detach().numpy()\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "        adj_grad = adj_grad.detach().numpy()\n",
    "\n",
    "    def adj_feat_grad(self, node_idx, pred_label_node):\n",
    "        \"\"\"\n",
    "        Compute the gradient of the prediction w.r.t. the adjacency matrix\n",
    "        and the node features.\n",
    "        \"\"\"\n",
    "        self.model.zero_grad()\n",
    "        self.adj.requires_grad = True\n",
    "        self.x.requires_grad = True\n",
    "        if self.adj.grad is not None:\n",
    "            print('self.adj.grad', self.adj.grad)\n",
    "            self.adj.grad.zero_() # zero out the gradient\n",
    "            self.x.grad.zero_() # zero out the gradient\n",
    "\n",
    "        else:\n",
    "            x, adj = self.x, self.adj\n",
    "        ypred, _ = self.model(x, adj)\n",
    "\n",
    "\n",
    "        logit = nn.Softmax(dim=0)(ypred[ node_idx, :])\n",
    "        logit = logit[pred_label_node]\n",
    "        loss = -torch.log(logit)\n",
    "        loss.backward()\n",
    "        print('adj grad', self.adj.grad)\n",
    "        print('x grad',self.x.grad)\n",
    "        return self.adj, self.x.grad    \n",
    "    \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the explain object\n",
    "Here we create the explain object and we specify the node to explain and how big we consider its neighborhood to be "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ww/33zq_rh50tx94n81lb4thx0w0000gn/T/ipykernel_5019/2699400499.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  adj = torch.tensor(adj, dtype=torch.float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "explainer = Explain(model = torch.load('cora_chk/model_cora'), data = data, node_idx = 0, n_hops=2)\n",
    "optimizer = torch.optim.Adam(explainer.parameters(), lr=0.01)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAINING LOOP\n",
    "Here we perform the training loop:\n",
    "- zero out explainer and optimizer\n",
    "- forward on subgraph model\n",
    "- loss and backprop\n",
    "- optimizer step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 1.9635,  1.7436,  1.4504, -0.0528,  1.3392,  0.3827,  0.9785,  0.1977],\n",
      "        [ 0.6239,  1.8244,  0.8038,  0.2982,  0.6361,  0.7203,  0.6156,  1.3812],\n",
      "        [ 1.8212,  0.9202,  0.7513,  1.2198,  0.6209,  1.5392,  1.4004,  1.8403],\n",
      "        [ 1.6396,  1.6482,  1.3052,  1.6674,  0.8842,  1.0209,  0.8742,  1.4299],\n",
      "        [ 0.3077,  0.5644,  0.8883,  1.8587,  1.1594,  0.7877,  1.1529,  0.6127],\n",
      "        [ 0.2212,  1.4978,  0.5601,  0.6994,  0.3629,  2.0614,  0.3827,  0.7560],\n",
      "        [ 0.5431,  0.6709,  1.0390,  1.2629,  0.7560,  1.5957,  0.5930,  0.6320],\n",
      "        [ 0.2984,  1.0180,  0.9683,  1.3378,  0.9511,  1.9223,  0.4077,  1.6918]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 1.9635,  1.7436,  1.4504, -0.0528,  1.3392,  0.3827,  0.9785,  0.1977],\n",
      "        [ 0.6239,  1.8244,  0.8038,  0.2982,  0.6361,  0.7203,  0.6156,  1.3812],\n",
      "        [ 1.8212,  0.9202,  0.7513,  1.2198,  0.6209,  1.5392,  1.4004,  1.8403],\n",
      "        [ 1.6396,  1.6482,  1.3052,  1.6674,  0.8842,  1.0209,  0.8742,  1.4299],\n",
      "        [ 0.3077,  0.5644,  0.8883,  1.8587,  1.1594,  0.7877,  1.1529,  0.6127],\n",
      "        [ 0.2212,  1.4978,  0.5601,  0.6994,  0.3629,  2.0614,  0.3827,  0.7560],\n",
      "        [ 0.5431,  0.6709,  1.0390,  1.2629,  0.7560,  1.5957,  0.5930,  0.6320],\n",
      "        [ 0.2984,  1.0180,  0.9683,  1.3378,  0.9511,  1.9223,  0.4077,  1.6918]],\n",
      "       requires_grad=True)\n",
      "adj grad None\n",
      "x grad tensor([[ 4.1203e-03, -3.3194e-03,  1.1395e-01,  ..., -7.1981e-02,\n",
      "         -2.1926e-02, -9.1704e-02],\n",
      "        [ 6.5486e-04,  2.6171e-03,  1.5889e-02,  ...,  8.0785e-03,\n",
      "          2.8308e-03,  5.5990e-03],\n",
      "        [ 1.3034e-04,  3.9236e-05,  4.1168e-03,  ...,  1.8827e-03,\n",
      "          5.9122e-05,  1.2444e-03],\n",
      "        ...,\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00]])\n",
      "epoch:  0 ; loss:  1.6697999238967896 ; mask density:  0.001298481016419828 ; pred:  tensor([0.0481, 0.0309, 0.0424, 0.7557, 0.0407, 0.0501, 0.0322],\n",
      "       grad_fn=<SoftmaxBackward0>) ; labels equal:  tensor(True)\n",
      "Parameter containing:\n",
      "tensor([[ 1.9735,  1.7536,  1.4604, -0.0628,  1.3492,  0.3927,  0.9885,  0.1877],\n",
      "        [ 0.6339,  1.8344,  0.8138,  0.2882,  0.6461,  0.7303,  0.6256,  1.3912],\n",
      "        [ 1.8312,  0.9302,  0.7613,  1.2298,  0.6309,  1.5492,  1.4104,  1.8503],\n",
      "        [ 1.6496,  1.6582,  1.3152,  1.6774,  0.8942,  1.0309,  0.8842,  1.4399],\n",
      "        [ 0.2977,  0.5744,  0.8983,  1.8687,  1.1694,  0.7977,  1.1629,  0.6227],\n",
      "        [ 0.2112,  1.5078,  0.5701,  0.7094,  0.3729,  2.0714,  0.3927,  0.7660],\n",
      "        [ 0.5531,  0.6809,  1.0490,  1.2729,  0.7660,  1.6057,  0.6030,  0.6420],\n",
      "        [ 0.2884,  1.0280,  0.9783,  1.3478,  0.9611,  1.9323,  0.4177,  1.7018]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 1.9735,  1.7536,  1.4604, -0.0628,  1.3492,  0.3927,  0.9885,  0.1877],\n",
      "        [ 0.6339,  1.8344,  0.8138,  0.2882,  0.6461,  0.7303,  0.6256,  1.3912],\n",
      "        [ 1.8312,  0.9302,  0.7613,  1.2298,  0.6309,  1.5492,  1.4104,  1.8503],\n",
      "        [ 1.6496,  1.6582,  1.3152,  1.6774,  0.8942,  1.0309,  0.8842,  1.4399],\n",
      "        [ 0.2977,  0.5744,  0.8983,  1.8687,  1.1694,  0.7977,  1.1629,  0.6227],\n",
      "        [ 0.2112,  1.5078,  0.5701,  0.7094,  0.3729,  2.0714,  0.3927,  0.7660],\n",
      "        [ 0.5531,  0.6809,  1.0490,  1.2729,  0.7660,  1.6057,  0.6030,  0.6420],\n",
      "        [ 0.2884,  1.0280,  0.9783,  1.3478,  0.9611,  1.9323,  0.4177,  1.7018]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 1.9835,  1.7636,  1.4704, -0.0728,  1.3592,  0.4027,  0.9985,  0.1777],\n",
      "        [ 0.6439,  1.8444,  0.8238,  0.2783,  0.6561,  0.7403,  0.6356,  1.4012],\n",
      "        [ 1.8412,  0.9402,  0.7713,  1.2398,  0.6409,  1.5592,  1.4204,  1.8603],\n",
      "        [ 1.6596,  1.6682,  1.3252,  1.6874,  0.9042,  1.0409,  0.8942,  1.4499],\n",
      "        [ 0.2879,  0.5844,  0.9083,  1.8787,  1.1794,  0.8077,  1.1729,  0.6327],\n",
      "        [ 0.2012,  1.5178,  0.5801,  0.7194,  0.3829,  2.0814,  0.4027,  0.7760],\n",
      "        [ 0.5631,  0.6909,  1.0590,  1.2829,  0.7760,  1.6157,  0.6130,  0.6520],\n",
      "        [ 0.2785,  1.0380,  0.9883,  1.3578,  0.9711,  1.9423,  0.4277,  1.7118]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 1.9835,  1.7636,  1.4704, -0.0728,  1.3592,  0.4027,  0.9985,  0.1777],\n",
      "        [ 0.6439,  1.8444,  0.8238,  0.2783,  0.6561,  0.7403,  0.6356,  1.4012],\n",
      "        [ 1.8412,  0.9402,  0.7713,  1.2398,  0.6409,  1.5592,  1.4204,  1.8603],\n",
      "        [ 1.6596,  1.6682,  1.3252,  1.6874,  0.9042,  1.0409,  0.8942,  1.4499],\n",
      "        [ 0.2879,  0.5844,  0.9083,  1.8787,  1.1794,  0.8077,  1.1729,  0.6327],\n",
      "        [ 0.2012,  1.5178,  0.5801,  0.7194,  0.3829,  2.0814,  0.4027,  0.7760],\n",
      "        [ 0.5631,  0.6909,  1.0590,  1.2829,  0.7760,  1.6157,  0.6130,  0.6520],\n",
      "        [ 0.2785,  1.0380,  0.9883,  1.3578,  0.9711,  1.9423,  0.4277,  1.7118]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 1.9935,  1.7736,  1.4804, -0.0828,  1.3692,  0.4128,  1.0085,  0.1676],\n",
      "        [ 0.6540,  1.8544,  0.8338,  0.2684,  0.6661,  0.7503,  0.6456,  1.4112],\n",
      "        [ 1.8512,  0.9502,  0.7813,  1.2498,  0.6510,  1.5692,  1.4304,  1.8703],\n",
      "        [ 1.6696,  1.6782,  1.3352,  1.6974,  0.9142,  1.0509,  0.9042,  1.4599],\n",
      "        [ 0.2782,  0.5944,  0.9183,  1.8887,  1.1894,  0.8178,  1.1829,  0.6427],\n",
      "        [ 0.1912,  1.5278,  0.5901,  0.7294,  0.3929,  2.0914,  0.4127,  0.7861],\n",
      "        [ 0.5731,  0.7010,  1.0690,  1.2929,  0.7860,  1.6257,  0.6230,  0.6620],\n",
      "        [ 0.2686,  1.0480,  0.9983,  1.3678,  0.9811,  1.9523,  0.4378,  1.7218]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 1.9935,  1.7736,  1.4804, -0.0828,  1.3692,  0.4128,  1.0085,  0.1676],\n",
      "        [ 0.6540,  1.8544,  0.8338,  0.2684,  0.6661,  0.7503,  0.6456,  1.4112],\n",
      "        [ 1.8512,  0.9502,  0.7813,  1.2498,  0.6510,  1.5692,  1.4304,  1.8703],\n",
      "        [ 1.6696,  1.6782,  1.3352,  1.6974,  0.9142,  1.0509,  0.9042,  1.4599],\n",
      "        [ 0.2782,  0.5944,  0.9183,  1.8887,  1.1894,  0.8178,  1.1829,  0.6427],\n",
      "        [ 0.1912,  1.5278,  0.5901,  0.7294,  0.3929,  2.0914,  0.4127,  0.7861],\n",
      "        [ 0.5731,  0.7010,  1.0690,  1.2929,  0.7860,  1.6257,  0.6230,  0.6620],\n",
      "        [ 0.2686,  1.0480,  0.9983,  1.3678,  0.9811,  1.9523,  0.4378,  1.7218]],\n",
      "       requires_grad=True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ww/33zq_rh50tx94n81lb4thx0w0000gn/T/ipykernel_5019/3195031680.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  adj = torch.tensor(self.sub_adj)\n",
      "/var/folders/ww/33zq_rh50tx94n81lb4thx0w0000gn/T/ipykernel_5019/507372444.py:45: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  pred_label_t = torch.tensor(pred_label, dtype=torch.float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 2.0035,  1.7836,  1.4904, -0.0928,  1.3792,  0.4228,  1.0185,  0.1576],\n",
      "        [ 0.6640,  1.8644,  0.8438,  0.2585,  0.6761,  0.7603,  0.6556,  1.4212],\n",
      "        [ 1.8612,  0.9602,  0.7913,  1.2598,  0.6610,  1.5792,  1.4404,  1.8803],\n",
      "        [ 1.6796,  1.6882,  1.3452,  1.7074,  0.9242,  1.0609,  0.9142,  1.4699],\n",
      "        [ 0.2685,  0.6044,  0.9283,  1.8987,  1.1995,  0.8278,  1.1929,  0.6528],\n",
      "        [ 0.1811,  1.5378,  0.6002,  0.7395,  0.4030,  2.1014,  0.4228,  0.7961],\n",
      "        [ 0.5832,  0.7110,  1.0790,  1.3029,  0.7960,  1.6357,  0.6331,  0.6721],\n",
      "        [ 0.2587,  1.0580,  1.0083,  1.3778,  0.9911,  1.9623,  0.4478,  1.7318]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.0035,  1.7836,  1.4904, -0.0928,  1.3792,  0.4228,  1.0185,  0.1576],\n",
      "        [ 0.6640,  1.8644,  0.8438,  0.2585,  0.6761,  0.7603,  0.6556,  1.4212],\n",
      "        [ 1.8612,  0.9602,  0.7913,  1.2598,  0.6610,  1.5792,  1.4404,  1.8803],\n",
      "        [ 1.6796,  1.6882,  1.3452,  1.7074,  0.9242,  1.0609,  0.9142,  1.4699],\n",
      "        [ 0.2685,  0.6044,  0.9283,  1.8987,  1.1995,  0.8278,  1.1929,  0.6528],\n",
      "        [ 0.1811,  1.5378,  0.6002,  0.7395,  0.4030,  2.1014,  0.4228,  0.7961],\n",
      "        [ 0.5832,  0.7110,  1.0790,  1.3029,  0.7960,  1.6357,  0.6331,  0.6721],\n",
      "        [ 0.2587,  1.0580,  1.0083,  1.3778,  0.9911,  1.9623,  0.4478,  1.7318]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.0135,  1.7936,  1.5004, -0.1029,  1.3892,  0.4329,  1.0285,  0.1475],\n",
      "        [ 0.6740,  1.8744,  0.8538,  0.2485,  0.6862,  0.7704,  0.6657,  1.4312],\n",
      "        [ 1.8712,  0.9702,  0.8014,  1.2698,  0.6710,  1.5892,  1.4504,  1.8903],\n",
      "        [ 1.6896,  1.6982,  1.3553,  1.7174,  0.9342,  1.0709,  0.9243,  1.4799],\n",
      "        [ 0.2588,  0.6145,  0.9384,  1.9087,  1.2095,  0.8378,  1.2029,  0.6628],\n",
      "        [ 0.1710,  1.5478,  0.6102,  0.7495,  0.4131,  2.1114,  0.4329,  0.8061],\n",
      "        [ 0.5932,  0.7210,  1.0890,  1.3129,  0.8061,  1.6457,  0.6431,  0.6821],\n",
      "        [ 0.2487,  1.0680,  1.0183,  1.3878,  1.0011,  1.9723,  0.4580,  1.7418]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.0135,  1.7936,  1.5004, -0.1029,  1.3892,  0.4329,  1.0285,  0.1475],\n",
      "        [ 0.6740,  1.8744,  0.8538,  0.2485,  0.6862,  0.7704,  0.6657,  1.4312],\n",
      "        [ 1.8712,  0.9702,  0.8014,  1.2698,  0.6710,  1.5892,  1.4504,  1.8903],\n",
      "        [ 1.6896,  1.6982,  1.3553,  1.7174,  0.9342,  1.0709,  0.9243,  1.4799],\n",
      "        [ 0.2588,  0.6145,  0.9384,  1.9087,  1.2095,  0.8378,  1.2029,  0.6628],\n",
      "        [ 0.1710,  1.5478,  0.6102,  0.7495,  0.4131,  2.1114,  0.4329,  0.8061],\n",
      "        [ 0.5932,  0.7210,  1.0890,  1.3129,  0.8061,  1.6457,  0.6431,  0.6821],\n",
      "        [ 0.2487,  1.0680,  1.0183,  1.3878,  1.0011,  1.9723,  0.4580,  1.7418]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.0234,  1.8036,  1.5104, -0.1129,  1.3992,  0.4431,  1.0385,  0.1373],\n",
      "        [ 0.6841,  1.8844,  0.8639,  0.2385,  0.6962,  0.7804,  0.6758,  1.4412],\n",
      "        [ 1.8812,  0.9803,  0.8114,  1.2798,  0.6811,  1.5992,  1.4604,  1.9003],\n",
      "        [ 1.6996,  1.7082,  1.3653,  1.7274,  0.9443,  1.0809,  0.9343,  1.4899],\n",
      "        [ 0.2489,  0.6246,  0.9484,  1.9187,  1.2195,  0.8479,  1.2129,  0.6729],\n",
      "        [ 0.1608,  1.5578,  0.6203,  0.7596,  0.4232,  2.1214,  0.4430,  0.8162],\n",
      "        [ 0.6033,  0.7311,  1.0991,  1.3229,  0.8161,  1.6557,  0.6532,  0.6922],\n",
      "        [ 0.2387,  1.0781,  1.0283,  1.3978,  1.0112,  1.9823,  0.4681,  1.7518]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.0234,  1.8036,  1.5104, -0.1129,  1.3992,  0.4431,  1.0385,  0.1373],\n",
      "        [ 0.6841,  1.8844,  0.8639,  0.2385,  0.6962,  0.7804,  0.6758,  1.4412],\n",
      "        [ 1.8812,  0.9803,  0.8114,  1.2798,  0.6811,  1.5992,  1.4604,  1.9003],\n",
      "        [ 1.6996,  1.7082,  1.3653,  1.7274,  0.9443,  1.0809,  0.9343,  1.4899],\n",
      "        [ 0.2489,  0.6246,  0.9484,  1.9187,  1.2195,  0.8479,  1.2129,  0.6729],\n",
      "        [ 0.1608,  1.5578,  0.6203,  0.7596,  0.4232,  2.1214,  0.4430,  0.8162],\n",
      "        [ 0.6033,  0.7311,  1.0991,  1.3229,  0.8161,  1.6557,  0.6532,  0.6922],\n",
      "        [ 0.2387,  1.0781,  1.0283,  1.3978,  1.0112,  1.9823,  0.4681,  1.7518]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.0334,  1.8136,  1.5204, -0.1230,  1.4093,  0.4533,  1.0486,  0.1271],\n",
      "        [ 0.6942,  1.8944,  0.8739,  0.2285,  0.7063,  0.7905,  0.6859,  1.4513],\n",
      "        [ 1.8911,  0.9903,  0.8215,  1.2899,  0.6912,  1.6092,  1.4704,  1.9103],\n",
      "        [ 1.7096,  1.7182,  1.3753,  1.7374,  0.9543,  1.0910,  0.9444,  1.5000],\n",
      "        [ 0.2390,  0.6347,  0.9585,  1.9287,  1.2295,  0.8579,  1.2229,  0.6830],\n",
      "        [ 0.1506,  1.5678,  0.6304,  0.7696,  0.4334,  2.1314,  0.4533,  0.8262],\n",
      "        [ 0.6134,  0.7412,  1.1091,  1.3330,  0.8262,  1.6657,  0.6633,  0.7023],\n",
      "        [ 0.2286,  1.0881,  1.0384,  1.4078,  1.0212,  1.9923,  0.4783,  1.7618]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.0334,  1.8136,  1.5204, -0.1230,  1.4093,  0.4533,  1.0486,  0.1271],\n",
      "        [ 0.6942,  1.8944,  0.8739,  0.2285,  0.7063,  0.7905,  0.6859,  1.4513],\n",
      "        [ 1.8911,  0.9903,  0.8215,  1.2899,  0.6912,  1.6092,  1.4704,  1.9103],\n",
      "        [ 1.7096,  1.7182,  1.3753,  1.7374,  0.9543,  1.0910,  0.9444,  1.5000],\n",
      "        [ 0.2390,  0.6347,  0.9585,  1.9287,  1.2295,  0.8579,  1.2229,  0.6830],\n",
      "        [ 0.1506,  1.5678,  0.6304,  0.7696,  0.4334,  2.1314,  0.4533,  0.8262],\n",
      "        [ 0.6134,  0.7412,  1.1091,  1.3330,  0.8262,  1.6657,  0.6633,  0.7023],\n",
      "        [ 0.2286,  1.0881,  1.0384,  1.4078,  1.0212,  1.9923,  0.4783,  1.7618]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.0434,  1.8236,  1.5304, -0.1331,  1.4193,  0.4636,  1.0586,  0.1169],\n",
      "        [ 0.7043,  1.9043,  0.8840,  0.2183,  0.7164,  0.8006,  0.6960,  1.4613],\n",
      "        [ 1.9011,  1.0004,  0.8316,  1.2999,  0.7013,  1.6192,  1.4804,  1.9203],\n",
      "        [ 1.7196,  1.7282,  1.3853,  1.7474,  0.9644,  1.1010,  0.9544,  1.5100],\n",
      "        [ 0.2290,  0.6449,  0.9685,  1.9387,  1.2395,  0.8680,  1.2330,  0.6931],\n",
      "        [ 0.1404,  1.5778,  0.6406,  0.7797,  0.4436,  2.1414,  0.4635,  0.8363],\n",
      "        [ 0.6236,  0.7513,  1.1191,  1.3430,  0.8363,  1.6757,  0.6734,  0.7124],\n",
      "        [ 0.2185,  1.0981,  1.0484,  1.4179,  1.0313,  2.0023,  0.4886,  1.7718]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.0434,  1.8236,  1.5304, -0.1331,  1.4193,  0.4636,  1.0586,  0.1169],\n",
      "        [ 0.7043,  1.9043,  0.8840,  0.2183,  0.7164,  0.8006,  0.6960,  1.4613],\n",
      "        [ 1.9011,  1.0004,  0.8316,  1.2999,  0.7013,  1.6192,  1.4804,  1.9203],\n",
      "        [ 1.7196,  1.7282,  1.3853,  1.7474,  0.9644,  1.1010,  0.9544,  1.5100],\n",
      "        [ 0.2290,  0.6449,  0.9685,  1.9387,  1.2395,  0.8680,  1.2330,  0.6931],\n",
      "        [ 0.1404,  1.5778,  0.6406,  0.7797,  0.4436,  2.1414,  0.4635,  0.8363],\n",
      "        [ 0.6236,  0.7513,  1.1191,  1.3430,  0.8363,  1.6757,  0.6734,  0.7124],\n",
      "        [ 0.2185,  1.0981,  1.0484,  1.4179,  1.0313,  2.0023,  0.4886,  1.7718]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.0534,  1.8336,  1.5404, -0.1433,  1.4293,  0.4739,  1.0687,  0.1066],\n",
      "        [ 0.7145,  1.9143,  0.8941,  0.2081,  0.7266,  0.8107,  0.7061,  1.4713],\n",
      "        [ 1.9111,  1.0105,  0.8417,  1.3099,  0.7115,  1.6292,  1.4905,  1.9303],\n",
      "        [ 1.7296,  1.7382,  1.3953,  1.7574,  0.9745,  1.1111,  0.9645,  1.5200],\n",
      "        [ 0.2188,  0.6550,  0.9786,  1.9487,  1.2496,  0.8781,  1.2430,  0.7033],\n",
      "        [ 0.1301,  1.5879,  0.6508,  0.7899,  0.4540,  2.1513,  0.4739,  0.8464],\n",
      "        [ 0.6338,  0.7614,  1.1292,  1.3530,  0.8464,  1.6857,  0.6836,  0.7225],\n",
      "        [ 0.2082,  1.1082,  1.0585,  1.4279,  1.0413,  2.0123,  0.4989,  1.7818]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.0534,  1.8336,  1.5404, -0.1433,  1.4293,  0.4739,  1.0687,  0.1066],\n",
      "        [ 0.7145,  1.9143,  0.8941,  0.2081,  0.7266,  0.8107,  0.7061,  1.4713],\n",
      "        [ 1.9111,  1.0105,  0.8417,  1.3099,  0.7115,  1.6292,  1.4905,  1.9303],\n",
      "        [ 1.7296,  1.7382,  1.3953,  1.7574,  0.9745,  1.1111,  0.9645,  1.5200],\n",
      "        [ 0.2188,  0.6550,  0.9786,  1.9487,  1.2496,  0.8781,  1.2430,  0.7033],\n",
      "        [ 0.1301,  1.5879,  0.6508,  0.7899,  0.4540,  2.1513,  0.4739,  0.8464],\n",
      "        [ 0.6338,  0.7614,  1.1292,  1.3530,  0.8464,  1.6857,  0.6836,  0.7225],\n",
      "        [ 0.2082,  1.1082,  1.0585,  1.4279,  1.0413,  2.0123,  0.4989,  1.7818]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.0634,  1.8436,  1.5504, -0.1534,  1.4393,  0.4843,  1.0788,  0.0962],\n",
      "        [ 0.7247,  1.9243,  0.9042,  0.1977,  0.7368,  0.8209,  0.7163,  1.4813],\n",
      "        [ 1.9211,  1.0205,  0.8518,  1.3200,  0.7217,  1.6392,  1.5005,  1.9403],\n",
      "        [ 1.7396,  1.7482,  1.4054,  1.7674,  0.9846,  1.1211,  0.9746,  1.5300],\n",
      "        [ 0.2086,  0.6653,  0.9887,  1.9586,  1.2596,  0.8882,  1.2531,  0.7135],\n",
      "        [ 0.1197,  1.5979,  0.6610,  0.8000,  0.4644,  2.1613,  0.4843,  0.8566],\n",
      "        [ 0.6440,  0.7716,  1.1393,  1.3630,  0.8565,  1.6957,  0.6938,  0.7327],\n",
      "        [ 0.1979,  1.1183,  1.0686,  1.4379,  1.0514,  2.0223,  0.5093,  1.7918]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.0634,  1.8436,  1.5504, -0.1534,  1.4393,  0.4843,  1.0788,  0.0962],\n",
      "        [ 0.7247,  1.9243,  0.9042,  0.1977,  0.7368,  0.8209,  0.7163,  1.4813],\n",
      "        [ 1.9211,  1.0205,  0.8518,  1.3200,  0.7217,  1.6392,  1.5005,  1.9403],\n",
      "        [ 1.7396,  1.7482,  1.4054,  1.7674,  0.9846,  1.1211,  0.9746,  1.5300],\n",
      "        [ 0.2086,  0.6653,  0.9887,  1.9586,  1.2596,  0.8882,  1.2531,  0.7135],\n",
      "        [ 0.1197,  1.5979,  0.6610,  0.8000,  0.4644,  2.1613,  0.4843,  0.8566],\n",
      "        [ 0.6440,  0.7716,  1.1393,  1.3630,  0.8565,  1.6957,  0.6938,  0.7327],\n",
      "        [ 0.1979,  1.1183,  1.0686,  1.4379,  1.0514,  2.0223,  0.5093,  1.7918]],\n",
      "       requires_grad=True)\n",
      "epoch:  10 ; loss:  1.5002042055130005 ; mask density:  0.001323027303442359 ; pred:  tensor([2.6529e-03, 4.0143e-05, 3.2432e-04, 9.9636e-01, 2.4840e-04, 3.0007e-04,\n",
      "        7.7121e-05], grad_fn=<SoftmaxBackward0>) ; labels equal:  tensor(True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.0734,  1.8536,  1.5605, -0.1637,  1.4494,  0.4948,  1.0889,  0.0858],\n",
      "        [ 0.7349,  1.9343,  0.9144,  0.1873,  0.7470,  0.8310,  0.7266,  1.4913],\n",
      "        [ 1.9311,  1.0307,  0.8620,  1.3300,  0.7319,  1.6492,  1.5105,  1.9503],\n",
      "        [ 1.7496,  1.7582,  1.4154,  1.7774,  0.9947,  1.1312,  0.9847,  1.5400],\n",
      "        [ 0.1982,  0.6755,  0.9988,  1.9686,  1.2697,  0.8984,  1.2631,  0.7237],\n",
      "        [ 0.1092,  1.6079,  0.6713,  0.8102,  0.4749,  2.1713,  0.4948,  0.8667],\n",
      "        [ 0.6543,  0.7818,  1.1493,  1.3731,  0.8667,  1.7057,  0.7041,  0.7429],\n",
      "        [ 0.1875,  1.1284,  1.0787,  1.4480,  1.0615,  2.0322,  0.5198,  1.8018]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.0734,  1.8536,  1.5605, -0.1637,  1.4494,  0.4948,  1.0889,  0.0858],\n",
      "        [ 0.7349,  1.9343,  0.9144,  0.1873,  0.7470,  0.8310,  0.7266,  1.4913],\n",
      "        [ 1.9311,  1.0307,  0.8620,  1.3300,  0.7319,  1.6492,  1.5105,  1.9503],\n",
      "        [ 1.7496,  1.7582,  1.4154,  1.7774,  0.9947,  1.1312,  0.9847,  1.5400],\n",
      "        [ 0.1982,  0.6755,  0.9988,  1.9686,  1.2697,  0.8984,  1.2631,  0.7237],\n",
      "        [ 0.1092,  1.6079,  0.6713,  0.8102,  0.4749,  2.1713,  0.4948,  0.8667],\n",
      "        [ 0.6543,  0.7818,  1.1493,  1.3731,  0.8667,  1.7057,  0.7041,  0.7429],\n",
      "        [ 0.1875,  1.1284,  1.0787,  1.4480,  1.0615,  2.0322,  0.5198,  1.8018]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.0834,  1.8636,  1.5705, -0.1739,  1.4594,  0.5054,  1.0990,  0.0753],\n",
      "        [ 0.7452,  1.9443,  0.9245,  0.1767,  0.7572,  0.8412,  0.7368,  1.5014],\n",
      "        [ 1.9411,  1.0408,  0.8722,  1.3401,  0.7422,  1.6592,  1.5205,  1.9603],\n",
      "        [ 1.7596,  1.7682,  1.4254,  1.7874,  1.0048,  1.1413,  0.9949,  1.5501],\n",
      "        [ 0.1877,  0.6859,  1.0089,  1.9786,  1.2798,  0.9085,  1.2732,  0.7340],\n",
      "        [ 0.0987,  1.6179,  0.6816,  0.8204,  0.4855,  2.1813,  0.5054,  0.8769],\n",
      "        [ 0.6647,  0.7920,  1.1594,  1.3831,  0.8769,  1.7157,  0.7143,  0.7532],\n",
      "        [ 0.1769,  1.1385,  1.0888,  1.4580,  1.0716,  2.0422,  0.5303,  1.8118]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.0834,  1.8636,  1.5705, -0.1739,  1.4594,  0.5054,  1.0990,  0.0753],\n",
      "        [ 0.7452,  1.9443,  0.9245,  0.1767,  0.7572,  0.8412,  0.7368,  1.5014],\n",
      "        [ 1.9411,  1.0408,  0.8722,  1.3401,  0.7422,  1.6592,  1.5205,  1.9603],\n",
      "        [ 1.7596,  1.7682,  1.4254,  1.7874,  1.0048,  1.1413,  0.9949,  1.5501],\n",
      "        [ 0.1877,  0.6859,  1.0089,  1.9786,  1.2798,  0.9085,  1.2732,  0.7340],\n",
      "        [ 0.0987,  1.6179,  0.6816,  0.8204,  0.4855,  2.1813,  0.5054,  0.8769],\n",
      "        [ 0.6647,  0.7920,  1.1594,  1.3831,  0.8769,  1.7157,  0.7143,  0.7532],\n",
      "        [ 0.1769,  1.1385,  1.0888,  1.4580,  1.0716,  2.0422,  0.5303,  1.8118]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.0933,  1.8736,  1.5805, -0.1842,  1.4694,  0.5161,  1.1091,  0.0648],\n",
      "        [ 0.7555,  1.9543,  0.9347,  0.1660,  0.7675,  0.8515,  0.7471,  1.5114],\n",
      "        [ 1.9511,  1.0509,  0.8824,  1.3501,  0.7525,  1.6692,  1.5306,  1.9702],\n",
      "        [ 1.7696,  1.7782,  1.4355,  1.7974,  1.0150,  1.1514,  1.0050,  1.5601],\n",
      "        [ 0.1771,  0.6962,  1.0191,  1.9886,  1.2898,  0.9187,  1.2833,  0.7443],\n",
      "        [ 0.0881,  1.6279,  0.6920,  0.8307,  0.4962,  2.1912,  0.5160,  0.8871],\n",
      "        [ 0.6751,  0.8023,  1.1695,  1.3932,  0.8871,  1.7257,  0.7247,  0.7635],\n",
      "        [ 0.1662,  1.1486,  1.0989,  1.4680,  1.0818,  2.0522,  0.5409,  1.8218]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.0933,  1.8736,  1.5805, -0.1842,  1.4694,  0.5161,  1.1091,  0.0648],\n",
      "        [ 0.7555,  1.9543,  0.9347,  0.1660,  0.7675,  0.8515,  0.7471,  1.5114],\n",
      "        [ 1.9511,  1.0509,  0.8824,  1.3501,  0.7525,  1.6692,  1.5306,  1.9702],\n",
      "        [ 1.7696,  1.7782,  1.4355,  1.7974,  1.0150,  1.1514,  1.0050,  1.5601],\n",
      "        [ 0.1771,  0.6962,  1.0191,  1.9886,  1.2898,  0.9187,  1.2833,  0.7443],\n",
      "        [ 0.0881,  1.6279,  0.6920,  0.8307,  0.4962,  2.1912,  0.5160,  0.8871],\n",
      "        [ 0.6751,  0.8023,  1.1695,  1.3932,  0.8871,  1.7257,  0.7247,  0.7635],\n",
      "        [ 0.1662,  1.1486,  1.0989,  1.4680,  1.0818,  2.0522,  0.5409,  1.8218]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.1033,  1.8836,  1.5905, -0.1945,  1.4795,  0.5268,  1.1192,  0.0541],\n",
      "        [ 0.7658,  1.9643,  0.9449,  0.1552,  0.7779,  0.8617,  0.7575,  1.5215],\n",
      "        [ 1.9611,  1.0611,  0.8926,  1.3602,  0.7628,  1.6793,  1.5406,  1.9802],\n",
      "        [ 1.7796,  1.7882,  1.4455,  1.8074,  1.0251,  1.1616,  1.0152,  1.5701],\n",
      "        [ 0.1663,  0.7066,  1.0293,  1.9986,  1.2999,  0.9290,  1.2934,  0.7546],\n",
      "        [ 0.0774,  1.6380,  0.7024,  0.8409,  0.5070,  2.2012,  0.5268,  0.8974],\n",
      "        [ 0.6855,  0.8126,  1.1797,  1.4033,  0.8973,  1.7357,  0.7350,  0.7738],\n",
      "        [ 0.1554,  1.1587,  1.1090,  1.4781,  1.0919,  2.0622,  0.5516,  1.8318]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.1033,  1.8836,  1.5905, -0.1945,  1.4795,  0.5268,  1.1192,  0.0541],\n",
      "        [ 0.7658,  1.9643,  0.9449,  0.1552,  0.7779,  0.8617,  0.7575,  1.5215],\n",
      "        [ 1.9611,  1.0611,  0.8926,  1.3602,  0.7628,  1.6793,  1.5406,  1.9802],\n",
      "        [ 1.7796,  1.7882,  1.4455,  1.8074,  1.0251,  1.1616,  1.0152,  1.5701],\n",
      "        [ 0.1663,  0.7066,  1.0293,  1.9986,  1.2999,  0.9290,  1.2934,  0.7546],\n",
      "        [ 0.0774,  1.6380,  0.7024,  0.8409,  0.5070,  2.2012,  0.5268,  0.8974],\n",
      "        [ 0.6855,  0.8126,  1.1797,  1.4033,  0.8973,  1.7357,  0.7350,  0.7738],\n",
      "        [ 0.1554,  1.1587,  1.1090,  1.4781,  1.0919,  2.0622,  0.5516,  1.8318]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.1133,  1.8936,  1.6006, -0.2049,  1.4895,  0.5377,  1.1294,  0.0434],\n",
      "        [ 0.7762,  1.9743,  0.9552,  0.1444,  0.7882,  0.8720,  0.7679,  1.5315],\n",
      "        [ 1.9711,  1.0713,  0.9029,  1.3703,  0.7732,  1.6893,  1.5507,  1.9902],\n",
      "        [ 1.7896,  1.7982,  1.4556,  1.8174,  1.0353,  1.1717,  1.0254,  1.5802],\n",
      "        [ 0.1555,  0.7171,  1.0395,  2.0086,  1.3100,  0.9392,  1.3035,  0.7650],\n",
      "        [ 0.0666,  1.6480,  0.7128,  0.8513,  0.5179,  2.2112,  0.5376,  0.9076],\n",
      "        [ 0.6960,  0.8229,  1.1898,  1.4133,  0.9076,  1.7458,  0.7455,  0.7842],\n",
      "        [ 0.1446,  1.1688,  1.1192,  1.4881,  1.1021,  2.0721,  0.5624,  1.8418]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.1133,  1.8936,  1.6006, -0.2049,  1.4895,  0.5377,  1.1294,  0.0434],\n",
      "        [ 0.7762,  1.9743,  0.9552,  0.1444,  0.7882,  0.8720,  0.7679,  1.5315],\n",
      "        [ 1.9711,  1.0713,  0.9029,  1.3703,  0.7732,  1.6893,  1.5507,  1.9902],\n",
      "        [ 1.7896,  1.7982,  1.4556,  1.8174,  1.0353,  1.1717,  1.0254,  1.5802],\n",
      "        [ 0.1555,  0.7171,  1.0395,  2.0086,  1.3100,  0.9392,  1.3035,  0.7650],\n",
      "        [ 0.0666,  1.6480,  0.7128,  0.8513,  0.5179,  2.2112,  0.5376,  0.9076],\n",
      "        [ 0.6960,  0.8229,  1.1898,  1.4133,  0.9076,  1.7458,  0.7455,  0.7842],\n",
      "        [ 0.1446,  1.1688,  1.1192,  1.4881,  1.1021,  2.0721,  0.5624,  1.8418]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.1232,  1.9036,  1.6106, -0.2153,  1.4996,  0.5486,  1.1396,  0.0326],\n",
      "        [ 0.7866,  1.9842,  0.9654,  0.1333,  0.7987,  0.8824,  0.7783,  1.5416],\n",
      "        [ 1.9810,  1.0815,  0.9132,  1.3804,  0.7837,  1.6993,  1.5607,  2.0002],\n",
      "        [ 1.7996,  1.8082,  1.4657,  1.8274,  1.0456,  1.1819,  1.0356,  1.5902],\n",
      "        [ 0.1445,  0.7276,  1.0497,  2.0185,  1.3201,  0.9495,  1.3136,  0.7755],\n",
      "        [ 0.0558,  1.6580,  0.7234,  0.8616,  0.5289,  2.2211,  0.5486,  0.9179],\n",
      "        [ 0.7065,  0.8333,  1.2000,  1.4234,  0.9179,  1.7558,  0.7559,  0.7946],\n",
      "        [ 0.1335,  1.1790,  1.1294,  1.4982,  1.1123,  2.0821,  0.5733,  1.8518]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.1232,  1.9036,  1.6106, -0.2153,  1.4996,  0.5486,  1.1396,  0.0326],\n",
      "        [ 0.7866,  1.9842,  0.9654,  0.1333,  0.7987,  0.8824,  0.7783,  1.5416],\n",
      "        [ 1.9810,  1.0815,  0.9132,  1.3804,  0.7837,  1.6993,  1.5607,  2.0002],\n",
      "        [ 1.7996,  1.8082,  1.4657,  1.8274,  1.0456,  1.1819,  1.0356,  1.5902],\n",
      "        [ 0.1445,  0.7276,  1.0497,  2.0185,  1.3201,  0.9495,  1.3136,  0.7755],\n",
      "        [ 0.0558,  1.6580,  0.7234,  0.8616,  0.5289,  2.2211,  0.5486,  0.9179],\n",
      "        [ 0.7065,  0.8333,  1.2000,  1.4234,  0.9179,  1.7558,  0.7559,  0.7946],\n",
      "        [ 0.1335,  1.1790,  1.1294,  1.4982,  1.1123,  2.0821,  0.5733,  1.8518]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.1332,  1.9136,  1.6207, -0.2257,  1.5097,  0.5597,  1.1498,  0.0217],\n",
      "        [ 0.7971,  1.9942,  0.9757,  0.1222,  0.8091,  0.8927,  0.7888,  1.5516],\n",
      "        [ 1.9910,  1.0917,  0.9236,  1.3905,  0.7941,  1.7093,  1.5708,  2.0101],\n",
      "        [ 1.8096,  1.8182,  1.4758,  1.8374,  1.0558,  1.1920,  1.0459,  1.6003],\n",
      "        [ 0.1333,  0.7382,  1.0599,  2.0285,  1.3303,  0.9598,  1.3237,  0.7860],\n",
      "        [ 0.0448,  1.6680,  0.7339,  0.8720,  0.5399,  2.2310,  0.5596,  0.9283],\n",
      "        [ 0.7171,  0.8437,  1.2101,  1.4335,  0.9282,  1.7658,  0.7665,  0.8051],\n",
      "        [ 0.1224,  1.1892,  1.1396,  1.5083,  1.1225,  2.0921,  0.5842,  1.8618]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.1332,  1.9136,  1.6207, -0.2257,  1.5097,  0.5597,  1.1498,  0.0217],\n",
      "        [ 0.7971,  1.9942,  0.9757,  0.1222,  0.8091,  0.8927,  0.7888,  1.5516],\n",
      "        [ 1.9910,  1.0917,  0.9236,  1.3905,  0.7941,  1.7093,  1.5708,  2.0101],\n",
      "        [ 1.8096,  1.8182,  1.4758,  1.8374,  1.0558,  1.1920,  1.0459,  1.6003],\n",
      "        [ 0.1333,  0.7382,  1.0599,  2.0285,  1.3303,  0.9598,  1.3237,  0.7860],\n",
      "        [ 0.0448,  1.6680,  0.7339,  0.8720,  0.5399,  2.2310,  0.5596,  0.9283],\n",
      "        [ 0.7171,  0.8437,  1.2101,  1.4335,  0.9282,  1.7658,  0.7665,  0.8051],\n",
      "        [ 0.1224,  1.1892,  1.1396,  1.5083,  1.1225,  2.0921,  0.5842,  1.8618]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.1431,  1.9236,  1.6307, -0.2363,  1.5198,  0.5708,  1.1600,  0.0108],\n",
      "        [ 0.8076,  2.0042,  0.9861,  0.1110,  0.8196,  0.9031,  0.7994,  1.5617],\n",
      "        [ 2.0010,  1.1019,  0.9339,  1.4006,  0.8047,  1.7194,  1.5808,  2.0201],\n",
      "        [ 1.8196,  1.8282,  1.4858,  1.8474,  1.0661,  1.2022,  1.0562,  1.6103],\n",
      "        [ 0.1221,  0.7488,  1.0702,  2.0385,  1.3404,  0.9701,  1.3338,  0.7965],\n",
      "        [ 0.0338,  1.6781,  0.7446,  0.8824,  0.5511,  2.2410,  0.5707,  0.9386],\n",
      "        [ 0.7278,  0.8542,  1.2203,  1.4436,  0.9386,  1.7758,  0.7770,  0.8156],\n",
      "        [ 0.1112,  1.1994,  1.1498,  1.5184,  1.1327,  2.1020,  0.5952,  1.8718]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.1431,  1.9236,  1.6307, -0.2363,  1.5198,  0.5708,  1.1600,  0.0108],\n",
      "        [ 0.8076,  2.0042,  0.9861,  0.1110,  0.8196,  0.9031,  0.7994,  1.5617],\n",
      "        [ 2.0010,  1.1019,  0.9339,  1.4006,  0.8047,  1.7194,  1.5808,  2.0201],\n",
      "        [ 1.8196,  1.8282,  1.4858,  1.8474,  1.0661,  1.2022,  1.0562,  1.6103],\n",
      "        [ 0.1221,  0.7488,  1.0702,  2.0385,  1.3404,  0.9701,  1.3338,  0.7965],\n",
      "        [ 0.0338,  1.6781,  0.7446,  0.8824,  0.5511,  2.2410,  0.5707,  0.9386],\n",
      "        [ 0.7278,  0.8542,  1.2203,  1.4436,  0.9386,  1.7758,  0.7770,  0.8156],\n",
      "        [ 0.1112,  1.1994,  1.1498,  1.5184,  1.1327,  2.1020,  0.5952,  1.8718]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.1531e+00,  1.9335e+00,  1.6407e+00, -2.4680e-01,  1.5298e+00,\n",
      "          5.8201e-01,  1.1702e+00, -2.9709e-04],\n",
      "        [ 8.1821e-01,  2.0141e+00,  9.9643e-01,  9.9620e-02,  8.3019e-01,\n",
      "          9.1358e-01,  8.0996e-01,  1.5718e+00],\n",
      "        [ 2.0109e+00,  1.1122e+00,  9.4435e-01,  1.4107e+00,  8.1524e-01,\n",
      "          1.7294e+00,  1.5909e+00,  2.0301e+00],\n",
      "        [ 1.8296e+00,  1.8382e+00,  1.4959e+00,  1.8574e+00,  1.0764e+00,\n",
      "          1.2125e+00,  1.0665e+00,  1.6204e+00],\n",
      "        [ 1.1074e-01,  7.5947e-01,  1.0805e+00,  2.0484e+00,  1.3505e+00,\n",
      "          9.8052e-01,  1.3440e+00,  8.0712e-01],\n",
      "        [ 2.2641e-02,  1.6881e+00,  7.5527e-01,  8.9291e-01,  5.6242e-01,\n",
      "          2.2509e+00,  5.8195e-01,  9.4905e-01],\n",
      "        [ 7.3853e-01,  8.6469e-01,  1.2305e+00,  1.4537e+00,  9.4901e-01,\n",
      "          1.7858e+00,  7.8767e-01,  8.2618e-01],\n",
      "        [ 9.9820e-02,  1.2096e+00,  1.1601e+00,  1.5284e+00,  1.1430e+00,\n",
      "          2.1120e+00,  6.0637e-01,  1.8817e+00]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.1531e+00,  1.9335e+00,  1.6407e+00, -2.4680e-01,  1.5298e+00,\n",
      "          5.8201e-01,  1.1702e+00, -2.9709e-04],\n",
      "        [ 8.1821e-01,  2.0141e+00,  9.9643e-01,  9.9620e-02,  8.3019e-01,\n",
      "          9.1358e-01,  8.0996e-01,  1.5718e+00],\n",
      "        [ 2.0109e+00,  1.1122e+00,  9.4435e-01,  1.4107e+00,  8.1524e-01,\n",
      "          1.7294e+00,  1.5909e+00,  2.0301e+00],\n",
      "        [ 1.8296e+00,  1.8382e+00,  1.4959e+00,  1.8574e+00,  1.0764e+00,\n",
      "          1.2125e+00,  1.0665e+00,  1.6204e+00],\n",
      "        [ 1.1074e-01,  7.5947e-01,  1.0805e+00,  2.0484e+00,  1.3505e+00,\n",
      "          9.8052e-01,  1.3440e+00,  8.0712e-01],\n",
      "        [ 2.2641e-02,  1.6881e+00,  7.5527e-01,  8.9291e-01,  5.6242e-01,\n",
      "          2.2509e+00,  5.8195e-01,  9.4905e-01],\n",
      "        [ 7.3853e-01,  8.6469e-01,  1.2305e+00,  1.4537e+00,  9.4901e-01,\n",
      "          1.7858e+00,  7.8767e-01,  8.2618e-01],\n",
      "        [ 9.9820e-02,  1.2096e+00,  1.1601e+00,  1.5284e+00,  1.1430e+00,\n",
      "          2.1120e+00,  6.0637e-01,  1.8817e+00]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.1630,  1.9435,  1.6508, -0.2574,  1.5399,  0.5933,  1.1805, -0.0114],\n",
      "        [ 0.8288,  2.0241,  1.0068,  0.0881,  0.8408,  0.9241,  0.8206,  1.5818],\n",
      "        [ 2.0209,  1.1225,  0.9548,  1.4209,  0.8259,  1.7394,  1.6010,  2.0400],\n",
      "        [ 1.8396,  1.8482,  1.5060,  1.8674,  1.0867,  1.2227,  1.0768,  1.6304],\n",
      "        [ 0.0993,  0.7702,  1.0908,  2.0584,  1.3607,  0.9909,  1.3541,  0.8178],\n",
      "        [ 0.0114,  1.6982,  0.7660,  0.9034,  0.5738,  2.2608,  0.5933,  0.9595],\n",
      "        [ 0.7493,  0.8752,  1.2407,  1.4638,  0.9595,  1.7958,  0.7983,  0.8368],\n",
      "        [ 0.0883,  1.2198,  1.1703,  1.5385,  1.1532,  2.1219,  0.6176,  1.8917]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.1630,  1.9435,  1.6508, -0.2574,  1.5399,  0.5933,  1.1805, -0.0114],\n",
      "        [ 0.8288,  2.0241,  1.0068,  0.0881,  0.8408,  0.9241,  0.8206,  1.5818],\n",
      "        [ 2.0209,  1.1225,  0.9548,  1.4209,  0.8259,  1.7394,  1.6010,  2.0400],\n",
      "        [ 1.8396,  1.8482,  1.5060,  1.8674,  1.0867,  1.2227,  1.0768,  1.6304],\n",
      "        [ 0.0993,  0.7702,  1.0908,  2.0584,  1.3607,  0.9909,  1.3541,  0.8178],\n",
      "        [ 0.0114,  1.6982,  0.7660,  0.9034,  0.5738,  2.2608,  0.5933,  0.9595],\n",
      "        [ 0.7493,  0.8752,  1.2407,  1.4638,  0.9595,  1.7958,  0.7983,  0.8368],\n",
      "        [ 0.0883,  1.2198,  1.1703,  1.5385,  1.1532,  2.1219,  0.6176,  1.8917]],\n",
      "       requires_grad=True)\n",
      "epoch:  20 ; loss:  1.4705199003219604 ; mask density:  0.001346623059362173 ; pred:  tensor([4.9204e-03, 6.8288e-05, 4.7744e-04, 9.9380e-01, 2.2854e-04, 3.7002e-04,\n",
      "        1.3222e-04], grad_fn=<SoftmaxBackward0>) ; labels equal:  tensor(True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.1729e+00,  1.9535e+00,  1.6609e+00, -2.6805e-01,  1.5500e+00,\n",
      "          6.0473e-01,  1.1908e+00, -2.2655e-02],\n",
      "        [ 8.3950e-01,  2.0341e+00,  1.0173e+00,  7.6560e-02,  8.5145e-01,\n",
      "          9.3459e-01,  8.3128e-01,  1.5919e+00],\n",
      "        [ 2.0309e+00,  1.1328e+00,  9.6528e-01,  1.4310e+00,  8.3655e-01,\n",
      "          1.7494e+00,  1.6110e+00,  2.0500e+00],\n",
      "        [ 1.8496e+00,  1.8582e+00,  1.5161e+00,  1.8774e+00,  1.0971e+00,\n",
      "          1.2329e+00,  1.0872e+00,  1.6405e+00],\n",
      "        [ 8.7644e-02,  7.8101e-01,  1.1012e+00,  2.0683e+00,  1.3709e+00,\n",
      "          1.0014e+00,  1.3643e+00,  8.2845e-01],\n",
      "        [ 1.1373e-04,  1.7082e+00,  7.7682e-01,  9.1397e-01,  5.8530e-01,\n",
      "          2.2707e+00,  6.0468e-01,  9.6998e-01],\n",
      "        [ 7.6016e-01,  8.8583e-01,  1.2510e+00,  1.4740e+00,  9.6994e-01,\n",
      "          1.8058e+00,  8.0908e-01,  8.4745e-01],\n",
      "        [ 7.6759e-02,  1.2301e+00,  1.1806e+00,  1.5486e+00,  1.1635e+00,\n",
      "          2.1318e+00,  6.2888e-01,  1.9017e+00]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.1729e+00,  1.9535e+00,  1.6609e+00, -2.6805e-01,  1.5500e+00,\n",
      "          6.0473e-01,  1.1908e+00, -2.2655e-02],\n",
      "        [ 8.3950e-01,  2.0341e+00,  1.0173e+00,  7.6560e-02,  8.5145e-01,\n",
      "          9.3459e-01,  8.3128e-01,  1.5919e+00],\n",
      "        [ 2.0309e+00,  1.1328e+00,  9.6528e-01,  1.4310e+00,  8.3655e-01,\n",
      "          1.7494e+00,  1.6110e+00,  2.0500e+00],\n",
      "        [ 1.8496e+00,  1.8582e+00,  1.5161e+00,  1.8774e+00,  1.0971e+00,\n",
      "          1.2329e+00,  1.0872e+00,  1.6405e+00],\n",
      "        [ 8.7644e-02,  7.8101e-01,  1.1012e+00,  2.0683e+00,  1.3709e+00,\n",
      "          1.0014e+00,  1.3643e+00,  8.2845e-01],\n",
      "        [ 1.1373e-04,  1.7082e+00,  7.7682e-01,  9.1397e-01,  5.8530e-01,\n",
      "          2.2707e+00,  6.0468e-01,  9.6998e-01],\n",
      "        [ 7.6016e-01,  8.8583e-01,  1.2510e+00,  1.4740e+00,  9.6994e-01,\n",
      "          1.8058e+00,  8.0908e-01,  8.4745e-01],\n",
      "        [ 7.6759e-02,  1.2301e+00,  1.1806e+00,  1.5486e+00,  1.1635e+00,\n",
      "          2.1318e+00,  6.2888e-01,  1.9017e+00]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.1829,  1.9635,  1.6709, -0.2788,  1.5601,  0.6162,  1.2011, -0.0340],\n",
      "        [ 0.8502,  2.0440,  1.0277,  0.0649,  0.8621,  0.9452,  0.8420,  1.6020],\n",
      "        [ 2.0408,  1.1432,  0.9758,  1.4412,  0.8473,  1.7595,  1.6211,  2.0599],\n",
      "        [ 1.8596,  1.8682,  1.5263,  1.8874,  1.1074,  1.2432,  1.0975,  1.6505],\n",
      "        [ 0.0759,  0.7919,  1.1115,  2.0783,  1.3811,  1.0119,  1.3745,  0.8392],\n",
      "        [-0.0113,  1.7182,  0.7877,  0.9246,  0.5969,  2.2806,  0.6162,  0.9805],\n",
      "        [ 0.7711,  0.8965,  1.2612,  1.4841,  0.9805,  1.8158,  0.8199,  0.8582],\n",
      "        [ 0.0651,  1.2403,  1.1909,  1.5587,  1.1739,  2.1418,  0.6403,  1.9117]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.1829,  1.9635,  1.6709, -0.2788,  1.5601,  0.6162,  1.2011, -0.0340],\n",
      "        [ 0.8502,  2.0440,  1.0277,  0.0649,  0.8621,  0.9452,  0.8420,  1.6020],\n",
      "        [ 2.0408,  1.1432,  0.9758,  1.4412,  0.8473,  1.7595,  1.6211,  2.0599],\n",
      "        [ 1.8596,  1.8682,  1.5263,  1.8874,  1.1074,  1.2432,  1.0975,  1.6505],\n",
      "        [ 0.0759,  0.7919,  1.1115,  2.0783,  1.3811,  1.0119,  1.3745,  0.8392],\n",
      "        [-0.0113,  1.7182,  0.7877,  0.9246,  0.5969,  2.2806,  0.6162,  0.9805],\n",
      "        [ 0.7711,  0.8965,  1.2612,  1.4841,  0.9805,  1.8158,  0.8199,  0.8582],\n",
      "        [ 0.0651,  1.2403,  1.1909,  1.5587,  1.1739,  2.1418,  0.6403,  1.9117]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.1928,  1.9734,  1.6810, -0.2895,  1.5702,  0.6278,  1.2114, -0.0454],\n",
      "        [ 0.8610,  2.0540,  1.0382,  0.0530,  0.8729,  0.9558,  0.8528,  1.6121],\n",
      "        [ 2.0508,  1.1536,  0.9864,  1.4513,  0.8581,  1.7695,  1.6312,  2.0699],\n",
      "        [ 1.8696,  1.8782,  1.5364,  1.8973,  1.1178,  1.2535,  1.1080,  1.6606],\n",
      "        [ 0.0641,  0.8028,  1.1219,  2.0882,  1.3913,  1.0224,  1.3847,  0.8500],\n",
      "        [-0.0228,  1.7283,  0.7986,  0.9352,  0.6086,  2.2905,  0.6278,  0.9910],\n",
      "        [ 0.7820,  0.9072,  1.2715,  1.4942,  0.9910,  1.8259,  0.8307,  0.8689],\n",
      "        [ 0.0532,  1.2506,  1.2012,  1.5688,  1.1842,  2.1517,  0.6518,  1.9217]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.1928,  1.9734,  1.6810, -0.2895,  1.5702,  0.6278,  1.2114, -0.0454],\n",
      "        [ 0.8610,  2.0540,  1.0382,  0.0530,  0.8729,  0.9558,  0.8528,  1.6121],\n",
      "        [ 2.0508,  1.1536,  0.9864,  1.4513,  0.8581,  1.7695,  1.6312,  2.0699],\n",
      "        [ 1.8696,  1.8782,  1.5364,  1.8973,  1.1178,  1.2535,  1.1080,  1.6606],\n",
      "        [ 0.0641,  0.8028,  1.1219,  2.0882,  1.3913,  1.0224,  1.3847,  0.8500],\n",
      "        [-0.0228,  1.7283,  0.7986,  0.9352,  0.6086,  2.2905,  0.6278,  0.9910],\n",
      "        [ 0.7820,  0.9072,  1.2715,  1.4942,  0.9910,  1.8259,  0.8307,  0.8689],\n",
      "        [ 0.0532,  1.2506,  1.2012,  1.5688,  1.1842,  2.1517,  0.6518,  1.9217]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.2027,  1.9834,  1.6910, -0.3003,  1.5803,  0.6395,  1.2217, -0.0568],\n",
      "        [ 0.8718,  2.0639,  1.0487,  0.0411,  0.8837,  0.9664,  0.8637,  1.6222],\n",
      "        [ 2.0607,  1.1640,  0.9970,  1.4615,  0.8689,  1.7795,  1.6413,  2.0798],\n",
      "        [ 1.8796,  1.8882,  1.5465,  1.9073,  1.1283,  1.2638,  1.1184,  1.6707],\n",
      "        [ 0.0521,  0.8137,  1.1324,  2.0981,  1.4015,  1.0329,  1.3949,  0.8609],\n",
      "        [-0.0344,  1.7383,  0.8096,  0.9459,  0.6204,  2.3004,  0.6395,  1.0016],\n",
      "        [ 0.7931,  0.9179,  1.2818,  1.5044,  1.0016,  1.8359,  0.8416,  0.8797],\n",
      "        [ 0.0413,  1.2609,  1.2116,  1.5789,  1.1946,  2.1616,  0.6633,  1.9317]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.2027,  1.9834,  1.6910, -0.3003,  1.5803,  0.6395,  1.2217, -0.0568],\n",
      "        [ 0.8718,  2.0639,  1.0487,  0.0411,  0.8837,  0.9664,  0.8637,  1.6222],\n",
      "        [ 2.0607,  1.1640,  0.9970,  1.4615,  0.8689,  1.7795,  1.6413,  2.0798],\n",
      "        [ 1.8796,  1.8882,  1.5465,  1.9073,  1.1283,  1.2638,  1.1184,  1.6707],\n",
      "        [ 0.0521,  0.8137,  1.1324,  2.0981,  1.4015,  1.0329,  1.3949,  0.8609],\n",
      "        [-0.0344,  1.7383,  0.8096,  0.9459,  0.6204,  2.3004,  0.6395,  1.0016],\n",
      "        [ 0.7931,  0.9179,  1.2818,  1.5044,  1.0016,  1.8359,  0.8416,  0.8797],\n",
      "        [ 0.0413,  1.2609,  1.2116,  1.5789,  1.1946,  2.1616,  0.6633,  1.9317]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.2126,  1.9933,  1.7011, -0.3112,  1.5905,  0.6513,  1.2321, -0.0684],\n",
      "        [ 0.8827,  2.0738,  1.0593,  0.0291,  0.8945,  0.9771,  0.8746,  1.6323],\n",
      "        [ 2.0706,  1.1744,  1.0076,  1.4717,  0.8798,  1.7895,  1.6514,  2.0897],\n",
      "        [ 1.8896,  1.8982,  1.5566,  1.9173,  1.1387,  1.2741,  1.1289,  1.6808],\n",
      "        [ 0.0400,  0.8248,  1.1428,  2.1081,  1.4117,  1.0435,  1.4052,  0.8718],\n",
      "        [-0.0460,  1.7484,  0.8206,  0.9566,  0.6323,  2.3103,  0.6513,  1.0123],\n",
      "        [ 0.8042,  0.9287,  1.2921,  1.5145,  1.0122,  1.8459,  0.8526,  0.8906],\n",
      "        [ 0.0293,  1.2713,  1.2220,  1.5891,  1.2050,  2.1715,  0.6750,  1.9416]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.2126,  1.9933,  1.7011, -0.3112,  1.5905,  0.6513,  1.2321, -0.0684],\n",
      "        [ 0.8827,  2.0738,  1.0593,  0.0291,  0.8945,  0.9771,  0.8746,  1.6323],\n",
      "        [ 2.0706,  1.1744,  1.0076,  1.4717,  0.8798,  1.7895,  1.6514,  2.0897],\n",
      "        [ 1.8896,  1.8982,  1.5566,  1.9173,  1.1387,  1.2741,  1.1289,  1.6808],\n",
      "        [ 0.0400,  0.8248,  1.1428,  2.1081,  1.4117,  1.0435,  1.4052,  0.8718],\n",
      "        [-0.0460,  1.7484,  0.8206,  0.9566,  0.6323,  2.3103,  0.6513,  1.0123],\n",
      "        [ 0.8042,  0.9287,  1.2921,  1.5145,  1.0122,  1.8459,  0.8526,  0.8906],\n",
      "        [ 0.0293,  1.2713,  1.2220,  1.5891,  1.2050,  2.1715,  0.6750,  1.9416]],\n",
      "       requires_grad=True)\n",
      "adj grad None\n",
      "x grad tensor([[ 6.4119e-03,  7.8600e-03,  3.0574e-01,  ..., -1.7975e-01,\n",
      "         -1.2773e-01, -2.5844e-01],\n",
      "        [ 6.5486e-04,  2.6171e-03,  1.5889e-02,  ...,  8.0785e-03,\n",
      "          2.8308e-03,  5.5990e-03],\n",
      "        [ 1.3034e-04,  3.9236e-05,  4.1168e-03,  ...,  1.8827e-03,\n",
      "          5.9122e-05,  1.2444e-03],\n",
      "        ...,\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00]])\n",
      "Parameter containing:\n",
      "tensor([[ 2.2224,  2.0033,  1.7112, -0.3221,  1.6006,  0.6632,  1.2425, -0.0801],\n",
      "        [ 0.8936,  2.0838,  1.0699,  0.0169,  0.9054,  0.9878,  0.8855,  1.6424],\n",
      "        [ 2.0806,  1.1848,  1.0183,  1.4819,  0.8907,  1.7996,  1.6615,  2.0997],\n",
      "        [ 1.8996,  1.9082,  1.5668,  1.9273,  1.1492,  1.2845,  1.1394,  1.6908],\n",
      "        [ 0.0278,  0.8359,  1.1533,  2.1180,  1.4219,  1.0541,  1.4154,  0.8827],\n",
      "        [-0.0578,  1.7584,  0.8317,  0.9674,  0.6443,  2.3201,  0.6631,  1.0229],\n",
      "        [ 0.8153,  0.9395,  1.3024,  1.5247,  1.0229,  1.8559,  0.8636,  0.9015],\n",
      "        [ 0.0171,  1.2816,  1.2324,  1.5992,  1.2154,  2.1814,  0.6867,  1.9516]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.2224,  2.0033,  1.7112, -0.3221,  1.6006,  0.6632,  1.2425, -0.0801],\n",
      "        [ 0.8936,  2.0838,  1.0699,  0.0169,  0.9054,  0.9878,  0.8855,  1.6424],\n",
      "        [ 2.0806,  1.1848,  1.0183,  1.4819,  0.8907,  1.7996,  1.6615,  2.0997],\n",
      "        [ 1.8996,  1.9082,  1.5668,  1.9273,  1.1492,  1.2845,  1.1394,  1.6908],\n",
      "        [ 0.0278,  0.8359,  1.1533,  2.1180,  1.4219,  1.0541,  1.4154,  0.8827],\n",
      "        [-0.0578,  1.7584,  0.8317,  0.9674,  0.6443,  2.3201,  0.6631,  1.0229],\n",
      "        [ 0.8153,  0.9395,  1.3024,  1.5247,  1.0229,  1.8559,  0.8636,  0.9015],\n",
      "        [ 0.0171,  1.2816,  1.2324,  1.5992,  1.2154,  2.1814,  0.6867,  1.9516]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.2323,  2.0132,  1.7212, -0.3331,  1.6107,  0.6752,  1.2529, -0.0918],\n",
      "        [ 0.9046,  2.0937,  1.0805,  0.0047,  0.9164,  0.9986,  0.8965,  1.6525],\n",
      "        [ 2.0905,  1.1953,  1.0290,  1.4921,  0.9017,  1.8096,  1.6715,  2.1096],\n",
      "        [ 1.9096,  1.9182,  1.5769,  1.9373,  1.1597,  1.2948,  1.1499,  1.7009],\n",
      "        [ 0.0155,  0.8470,  1.1638,  2.1279,  1.4322,  1.0648,  1.4256,  0.8937],\n",
      "        [-0.0697,  1.7685,  0.8429,  0.9782,  0.6563,  2.3300,  0.6751,  1.0336],\n",
      "        [ 0.8266,  0.9504,  1.3128,  1.5349,  1.0336,  1.8659,  0.8746,  0.9125],\n",
      "        [ 0.0049,  1.2920,  1.2428,  1.6093,  1.2258,  2.1913,  0.6985,  1.9616]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.2323,  2.0132,  1.7212, -0.3331,  1.6107,  0.6752,  1.2529, -0.0918],\n",
      "        [ 0.9046,  2.0937,  1.0805,  0.0047,  0.9164,  0.9986,  0.8965,  1.6525],\n",
      "        [ 2.0905,  1.1953,  1.0290,  1.4921,  0.9017,  1.8096,  1.6715,  2.1096],\n",
      "        [ 1.9096,  1.9182,  1.5769,  1.9373,  1.1597,  1.2948,  1.1499,  1.7009],\n",
      "        [ 0.0155,  0.8470,  1.1638,  2.1279,  1.4322,  1.0648,  1.4256,  0.8937],\n",
      "        [-0.0697,  1.7685,  0.8429,  0.9782,  0.6563,  2.3300,  0.6751,  1.0336],\n",
      "        [ 0.8266,  0.9504,  1.3128,  1.5349,  1.0336,  1.8659,  0.8746,  0.9125],\n",
      "        [ 0.0049,  1.2920,  1.2428,  1.6093,  1.2258,  2.1913,  0.6985,  1.9616]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.2422,  2.0232,  1.7313, -0.3441,  1.6208,  0.6872,  1.2633, -0.1037],\n",
      "        [ 0.9156,  2.1036,  1.0912, -0.0077,  0.9274,  1.0094,  0.9076,  1.6626],\n",
      "        [ 2.1004,  1.2058,  1.0397,  1.5023,  0.9127,  1.8196,  1.6816,  2.1195],\n",
      "        [ 1.9195,  1.9282,  1.5871,  1.9472,  1.1703,  1.3052,  1.1605,  1.7110],\n",
      "        [ 0.0031,  0.8582,  1.1743,  2.1378,  1.4424,  1.0754,  1.4359,  0.9048],\n",
      "        [-0.0816,  1.7785,  0.8541,  0.9890,  0.6685,  2.3398,  0.6872,  1.0444],\n",
      "        [ 0.8378,  0.9613,  1.3232,  1.5451,  1.0443,  1.8759,  0.8858,  0.9235],\n",
      "        [-0.0075,  1.3024,  1.2532,  1.6195,  1.2363,  2.2012,  0.7104,  1.9715]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.2422,  2.0232,  1.7313, -0.3441,  1.6208,  0.6872,  1.2633, -0.1037],\n",
      "        [ 0.9156,  2.1036,  1.0912, -0.0077,  0.9274,  1.0094,  0.9076,  1.6626],\n",
      "        [ 2.1004,  1.2058,  1.0397,  1.5023,  0.9127,  1.8196,  1.6816,  2.1195],\n",
      "        [ 1.9195,  1.9282,  1.5871,  1.9472,  1.1703,  1.3052,  1.1605,  1.7110],\n",
      "        [ 0.0031,  0.8582,  1.1743,  2.1378,  1.4424,  1.0754,  1.4359,  0.9048],\n",
      "        [-0.0816,  1.7785,  0.8541,  0.9890,  0.6685,  2.3398,  0.6872,  1.0444],\n",
      "        [ 0.8378,  0.9613,  1.3232,  1.5451,  1.0443,  1.8759,  0.8858,  0.9235],\n",
      "        [-0.0075,  1.3024,  1.2532,  1.6195,  1.2363,  2.2012,  0.7104,  1.9715]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.2520,  2.0331,  1.7414, -0.3551,  1.6310,  0.6993,  1.2738, -0.1156],\n",
      "        [ 0.9267,  2.1135,  1.1019, -0.0201,  0.9384,  1.0202,  0.9187,  1.6727],\n",
      "        [ 2.1103,  1.2163,  1.0505,  1.5125,  0.9238,  1.8297,  1.6917,  2.1294],\n",
      "        [ 1.9295,  1.9381,  1.5973,  1.9572,  1.1808,  1.3156,  1.1710,  1.7211],\n",
      "        [-0.0094,  0.8695,  1.1849,  2.1477,  1.4527,  1.0862,  1.4462,  0.9159],\n",
      "        [-0.0937,  1.7886,  0.8654,  0.9999,  0.6808,  2.3496,  0.6993,  1.0552],\n",
      "        [ 0.8492,  0.9722,  1.3335,  1.5553,  1.0551,  1.8859,  0.8969,  0.9345],\n",
      "        [-0.0200,  1.3128,  1.2637,  1.6296,  1.2468,  2.2110,  0.7224,  1.9815]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.2520,  2.0331,  1.7414, -0.3551,  1.6310,  0.6993,  1.2738, -0.1156],\n",
      "        [ 0.9267,  2.1135,  1.1019, -0.0201,  0.9384,  1.0202,  0.9187,  1.6727],\n",
      "        [ 2.1103,  1.2163,  1.0505,  1.5125,  0.9238,  1.8297,  1.6917,  2.1294],\n",
      "        [ 1.9295,  1.9381,  1.5973,  1.9572,  1.1808,  1.3156,  1.1710,  1.7211],\n",
      "        [-0.0094,  0.8695,  1.1849,  2.1477,  1.4527,  1.0862,  1.4462,  0.9159],\n",
      "        [-0.0937,  1.7886,  0.8654,  0.9999,  0.6808,  2.3496,  0.6993,  1.0552],\n",
      "        [ 0.8492,  0.9722,  1.3335,  1.5553,  1.0551,  1.8859,  0.8969,  0.9345],\n",
      "        [-0.0200,  1.3128,  1.2637,  1.6296,  1.2468,  2.2110,  0.7224,  1.9815]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.2619,  2.0430,  1.7515, -0.3663,  1.6411,  0.7116,  1.2842, -0.1276],\n",
      "        [ 0.9379,  2.1234,  1.1126, -0.0327,  0.9495,  1.0311,  0.9298,  1.6828],\n",
      "        [ 2.1202,  1.2268,  1.0613,  1.5227,  0.9350,  1.8397,  1.7019,  2.1393],\n",
      "        [ 1.9395,  1.9481,  1.6074,  1.9672,  1.1914,  1.3261,  1.1816,  1.7312],\n",
      "        [-0.0220,  0.8808,  1.1955,  2.1575,  1.4630,  1.0969,  1.4565,  0.9271],\n",
      "        [-0.1058,  1.7986,  0.8768,  1.0109,  0.6931,  2.3595,  0.7115,  1.0660],\n",
      "        [ 0.8606,  0.9832,  1.3439,  1.5655,  1.0659,  1.8959,  0.9082,  0.9456],\n",
      "        [-0.0325,  1.3232,  1.2742,  1.6397,  1.2573,  2.2209,  0.7345,  1.9914]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.2619,  2.0430,  1.7515, -0.3663,  1.6411,  0.7116,  1.2842, -0.1276],\n",
      "        [ 0.9379,  2.1234,  1.1126, -0.0327,  0.9495,  1.0311,  0.9298,  1.6828],\n",
      "        [ 2.1202,  1.2268,  1.0613,  1.5227,  0.9350,  1.8397,  1.7019,  2.1393],\n",
      "        [ 1.9395,  1.9481,  1.6074,  1.9672,  1.1914,  1.3261,  1.1816,  1.7312],\n",
      "        [-0.0220,  0.8808,  1.1955,  2.1575,  1.4630,  1.0969,  1.4565,  0.9271],\n",
      "        [-0.1058,  1.7986,  0.8768,  1.0109,  0.6931,  2.3595,  0.7115,  1.0660],\n",
      "        [ 0.8606,  0.9832,  1.3439,  1.5655,  1.0659,  1.8959,  0.9082,  0.9456],\n",
      "        [-0.0325,  1.3232,  1.2742,  1.6397,  1.2573,  2.2209,  0.7345,  1.9914]],\n",
      "       requires_grad=True)\n",
      "epoch:  30 ; loss:  1.4403722286224365 ; mask density:  0.001369296107441187 ; pred:  tensor([5.7723e-03, 3.8546e-02, 6.3186e-04, 9.2749e-01, 1.1357e-03, 2.5518e-02,\n",
      "        9.0735e-04], grad_fn=<SoftmaxBackward0>) ; labels equal:  tensor(True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.2717,  2.0530,  1.7615, -0.3774,  1.6513,  0.7239,  1.2947, -0.1397],\n",
      "        [ 0.9490,  2.1333,  1.1233, -0.0454,  0.9607,  1.0421,  0.9410,  1.6929],\n",
      "        [ 2.1301,  1.2374,  1.0722,  1.5330,  0.9462,  1.8497,  1.7120,  2.1491],\n",
      "        [ 1.9495,  1.9581,  1.6176,  1.9771,  1.2021,  1.3365,  1.1923,  1.7413],\n",
      "        [-0.0347,  0.8922,  1.2061,  2.1674,  1.4733,  1.1077,  1.4668,  0.9383],\n",
      "        [-0.1180,  1.8086,  0.8882,  1.0218,  0.7056,  2.3693,  0.7238,  1.0768],\n",
      "        [ 0.8720,  0.9943,  1.3544,  1.5757,  1.0768,  1.9059,  0.9194,  0.9568],\n",
      "        [-0.0452,  1.3337,  1.2847,  1.6499,  1.2678,  2.2307,  0.7467,  2.0014]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.2717,  2.0530,  1.7615, -0.3774,  1.6513,  0.7239,  1.2947, -0.1397],\n",
      "        [ 0.9490,  2.1333,  1.1233, -0.0454,  0.9607,  1.0421,  0.9410,  1.6929],\n",
      "        [ 2.1301,  1.2374,  1.0722,  1.5330,  0.9462,  1.8497,  1.7120,  2.1491],\n",
      "        [ 1.9495,  1.9581,  1.6176,  1.9771,  1.2021,  1.3365,  1.1923,  1.7413],\n",
      "        [-0.0347,  0.8922,  1.2061,  2.1674,  1.4733,  1.1077,  1.4668,  0.9383],\n",
      "        [-0.1180,  1.8086,  0.8882,  1.0218,  0.7056,  2.3693,  0.7238,  1.0768],\n",
      "        [ 0.8720,  0.9943,  1.3544,  1.5757,  1.0768,  1.9059,  0.9194,  0.9568],\n",
      "        [-0.0452,  1.3337,  1.2847,  1.6499,  1.2678,  2.2307,  0.7467,  2.0014]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.2815,  2.0629,  1.7716, -0.3887,  1.6614,  0.7363,  1.3052, -0.1518],\n",
      "        [ 0.9603,  2.1431,  1.1341, -0.0582,  0.9719,  1.0530,  0.9523,  1.7031],\n",
      "        [ 2.1400,  1.2480,  1.0831,  1.5432,  0.9574,  1.8597,  1.7221,  2.1590],\n",
      "        [ 1.9594,  1.9680,  1.6278,  1.9871,  1.2127,  1.3469,  1.2030,  1.7513],\n",
      "        [-0.0476,  0.9036,  1.2168,  2.1773,  1.4836,  1.1185,  1.4771,  0.9496],\n",
      "        [-0.1303,  1.8187,  0.8996,  1.0329,  0.7181,  2.3790,  0.7362,  1.0877],\n",
      "        [ 0.8836,  1.0054,  1.3648,  1.5859,  1.0877,  1.9158,  0.9308,  0.9680],\n",
      "        [-0.0580,  1.3441,  1.2952,  1.6600,  1.2783,  2.2406,  0.7589,  2.0113]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.2815,  2.0629,  1.7716, -0.3887,  1.6614,  0.7363,  1.3052, -0.1518],\n",
      "        [ 0.9603,  2.1431,  1.1341, -0.0582,  0.9719,  1.0530,  0.9523,  1.7031],\n",
      "        [ 2.1400,  1.2480,  1.0831,  1.5432,  0.9574,  1.8597,  1.7221,  2.1590],\n",
      "        [ 1.9594,  1.9680,  1.6278,  1.9871,  1.2127,  1.3469,  1.2030,  1.7513],\n",
      "        [-0.0476,  0.9036,  1.2168,  2.1773,  1.4836,  1.1185,  1.4771,  0.9496],\n",
      "        [-0.1303,  1.8187,  0.8996,  1.0329,  0.7181,  2.3790,  0.7362,  1.0877],\n",
      "        [ 0.8836,  1.0054,  1.3648,  1.5859,  1.0877,  1.9158,  0.9308,  0.9680],\n",
      "        [-0.0580,  1.3441,  1.2952,  1.6600,  1.2783,  2.2406,  0.7589,  2.0113]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.2914,  2.0728,  1.7817, -0.4000,  1.6716,  0.7488,  1.3158, -0.1641],\n",
      "        [ 0.9716,  2.1530,  1.1450, -0.0710,  0.9831,  1.0640,  0.9636,  1.7132],\n",
      "        [ 2.1498,  1.2586,  1.0940,  1.5535,  0.9687,  1.8697,  1.7322,  2.1689],\n",
      "        [ 1.9694,  1.9780,  1.6380,  1.9970,  1.2234,  1.3574,  1.2136,  1.7614],\n",
      "        [-0.0605,  0.9151,  1.2274,  2.1871,  1.4939,  1.1294,  1.4874,  0.9609],\n",
      "        [-0.1427,  1.8287,  0.9111,  1.0439,  0.7307,  2.3888,  0.7487,  1.0986],\n",
      "        [ 0.8952,  1.0165,  1.3753,  1.5961,  1.0986,  1.9258,  0.9422,  0.9793],\n",
      "        [-0.0708,  1.3546,  1.3057,  1.6702,  1.2889,  2.2504,  0.7712,  2.0212]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.2914,  2.0728,  1.7817, -0.4000,  1.6716,  0.7488,  1.3158, -0.1641],\n",
      "        [ 0.9716,  2.1530,  1.1450, -0.0710,  0.9831,  1.0640,  0.9636,  1.7132],\n",
      "        [ 2.1498,  1.2586,  1.0940,  1.5535,  0.9687,  1.8697,  1.7322,  2.1689],\n",
      "        [ 1.9694,  1.9780,  1.6380,  1.9970,  1.2234,  1.3574,  1.2136,  1.7614],\n",
      "        [-0.0605,  0.9151,  1.2274,  2.1871,  1.4939,  1.1294,  1.4874,  0.9609],\n",
      "        [-0.1427,  1.8287,  0.9111,  1.0439,  0.7307,  2.3888,  0.7487,  1.0986],\n",
      "        [ 0.8952,  1.0165,  1.3753,  1.5961,  1.0986,  1.9258,  0.9422,  0.9793],\n",
      "        [-0.0708,  1.3546,  1.3057,  1.6702,  1.2889,  2.2504,  0.7712,  2.0212]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.3012,  2.0827,  1.7918, -0.4113,  1.6817,  0.7613,  1.3263, -0.1764],\n",
      "        [ 0.9829,  2.1629,  1.1558, -0.0840,  0.9944,  1.0751,  0.9750,  1.7233],\n",
      "        [ 2.1597,  1.2693,  1.1050,  1.5638,  0.9800,  1.8798,  1.7423,  2.1787],\n",
      "        [ 1.9793,  1.9879,  1.6482,  2.0069,  1.2341,  1.3679,  1.2244,  1.7715],\n",
      "        [-0.0735,  0.9267,  1.2381,  2.1970,  1.5042,  1.1403,  1.4978,  0.9722],\n",
      "        [-0.1551,  1.8388,  0.9227,  1.0550,  0.7434,  2.3986,  0.7613,  1.1096],\n",
      "        [ 0.9068,  1.0277,  1.3857,  1.6063,  1.1095,  1.9358,  0.9536,  0.9906],\n",
      "        [-0.0838,  1.3651,  1.3163,  1.6804,  1.2995,  2.2602,  0.7836,  2.0312]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.3012,  2.0827,  1.7918, -0.4113,  1.6817,  0.7613,  1.3263, -0.1764],\n",
      "        [ 0.9829,  2.1629,  1.1558, -0.0840,  0.9944,  1.0751,  0.9750,  1.7233],\n",
      "        [ 2.1597,  1.2693,  1.1050,  1.5638,  0.9800,  1.8798,  1.7423,  2.1787],\n",
      "        [ 1.9793,  1.9879,  1.6482,  2.0069,  1.2341,  1.3679,  1.2244,  1.7715],\n",
      "        [-0.0735,  0.9267,  1.2381,  2.1970,  1.5042,  1.1403,  1.4978,  0.9722],\n",
      "        [-0.1551,  1.8388,  0.9227,  1.0550,  0.7434,  2.3986,  0.7613,  1.1096],\n",
      "        [ 0.9068,  1.0277,  1.3857,  1.6063,  1.1095,  1.9358,  0.9536,  0.9906],\n",
      "        [-0.0838,  1.3651,  1.3163,  1.6804,  1.2995,  2.2602,  0.7836,  2.0312]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.3109,  2.0926,  1.8018, -0.4227,  1.6919,  0.7739,  1.3369, -0.1888],\n",
      "        [ 0.9943,  2.1727,  1.1667, -0.0970,  1.0058,  1.0862,  0.9864,  1.7334],\n",
      "        [ 2.1695,  1.2799,  1.1160,  1.5740,  0.9914,  1.8898,  1.7524,  2.1886],\n",
      "        [ 1.9893,  1.9979,  1.6583,  2.0169,  1.2448,  1.3784,  1.2351,  1.7816],\n",
      "        [-0.0866,  0.9383,  1.2488,  2.2068,  1.5146,  1.1512,  1.5081,  0.9837],\n",
      "        [-0.1677,  1.8488,  0.9343,  1.0662,  0.7562,  2.4083,  0.7739,  1.1206],\n",
      "        [ 0.9185,  1.0389,  1.3962,  1.6166,  1.1205,  1.9458,  0.9651,  1.0019],\n",
      "        [-0.0968,  1.3756,  1.3269,  1.6905,  1.3101,  2.2700,  0.7960,  2.0411]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.3109,  2.0926,  1.8018, -0.4227,  1.6919,  0.7739,  1.3369, -0.1888],\n",
      "        [ 0.9943,  2.1727,  1.1667, -0.0970,  1.0058,  1.0862,  0.9864,  1.7334],\n",
      "        [ 2.1695,  1.2799,  1.1160,  1.5740,  0.9914,  1.8898,  1.7524,  2.1886],\n",
      "        [ 1.9893,  1.9979,  1.6583,  2.0169,  1.2448,  1.3784,  1.2351,  1.7816],\n",
      "        [-0.0866,  0.9383,  1.2488,  2.2068,  1.5146,  1.1512,  1.5081,  0.9837],\n",
      "        [-0.1677,  1.8488,  0.9343,  1.0662,  0.7562,  2.4083,  0.7739,  1.1206],\n",
      "        [ 0.9185,  1.0389,  1.3962,  1.6166,  1.1205,  1.9458,  0.9651,  1.0019],\n",
      "        [-0.0968,  1.3756,  1.3269,  1.6905,  1.3101,  2.2700,  0.7960,  2.0411]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.3207,  2.1024,  1.8119, -0.4341,  1.7021,  0.7866,  1.3475, -0.2013],\n",
      "        [ 1.0057,  2.1826,  1.1776, -0.1102,  1.0171,  1.0973,  0.9978,  1.7436],\n",
      "        [ 2.1794,  1.2906,  1.1270,  1.5843,  1.0029,  1.8998,  1.7625,  2.1984],\n",
      "        [ 1.9992,  2.0078,  1.6685,  2.0268,  1.2556,  1.3889,  1.2459,  1.7917],\n",
      "        [-0.0998,  0.9500,  1.2596,  2.2166,  1.5249,  1.1621,  1.5185,  0.9951],\n",
      "        [-0.1803,  1.8589,  0.9460,  1.0773,  0.7690,  2.4181,  0.7866,  1.1316],\n",
      "        [ 0.9302,  1.0502,  1.4067,  1.6268,  1.1316,  1.9558,  0.9766,  1.0133],\n",
      "        [-0.1100,  1.3861,  1.3375,  1.7007,  1.3207,  2.2798,  0.8086,  2.0510]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.3207,  2.1024,  1.8119, -0.4341,  1.7021,  0.7866,  1.3475, -0.2013],\n",
      "        [ 1.0057,  2.1826,  1.1776, -0.1102,  1.0171,  1.0973,  0.9978,  1.7436],\n",
      "        [ 2.1794,  1.2906,  1.1270,  1.5843,  1.0029,  1.8998,  1.7625,  2.1984],\n",
      "        [ 1.9992,  2.0078,  1.6685,  2.0268,  1.2556,  1.3889,  1.2459,  1.7917],\n",
      "        [-0.0998,  0.9500,  1.2596,  2.2166,  1.5249,  1.1621,  1.5185,  0.9951],\n",
      "        [-0.1803,  1.8589,  0.9460,  1.0773,  0.7690,  2.4181,  0.7866,  1.1316],\n",
      "        [ 0.9302,  1.0502,  1.4067,  1.6268,  1.1316,  1.9558,  0.9766,  1.0133],\n",
      "        [-0.1100,  1.3861,  1.3375,  1.7007,  1.3207,  2.2798,  0.8086,  2.0510]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.3305,  2.1123,  1.8220, -0.4456,  1.7122,  0.7994,  1.3581, -0.2139],\n",
      "        [ 1.0172,  2.1924,  1.1886, -0.1234,  1.0286,  1.1084,  1.0093,  1.7537],\n",
      "        [ 2.1892,  1.3013,  1.1381,  1.5946,  1.0143,  1.9098,  1.7726,  2.2082],\n",
      "        [ 2.0092,  2.0177,  1.6787,  2.0367,  1.2663,  1.3995,  1.2567,  1.8018],\n",
      "        [-0.1131,  0.9617,  1.2703,  2.2264,  1.5353,  1.1731,  1.5288,  1.0066],\n",
      "        [-0.1930,  1.8689,  0.9577,  1.0886,  0.7820,  2.4278,  0.7994,  1.1427],\n",
      "        [ 0.9421,  1.0615,  1.4172,  1.6370,  1.1426,  1.9657,  0.9882,  1.0247],\n",
      "        [-0.1232,  1.3967,  1.3481,  1.7109,  1.3314,  2.2896,  0.8212,  2.0609]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.3305,  2.1123,  1.8220, -0.4456,  1.7122,  0.7994,  1.3581, -0.2139],\n",
      "        [ 1.0172,  2.1924,  1.1886, -0.1234,  1.0286,  1.1084,  1.0093,  1.7537],\n",
      "        [ 2.1892,  1.3013,  1.1381,  1.5946,  1.0143,  1.9098,  1.7726,  2.2082],\n",
      "        [ 2.0092,  2.0177,  1.6787,  2.0367,  1.2663,  1.3995,  1.2567,  1.8018],\n",
      "        [-0.1131,  0.9617,  1.2703,  2.2264,  1.5353,  1.1731,  1.5288,  1.0066],\n",
      "        [-0.1930,  1.8689,  0.9577,  1.0886,  0.7820,  2.4278,  0.7994,  1.1427],\n",
      "        [ 0.9421,  1.0615,  1.4172,  1.6370,  1.1426,  1.9657,  0.9882,  1.0247],\n",
      "        [-0.1232,  1.3967,  1.3481,  1.7109,  1.3314,  2.2896,  0.8212,  2.0609]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.3402,  2.1222,  1.8320, -0.4572,  1.7224,  0.8123,  1.3687, -0.2265],\n",
      "        [ 1.0287,  2.2022,  1.1995, -0.1367,  1.0400,  1.1196,  1.0209,  1.7638],\n",
      "        [ 2.1990,  1.3120,  1.1492,  1.6049,  1.0259,  1.9198,  1.7827,  2.2180],\n",
      "        [ 2.0191,  2.0277,  1.6889,  2.0466,  1.2771,  1.4100,  1.2675,  1.8119],\n",
      "        [-0.1265,  0.9734,  1.2811,  2.2362,  1.5456,  1.1841,  1.5392,  1.0182],\n",
      "        [-0.2058,  1.8789,  0.9695,  1.0998,  0.7950,  2.4375,  0.8122,  1.1538],\n",
      "        [ 0.9539,  1.0728,  1.4277,  1.6473,  1.1537,  1.9757,  0.9999,  1.0362],\n",
      "        [-0.1365,  1.4072,  1.3588,  1.7210,  1.3421,  2.2994,  0.8338,  2.0708]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.3402,  2.1222,  1.8320, -0.4572,  1.7224,  0.8123,  1.3687, -0.2265],\n",
      "        [ 1.0287,  2.2022,  1.1995, -0.1367,  1.0400,  1.1196,  1.0209,  1.7638],\n",
      "        [ 2.1990,  1.3120,  1.1492,  1.6049,  1.0259,  1.9198,  1.7827,  2.2180],\n",
      "        [ 2.0191,  2.0277,  1.6889,  2.0466,  1.2771,  1.4100,  1.2675,  1.8119],\n",
      "        [-0.1265,  0.9734,  1.2811,  2.2362,  1.5456,  1.1841,  1.5392,  1.0182],\n",
      "        [-0.2058,  1.8789,  0.9695,  1.0998,  0.7950,  2.4375,  0.8122,  1.1538],\n",
      "        [ 0.9539,  1.0728,  1.4277,  1.6473,  1.1537,  1.9757,  0.9999,  1.0362],\n",
      "        [-0.1365,  1.4072,  1.3588,  1.7210,  1.3421,  2.2994,  0.8338,  2.0708]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.3499,  2.1320,  1.8421, -0.4688,  1.7326,  0.8252,  1.3793, -0.2392],\n",
      "        [ 1.0402,  2.2120,  1.2105, -0.1501,  1.0515,  1.1309,  1.0325,  1.7740],\n",
      "        [ 2.2088,  1.3228,  1.1604,  1.6152,  1.0374,  1.9298,  1.7929,  2.2278],\n",
      "        [ 2.0290,  2.0376,  1.6992,  2.0565,  1.2880,  1.4206,  1.2783,  1.8220],\n",
      "        [-0.1400,  0.9852,  1.2919,  2.2460,  1.5560,  1.1952,  1.5496,  1.0298],\n",
      "        [-0.2187,  1.8890,  0.9813,  1.1111,  0.8080,  2.4472,  0.8251,  1.1649],\n",
      "        [ 0.9658,  1.0842,  1.4383,  1.6575,  1.1648,  1.9856,  1.0115,  1.0478],\n",
      "        [-0.1499,  1.4178,  1.3694,  1.7312,  1.3528,  2.3091,  0.8466,  2.0807]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.3499,  2.1320,  1.8421, -0.4688,  1.7326,  0.8252,  1.3793, -0.2392],\n",
      "        [ 1.0402,  2.2120,  1.2105, -0.1501,  1.0515,  1.1309,  1.0325,  1.7740],\n",
      "        [ 2.2088,  1.3228,  1.1604,  1.6152,  1.0374,  1.9298,  1.7929,  2.2278],\n",
      "        [ 2.0290,  2.0376,  1.6992,  2.0565,  1.2880,  1.4206,  1.2783,  1.8220],\n",
      "        [-0.1400,  0.9852,  1.2919,  2.2460,  1.5560,  1.1952,  1.5496,  1.0298],\n",
      "        [-0.2187,  1.8890,  0.9813,  1.1111,  0.8080,  2.4472,  0.8251,  1.1649],\n",
      "        [ 0.9658,  1.0842,  1.4383,  1.6575,  1.1648,  1.9856,  1.0115,  1.0478],\n",
      "        [-0.1499,  1.4178,  1.3694,  1.7312,  1.3528,  2.3091,  0.8466,  2.0807]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.3597,  2.1419,  1.8522, -0.4804,  1.7428,  0.8382,  1.3900, -0.2520],\n",
      "        [ 1.0518,  2.2218,  1.2216, -0.1636,  1.0631,  1.1421,  1.0441,  1.7841],\n",
      "        [ 2.2186,  1.3336,  1.1715,  1.6255,  1.0491,  1.9398,  1.8030,  2.2376],\n",
      "        [ 2.0389,  2.0475,  1.7094,  2.0664,  1.2988,  1.4312,  1.2892,  1.8321],\n",
      "        [-0.1535,  0.9971,  1.3028,  2.2558,  1.5664,  1.2062,  1.5600,  1.0414],\n",
      "        [-0.2316,  1.8990,  0.9932,  1.1224,  0.8212,  2.4568,  0.8381,  1.1760],\n",
      "        [ 0.9778,  1.0957,  1.4488,  1.6678,  1.1760,  1.9956,  1.0233,  1.0593],\n",
      "        [-0.1634,  1.4284,  1.3801,  1.7414,  1.3635,  2.3188,  0.8594,  2.0906]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.3597,  2.1419,  1.8522, -0.4804,  1.7428,  0.8382,  1.3900, -0.2520],\n",
      "        [ 1.0518,  2.2218,  1.2216, -0.1636,  1.0631,  1.1421,  1.0441,  1.7841],\n",
      "        [ 2.2186,  1.3336,  1.1715,  1.6255,  1.0491,  1.9398,  1.8030,  2.2376],\n",
      "        [ 2.0389,  2.0475,  1.7094,  2.0664,  1.2988,  1.4312,  1.2892,  1.8321],\n",
      "        [-0.1535,  0.9971,  1.3028,  2.2558,  1.5664,  1.2062,  1.5600,  1.0414],\n",
      "        [-0.2316,  1.8990,  0.9932,  1.1224,  0.8212,  2.4568,  0.8381,  1.1760],\n",
      "        [ 0.9778,  1.0957,  1.4488,  1.6678,  1.1760,  1.9956,  1.0233,  1.0593],\n",
      "        [-0.1634,  1.4284,  1.3801,  1.7414,  1.3635,  2.3188,  0.8594,  2.0906]],\n",
      "       requires_grad=True)\n",
      "epoch:  40 ; loss:  1.387184977531433 ; mask density:  0.00139088265132159 ; pred:  tensor([2.4111e-04, 1.1577e-04, 4.9075e-05, 9.9894e-01, 5.7092e-04, 7.1045e-05,\n",
      "        1.5470e-05], grad_fn=<SoftmaxBackward0>) ; labels equal:  tensor(True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.3694,  2.1517,  1.8622, -0.4921,  1.7529,  0.8512,  1.4007, -0.2649],\n",
      "        [ 1.0635,  2.2316,  1.2326, -0.1771,  1.0747,  1.1534,  1.0558,  1.7942],\n",
      "        [ 2.2284,  1.3444,  1.1827,  1.6358,  1.0607,  1.9498,  1.8131,  2.2474],\n",
      "        [ 2.0488,  2.0574,  1.7196,  2.0763,  1.3097,  1.4418,  1.3001,  1.8421],\n",
      "        [-0.1672,  1.0090,  1.3136,  2.2656,  1.5768,  1.2173,  1.5704,  1.0531],\n",
      "        [-0.2446,  1.9090,  1.0051,  1.1338,  0.8344,  2.4665,  0.8512,  1.1872],\n",
      "        [ 0.9898,  1.1071,  1.4594,  1.6780,  1.1872,  2.0055,  1.0351,  1.0709],\n",
      "        [-0.1770,  1.4390,  1.3908,  1.7516,  1.3742,  2.3286,  0.8722,  2.1004]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.3694,  2.1517,  1.8622, -0.4921,  1.7529,  0.8512,  1.4007, -0.2649],\n",
      "        [ 1.0635,  2.2316,  1.2326, -0.1771,  1.0747,  1.1534,  1.0558,  1.7942],\n",
      "        [ 2.2284,  1.3444,  1.1827,  1.6358,  1.0607,  1.9498,  1.8131,  2.2474],\n",
      "        [ 2.0488,  2.0574,  1.7196,  2.0763,  1.3097,  1.4418,  1.3001,  1.8421],\n",
      "        [-0.1672,  1.0090,  1.3136,  2.2656,  1.5768,  1.2173,  1.5704,  1.0531],\n",
      "        [-0.2446,  1.9090,  1.0051,  1.1338,  0.8344,  2.4665,  0.8512,  1.1872],\n",
      "        [ 0.9898,  1.1071,  1.4594,  1.6780,  1.1872,  2.0055,  1.0351,  1.0709],\n",
      "        [-0.1770,  1.4390,  1.3908,  1.7516,  1.3742,  2.3286,  0.8722,  2.1004]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.3791,  2.1615,  1.8723, -0.5038,  1.7631,  0.8643,  1.4114, -0.2778],\n",
      "        [ 1.0752,  2.2414,  1.2437, -0.1908,  1.0863,  1.1648,  1.0675,  1.8043],\n",
      "        [ 2.2382,  1.3552,  1.1940,  1.6461,  1.0724,  1.9597,  1.8232,  2.2571],\n",
      "        [ 2.0587,  2.0673,  1.7298,  2.0862,  1.3206,  1.4524,  1.3110,  1.8522],\n",
      "        [-0.1809,  1.0210,  1.3245,  2.2753,  1.5872,  1.2285,  1.5808,  1.0649],\n",
      "        [-0.2577,  1.9190,  1.0171,  1.1452,  0.8476,  2.4761,  0.8643,  1.1985],\n",
      "        [ 1.0019,  1.1186,  1.4700,  1.6883,  1.1984,  2.0155,  1.0469,  1.0826],\n",
      "        [-0.1906,  1.4496,  1.4015,  1.7617,  1.3849,  2.3383,  0.8851,  2.1103]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.3791,  2.1615,  1.8723, -0.5038,  1.7631,  0.8643,  1.4114, -0.2778],\n",
      "        [ 1.0752,  2.2414,  1.2437, -0.1908,  1.0863,  1.1648,  1.0675,  1.8043],\n",
      "        [ 2.2382,  1.3552,  1.1940,  1.6461,  1.0724,  1.9597,  1.8232,  2.2571],\n",
      "        [ 2.0587,  2.0673,  1.7298,  2.0862,  1.3206,  1.4524,  1.3110,  1.8522],\n",
      "        [-0.1809,  1.0210,  1.3245,  2.2753,  1.5872,  1.2285,  1.5808,  1.0649],\n",
      "        [-0.2577,  1.9190,  1.0171,  1.1452,  0.8476,  2.4761,  0.8643,  1.1985],\n",
      "        [ 1.0019,  1.1186,  1.4700,  1.6883,  1.1984,  2.0155,  1.0469,  1.0826],\n",
      "        [-0.1906,  1.4496,  1.4015,  1.7617,  1.3849,  2.3383,  0.8851,  2.1103]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.3887,  2.1714,  1.8824, -0.5156,  1.7733,  0.8775,  1.4221, -0.2908],\n",
      "        [ 1.0869,  2.2511,  1.2548, -0.2045,  1.0980,  1.1761,  1.0793,  1.8145],\n",
      "        [ 2.2479,  1.3660,  1.2052,  1.6564,  1.0841,  1.9697,  1.8333,  2.2669],\n",
      "        [ 2.0686,  2.0772,  1.7400,  2.0961,  1.3315,  1.4630,  1.3219,  1.8623],\n",
      "        [-0.1947,  1.0330,  1.3354,  2.2850,  1.5976,  1.2396,  1.5912,  1.0766],\n",
      "        [-0.2709,  1.9290,  1.0291,  1.1566,  0.8610,  2.4857,  0.8775,  1.2097],\n",
      "        [ 1.0140,  1.1302,  1.4806,  1.6986,  1.2097,  2.0254,  1.0587,  1.0943],\n",
      "        [-0.2043,  1.4603,  1.4122,  1.7719,  1.3957,  2.3480,  0.8981,  2.1201]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.3887,  2.1714,  1.8824, -0.5156,  1.7733,  0.8775,  1.4221, -0.2908],\n",
      "        [ 1.0869,  2.2511,  1.2548, -0.2045,  1.0980,  1.1761,  1.0793,  1.8145],\n",
      "        [ 2.2479,  1.3660,  1.2052,  1.6564,  1.0841,  1.9697,  1.8333,  2.2669],\n",
      "        [ 2.0686,  2.0772,  1.7400,  2.0961,  1.3315,  1.4630,  1.3219,  1.8623],\n",
      "        [-0.1947,  1.0330,  1.3354,  2.2850,  1.5976,  1.2396,  1.5912,  1.0766],\n",
      "        [-0.2709,  1.9290,  1.0291,  1.1566,  0.8610,  2.4857,  0.8775,  1.2097],\n",
      "        [ 1.0140,  1.1302,  1.4806,  1.6986,  1.2097,  2.0254,  1.0587,  1.0943],\n",
      "        [-0.2043,  1.4603,  1.4122,  1.7719,  1.3957,  2.3480,  0.8981,  2.1201]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.3984,  2.1812,  1.8924, -0.5274,  1.7834,  0.8907,  1.4328, -0.3039],\n",
      "        [ 1.0986,  2.2609,  1.2659, -0.2183,  1.1097,  1.1875,  1.0911,  1.8246],\n",
      "        [ 2.2577,  1.3768,  1.2165,  1.6668,  1.0959,  1.9797,  1.8434,  2.2766],\n",
      "        [ 2.0785,  2.0870,  1.7502,  2.1059,  1.3424,  1.4737,  1.3329,  1.8724],\n",
      "        [-0.2085,  1.0450,  1.3463,  2.2948,  1.6080,  1.2508,  1.6016,  1.0884],\n",
      "        [-0.2841,  1.9390,  1.0412,  1.1681,  0.8743,  2.4953,  0.8907,  1.2210],\n",
      "        [ 1.0261,  1.1417,  1.4912,  1.7088,  1.2209,  2.0353,  1.0706,  1.1060],\n",
      "        [-0.2181,  1.4709,  1.4230,  1.7821,  1.4065,  2.3577,  0.9111,  2.1300]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.3984,  2.1812,  1.8924, -0.5274,  1.7834,  0.8907,  1.4328, -0.3039],\n",
      "        [ 1.0986,  2.2609,  1.2659, -0.2183,  1.1097,  1.1875,  1.0911,  1.8246],\n",
      "        [ 2.2577,  1.3768,  1.2165,  1.6668,  1.0959,  1.9797,  1.8434,  2.2766],\n",
      "        [ 2.0785,  2.0870,  1.7502,  2.1059,  1.3424,  1.4737,  1.3329,  1.8724],\n",
      "        [-0.2085,  1.0450,  1.3463,  2.2948,  1.6080,  1.2508,  1.6016,  1.0884],\n",
      "        [-0.2841,  1.9390,  1.0412,  1.1681,  0.8743,  2.4953,  0.8907,  1.2210],\n",
      "        [ 1.0261,  1.1417,  1.4912,  1.7088,  1.2209,  2.0353,  1.0706,  1.1060],\n",
      "        [-0.2181,  1.4709,  1.4230,  1.7821,  1.4065,  2.3577,  0.9111,  2.1300]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.4080,  2.1909,  1.9025, -0.5393,  1.7936,  0.9040,  1.4435, -0.3170],\n",
      "        [ 1.1104,  2.2706,  1.2771, -0.2321,  1.1215,  1.1989,  1.1029,  1.8347],\n",
      "        [ 2.2674,  1.3877,  1.2278,  1.6771,  1.1077,  1.9896,  1.8535,  2.2863],\n",
      "        [ 2.0884,  2.0969,  1.7604,  2.1158,  1.3533,  1.4843,  1.3438,  1.8824],\n",
      "        [-0.2225,  1.0571,  1.3573,  2.3045,  1.6184,  1.2620,  1.6120,  1.1003],\n",
      "        [-0.2974,  1.9490,  1.0533,  1.1796,  0.8878,  2.5049,  0.9040,  1.2323],\n",
      "        [ 1.0383,  1.1533,  1.5018,  1.7191,  1.2322,  2.0452,  1.0826,  1.1178],\n",
      "        [-0.2319,  1.4816,  1.4337,  1.7922,  1.4173,  2.3673,  0.9242,  2.1398]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.4080,  2.1909,  1.9025, -0.5393,  1.7936,  0.9040,  1.4435, -0.3170],\n",
      "        [ 1.1104,  2.2706,  1.2771, -0.2321,  1.1215,  1.1989,  1.1029,  1.8347],\n",
      "        [ 2.2674,  1.3877,  1.2278,  1.6771,  1.1077,  1.9896,  1.8535,  2.2863],\n",
      "        [ 2.0884,  2.0969,  1.7604,  2.1158,  1.3533,  1.4843,  1.3438,  1.8824],\n",
      "        [-0.2225,  1.0571,  1.3573,  2.3045,  1.6184,  1.2620,  1.6120,  1.1003],\n",
      "        [-0.2974,  1.9490,  1.0533,  1.1796,  0.8878,  2.5049,  0.9040,  1.2323],\n",
      "        [ 1.0383,  1.1533,  1.5018,  1.7191,  1.2322,  2.0452,  1.0826,  1.1178],\n",
      "        [-0.2319,  1.4816,  1.4337,  1.7922,  1.4173,  2.3673,  0.9242,  2.1398]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.4177,  2.2007,  1.9125, -0.5512,  1.8038,  0.9173,  1.4543, -0.3302],\n",
      "        [ 1.1223,  2.2803,  1.2883, -0.2460,  1.1332,  1.2104,  1.1148,  1.8448],\n",
      "        [ 2.2771,  1.3986,  1.2392,  1.6874,  1.1196,  1.9996,  1.8636,  2.2960],\n",
      "        [ 2.0982,  2.1068,  1.7706,  2.1256,  1.3643,  1.4950,  1.3548,  1.8925],\n",
      "        [-0.2365,  1.0692,  1.3682,  2.3142,  1.6288,  1.2732,  1.6224,  1.1122],\n",
      "        [-0.3107,  1.9590,  1.0655,  1.1911,  0.9013,  2.5145,  0.9173,  1.2436],\n",
      "        [ 1.0506,  1.1650,  1.5124,  1.7294,  1.2436,  2.0551,  1.0946,  1.1296],\n",
      "        [-0.2459,  1.4922,  1.4445,  1.8024,  1.4281,  2.3770,  0.9374,  2.1496]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.4177,  2.2007,  1.9125, -0.5512,  1.8038,  0.9173,  1.4543, -0.3302],\n",
      "        [ 1.1223,  2.2803,  1.2883, -0.2460,  1.1332,  1.2104,  1.1148,  1.8448],\n",
      "        [ 2.2771,  1.3986,  1.2392,  1.6874,  1.1196,  1.9996,  1.8636,  2.2960],\n",
      "        [ 2.0982,  2.1068,  1.7706,  2.1256,  1.3643,  1.4950,  1.3548,  1.8925],\n",
      "        [-0.2365,  1.0692,  1.3682,  2.3142,  1.6288,  1.2732,  1.6224,  1.1122],\n",
      "        [-0.3107,  1.9590,  1.0655,  1.1911,  0.9013,  2.5145,  0.9173,  1.2436],\n",
      "        [ 1.0506,  1.1650,  1.5124,  1.7294,  1.2436,  2.0551,  1.0946,  1.1296],\n",
      "        [-0.2459,  1.4922,  1.4445,  1.8024,  1.4281,  2.3770,  0.9374,  2.1496]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.4273,  2.2105,  1.9225, -0.5632,  1.8140,  0.9307,  1.4650, -0.3434],\n",
      "        [ 1.1342,  2.2900,  1.2995, -0.2600,  1.1451,  1.2219,  1.1267,  1.8550],\n",
      "        [ 2.2869,  1.4095,  1.2506,  1.6978,  1.1315,  2.0095,  1.8737,  2.3057],\n",
      "        [ 2.1081,  2.1166,  1.7808,  2.1354,  1.3753,  1.5057,  1.3658,  1.9026],\n",
      "        [-0.2505,  1.0814,  1.3792,  2.3239,  1.6392,  1.2845,  1.6329,  1.1241],\n",
      "        [-0.3242,  1.9690,  1.0776,  1.2027,  0.9148,  2.5241,  0.9307,  1.2550],\n",
      "        [ 1.0628,  1.1766,  1.5231,  1.7396,  1.2549,  2.0650,  1.1066,  1.1414],\n",
      "        [-0.2598,  1.5029,  1.4553,  1.8126,  1.4389,  2.3866,  0.9505,  2.1594]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.4273,  2.2105,  1.9225, -0.5632,  1.8140,  0.9307,  1.4650, -0.3434],\n",
      "        [ 1.1342,  2.2900,  1.2995, -0.2600,  1.1451,  1.2219,  1.1267,  1.8550],\n",
      "        [ 2.2869,  1.4095,  1.2506,  1.6978,  1.1315,  2.0095,  1.8737,  2.3057],\n",
      "        [ 2.1081,  2.1166,  1.7808,  2.1354,  1.3753,  1.5057,  1.3658,  1.9026],\n",
      "        [-0.2505,  1.0814,  1.3792,  2.3239,  1.6392,  1.2845,  1.6329,  1.1241],\n",
      "        [-0.3242,  1.9690,  1.0776,  1.2027,  0.9148,  2.5241,  0.9307,  1.2550],\n",
      "        [ 1.0628,  1.1766,  1.5231,  1.7396,  1.2549,  2.0650,  1.1066,  1.1414],\n",
      "        [-0.2598,  1.5029,  1.4553,  1.8126,  1.4389,  2.3866,  0.9505,  2.1594]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.4369,  2.2202,  1.9326, -0.5752,  1.8241,  0.9442,  1.4758, -0.3568],\n",
      "        [ 1.1461,  2.2997,  1.3107, -0.2741,  1.1569,  1.2334,  1.1386,  1.8651],\n",
      "        [ 2.2965,  1.4204,  1.2620,  1.7081,  1.1434,  2.0195,  1.8838,  2.3154],\n",
      "        [ 2.1179,  2.1264,  1.7910,  2.1453,  1.3863,  1.5163,  1.3768,  1.9126],\n",
      "        [-0.2647,  1.0936,  1.3902,  2.3335,  1.6497,  1.2958,  1.6433,  1.1361],\n",
      "        [-0.3376,  1.9790,  1.0899,  1.2143,  0.9284,  2.5336,  0.9441,  1.2663],\n",
      "        [ 1.0752,  1.1883,  1.5337,  1.7499,  1.2663,  2.0749,  1.1186,  1.1533],\n",
      "        [-0.2739,  1.5136,  1.4661,  1.8228,  1.4497,  2.3962,  0.9638,  2.1692]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.4369,  2.2202,  1.9326, -0.5752,  1.8241,  0.9442,  1.4758, -0.3568],\n",
      "        [ 1.1461,  2.2997,  1.3107, -0.2741,  1.1569,  1.2334,  1.1386,  1.8651],\n",
      "        [ 2.2965,  1.4204,  1.2620,  1.7081,  1.1434,  2.0195,  1.8838,  2.3154],\n",
      "        [ 2.1179,  2.1264,  1.7910,  2.1453,  1.3863,  1.5163,  1.3768,  1.9126],\n",
      "        [-0.2647,  1.0936,  1.3902,  2.3335,  1.6497,  1.2958,  1.6433,  1.1361],\n",
      "        [-0.3376,  1.9790,  1.0899,  1.2143,  0.9284,  2.5336,  0.9441,  1.2663],\n",
      "        [ 1.0752,  1.1883,  1.5337,  1.7499,  1.2663,  2.0749,  1.1186,  1.1533],\n",
      "        [-0.2739,  1.5136,  1.4661,  1.8228,  1.4497,  2.3962,  0.9638,  2.1692]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.4464,  2.2300,  1.9426, -0.5872,  1.8343,  0.9576,  1.4866, -0.3701],\n",
      "        [ 1.1580,  2.3094,  1.3220, -0.2882,  1.1688,  1.2449,  1.1506,  1.8752],\n",
      "        [ 2.3062,  1.4313,  1.2734,  1.7184,  1.1553,  2.0294,  1.8939,  2.3251],\n",
      "        [ 2.1278,  2.1363,  1.8013,  2.1551,  1.3973,  1.5270,  1.3879,  1.9227],\n",
      "        [-0.2789,  1.1058,  1.4012,  2.3432,  1.6601,  1.3071,  1.6538,  1.1481],\n",
      "        [-0.3512,  1.9890,  1.1021,  1.2259,  0.9421,  2.5431,  0.9576,  1.2777],\n",
      "        [ 1.0875,  1.2001,  1.5444,  1.7602,  1.2777,  2.0848,  1.1307,  1.1652],\n",
      "        [-0.2880,  1.5243,  1.4769,  1.8329,  1.4606,  2.4059,  0.9771,  2.1790]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.4464,  2.2300,  1.9426, -0.5872,  1.8343,  0.9576,  1.4866, -0.3701],\n",
      "        [ 1.1580,  2.3094,  1.3220, -0.2882,  1.1688,  1.2449,  1.1506,  1.8752],\n",
      "        [ 2.3062,  1.4313,  1.2734,  1.7184,  1.1553,  2.0294,  1.8939,  2.3251],\n",
      "        [ 2.1278,  2.1363,  1.8013,  2.1551,  1.3973,  1.5270,  1.3879,  1.9227],\n",
      "        [-0.2789,  1.1058,  1.4012,  2.3432,  1.6601,  1.3071,  1.6538,  1.1481],\n",
      "        [-0.3512,  1.9890,  1.1021,  1.2259,  0.9421,  2.5431,  0.9576,  1.2777],\n",
      "        [ 1.0875,  1.2001,  1.5444,  1.7602,  1.2777,  2.0848,  1.1307,  1.1652],\n",
      "        [-0.2880,  1.5243,  1.4769,  1.8329,  1.4606,  2.4059,  0.9771,  2.1790]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.4560,  2.2397,  1.9526, -0.5993,  1.8444,  0.9712,  1.4974, -0.3835],\n",
      "        [ 1.1700,  2.3190,  1.3332, -0.3023,  1.1807,  1.2565,  1.1626,  1.8853],\n",
      "        [ 2.3159,  1.4422,  1.2848,  1.7287,  1.1673,  2.0393,  1.9039,  2.3347],\n",
      "        [ 2.1376,  2.1461,  1.8115,  2.1649,  1.4083,  1.5377,  1.3989,  1.9327],\n",
      "        [-0.2931,  1.1181,  1.4122,  2.3528,  1.6705,  1.3184,  1.6642,  1.1601],\n",
      "        [-0.3647,  1.9989,  1.1144,  1.2375,  0.9558,  2.5526,  0.9711,  1.2892],\n",
      "        [ 1.0999,  1.2118,  1.5550,  1.7704,  1.2891,  2.0946,  1.1428,  1.1771],\n",
      "        [-0.3022,  1.5350,  1.4877,  1.8431,  1.4714,  2.4154,  0.9904,  2.1888]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.4560,  2.2397,  1.9526, -0.5993,  1.8444,  0.9712,  1.4974, -0.3835],\n",
      "        [ 1.1700,  2.3190,  1.3332, -0.3023,  1.1807,  1.2565,  1.1626,  1.8853],\n",
      "        [ 2.3159,  1.4422,  1.2848,  1.7287,  1.1673,  2.0393,  1.9039,  2.3347],\n",
      "        [ 2.1376,  2.1461,  1.8115,  2.1649,  1.4083,  1.5377,  1.3989,  1.9327],\n",
      "        [-0.2931,  1.1181,  1.4122,  2.3528,  1.6705,  1.3184,  1.6642,  1.1601],\n",
      "        [-0.3647,  1.9989,  1.1144,  1.2375,  0.9558,  2.5526,  0.9711,  1.2892],\n",
      "        [ 1.0999,  1.2118,  1.5550,  1.7704,  1.2891,  2.0946,  1.1428,  1.1771],\n",
      "        [-0.3022,  1.5350,  1.4877,  1.8431,  1.4714,  2.4154,  0.9904,  2.1888]],\n",
      "       requires_grad=True)\n",
      "adj grad None\n",
      "x grad tensor([[ 5.3219e-03,  8.3039e-03,  3.2589e-01,  ..., -1.8961e-01,\n",
      "         -1.5268e-01, -2.7790e-01],\n",
      "        [ 6.5486e-04,  2.6171e-03,  1.5889e-02,  ...,  8.0785e-03,\n",
      "          2.8308e-03,  5.5990e-03],\n",
      "        [ 1.3034e-04,  3.9236e-05,  4.1168e-03,  ...,  1.8827e-03,\n",
      "          5.9122e-05,  1.2444e-03],\n",
      "        ...,\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00]])\n",
      "epoch:  50 ; loss:  1.3455979824066162 ; mask density:  0.0014111553318798542 ; pred:  tensor([4.5240e-03, 3.1098e-05, 3.9749e-03, 9.9102e-01, 3.4737e-04, 8.1964e-05,\n",
      "        1.6874e-05], grad_fn=<SoftmaxBackward0>) ; labels equal:  tensor(True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.4655,  2.2494,  1.9626, -0.6114,  1.8546,  0.9847,  1.5082, -0.3970],\n",
      "        [ 1.1820,  2.3287,  1.3445, -0.3166,  1.1926,  1.2680,  1.1746,  1.8954],\n",
      "        [ 2.3255,  1.4532,  1.2963,  1.7391,  1.1793,  2.0492,  1.9140,  2.3444],\n",
      "        [ 2.1474,  2.1559,  1.8217,  2.1746,  1.4193,  1.5484,  1.4100,  1.9427],\n",
      "        [-0.3074,  1.1304,  1.4232,  2.3624,  1.6810,  1.3297,  1.6746,  1.1721],\n",
      "        [-0.3784,  2.0089,  1.1268,  1.2492,  0.9695,  2.5621,  0.9847,  1.3006],\n",
      "        [ 1.1124,  1.2236,  1.5657,  1.7807,  1.3006,  2.1045,  1.1550,  1.1891],\n",
      "        [-0.3164,  1.5457,  1.4985,  1.8532,  1.4823,  2.4250,  1.0038,  2.1986]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.4655,  2.2494,  1.9626, -0.6114,  1.8546,  0.9847,  1.5082, -0.3970],\n",
      "        [ 1.1820,  2.3287,  1.3445, -0.3166,  1.1926,  1.2680,  1.1746,  1.8954],\n",
      "        [ 2.3255,  1.4532,  1.2963,  1.7391,  1.1793,  2.0492,  1.9140,  2.3444],\n",
      "        [ 2.1474,  2.1559,  1.8217,  2.1746,  1.4193,  1.5484,  1.4100,  1.9427],\n",
      "        [-0.3074,  1.1304,  1.4232,  2.3624,  1.6810,  1.3297,  1.6746,  1.1721],\n",
      "        [-0.3784,  2.0089,  1.1268,  1.2492,  0.9695,  2.5621,  0.9847,  1.3006],\n",
      "        [ 1.1124,  1.2236,  1.5657,  1.7807,  1.3006,  2.1045,  1.1550,  1.1891],\n",
      "        [-0.3164,  1.5457,  1.4985,  1.8532,  1.4823,  2.4250,  1.0038,  2.1986]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.4751,  2.2591,  1.9726, -0.6236,  1.8647,  0.9983,  1.5190, -0.4106],\n",
      "        [ 1.1940,  2.3383,  1.3558, -0.3309,  1.2046,  1.2796,  1.1867,  1.9055],\n",
      "        [ 2.3352,  1.4641,  1.3078,  1.7494,  1.1914,  2.0591,  1.9241,  2.3540],\n",
      "        [ 2.1572,  2.1657,  1.8319,  2.1844,  1.4304,  1.5592,  1.4211,  1.9528],\n",
      "        [-0.3218,  1.1427,  1.4343,  2.3720,  1.6914,  1.3411,  1.6851,  1.1842],\n",
      "        [-0.3921,  2.0188,  1.1391,  1.2609,  0.9833,  2.5715,  0.9983,  1.3121],\n",
      "        [ 1.1248,  1.2354,  1.5764,  1.7910,  1.3120,  2.1143,  1.1672,  1.2010],\n",
      "        [-0.3307,  1.5564,  1.5094,  1.8634,  1.4932,  2.4346,  1.0172,  2.2083]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.4751,  2.2591,  1.9726, -0.6236,  1.8647,  0.9983,  1.5190, -0.4106],\n",
      "        [ 1.1940,  2.3383,  1.3558, -0.3309,  1.2046,  1.2796,  1.1867,  1.9055],\n",
      "        [ 2.3352,  1.4641,  1.3078,  1.7494,  1.1914,  2.0591,  1.9241,  2.3540],\n",
      "        [ 2.1572,  2.1657,  1.8319,  2.1844,  1.4304,  1.5592,  1.4211,  1.9528],\n",
      "        [-0.3218,  1.1427,  1.4343,  2.3720,  1.6914,  1.3411,  1.6851,  1.1842],\n",
      "        [-0.3921,  2.0188,  1.1391,  1.2609,  0.9833,  2.5715,  0.9983,  1.3121],\n",
      "        [ 1.1248,  1.2354,  1.5764,  1.7910,  1.3120,  2.1143,  1.1672,  1.2010],\n",
      "        [-0.3307,  1.5564,  1.5094,  1.8634,  1.4932,  2.4346,  1.0172,  2.2083]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.4846,  2.2688,  1.9826, -0.6358,  1.8749,  1.0120,  1.5298, -0.4241],\n",
      "        [ 1.2060,  2.3479,  1.3671, -0.3452,  1.2166,  1.2913,  1.1988,  1.9156],\n",
      "        [ 2.3448,  1.4751,  1.3193,  1.7597,  1.2034,  2.0690,  1.9341,  2.3636],\n",
      "        [ 2.1670,  2.1754,  1.8421,  2.1942,  1.4415,  1.5699,  1.4322,  1.9628],\n",
      "        [-0.3362,  1.1551,  1.4453,  2.3816,  1.7018,  1.3524,  1.6955,  1.1963],\n",
      "        [-0.4058,  2.0288,  1.1515,  1.2726,  0.9971,  2.5810,  1.0119,  1.3236],\n",
      "        [ 1.1373,  1.2472,  1.5871,  1.8012,  1.3235,  2.1241,  1.1794,  1.2131],\n",
      "        [-0.3450,  1.5671,  1.5202,  1.8735,  1.5041,  2.4441,  1.0306,  2.2180]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.4846,  2.2688,  1.9826, -0.6358,  1.8749,  1.0120,  1.5298, -0.4241],\n",
      "        [ 1.2060,  2.3479,  1.3671, -0.3452,  1.2166,  1.2913,  1.1988,  1.9156],\n",
      "        [ 2.3448,  1.4751,  1.3193,  1.7597,  1.2034,  2.0690,  1.9341,  2.3636],\n",
      "        [ 2.1670,  2.1754,  1.8421,  2.1942,  1.4415,  1.5699,  1.4322,  1.9628],\n",
      "        [-0.3362,  1.1551,  1.4453,  2.3816,  1.7018,  1.3524,  1.6955,  1.1963],\n",
      "        [-0.4058,  2.0288,  1.1515,  1.2726,  0.9971,  2.5810,  1.0119,  1.3236],\n",
      "        [ 1.1373,  1.2472,  1.5871,  1.8012,  1.3235,  2.1241,  1.1794,  1.2131],\n",
      "        [-0.3450,  1.5671,  1.5202,  1.8735,  1.5041,  2.4441,  1.0306,  2.2180]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.4941,  2.2785,  1.9926, -0.6480,  1.8850,  1.0257,  1.5406, -0.4378],\n",
      "        [ 1.2181,  2.3575,  1.3785, -0.3596,  1.2286,  1.3029,  1.2109,  1.9256],\n",
      "        [ 2.3544,  1.4861,  1.3308,  1.7701,  1.2155,  2.0789,  1.9442,  2.3732],\n",
      "        [ 2.1768,  2.1852,  1.8523,  2.2039,  1.4526,  1.5806,  1.4433,  1.9728],\n",
      "        [-0.3507,  1.1675,  1.4564,  2.3912,  1.7123,  1.3638,  1.7060,  1.2085],\n",
      "        [-0.4196,  2.0387,  1.1639,  1.2843,  1.0109,  2.5904,  1.0256,  1.3351],\n",
      "        [ 1.1498,  1.2591,  1.5977,  1.8115,  1.3350,  2.1340,  1.1916,  1.2251],\n",
      "        [-0.3594,  1.5779,  1.5310,  1.8837,  1.5150,  2.4536,  1.0441,  2.2278]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.4941,  2.2785,  1.9926, -0.6480,  1.8850,  1.0257,  1.5406, -0.4378],\n",
      "        [ 1.2181,  2.3575,  1.3785, -0.3596,  1.2286,  1.3029,  1.2109,  1.9256],\n",
      "        [ 2.3544,  1.4861,  1.3308,  1.7701,  1.2155,  2.0789,  1.9442,  2.3732],\n",
      "        [ 2.1768,  2.1852,  1.8523,  2.2039,  1.4526,  1.5806,  1.4433,  1.9728],\n",
      "        [-0.3507,  1.1675,  1.4564,  2.3912,  1.7123,  1.3638,  1.7060,  1.2085],\n",
      "        [-0.4196,  2.0387,  1.1639,  1.2843,  1.0109,  2.5904,  1.0256,  1.3351],\n",
      "        [ 1.1498,  1.2591,  1.5977,  1.8115,  1.3350,  2.1340,  1.1916,  1.2251],\n",
      "        [-0.3594,  1.5779,  1.5310,  1.8837,  1.5150,  2.4536,  1.0441,  2.2278]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.5036,  2.2881,  2.0026, -0.6603,  1.8952,  1.0394,  1.5515, -0.4514],\n",
      "        [ 1.2302,  2.3671,  1.3898, -0.3740,  1.2407,  1.3146,  1.2231,  1.9357],\n",
      "        [ 2.3640,  1.4971,  1.3423,  1.7804,  1.2277,  2.0888,  1.9543,  2.3828],\n",
      "        [ 2.1865,  2.1950,  1.8624,  2.2136,  1.4636,  1.5913,  1.4544,  1.9828],\n",
      "        [-0.3652,  1.1799,  1.4675,  2.4008,  1.7227,  1.3752,  1.7164,  1.2206],\n",
      "        [-0.4335,  2.0486,  1.1764,  1.2961,  1.0248,  2.5998,  1.0393,  1.3466],\n",
      "        [ 1.1624,  1.2710,  1.6084,  1.8217,  1.3466,  2.1438,  1.2039,  1.2372],\n",
      "        [-0.3739,  1.5886,  1.5419,  1.8938,  1.5259,  2.4631,  1.0576,  2.2375]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.5036,  2.2881,  2.0026, -0.6603,  1.8952,  1.0394,  1.5515, -0.4514],\n",
      "        [ 1.2302,  2.3671,  1.3898, -0.3740,  1.2407,  1.3146,  1.2231,  1.9357],\n",
      "        [ 2.3640,  1.4971,  1.3423,  1.7804,  1.2277,  2.0888,  1.9543,  2.3828],\n",
      "        [ 2.1865,  2.1950,  1.8624,  2.2136,  1.4636,  1.5913,  1.4544,  1.9828],\n",
      "        [-0.3652,  1.1799,  1.4675,  2.4008,  1.7227,  1.3752,  1.7164,  1.2206],\n",
      "        [-0.4335,  2.0486,  1.1764,  1.2961,  1.0248,  2.5998,  1.0393,  1.3466],\n",
      "        [ 1.1624,  1.2710,  1.6084,  1.8217,  1.3466,  2.1438,  1.2039,  1.2372],\n",
      "        [-0.3739,  1.5886,  1.5419,  1.8938,  1.5259,  2.4631,  1.0576,  2.2375]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.5130,  2.2978,  2.0126, -0.6726,  1.9053,  1.0531,  1.5623, -0.4652],\n",
      "        [ 1.2424,  2.3767,  1.4012, -0.3885,  1.2527,  1.3263,  1.2353,  1.9458],\n",
      "        [ 2.3736,  1.5081,  1.3539,  1.7907,  1.2398,  2.0986,  1.9643,  2.3923],\n",
      "        [ 2.1963,  2.2047,  1.8726,  2.2234,  1.4748,  1.6021,  1.4656,  1.9928],\n",
      "        [-0.3798,  1.1924,  1.4786,  2.4103,  1.7332,  1.3867,  1.7269,  1.2328],\n",
      "        [-0.4474,  2.0585,  1.1889,  1.3078,  1.0387,  2.6092,  1.0531,  1.3581],\n",
      "        [ 1.1750,  1.2829,  1.6191,  1.8320,  1.3581,  2.1536,  1.2162,  1.2493],\n",
      "        [-0.3883,  1.5994,  1.5528,  1.9040,  1.5368,  2.4726,  1.0711,  2.2472]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.5130,  2.2978,  2.0126, -0.6726,  1.9053,  1.0531,  1.5623, -0.4652],\n",
      "        [ 1.2424,  2.3767,  1.4012, -0.3885,  1.2527,  1.3263,  1.2353,  1.9458],\n",
      "        [ 2.3736,  1.5081,  1.3539,  1.7907,  1.2398,  2.0986,  1.9643,  2.3923],\n",
      "        [ 2.1963,  2.2047,  1.8726,  2.2234,  1.4748,  1.6021,  1.4656,  1.9928],\n",
      "        [-0.3798,  1.1924,  1.4786,  2.4103,  1.7332,  1.3867,  1.7269,  1.2328],\n",
      "        [-0.4474,  2.0585,  1.1889,  1.3078,  1.0387,  2.6092,  1.0531,  1.3581],\n",
      "        [ 1.1750,  1.2829,  1.6191,  1.8320,  1.3581,  2.1536,  1.2162,  1.2493],\n",
      "        [-0.3883,  1.5994,  1.5528,  1.9040,  1.5368,  2.4726,  1.0711,  2.2472]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.5225,  2.3074,  2.0225, -0.6849,  1.9154,  1.0669,  1.5732, -0.4789],\n",
      "        [ 1.2545,  2.3863,  1.4126, -0.4030,  1.2648,  1.3380,  1.2475,  1.9558],\n",
      "        [ 2.3831,  1.5191,  1.3655,  1.8011,  1.2520,  2.1085,  1.9743,  2.4019],\n",
      "        [ 2.2060,  2.2144,  1.8828,  2.2331,  1.4859,  1.6128,  1.4767,  2.0028],\n",
      "        [-0.3944,  1.2049,  1.4897,  2.4198,  1.7436,  1.3981,  1.7373,  1.2450],\n",
      "        [-0.4613,  2.0684,  1.2014,  1.3196,  1.0527,  2.6186,  1.0669,  1.3697],\n",
      "        [ 1.1876,  1.2948,  1.6298,  1.8422,  1.3697,  2.1634,  1.2285,  1.2614],\n",
      "        [-0.4029,  1.6101,  1.5636,  1.9141,  1.5477,  2.4821,  1.0847,  2.2568]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.5225,  2.3074,  2.0225, -0.6849,  1.9154,  1.0669,  1.5732, -0.4789],\n",
      "        [ 1.2545,  2.3863,  1.4126, -0.4030,  1.2648,  1.3380,  1.2475,  1.9558],\n",
      "        [ 2.3831,  1.5191,  1.3655,  1.8011,  1.2520,  2.1085,  1.9743,  2.4019],\n",
      "        [ 2.2060,  2.2144,  1.8828,  2.2331,  1.4859,  1.6128,  1.4767,  2.0028],\n",
      "        [-0.3944,  1.2049,  1.4897,  2.4198,  1.7436,  1.3981,  1.7373,  1.2450],\n",
      "        [-0.4613,  2.0684,  1.2014,  1.3196,  1.0527,  2.6186,  1.0669,  1.3697],\n",
      "        [ 1.1876,  1.2948,  1.6298,  1.8422,  1.3697,  2.1634,  1.2285,  1.2614],\n",
      "        [-0.4029,  1.6101,  1.5636,  1.9141,  1.5477,  2.4821,  1.0847,  2.2568]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.5319,  2.3170,  2.0325, -0.6973,  1.9255,  1.0807,  1.5840, -0.4927],\n",
      "        [ 1.2667,  2.3958,  1.4239, -0.4176,  1.2769,  1.3497,  1.2597,  1.9659],\n",
      "        [ 2.3927,  1.5301,  1.3771,  1.8114,  1.2642,  2.1183,  1.9844,  2.4114],\n",
      "        [ 2.2157,  2.2241,  1.8930,  2.2428,  1.4970,  1.6235,  1.4879,  2.0128],\n",
      "        [-0.4090,  1.2174,  1.5008,  2.4293,  1.7540,  1.4096,  1.7478,  1.2573],\n",
      "        [-0.4753,  2.0783,  1.2139,  1.3314,  1.0666,  2.6279,  1.0807,  1.3813],\n",
      "        [ 1.2002,  1.3067,  1.6405,  1.8525,  1.3812,  2.1731,  1.2409,  1.2735],\n",
      "        [-0.4174,  1.6209,  1.5745,  1.9242,  1.5586,  2.4916,  1.0983,  2.2665]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.5319,  2.3170,  2.0325, -0.6973,  1.9255,  1.0807,  1.5840, -0.4927],\n",
      "        [ 1.2667,  2.3958,  1.4239, -0.4176,  1.2769,  1.3497,  1.2597,  1.9659],\n",
      "        [ 2.3927,  1.5301,  1.3771,  1.8114,  1.2642,  2.1183,  1.9844,  2.4114],\n",
      "        [ 2.2157,  2.2241,  1.8930,  2.2428,  1.4970,  1.6235,  1.4879,  2.0128],\n",
      "        [-0.4090,  1.2174,  1.5008,  2.4293,  1.7540,  1.4096,  1.7478,  1.2573],\n",
      "        [-0.4753,  2.0783,  1.2139,  1.3314,  1.0666,  2.6279,  1.0807,  1.3813],\n",
      "        [ 1.2002,  1.3067,  1.6405,  1.8525,  1.3812,  2.1731,  1.2409,  1.2735],\n",
      "        [-0.4174,  1.6209,  1.5745,  1.9242,  1.5586,  2.4916,  1.0983,  2.2665]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.5413,  2.3266,  2.0424, -0.7097,  1.9357,  1.0945,  1.5949, -0.5066],\n",
      "        [ 1.2789,  2.4053,  1.4353, -0.4322,  1.2891,  1.3614,  1.2719,  1.9759],\n",
      "        [ 2.4022,  1.5411,  1.3887,  1.8217,  1.2764,  2.1282,  1.9944,  2.4209],\n",
      "        [ 2.2254,  2.2338,  1.9031,  2.2524,  1.5081,  1.6343,  1.4990,  2.0228],\n",
      "        [-0.4237,  1.2299,  1.5119,  2.4388,  1.7645,  1.4210,  1.7582,  1.2695],\n",
      "        [-0.4893,  2.0882,  1.2265,  1.3433,  1.0806,  2.6372,  1.0945,  1.3929],\n",
      "        [ 1.2129,  1.3187,  1.6512,  1.8627,  1.3928,  2.1829,  1.2532,  1.2857],\n",
      "        [-0.4320,  1.6316,  1.5854,  1.9343,  1.5695,  2.5010,  1.1119,  2.2762]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.5413,  2.3266,  2.0424, -0.7097,  1.9357,  1.0945,  1.5949, -0.5066],\n",
      "        [ 1.2789,  2.4053,  1.4353, -0.4322,  1.2891,  1.3614,  1.2719,  1.9759],\n",
      "        [ 2.4022,  1.5411,  1.3887,  1.8217,  1.2764,  2.1282,  1.9944,  2.4209],\n",
      "        [ 2.2254,  2.2338,  1.9031,  2.2524,  1.5081,  1.6343,  1.4990,  2.0228],\n",
      "        [-0.4237,  1.2299,  1.5119,  2.4388,  1.7645,  1.4210,  1.7582,  1.2695],\n",
      "        [-0.4893,  2.0882,  1.2265,  1.3433,  1.0806,  2.6372,  1.0945,  1.3929],\n",
      "        [ 1.2129,  1.3187,  1.6512,  1.8627,  1.3928,  2.1829,  1.2532,  1.2857],\n",
      "        [-0.4320,  1.6316,  1.5854,  1.9343,  1.5695,  2.5010,  1.1119,  2.2762]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.5507,  2.3362,  2.0524, -0.7221,  1.9458,  1.1084,  1.6057, -0.5205],\n",
      "        [ 1.2911,  2.4148,  1.4468, -0.4468,  1.3012,  1.3732,  1.2842,  1.9860],\n",
      "        [ 2.4117,  1.5521,  1.4003,  1.8320,  1.2886,  2.1380,  2.0044,  2.4304],\n",
      "        [ 2.2351,  2.2435,  1.9133,  2.2621,  1.5193,  1.6450,  1.5102,  2.0327],\n",
      "        [-0.4385,  1.2425,  1.5230,  2.4483,  1.7749,  1.4325,  1.7687,  1.2818],\n",
      "        [-0.5033,  2.0981,  1.2391,  1.3551,  1.0947,  2.6466,  1.1084,  1.4045],\n",
      "        [ 1.2256,  1.3307,  1.6619,  1.8730,  1.4044,  2.1926,  1.2656,  1.2978],\n",
      "        [-0.4467,  1.6424,  1.5963,  1.9444,  1.5805,  2.5104,  1.1256,  2.2858]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.5507,  2.3362,  2.0524, -0.7221,  1.9458,  1.1084,  1.6057, -0.5205],\n",
      "        [ 1.2911,  2.4148,  1.4468, -0.4468,  1.3012,  1.3732,  1.2842,  1.9860],\n",
      "        [ 2.4117,  1.5521,  1.4003,  1.8320,  1.2886,  2.1380,  2.0044,  2.4304],\n",
      "        [ 2.2351,  2.2435,  1.9133,  2.2621,  1.5193,  1.6450,  1.5102,  2.0327],\n",
      "        [-0.4385,  1.2425,  1.5230,  2.4483,  1.7749,  1.4325,  1.7687,  1.2818],\n",
      "        [-0.5033,  2.0981,  1.2391,  1.3551,  1.0947,  2.6466,  1.1084,  1.4045],\n",
      "        [ 1.2256,  1.3307,  1.6619,  1.8730,  1.4044,  2.1926,  1.2656,  1.2978],\n",
      "        [-0.4467,  1.6424,  1.5963,  1.9444,  1.5805,  2.5104,  1.1256,  2.2858]],\n",
      "       requires_grad=True)\n",
      "epoch:  60 ; loss:  1.302696943283081 ; mask density:  0.0014299199683591723 ; pred:  tensor([2.7946e-06, 5.5742e-06, 1.9562e-07, 9.9995e-01, 1.0843e-05, 3.0840e-05,\n",
      "        2.0645e-07], grad_fn=<SoftmaxBackward0>) ; labels equal:  tensor(True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.5600,  2.3458,  2.0623, -0.7346,  1.9559,  1.1223,  1.6166, -0.5344],\n",
      "        [ 1.3033,  2.4243,  1.4582, -0.4615,  1.3134,  1.3849,  1.2964,  1.9960],\n",
      "        [ 2.4212,  1.5631,  1.4119,  1.8423,  1.3009,  2.1478,  2.0144,  2.4398],\n",
      "        [ 2.2448,  2.2532,  1.9235,  2.2718,  1.5304,  1.6558,  1.5214,  2.0427],\n",
      "        [-0.4532,  1.2550,  1.5341,  2.4578,  1.7853,  1.4440,  1.7791,  1.2941],\n",
      "        [-0.5174,  2.1079,  1.2517,  1.3670,  1.1087,  2.6558,  1.1222,  1.4161],\n",
      "        [ 1.2383,  1.3427,  1.6726,  1.8832,  1.4160,  2.2024,  1.2780,  1.3100],\n",
      "        [-0.4614,  1.6531,  1.6072,  1.9545,  1.5914,  2.5198,  1.1393,  2.2954]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.5600,  2.3458,  2.0623, -0.7346,  1.9559,  1.1223,  1.6166, -0.5344],\n",
      "        [ 1.3033,  2.4243,  1.4582, -0.4615,  1.3134,  1.3849,  1.2964,  1.9960],\n",
      "        [ 2.4212,  1.5631,  1.4119,  1.8423,  1.3009,  2.1478,  2.0144,  2.4398],\n",
      "        [ 2.2448,  2.2532,  1.9235,  2.2718,  1.5304,  1.6558,  1.5214,  2.0427],\n",
      "        [-0.4532,  1.2550,  1.5341,  2.4578,  1.7853,  1.4440,  1.7791,  1.2941],\n",
      "        [-0.5174,  2.1079,  1.2517,  1.3670,  1.1087,  2.6558,  1.1222,  1.4161],\n",
      "        [ 1.2383,  1.3427,  1.6726,  1.8832,  1.4160,  2.2024,  1.2780,  1.3100],\n",
      "        [-0.4614,  1.6531,  1.6072,  1.9545,  1.5914,  2.5198,  1.1393,  2.2954]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.5694,  2.3553,  2.0722, -0.7470,  1.9659,  1.1362,  1.6275, -0.5483],\n",
      "        [ 1.3156,  2.4338,  1.4696, -0.4762,  1.3256,  1.3967,  1.3087,  2.0060],\n",
      "        [ 2.4307,  1.5742,  1.4236,  1.8526,  1.3131,  2.1576,  2.0244,  2.4493],\n",
      "        [ 2.2545,  2.2629,  1.9336,  2.2814,  1.5415,  1.6665,  1.5325,  2.0526],\n",
      "        [-0.4680,  1.2676,  1.5453,  2.4672,  1.7957,  1.4555,  1.7896,  1.3064],\n",
      "        [-0.5316,  2.1178,  1.2643,  1.3788,  1.1228,  2.6651,  1.1361,  1.4277],\n",
      "        [ 1.2510,  1.3547,  1.6833,  1.8934,  1.4276,  2.2121,  1.2904,  1.3222],\n",
      "        [-0.4761,  1.6639,  1.6181,  1.9646,  1.6024,  2.5292,  1.1530,  2.3050]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.5694,  2.3553,  2.0722, -0.7470,  1.9659,  1.1362,  1.6275, -0.5483],\n",
      "        [ 1.3156,  2.4338,  1.4696, -0.4762,  1.3256,  1.3967,  1.3087,  2.0060],\n",
      "        [ 2.4307,  1.5742,  1.4236,  1.8526,  1.3131,  2.1576,  2.0244,  2.4493],\n",
      "        [ 2.2545,  2.2629,  1.9336,  2.2814,  1.5415,  1.6665,  1.5325,  2.0526],\n",
      "        [-0.4680,  1.2676,  1.5453,  2.4672,  1.7957,  1.4555,  1.7896,  1.3064],\n",
      "        [-0.5316,  2.1178,  1.2643,  1.3788,  1.1228,  2.6651,  1.1361,  1.4277],\n",
      "        [ 1.2510,  1.3547,  1.6833,  1.8934,  1.4276,  2.2121,  1.2904,  1.3222],\n",
      "        [-0.4761,  1.6639,  1.6181,  1.9646,  1.6024,  2.5292,  1.1530,  2.3050]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.5787,  2.3649,  2.0821, -0.7595,  1.9760,  1.1501,  1.6383, -0.5623],\n",
      "        [ 1.3278,  2.4432,  1.4811, -0.4910,  1.3378,  1.4085,  1.3210,  2.0160],\n",
      "        [ 2.4401,  1.5852,  1.4352,  1.8629,  1.3254,  2.1673,  2.0343,  2.4587],\n",
      "        [ 2.2641,  2.2725,  1.9437,  2.2910,  1.5527,  1.6773,  1.5437,  2.0626],\n",
      "        [-0.4829,  1.2802,  1.5564,  2.4766,  1.8062,  1.4670,  1.8000,  1.3187],\n",
      "        [-0.5457,  2.1276,  1.2769,  1.3907,  1.1368,  2.6744,  1.1500,  1.4393],\n",
      "        [ 1.2637,  1.3667,  1.6940,  1.9036,  1.4393,  2.2218,  1.3029,  1.3344],\n",
      "        [-0.4908,  1.6746,  1.6290,  1.9747,  1.6133,  2.5386,  1.1667,  2.3146]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.5787,  2.3649,  2.0821, -0.7595,  1.9760,  1.1501,  1.6383, -0.5623],\n",
      "        [ 1.3278,  2.4432,  1.4811, -0.4910,  1.3378,  1.4085,  1.3210,  2.0160],\n",
      "        [ 2.4401,  1.5852,  1.4352,  1.8629,  1.3254,  2.1673,  2.0343,  2.4587],\n",
      "        [ 2.2641,  2.2725,  1.9437,  2.2910,  1.5527,  1.6773,  1.5437,  2.0626],\n",
      "        [-0.4829,  1.2802,  1.5564,  2.4766,  1.8062,  1.4670,  1.8000,  1.3187],\n",
      "        [-0.5457,  2.1276,  1.2769,  1.3907,  1.1368,  2.6744,  1.1500,  1.4393],\n",
      "        [ 1.2637,  1.3667,  1.6940,  1.9036,  1.4393,  2.2218,  1.3029,  1.3344],\n",
      "        [-0.4908,  1.6746,  1.6290,  1.9747,  1.6133,  2.5386,  1.1667,  2.3146]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.5880,  2.3744,  2.0920, -0.7721,  1.9861,  1.1640,  1.6492, -0.5764],\n",
      "        [ 1.3401,  2.4527,  1.4925, -0.5058,  1.3500,  1.4203,  1.3334,  2.0260],\n",
      "        [ 2.4496,  1.5963,  1.4469,  1.8732,  1.3377,  2.1771,  2.0443,  2.4682],\n",
      "        [ 2.2738,  2.2821,  1.9539,  2.3006,  1.5639,  1.6880,  1.5549,  2.0725],\n",
      "        [-0.4977,  1.2929,  1.5676,  2.4860,  1.8166,  1.4785,  1.8104,  1.3310],\n",
      "        [-0.5599,  2.1374,  1.2895,  1.4026,  1.1509,  2.6836,  1.1640,  1.4510],\n",
      "        [ 1.2765,  1.3787,  1.7047,  1.9138,  1.4509,  2.2315,  1.3153,  1.3467],\n",
      "        [-0.5056,  1.6854,  1.6399,  1.9848,  1.6243,  2.5479,  1.1804,  2.3242]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.5880,  2.3744,  2.0920, -0.7721,  1.9861,  1.1640,  1.6492, -0.5764],\n",
      "        [ 1.3401,  2.4527,  1.4925, -0.5058,  1.3500,  1.4203,  1.3334,  2.0260],\n",
      "        [ 2.4496,  1.5963,  1.4469,  1.8732,  1.3377,  2.1771,  2.0443,  2.4682],\n",
      "        [ 2.2738,  2.2821,  1.9539,  2.3006,  1.5639,  1.6880,  1.5549,  2.0725],\n",
      "        [-0.4977,  1.2929,  1.5676,  2.4860,  1.8166,  1.4785,  1.8104,  1.3310],\n",
      "        [-0.5599,  2.1374,  1.2895,  1.4026,  1.1509,  2.6836,  1.1640,  1.4510],\n",
      "        [ 1.2765,  1.3787,  1.7047,  1.9138,  1.4509,  2.2315,  1.3153,  1.3467],\n",
      "        [-0.5056,  1.6854,  1.6399,  1.9848,  1.6243,  2.5479,  1.1804,  2.3242]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.5973,  2.3839,  2.1019, -0.7846,  1.9961,  1.1780,  1.6601, -0.5904],\n",
      "        [ 1.3524,  2.4621,  1.5040, -0.5206,  1.3622,  1.4321,  1.3457,  2.0360],\n",
      "        [ 2.4590,  1.6073,  1.4585,  1.8835,  1.3500,  2.1869,  2.0543,  2.4776],\n",
      "        [ 2.2834,  2.2917,  1.9640,  2.3102,  1.5750,  1.6988,  1.5661,  2.0824],\n",
      "        [-0.5126,  1.3055,  1.5787,  2.4954,  1.8270,  1.4900,  1.8209,  1.3434],\n",
      "        [-0.5741,  2.1473,  1.3022,  1.4145,  1.1650,  2.6928,  1.1779,  1.4626],\n",
      "        [ 1.2893,  1.3908,  1.7154,  1.9240,  1.4626,  2.2412,  1.3278,  1.3589],\n",
      "        [-0.5204,  1.6962,  1.6508,  1.9948,  1.6352,  2.5572,  1.1941,  2.3338]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.5973,  2.3839,  2.1019, -0.7846,  1.9961,  1.1780,  1.6601, -0.5904],\n",
      "        [ 1.3524,  2.4621,  1.5040, -0.5206,  1.3622,  1.4321,  1.3457,  2.0360],\n",
      "        [ 2.4590,  1.6073,  1.4585,  1.8835,  1.3500,  2.1869,  2.0543,  2.4776],\n",
      "        [ 2.2834,  2.2917,  1.9640,  2.3102,  1.5750,  1.6988,  1.5661,  2.0824],\n",
      "        [-0.5126,  1.3055,  1.5787,  2.4954,  1.8270,  1.4900,  1.8209,  1.3434],\n",
      "        [-0.5741,  2.1473,  1.3022,  1.4145,  1.1650,  2.6928,  1.1779,  1.4626],\n",
      "        [ 1.2893,  1.3908,  1.7154,  1.9240,  1.4626,  2.2412,  1.3278,  1.3589],\n",
      "        [-0.5204,  1.6962,  1.6508,  1.9948,  1.6352,  2.5572,  1.1941,  2.3338]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.6066,  2.3934,  2.1118, -0.7972,  2.0062,  1.1919,  1.6709, -0.6045],\n",
      "        [ 1.3647,  2.4715,  1.5154, -0.5354,  1.3744,  1.4439,  1.3580,  2.0460],\n",
      "        [ 2.4684,  1.6184,  1.4702,  1.8938,  1.3623,  2.1966,  2.0642,  2.4869],\n",
      "        [ 2.2930,  2.3013,  1.9741,  2.3198,  1.5862,  1.7095,  1.5773,  2.0923],\n",
      "        [-0.5275,  1.3181,  1.5899,  2.5048,  1.8374,  1.5015,  1.8313,  1.3558],\n",
      "        [-0.5884,  2.1571,  1.3149,  1.4264,  1.1792,  2.7020,  1.1919,  1.4743],\n",
      "        [ 1.3020,  1.4028,  1.7261,  1.9342,  1.4742,  2.2508,  1.3402,  1.3712],\n",
      "        [-0.5353,  1.7069,  1.6617,  2.0049,  1.6462,  2.5665,  1.2079,  2.3433]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.6066,  2.3934,  2.1118, -0.7972,  2.0062,  1.1919,  1.6709, -0.6045],\n",
      "        [ 1.3647,  2.4715,  1.5154, -0.5354,  1.3744,  1.4439,  1.3580,  2.0460],\n",
      "        [ 2.4684,  1.6184,  1.4702,  1.8938,  1.3623,  2.1966,  2.0642,  2.4869],\n",
      "        [ 2.2930,  2.3013,  1.9741,  2.3198,  1.5862,  1.7095,  1.5773,  2.0923],\n",
      "        [-0.5275,  1.3181,  1.5899,  2.5048,  1.8374,  1.5015,  1.8313,  1.3558],\n",
      "        [-0.5884,  2.1571,  1.3149,  1.4264,  1.1792,  2.7020,  1.1919,  1.4743],\n",
      "        [ 1.3020,  1.4028,  1.7261,  1.9342,  1.4742,  2.2508,  1.3402,  1.3712],\n",
      "        [-0.5353,  1.7069,  1.6617,  2.0049,  1.6462,  2.5665,  1.2079,  2.3433]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.6158,  2.4028,  2.1216, -0.8098,  2.0162,  1.2059,  1.6818, -0.6186],\n",
      "        [ 1.3770,  2.4809,  1.5269, -0.5503,  1.3867,  1.4557,  1.3704,  2.0560],\n",
      "        [ 2.4778,  1.6294,  1.4819,  1.9040,  1.3746,  2.2063,  2.0741,  2.4963],\n",
      "        [ 2.3026,  2.3109,  1.9842,  2.3293,  1.5974,  1.7203,  1.5885,  2.1022],\n",
      "        [-0.5425,  1.3308,  1.6010,  2.5141,  1.8478,  1.5131,  1.8417,  1.3681],\n",
      "        [-0.6027,  2.1668,  1.3276,  1.4383,  1.1933,  2.7112,  1.2058,  1.4859],\n",
      "        [ 1.3148,  1.4149,  1.7368,  1.9444,  1.4859,  2.2605,  1.3527,  1.3834],\n",
      "        [-0.5501,  1.7177,  1.6726,  2.0149,  1.6571,  2.5758,  1.2217,  2.3528]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.6158,  2.4028,  2.1216, -0.8098,  2.0162,  1.2059,  1.6818, -0.6186],\n",
      "        [ 1.3770,  2.4809,  1.5269, -0.5503,  1.3867,  1.4557,  1.3704,  2.0560],\n",
      "        [ 2.4778,  1.6294,  1.4819,  1.9040,  1.3746,  2.2063,  2.0741,  2.4963],\n",
      "        [ 2.3026,  2.3109,  1.9842,  2.3293,  1.5974,  1.7203,  1.5885,  2.1022],\n",
      "        [-0.5425,  1.3308,  1.6010,  2.5141,  1.8478,  1.5131,  1.8417,  1.3681],\n",
      "        [-0.6027,  2.1668,  1.3276,  1.4383,  1.1933,  2.7112,  1.2058,  1.4859],\n",
      "        [ 1.3148,  1.4149,  1.7368,  1.9444,  1.4859,  2.2605,  1.3527,  1.3834],\n",
      "        [-0.5501,  1.7177,  1.6726,  2.0149,  1.6571,  2.5758,  1.2217,  2.3528]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.6250,  2.4123,  2.1315, -0.8224,  2.0263,  1.2199,  1.6926, -0.6327],\n",
      "        [ 1.3893,  2.4902,  1.5383, -0.5651,  1.3989,  1.4676,  1.3828,  2.0659],\n",
      "        [ 2.4871,  1.6404,  1.4936,  1.9143,  1.3870,  2.2160,  2.0841,  2.5057],\n",
      "        [ 2.3122,  2.3205,  1.9943,  2.3389,  1.6085,  1.7310,  1.5997,  2.1121],\n",
      "        [-0.5575,  1.3435,  1.6122,  2.5234,  1.8582,  1.5246,  1.8521,  1.3805],\n",
      "        [-0.6170,  2.1766,  1.3403,  1.4503,  1.2074,  2.7203,  1.2198,  1.4976],\n",
      "        [ 1.3276,  1.4269,  1.7475,  1.9546,  1.4976,  2.2701,  1.3652,  1.3957],\n",
      "        [-0.5650,  1.7284,  1.6835,  2.0249,  1.6680,  2.5851,  1.2354,  2.3623]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.6250,  2.4123,  2.1315, -0.8224,  2.0263,  1.2199,  1.6926, -0.6327],\n",
      "        [ 1.3893,  2.4902,  1.5383, -0.5651,  1.3989,  1.4676,  1.3828,  2.0659],\n",
      "        [ 2.4871,  1.6404,  1.4936,  1.9143,  1.3870,  2.2160,  2.0841,  2.5057],\n",
      "        [ 2.3122,  2.3205,  1.9943,  2.3389,  1.6085,  1.7310,  1.5997,  2.1121],\n",
      "        [-0.5575,  1.3435,  1.6122,  2.5234,  1.8582,  1.5246,  1.8521,  1.3805],\n",
      "        [-0.6170,  2.1766,  1.3403,  1.4503,  1.2074,  2.7203,  1.2198,  1.4976],\n",
      "        [ 1.3276,  1.4269,  1.7475,  1.9546,  1.4976,  2.2701,  1.3652,  1.3957],\n",
      "        [-0.5650,  1.7284,  1.6835,  2.0249,  1.6680,  2.5851,  1.2354,  2.3623]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.6343,  2.4217,  2.1413, -0.8350,  2.0363,  1.2338,  1.7035, -0.6469],\n",
      "        [ 1.4017,  2.4996,  1.5498, -0.5800,  1.4112,  1.4794,  1.3951,  2.0759],\n",
      "        [ 2.4965,  1.6515,  1.5053,  1.9245,  1.3993,  2.2257,  2.0940,  2.5150],\n",
      "        [ 2.3217,  2.3300,  2.0044,  2.3484,  1.6197,  1.7418,  1.6109,  2.1219],\n",
      "        [-0.5724,  1.3561,  1.6233,  2.5327,  1.8686,  1.5361,  1.8625,  1.3929],\n",
      "        [-0.6313,  2.1864,  1.3530,  1.4622,  1.2216,  2.7294,  1.2338,  1.5093],\n",
      "        [ 1.3405,  1.4390,  1.7582,  1.9647,  1.5092,  2.2797,  1.3777,  1.4080],\n",
      "        [-0.5799,  1.7392,  1.6944,  2.0350,  1.6790,  2.5943,  1.2492,  2.3718]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.6343,  2.4217,  2.1413, -0.8350,  2.0363,  1.2338,  1.7035, -0.6469],\n",
      "        [ 1.4017,  2.4996,  1.5498, -0.5800,  1.4112,  1.4794,  1.3951,  2.0759],\n",
      "        [ 2.4965,  1.6515,  1.5053,  1.9245,  1.3993,  2.2257,  2.0940,  2.5150],\n",
      "        [ 2.3217,  2.3300,  2.0044,  2.3484,  1.6197,  1.7418,  1.6109,  2.1219],\n",
      "        [-0.5724,  1.3561,  1.6233,  2.5327,  1.8686,  1.5361,  1.8625,  1.3929],\n",
      "        [-0.6313,  2.1864,  1.3530,  1.4622,  1.2216,  2.7294,  1.2338,  1.5093],\n",
      "        [ 1.3405,  1.4390,  1.7582,  1.9647,  1.5092,  2.2797,  1.3777,  1.4080],\n",
      "        [-0.5799,  1.7392,  1.6944,  2.0350,  1.6790,  2.5943,  1.2492,  2.3718]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.6434,  2.4311,  2.1511, -0.8477,  2.0463,  1.2478,  1.7144, -0.6611],\n",
      "        [ 1.4140,  2.5089,  1.5613, -0.5950,  1.4235,  1.4912,  1.4075,  2.0858],\n",
      "        [ 2.5058,  1.6625,  1.5170,  1.9348,  1.4117,  2.2354,  2.1039,  2.5243],\n",
      "        [ 2.3313,  2.3396,  2.0144,  2.3579,  1.6309,  1.7525,  1.6221,  2.1318],\n",
      "        [-0.5875,  1.3688,  1.6345,  2.5420,  1.8790,  1.5477,  1.8729,  1.4053],\n",
      "        [-0.6456,  2.1961,  1.3657,  1.4741,  1.2357,  2.7385,  1.2478,  1.5209],\n",
      "        [ 1.3533,  1.4511,  1.7689,  1.9749,  1.5209,  2.2893,  1.3902,  1.4203],\n",
      "        [-0.5948,  1.7499,  1.7052,  2.0450,  1.6899,  2.6036,  1.2630,  2.3813]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.6434,  2.4311,  2.1511, -0.8477,  2.0463,  1.2478,  1.7144, -0.6611],\n",
      "        [ 1.4140,  2.5089,  1.5613, -0.5950,  1.4235,  1.4912,  1.4075,  2.0858],\n",
      "        [ 2.5058,  1.6625,  1.5170,  1.9348,  1.4117,  2.2354,  2.1039,  2.5243],\n",
      "        [ 2.3313,  2.3396,  2.0144,  2.3579,  1.6309,  1.7525,  1.6221,  2.1318],\n",
      "        [-0.5875,  1.3688,  1.6345,  2.5420,  1.8790,  1.5477,  1.8729,  1.4053],\n",
      "        [-0.6456,  2.1961,  1.3657,  1.4741,  1.2357,  2.7385,  1.2478,  1.5209],\n",
      "        [ 1.3533,  1.4511,  1.7689,  1.9749,  1.5209,  2.2893,  1.3902,  1.4203],\n",
      "        [-0.5948,  1.7499,  1.7052,  2.0450,  1.6899,  2.6036,  1.2630,  2.3813]],\n",
      "       requires_grad=True)\n",
      "epoch:  70 ; loss:  1.2568235397338867 ; mask density:  0.0014470630558207631 ; pred:  tensor([5.9998e-05, 8.6017e-04, 5.2761e-06, 9.9841e-01, 6.3748e-04, 2.7236e-05,\n",
      "        1.6734e-06], grad_fn=<SoftmaxBackward0>) ; labels equal:  tensor(True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.6526,  2.4405,  2.1609, -0.8603,  2.0563,  1.2618,  1.7252, -0.6752],\n",
      "        [ 1.4263,  2.5182,  1.5728, -0.6099,  1.4357,  1.5031,  1.4199,  2.0957],\n",
      "        [ 2.5151,  1.6736,  1.5287,  1.9450,  1.4240,  2.2451,  2.1138,  2.5336],\n",
      "        [ 2.3408,  2.3491,  2.0245,  2.3674,  1.6420,  1.7632,  1.6333,  2.1416],\n",
      "        [-0.6025,  1.3815,  1.6456,  2.5513,  1.8893,  1.5592,  1.8833,  1.4177],\n",
      "        [-0.6600,  2.2058,  1.3784,  1.4861,  1.2499,  2.7476,  1.2618,  1.5326],\n",
      "        [ 1.3661,  1.4632,  1.7796,  1.9850,  1.5326,  2.2989,  1.4028,  1.4326],\n",
      "        [-0.6098,  1.7606,  1.7161,  2.0550,  1.7009,  2.6128,  1.2768,  2.3908]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.6526,  2.4405,  2.1609, -0.8603,  2.0563,  1.2618,  1.7252, -0.6752],\n",
      "        [ 1.4263,  2.5182,  1.5728, -0.6099,  1.4357,  1.5031,  1.4199,  2.0957],\n",
      "        [ 2.5151,  1.6736,  1.5287,  1.9450,  1.4240,  2.2451,  2.1138,  2.5336],\n",
      "        [ 2.3408,  2.3491,  2.0245,  2.3674,  1.6420,  1.7632,  1.6333,  2.1416],\n",
      "        [-0.6025,  1.3815,  1.6456,  2.5513,  1.8893,  1.5592,  1.8833,  1.4177],\n",
      "        [-0.6600,  2.2058,  1.3784,  1.4861,  1.2499,  2.7476,  1.2618,  1.5326],\n",
      "        [ 1.3661,  1.4632,  1.7796,  1.9850,  1.5326,  2.2989,  1.4028,  1.4326],\n",
      "        [-0.6098,  1.7606,  1.7161,  2.0550,  1.7009,  2.6128,  1.2768,  2.3908]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.6618,  2.4499,  2.1707, -0.8730,  2.0663,  1.2758,  1.7361, -0.6895],\n",
      "        [ 1.4387,  2.5275,  1.5842, -0.6249,  1.4480,  1.5149,  1.4323,  2.1056],\n",
      "        [ 2.5244,  1.6846,  1.5404,  1.9552,  1.4364,  2.2547,  2.1236,  2.5429],\n",
      "        [ 2.3503,  2.3586,  2.0345,  2.3769,  1.6532,  1.7740,  1.6445,  2.1514],\n",
      "        [-0.6175,  1.3942,  1.6568,  2.5606,  1.8997,  1.5707,  1.8937,  1.4301],\n",
      "        [-0.6744,  2.2155,  1.3911,  1.4980,  1.2640,  2.7567,  1.2758,  1.5443],\n",
      "        [ 1.3789,  1.4753,  1.7902,  1.9952,  1.5443,  2.3085,  1.4153,  1.4449],\n",
      "        [-0.6247,  1.7714,  1.7270,  2.0649,  1.7118,  2.6220,  1.2906,  2.4002]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.6618,  2.4499,  2.1707, -0.8730,  2.0663,  1.2758,  1.7361, -0.6895],\n",
      "        [ 1.4387,  2.5275,  1.5842, -0.6249,  1.4480,  1.5149,  1.4323,  2.1056],\n",
      "        [ 2.5244,  1.6846,  1.5404,  1.9552,  1.4364,  2.2547,  2.1236,  2.5429],\n",
      "        [ 2.3503,  2.3586,  2.0345,  2.3769,  1.6532,  1.7740,  1.6445,  2.1514],\n",
      "        [-0.6175,  1.3942,  1.6568,  2.5606,  1.8997,  1.5707,  1.8937,  1.4301],\n",
      "        [-0.6744,  2.2155,  1.3911,  1.4980,  1.2640,  2.7567,  1.2758,  1.5443],\n",
      "        [ 1.3789,  1.4753,  1.7902,  1.9952,  1.5443,  2.3085,  1.4153,  1.4449],\n",
      "        [-0.6247,  1.7714,  1.7270,  2.0649,  1.7118,  2.6220,  1.2906,  2.4002]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.6709,  2.4593,  2.1805, -0.8857,  2.0762,  1.2898,  1.7469, -0.7037],\n",
      "        [ 1.4510,  2.5368,  1.5957, -0.6398,  1.4603,  1.5268,  1.4447,  2.1155],\n",
      "        [ 2.5337,  1.6957,  1.5521,  1.9655,  1.4487,  2.2644,  2.1335,  2.5521],\n",
      "        [ 2.3598,  2.3681,  2.0446,  2.3863,  1.6644,  1.7847,  1.6557,  2.1612],\n",
      "        [-0.6326,  1.4069,  1.6679,  2.5698,  1.9100,  1.5823,  1.9040,  1.4425],\n",
      "        [-0.6888,  2.2252,  1.4039,  1.5100,  1.2782,  2.7657,  1.2898,  1.5560],\n",
      "        [ 1.3918,  1.4874,  1.8009,  2.0053,  1.5559,  2.3180,  1.4278,  1.4572],\n",
      "        [-0.6397,  1.7821,  1.7379,  2.0749,  1.7228,  2.6311,  1.3044,  2.4096]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.6709,  2.4593,  2.1805, -0.8857,  2.0762,  1.2898,  1.7469, -0.7037],\n",
      "        [ 1.4510,  2.5368,  1.5957, -0.6398,  1.4603,  1.5268,  1.4447,  2.1155],\n",
      "        [ 2.5337,  1.6957,  1.5521,  1.9655,  1.4487,  2.2644,  2.1335,  2.5521],\n",
      "        [ 2.3598,  2.3681,  2.0446,  2.3863,  1.6644,  1.7847,  1.6557,  2.1612],\n",
      "        [-0.6326,  1.4069,  1.6679,  2.5698,  1.9100,  1.5823,  1.9040,  1.4425],\n",
      "        [-0.6888,  2.2252,  1.4039,  1.5100,  1.2782,  2.7657,  1.2898,  1.5560],\n",
      "        [ 1.3918,  1.4874,  1.8009,  2.0053,  1.5559,  2.3180,  1.4278,  1.4572],\n",
      "        [-0.6397,  1.7821,  1.7379,  2.0749,  1.7228,  2.6311,  1.3044,  2.4096]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.6800,  2.4686,  2.1902, -0.8984,  2.0862,  1.3038,  1.7578, -0.7179],\n",
      "        [ 1.4634,  2.5460,  1.6072, -0.6548,  1.4726,  1.5386,  1.4571,  2.1254],\n",
      "        [ 2.5430,  1.7067,  1.5638,  1.9757,  1.4611,  2.2740,  2.1433,  2.5614],\n",
      "        [ 2.3693,  2.3775,  2.0546,  2.3958,  1.6755,  1.7954,  1.6669,  2.1710],\n",
      "        [-0.6476,  1.4196,  1.6791,  2.5790,  1.9204,  1.5938,  1.9144,  1.4549],\n",
      "        [-0.7032,  2.2349,  1.4166,  1.5219,  1.2924,  2.7747,  1.3038,  1.5676],\n",
      "        [ 1.4046,  1.4995,  1.8116,  2.0154,  1.5676,  2.3276,  1.4403,  1.4695],\n",
      "        [-0.6547,  1.7928,  1.7488,  2.0849,  1.7337,  2.6403,  1.3182,  2.4191]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.6800,  2.4686,  2.1902, -0.8984,  2.0862,  1.3038,  1.7578, -0.7179],\n",
      "        [ 1.4634,  2.5460,  1.6072, -0.6548,  1.4726,  1.5386,  1.4571,  2.1254],\n",
      "        [ 2.5430,  1.7067,  1.5638,  1.9757,  1.4611,  2.2740,  2.1433,  2.5614],\n",
      "        [ 2.3693,  2.3775,  2.0546,  2.3958,  1.6755,  1.7954,  1.6669,  2.1710],\n",
      "        [-0.6476,  1.4196,  1.6791,  2.5790,  1.9204,  1.5938,  1.9144,  1.4549],\n",
      "        [-0.7032,  2.2349,  1.4166,  1.5219,  1.2924,  2.7747,  1.3038,  1.5676],\n",
      "        [ 1.4046,  1.4995,  1.8116,  2.0154,  1.5676,  2.3276,  1.4403,  1.4695],\n",
      "        [-0.6547,  1.7928,  1.7488,  2.0849,  1.7337,  2.6403,  1.3182,  2.4191]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.6891,  2.4780,  2.2000, -0.9111,  2.0961,  1.3178,  1.7686, -0.7322],\n",
      "        [ 1.4757,  2.5553,  1.6187, -0.6698,  1.4848,  1.5504,  1.4695,  2.1353],\n",
      "        [ 2.5522,  1.7177,  1.5755,  1.9858,  1.4735,  2.2836,  2.1532,  2.5706],\n",
      "        [ 2.3787,  2.3870,  2.0646,  2.4052,  1.6867,  1.8061,  1.6781,  2.1808],\n",
      "        [-0.6627,  1.4323,  1.6902,  2.5882,  1.9307,  1.6054,  1.9247,  1.4674],\n",
      "        [-0.7176,  2.2446,  1.4293,  1.5339,  1.3065,  2.7837,  1.3178,  1.5793],\n",
      "        [ 1.4175,  1.5116,  1.8222,  2.0255,  1.5793,  2.3371,  1.4529,  1.4818],\n",
      "        [-0.6697,  1.8035,  1.7596,  2.0948,  1.7446,  2.6494,  1.3320,  2.4284]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.6891,  2.4780,  2.2000, -0.9111,  2.0961,  1.3178,  1.7686, -0.7322],\n",
      "        [ 1.4757,  2.5553,  1.6187, -0.6698,  1.4848,  1.5504,  1.4695,  2.1353],\n",
      "        [ 2.5522,  1.7177,  1.5755,  1.9858,  1.4735,  2.2836,  2.1532,  2.5706],\n",
      "        [ 2.3787,  2.3870,  2.0646,  2.4052,  1.6867,  1.8061,  1.6781,  2.1808],\n",
      "        [-0.6627,  1.4323,  1.6902,  2.5882,  1.9307,  1.6054,  1.9247,  1.4674],\n",
      "        [-0.7176,  2.2446,  1.4293,  1.5339,  1.3065,  2.7837,  1.3178,  1.5793],\n",
      "        [ 1.4175,  1.5116,  1.8222,  2.0255,  1.5793,  2.3371,  1.4529,  1.4818],\n",
      "        [-0.6697,  1.8035,  1.7596,  2.0948,  1.7446,  2.6494,  1.3320,  2.4284]],\n",
      "       requires_grad=True)\n",
      "adj grad None\n",
      "x grad tensor([[ 4.1255e-03,  5.6486e-03,  3.4330e-01,  ..., -1.9802e-01,\n",
      "         -1.7938e-01, -2.9459e-01],\n",
      "        [ 6.5486e-04,  2.6171e-03,  1.5889e-02,  ...,  8.0785e-03,\n",
      "          2.8308e-03,  5.5990e-03],\n",
      "        [ 1.3034e-04,  3.9236e-05,  4.1168e-03,  ...,  1.8827e-03,\n",
      "          5.9122e-05,  1.2444e-03],\n",
      "        ...,\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00]])\n",
      "Parameter containing:\n",
      "tensor([[ 2.6982,  2.4873,  2.2097, -0.9239,  2.1061,  1.3318,  1.7794, -0.7465],\n",
      "        [ 1.4881,  2.5645,  1.6301, -0.6848,  1.4971,  1.5623,  1.4819,  2.1451],\n",
      "        [ 2.5614,  1.7287,  1.5872,  1.9960,  1.4858,  2.2932,  2.1630,  2.5798],\n",
      "        [ 2.3882,  2.3964,  2.0746,  2.4146,  1.6978,  1.8168,  1.6893,  2.1906],\n",
      "        [-0.6778,  1.4451,  1.7013,  2.5974,  1.9410,  1.6169,  1.9351,  1.4798],\n",
      "        [-0.7320,  2.2542,  1.4421,  1.5458,  1.3207,  2.7927,  1.3318,  1.5910],\n",
      "        [ 1.4303,  1.5236,  1.8329,  2.0356,  1.5910,  2.3466,  1.4654,  1.4941],\n",
      "        [-0.6847,  1.8142,  1.7705,  2.1048,  1.7555,  2.6585,  1.3459,  2.4378]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.6982,  2.4873,  2.2097, -0.9239,  2.1061,  1.3318,  1.7794, -0.7465],\n",
      "        [ 1.4881,  2.5645,  1.6301, -0.6848,  1.4971,  1.5623,  1.4819,  2.1451],\n",
      "        [ 2.5614,  1.7287,  1.5872,  1.9960,  1.4858,  2.2932,  2.1630,  2.5798],\n",
      "        [ 2.3882,  2.3964,  2.0746,  2.4146,  1.6978,  1.8168,  1.6893,  2.1906],\n",
      "        [-0.6778,  1.4451,  1.7013,  2.5974,  1.9410,  1.6169,  1.9351,  1.4798],\n",
      "        [-0.7320,  2.2542,  1.4421,  1.5458,  1.3207,  2.7927,  1.3318,  1.5910],\n",
      "        [ 1.4303,  1.5236,  1.8329,  2.0356,  1.5910,  2.3466,  1.4654,  1.4941],\n",
      "        [-0.6847,  1.8142,  1.7705,  2.1048,  1.7555,  2.6585,  1.3459,  2.4378]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.7072,  2.4966,  2.2194, -0.9366,  2.1160,  1.3458,  1.7902, -0.7607],\n",
      "        [ 1.5004,  2.5737,  1.6416, -0.6998,  1.5094,  1.5741,  1.4943,  2.1549],\n",
      "        [ 2.5706,  1.7398,  1.5989,  2.0062,  1.4982,  2.3027,  2.1728,  2.5889],\n",
      "        [ 2.3976,  2.4058,  2.0846,  2.4240,  1.7090,  1.8275,  1.7005,  2.2003],\n",
      "        [-0.6929,  1.4578,  1.7125,  2.6065,  1.9513,  1.6284,  1.9454,  1.4922],\n",
      "        [-0.7465,  2.2639,  1.4548,  1.5578,  1.3348,  2.8017,  1.3458,  1.6027],\n",
      "        [ 1.4432,  1.5357,  1.8435,  2.0457,  1.6026,  2.3561,  1.4779,  1.5064],\n",
      "        [-0.6997,  1.8249,  1.7813,  2.1147,  1.7664,  2.6676,  1.3596,  2.4472]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.7072,  2.4966,  2.2194, -0.9366,  2.1160,  1.3458,  1.7902, -0.7607],\n",
      "        [ 1.5004,  2.5737,  1.6416, -0.6998,  1.5094,  1.5741,  1.4943,  2.1549],\n",
      "        [ 2.5706,  1.7398,  1.5989,  2.0062,  1.4982,  2.3027,  2.1728,  2.5889],\n",
      "        [ 2.3976,  2.4058,  2.0846,  2.4240,  1.7090,  1.8275,  1.7005,  2.2003],\n",
      "        [-0.6929,  1.4578,  1.7125,  2.6065,  1.9513,  1.6284,  1.9454,  1.4922],\n",
      "        [-0.7465,  2.2639,  1.4548,  1.5578,  1.3348,  2.8017,  1.3458,  1.6027],\n",
      "        [ 1.4432,  1.5357,  1.8435,  2.0457,  1.6026,  2.3561,  1.4779,  1.5064],\n",
      "        [-0.6997,  1.8249,  1.7813,  2.1147,  1.7664,  2.6676,  1.3596,  2.4472]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.7162,  2.5058,  2.2291, -0.9493,  2.1259,  1.3598,  1.8011, -0.7750],\n",
      "        [ 1.5128,  2.5828,  1.6530, -0.7148,  1.5217,  1.5860,  1.5067,  2.1648],\n",
      "        [ 2.5798,  1.7508,  1.6106,  2.0164,  1.5106,  2.3123,  2.1826,  2.5981],\n",
      "        [ 2.4070,  2.4152,  2.0946,  2.4334,  1.7201,  1.8382,  1.7117,  2.2101],\n",
      "        [-0.7080,  1.4705,  1.7236,  2.6157,  1.9617,  1.6400,  1.9557,  1.5046],\n",
      "        [-0.7609,  2.2735,  1.4675,  1.5697,  1.3490,  2.8106,  1.3598,  1.6143],\n",
      "        [ 1.4560,  1.5478,  1.8541,  2.0557,  1.6143,  2.3655,  1.4905,  1.5187],\n",
      "        [-0.7147,  1.8356,  1.7922,  2.1246,  1.7773,  2.6767,  1.3734,  2.4565]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.7162,  2.5058,  2.2291, -0.9493,  2.1259,  1.3598,  1.8011, -0.7750],\n",
      "        [ 1.5128,  2.5828,  1.6530, -0.7148,  1.5217,  1.5860,  1.5067,  2.1648],\n",
      "        [ 2.5798,  1.7508,  1.6106,  2.0164,  1.5106,  2.3123,  2.1826,  2.5981],\n",
      "        [ 2.4070,  2.4152,  2.0946,  2.4334,  1.7201,  1.8382,  1.7117,  2.2101],\n",
      "        [-0.7080,  1.4705,  1.7236,  2.6157,  1.9617,  1.6400,  1.9557,  1.5046],\n",
      "        [-0.7609,  2.2735,  1.4675,  1.5697,  1.3490,  2.8106,  1.3598,  1.6143],\n",
      "        [ 1.4560,  1.5478,  1.8541,  2.0557,  1.6143,  2.3655,  1.4905,  1.5187],\n",
      "        [-0.7147,  1.8356,  1.7922,  2.1246,  1.7773,  2.6767,  1.3734,  2.4565]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.7252,  2.5151,  2.2388, -0.9621,  2.1358,  1.3738,  1.8119, -0.7893],\n",
      "        [ 1.5251,  2.5920,  1.6645, -0.7299,  1.5340,  1.5978,  1.5191,  2.1746],\n",
      "        [ 2.5889,  1.7618,  1.6222,  2.0265,  1.5229,  2.3218,  2.1923,  2.6072],\n",
      "        [ 2.4164,  2.4246,  2.1045,  2.4427,  1.7312,  1.8488,  1.7229,  2.2198],\n",
      "        [-0.7231,  1.4832,  1.7347,  2.6248,  1.9719,  1.6515,  1.9660,  1.5170],\n",
      "        [-0.7754,  2.2831,  1.4803,  1.5816,  1.3631,  2.8195,  1.3737,  1.6260],\n",
      "        [ 1.4688,  1.5599,  1.8647,  2.0658,  1.6260,  2.3750,  1.5030,  1.5310],\n",
      "        [-0.7298,  1.8463,  1.8030,  2.1345,  1.7882,  2.6857,  1.3872,  2.4658]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.7252,  2.5151,  2.2388, -0.9621,  2.1358,  1.3738,  1.8119, -0.7893],\n",
      "        [ 1.5251,  2.5920,  1.6645, -0.7299,  1.5340,  1.5978,  1.5191,  2.1746],\n",
      "        [ 2.5889,  1.7618,  1.6222,  2.0265,  1.5229,  2.3218,  2.1923,  2.6072],\n",
      "        [ 2.4164,  2.4246,  2.1045,  2.4427,  1.7312,  1.8488,  1.7229,  2.2198],\n",
      "        [-0.7231,  1.4832,  1.7347,  2.6248,  1.9719,  1.6515,  1.9660,  1.5170],\n",
      "        [-0.7754,  2.2831,  1.4803,  1.5816,  1.3631,  2.8195,  1.3737,  1.6260],\n",
      "        [ 1.4688,  1.5599,  1.8647,  2.0658,  1.6260,  2.3750,  1.5030,  1.5310],\n",
      "        [-0.7298,  1.8463,  1.8030,  2.1345,  1.7882,  2.6857,  1.3872,  2.4658]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.7342,  2.5243,  2.2485, -0.9749,  2.1456,  1.3877,  1.8227, -0.8036],\n",
      "        [ 1.5375,  2.6011,  1.6760, -0.7449,  1.5462,  1.6096,  1.5315,  2.1844],\n",
      "        [ 2.5981,  1.7728,  1.6339,  2.0366,  1.5353,  2.3314,  2.2021,  2.6163],\n",
      "        [ 2.4258,  2.4339,  2.1145,  2.4520,  1.7424,  1.8595,  1.7340,  2.2295],\n",
      "        [-0.7382,  1.4959,  1.7458,  2.6339,  1.9822,  1.6630,  1.9763,  1.5294],\n",
      "        [-0.7899,  2.2927,  1.4930,  1.5936,  1.3772,  2.8284,  1.3877,  1.6377],\n",
      "        [ 1.4817,  1.5720,  1.8754,  2.0758,  1.6376,  2.3844,  1.5155,  1.5433],\n",
      "        [-0.7448,  1.8570,  1.8139,  2.1444,  1.7991,  2.6947,  1.4010,  2.4751]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.7342,  2.5243,  2.2485, -0.9749,  2.1456,  1.3877,  1.8227, -0.8036],\n",
      "        [ 1.5375,  2.6011,  1.6760, -0.7449,  1.5462,  1.6096,  1.5315,  2.1844],\n",
      "        [ 2.5981,  1.7728,  1.6339,  2.0366,  1.5353,  2.3314,  2.2021,  2.6163],\n",
      "        [ 2.4258,  2.4339,  2.1145,  2.4520,  1.7424,  1.8595,  1.7340,  2.2295],\n",
      "        [-0.7382,  1.4959,  1.7458,  2.6339,  1.9822,  1.6630,  1.9763,  1.5294],\n",
      "        [-0.7899,  2.2927,  1.4930,  1.5936,  1.3772,  2.8284,  1.3877,  1.6377],\n",
      "        [ 1.4817,  1.5720,  1.8754,  2.0758,  1.6376,  2.3844,  1.5155,  1.5433],\n",
      "        [-0.7448,  1.8570,  1.8139,  2.1444,  1.7991,  2.6947,  1.4010,  2.4751]],\n",
      "       requires_grad=True)\n",
      "epoch:  80 ; loss:  1.2109379768371582 ; mask density:  0.001462558633647859 ; pred:  tensor([5.5833e-05, 1.1848e-06, 1.6931e-06, 9.9989e-01, 7.7963e-06, 3.7145e-05,\n",
      "        7.0168e-06], grad_fn=<SoftmaxBackward0>) ; labels equal:  tensor(True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.7432,  2.5335,  2.2581, -0.9876,  2.1555,  1.4017,  1.8335, -0.8179],\n",
      "        [ 1.5498,  2.6102,  1.6874, -0.7599,  1.5585,  1.6215,  1.5439,  2.1941],\n",
      "        [ 2.6072,  1.7837,  1.6456,  2.0468,  1.5477,  2.3409,  2.2118,  2.6254],\n",
      "        [ 2.4351,  2.4433,  2.1244,  2.4613,  1.7535,  1.8701,  1.7452,  2.2392],\n",
      "        [-0.7533,  1.5086,  1.7569,  2.6430,  1.9925,  1.6745,  1.9866,  1.5418],\n",
      "        [-0.8043,  2.3023,  1.5057,  1.6055,  1.3913,  2.8373,  1.4017,  1.6493],\n",
      "        [ 1.4945,  1.5841,  1.8860,  2.0858,  1.6493,  2.3938,  1.5280,  1.5556],\n",
      "        [-0.7598,  1.8676,  1.8247,  2.1542,  1.8100,  2.7038,  1.4148,  2.4844]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.7432,  2.5335,  2.2581, -0.9876,  2.1555,  1.4017,  1.8335, -0.8179],\n",
      "        [ 1.5498,  2.6102,  1.6874, -0.7599,  1.5585,  1.6215,  1.5439,  2.1941],\n",
      "        [ 2.6072,  1.7837,  1.6456,  2.0468,  1.5477,  2.3409,  2.2118,  2.6254],\n",
      "        [ 2.4351,  2.4433,  2.1244,  2.4613,  1.7535,  1.8701,  1.7452,  2.2392],\n",
      "        [-0.7533,  1.5086,  1.7569,  2.6430,  1.9925,  1.6745,  1.9866,  1.5418],\n",
      "        [-0.8043,  2.3023,  1.5057,  1.6055,  1.3913,  2.8373,  1.4017,  1.6493],\n",
      "        [ 1.4945,  1.5841,  1.8860,  2.0858,  1.6493,  2.3938,  1.5280,  1.5556],\n",
      "        [-0.7598,  1.8676,  1.8247,  2.1542,  1.8100,  2.7038,  1.4148,  2.4844]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.7521,  2.5427,  2.2677, -1.0004,  2.1654,  1.4156,  1.8442, -0.8323],\n",
      "        [ 1.5621,  2.6193,  1.6988, -0.7750,  1.5708,  1.6333,  1.5562,  2.2039],\n",
      "        [ 2.6163,  1.7947,  1.6573,  2.0569,  1.5600,  2.3503,  2.2216,  2.6345],\n",
      "        [ 2.4445,  2.4526,  2.1343,  2.4706,  1.7646,  1.8808,  1.7563,  2.2488],\n",
      "        [-0.7684,  1.5212,  1.7680,  2.6520,  2.0027,  1.6860,  1.9969,  1.5542],\n",
      "        [-0.8188,  2.3118,  1.5184,  1.6174,  1.4054,  2.8461,  1.4156,  1.6610],\n",
      "        [ 1.5073,  1.5961,  1.8965,  2.0958,  1.6610,  2.4032,  1.5406,  1.5679],\n",
      "        [-0.7748,  1.8783,  1.8355,  2.1641,  1.8209,  2.7127,  1.4285,  2.4937]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.7521,  2.5427,  2.2677, -1.0004,  2.1654,  1.4156,  1.8442, -0.8323],\n",
      "        [ 1.5621,  2.6193,  1.6988, -0.7750,  1.5708,  1.6333,  1.5562,  2.2039],\n",
      "        [ 2.6163,  1.7947,  1.6573,  2.0569,  1.5600,  2.3503,  2.2216,  2.6345],\n",
      "        [ 2.4445,  2.4526,  2.1343,  2.4706,  1.7646,  1.8808,  1.7563,  2.2488],\n",
      "        [-0.7684,  1.5212,  1.7680,  2.6520,  2.0027,  1.6860,  1.9969,  1.5542],\n",
      "        [-0.8188,  2.3118,  1.5184,  1.6174,  1.4054,  2.8461,  1.4156,  1.6610],\n",
      "        [ 1.5073,  1.5961,  1.8965,  2.0958,  1.6610,  2.4032,  1.5406,  1.5679],\n",
      "        [-0.7748,  1.8783,  1.8355,  2.1641,  1.8209,  2.7127,  1.4285,  2.4937]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.7610,  2.5519,  2.2774, -1.0132,  2.1752,  1.4296,  1.8550, -0.8466],\n",
      "        [ 1.5745,  2.6284,  1.7103, -0.7900,  1.5830,  1.6451,  1.5686,  2.2137],\n",
      "        [ 2.6254,  1.8057,  1.6689,  2.0669,  1.5724,  2.3598,  2.2313,  2.6436],\n",
      "        [ 2.4538,  2.4619,  2.1442,  2.4799,  1.7757,  1.8914,  1.7675,  2.2585],\n",
      "        [-0.7835,  1.5339,  1.7791,  2.6610,  2.0130,  1.6975,  2.0071,  1.5666],\n",
      "        [-0.8333,  2.3213,  1.5311,  1.6293,  1.4195,  2.8549,  1.4295,  1.6726],\n",
      "        [ 1.5201,  1.6082,  1.9071,  2.1058,  1.6726,  2.4126,  1.5531,  1.5801],\n",
      "        [-0.7899,  1.8889,  1.8463,  2.1739,  1.8317,  2.7217,  1.4423,  2.5029]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.7610,  2.5519,  2.2774, -1.0132,  2.1752,  1.4296,  1.8550, -0.8466],\n",
      "        [ 1.5745,  2.6284,  1.7103, -0.7900,  1.5830,  1.6451,  1.5686,  2.2137],\n",
      "        [ 2.6254,  1.8057,  1.6689,  2.0669,  1.5724,  2.3598,  2.2313,  2.6436],\n",
      "        [ 2.4538,  2.4619,  2.1442,  2.4799,  1.7757,  1.8914,  1.7675,  2.2585],\n",
      "        [-0.7835,  1.5339,  1.7791,  2.6610,  2.0130,  1.6975,  2.0071,  1.5666],\n",
      "        [-0.8333,  2.3213,  1.5311,  1.6293,  1.4195,  2.8549,  1.4295,  1.6726],\n",
      "        [ 1.5201,  1.6082,  1.9071,  2.1058,  1.6726,  2.4126,  1.5531,  1.5801],\n",
      "        [-0.7899,  1.8889,  1.8463,  2.1739,  1.8317,  2.7217,  1.4423,  2.5029]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.7699,  2.5610,  2.2870, -1.0259,  2.1850,  1.4435,  1.8658, -0.8609],\n",
      "        [ 1.5868,  2.6375,  1.7217, -0.8050,  1.5953,  1.6569,  1.5810,  2.2234],\n",
      "        [ 2.6344,  1.8167,  1.6806,  2.0770,  1.5847,  2.3693,  2.2410,  2.6526],\n",
      "        [ 2.4631,  2.4712,  2.1541,  2.4892,  1.7868,  1.9020,  1.7786,  2.2681],\n",
      "        [-0.7986,  1.5466,  1.7902,  2.6701,  2.0232,  1.7090,  2.0174,  1.5790],\n",
      "        [-0.8478,  2.3309,  1.5438,  1.6413,  1.4336,  2.8637,  1.4435,  1.6843],\n",
      "        [ 1.5329,  1.6203,  1.9177,  2.1158,  1.6842,  2.4220,  1.5656,  1.5924],\n",
      "        [-0.8049,  1.8996,  1.8571,  2.1837,  1.8426,  2.7306,  1.4560,  2.5121]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.7699,  2.5610,  2.2870, -1.0259,  2.1850,  1.4435,  1.8658, -0.8609],\n",
      "        [ 1.5868,  2.6375,  1.7217, -0.8050,  1.5953,  1.6569,  1.5810,  2.2234],\n",
      "        [ 2.6344,  1.8167,  1.6806,  2.0770,  1.5847,  2.3693,  2.2410,  2.6526],\n",
      "        [ 2.4631,  2.4712,  2.1541,  2.4892,  1.7868,  1.9020,  1.7786,  2.2681],\n",
      "        [-0.7986,  1.5466,  1.7902,  2.6701,  2.0232,  1.7090,  2.0174,  1.5790],\n",
      "        [-0.8478,  2.3309,  1.5438,  1.6413,  1.4336,  2.8637,  1.4435,  1.6843],\n",
      "        [ 1.5329,  1.6203,  1.9177,  2.1158,  1.6842,  2.4220,  1.5656,  1.5924],\n",
      "        [-0.8049,  1.8996,  1.8571,  2.1837,  1.8426,  2.7306,  1.4560,  2.5121]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.7788,  2.5702,  2.2965, -1.0387,  2.1948,  1.4574,  1.8765, -0.8752],\n",
      "        [ 1.5991,  2.6465,  1.7331, -0.8200,  1.6075,  1.6687,  1.5933,  2.2331],\n",
      "        [ 2.6435,  1.8276,  1.6922,  2.0871,  1.5970,  2.3787,  2.2506,  2.6616],\n",
      "        [ 2.4723,  2.4804,  2.1640,  2.4984,  1.7979,  1.9126,  1.7897,  2.2777],\n",
      "        [-0.8137,  1.5593,  1.8013,  2.6791,  2.0334,  1.7205,  2.0276,  1.5914],\n",
      "        [-0.8622,  2.3404,  1.5565,  1.6532,  1.4476,  2.8725,  1.4574,  1.6959],\n",
      "        [ 1.5457,  1.6323,  1.9282,  2.1258,  1.6959,  2.4313,  1.5781,  1.6047],\n",
      "        [-0.8199,  1.9102,  1.8679,  2.1935,  1.8534,  2.7396,  1.4697,  2.5213]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.7788,  2.5702,  2.2965, -1.0387,  2.1948,  1.4574,  1.8765, -0.8752],\n",
      "        [ 1.5991,  2.6465,  1.7331, -0.8200,  1.6075,  1.6687,  1.5933,  2.2331],\n",
      "        [ 2.6435,  1.8276,  1.6922,  2.0871,  1.5970,  2.3787,  2.2506,  2.6616],\n",
      "        [ 2.4723,  2.4804,  2.1640,  2.4984,  1.7979,  1.9126,  1.7897,  2.2777],\n",
      "        [-0.8137,  1.5593,  1.8013,  2.6791,  2.0334,  1.7205,  2.0276,  1.5914],\n",
      "        [-0.8622,  2.3404,  1.5565,  1.6532,  1.4476,  2.8725,  1.4574,  1.6959],\n",
      "        [ 1.5457,  1.6323,  1.9282,  2.1258,  1.6959,  2.4313,  1.5781,  1.6047],\n",
      "        [-0.8199,  1.9102,  1.8679,  2.1935,  1.8534,  2.7396,  1.4697,  2.5213]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.7877,  2.5793,  2.3061, -1.0515,  2.2046,  1.4713,  1.8872, -0.8895],\n",
      "        [ 1.6114,  2.6555,  1.7445, -0.8350,  1.6197,  1.6805,  1.6057,  2.2428],\n",
      "        [ 2.6525,  1.8385,  1.7039,  2.0971,  1.6093,  2.3881,  2.2603,  2.6706],\n",
      "        [ 2.4816,  2.4897,  2.1738,  2.5076,  1.8089,  1.9232,  1.8008,  2.2873],\n",
      "        [-0.8288,  1.5719,  1.8123,  2.6880,  2.0436,  1.7320,  2.0378,  1.6037],\n",
      "        [-0.8767,  2.3498,  1.5692,  1.6651,  1.4616,  2.8813,  1.4713,  1.7075],\n",
      "        [ 1.5585,  1.6444,  1.9388,  2.1357,  1.7075,  2.4406,  1.5905,  1.6169],\n",
      "        [-0.8349,  1.9208,  1.8786,  2.2033,  1.8642,  2.7485,  1.4835,  2.5305]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.7877,  2.5793,  2.3061, -1.0515,  2.2046,  1.4713,  1.8872, -0.8895],\n",
      "        [ 1.6114,  2.6555,  1.7445, -0.8350,  1.6197,  1.6805,  1.6057,  2.2428],\n",
      "        [ 2.6525,  1.8385,  1.7039,  2.0971,  1.6093,  2.3881,  2.2603,  2.6706],\n",
      "        [ 2.4816,  2.4897,  2.1738,  2.5076,  1.8089,  1.9232,  1.8008,  2.2873],\n",
      "        [-0.8288,  1.5719,  1.8123,  2.6880,  2.0436,  1.7320,  2.0378,  1.6037],\n",
      "        [-0.8767,  2.3498,  1.5692,  1.6651,  1.4616,  2.8813,  1.4713,  1.7075],\n",
      "        [ 1.5585,  1.6444,  1.9388,  2.1357,  1.7075,  2.4406,  1.5905,  1.6169],\n",
      "        [-0.8349,  1.9208,  1.8786,  2.2033,  1.8642,  2.7485,  1.4835,  2.5305]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.7965,  2.5884,  2.3157, -1.0643,  2.2143,  1.4851,  1.8980, -0.9038],\n",
      "        [ 1.6237,  2.6645,  1.7559, -0.8500,  1.6320,  1.6923,  1.6180,  2.2525],\n",
      "        [ 2.6615,  1.8495,  1.7155,  2.1072,  1.6216,  2.3975,  2.2699,  2.6796],\n",
      "        [ 2.4908,  2.4989,  2.1837,  2.5168,  1.8200,  1.9338,  1.8119,  2.2969],\n",
      "        [-0.8439,  1.5846,  1.8234,  2.6970,  2.0538,  1.7434,  2.0480,  1.6161],\n",
      "        [-0.8912,  2.3593,  1.5819,  1.6769,  1.4756,  2.8900,  1.4851,  1.7191],\n",
      "        [ 1.5713,  1.6564,  1.9493,  2.1456,  1.7191,  2.4499,  1.6030,  1.6292],\n",
      "        [-0.8499,  1.9314,  1.8894,  2.2131,  1.8751,  2.7573,  1.4971,  2.5397]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.7965,  2.5884,  2.3157, -1.0643,  2.2143,  1.4851,  1.8980, -0.9038],\n",
      "        [ 1.6237,  2.6645,  1.7559, -0.8500,  1.6320,  1.6923,  1.6180,  2.2525],\n",
      "        [ 2.6615,  1.8495,  1.7155,  2.1072,  1.6216,  2.3975,  2.2699,  2.6796],\n",
      "        [ 2.4908,  2.4989,  2.1837,  2.5168,  1.8200,  1.9338,  1.8119,  2.2969],\n",
      "        [-0.8439,  1.5846,  1.8234,  2.6970,  2.0538,  1.7434,  2.0480,  1.6161],\n",
      "        [-0.8912,  2.3593,  1.5819,  1.6769,  1.4756,  2.8900,  1.4851,  1.7191],\n",
      "        [ 1.5713,  1.6564,  1.9493,  2.1456,  1.7191,  2.4499,  1.6030,  1.6292],\n",
      "        [-0.8499,  1.9314,  1.8894,  2.2131,  1.8751,  2.7573,  1.4971,  2.5397]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.8053,  2.5975,  2.3252, -1.0770,  2.2241,  1.4990,  1.9087, -0.9181],\n",
      "        [ 1.6359,  2.6735,  1.7673, -0.8650,  1.6442,  1.7040,  1.6303,  2.2621],\n",
      "        [ 2.6705,  1.8604,  1.7271,  2.1172,  1.6339,  2.4069,  2.2796,  2.6885],\n",
      "        [ 2.5001,  2.5081,  2.1935,  2.5260,  1.8311,  1.9444,  1.8230,  2.3065],\n",
      "        [-0.8590,  1.5972,  1.8344,  2.7059,  2.0640,  1.7549,  2.0582,  1.6284],\n",
      "        [-0.9056,  2.3688,  1.5945,  1.6888,  1.4896,  2.8987,  1.4990,  1.7307],\n",
      "        [ 1.5841,  1.6684,  1.9598,  2.1555,  1.7307,  2.4592,  1.6155,  1.6414],\n",
      "        [-0.8649,  1.9420,  1.9001,  2.2228,  1.8859,  2.7662,  1.5108,  2.5488]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.8053,  2.5975,  2.3252, -1.0770,  2.2241,  1.4990,  1.9087, -0.9181],\n",
      "        [ 1.6359,  2.6735,  1.7673, -0.8650,  1.6442,  1.7040,  1.6303,  2.2621],\n",
      "        [ 2.6705,  1.8604,  1.7271,  2.1172,  1.6339,  2.4069,  2.2796,  2.6885],\n",
      "        [ 2.5001,  2.5081,  2.1935,  2.5260,  1.8311,  1.9444,  1.8230,  2.3065],\n",
      "        [-0.8590,  1.5972,  1.8344,  2.7059,  2.0640,  1.7549,  2.0582,  1.6284],\n",
      "        [-0.9056,  2.3688,  1.5945,  1.6888,  1.4896,  2.8987,  1.4990,  1.7307],\n",
      "        [ 1.5841,  1.6684,  1.9598,  2.1555,  1.7307,  2.4592,  1.6155,  1.6414],\n",
      "        [-0.8649,  1.9420,  1.9001,  2.2228,  1.8859,  2.7662,  1.5108,  2.5488]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.8141,  2.6065,  2.3347, -1.0898,  2.2338,  1.5128,  1.9194, -0.9324],\n",
      "        [ 1.6482,  2.6824,  1.7786, -0.8800,  1.6564,  1.7158,  1.6427,  2.2718],\n",
      "        [ 2.6794,  1.8713,  1.7387,  2.1272,  1.6462,  2.4162,  2.2892,  2.6975],\n",
      "        [ 2.5093,  2.5173,  2.2033,  2.5352,  1.8421,  1.9550,  1.8341,  2.3161],\n",
      "        [-0.8741,  1.6098,  1.8454,  2.7148,  2.0741,  1.7663,  2.0684,  1.6408],\n",
      "        [-0.9201,  2.3782,  1.6072,  1.7007,  1.5036,  2.9074,  1.5128,  1.7423],\n",
      "        [ 1.5968,  1.6804,  1.9703,  2.1654,  1.7423,  2.4685,  1.6279,  1.6536],\n",
      "        [-0.8799,  1.9525,  1.9109,  2.2326,  1.8966,  2.7750,  1.5245,  2.5579]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.8141,  2.6065,  2.3347, -1.0898,  2.2338,  1.5128,  1.9194, -0.9324],\n",
      "        [ 1.6482,  2.6824,  1.7786, -0.8800,  1.6564,  1.7158,  1.6427,  2.2718],\n",
      "        [ 2.6794,  1.8713,  1.7387,  2.1272,  1.6462,  2.4162,  2.2892,  2.6975],\n",
      "        [ 2.5093,  2.5173,  2.2033,  2.5352,  1.8421,  1.9550,  1.8341,  2.3161],\n",
      "        [-0.8741,  1.6098,  1.8454,  2.7148,  2.0741,  1.7663,  2.0684,  1.6408],\n",
      "        [-0.9201,  2.3782,  1.6072,  1.7007,  1.5036,  2.9074,  1.5128,  1.7423],\n",
      "        [ 1.5968,  1.6804,  1.9703,  2.1654,  1.7423,  2.4685,  1.6279,  1.6536],\n",
      "        [-0.8799,  1.9525,  1.9109,  2.2326,  1.8966,  2.7750,  1.5245,  2.5579]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.8229,  2.6155,  2.3442, -1.1026,  2.2435,  1.5266,  1.9300, -0.9467],\n",
      "        [ 1.6605,  2.6914,  1.7900, -0.8950,  1.6686,  1.7275,  1.6550,  2.2814],\n",
      "        [ 2.6883,  1.8822,  1.7503,  2.1372,  1.6585,  2.4256,  2.2988,  2.7064],\n",
      "        [ 2.5184,  2.5265,  2.2131,  2.5443,  1.8531,  1.9655,  1.8451,  2.3256],\n",
      "        [-0.8891,  1.6224,  1.8564,  2.7237,  2.0843,  1.7777,  2.0785,  1.6531],\n",
      "        [-0.9345,  2.3876,  1.6198,  1.7125,  1.5176,  2.9161,  1.5266,  1.7539],\n",
      "        [ 1.6096,  1.6924,  1.9808,  2.1753,  1.7538,  2.4777,  1.6403,  1.6658],\n",
      "        [-0.8949,  1.9631,  1.9216,  2.2423,  1.9074,  2.7839,  1.5381,  2.5671]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.8229,  2.6155,  2.3442, -1.1026,  2.2435,  1.5266,  1.9300, -0.9467],\n",
      "        [ 1.6605,  2.6914,  1.7900, -0.8950,  1.6686,  1.7275,  1.6550,  2.2814],\n",
      "        [ 2.6883,  1.8822,  1.7503,  2.1372,  1.6585,  2.4256,  2.2988,  2.7064],\n",
      "        [ 2.5184,  2.5265,  2.2131,  2.5443,  1.8531,  1.9655,  1.8451,  2.3256],\n",
      "        [-0.8891,  1.6224,  1.8564,  2.7237,  2.0843,  1.7777,  2.0785,  1.6531],\n",
      "        [-0.9345,  2.3876,  1.6198,  1.7125,  1.5176,  2.9161,  1.5266,  1.7539],\n",
      "        [ 1.6096,  1.6924,  1.9808,  2.1753,  1.7538,  2.4777,  1.6403,  1.6658],\n",
      "        [-0.8949,  1.9631,  1.9216,  2.2423,  1.9074,  2.7839,  1.5381,  2.5671]],\n",
      "       requires_grad=True)\n",
      "epoch:  90 ; loss:  1.1669013500213623 ; mask density:  0.001476456644013524 ; pred:  tensor([2.2477e-07, 1.3604e-06, 2.8781e-09, 9.9999e-01, 4.0263e-06, 4.1746e-07,\n",
      "        3.3278e-09], grad_fn=<SoftmaxBackward0>) ; labels equal:  tensor(True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.8316,  2.6246,  2.3537, -1.1153,  2.2532,  1.5404,  1.9407, -0.9610],\n",
      "        [ 1.6727,  2.7003,  1.8013, -0.9100,  1.6807,  1.7393,  1.6672,  2.2910],\n",
      "        [ 2.6973,  1.8930,  1.7619,  2.1471,  1.6707,  2.4349,  2.3083,  2.7153],\n",
      "        [ 2.5276,  2.5356,  2.2228,  2.5534,  1.8641,  1.9760,  1.8562,  2.3351],\n",
      "        [-0.9042,  1.6350,  1.8674,  2.7326,  2.0944,  1.7891,  2.0887,  1.6654],\n",
      "        [-0.9490,  2.3970,  1.6324,  1.7243,  1.5315,  2.9248,  1.5404,  1.7654],\n",
      "        [ 1.6223,  1.7044,  1.9913,  2.1852,  1.7654,  2.4870,  1.6528,  1.6780],\n",
      "        [-0.9099,  1.9736,  1.9323,  2.2520,  1.9182,  2.7927,  1.5517,  2.5761]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.8316,  2.6246,  2.3537, -1.1153,  2.2532,  1.5404,  1.9407, -0.9610],\n",
      "        [ 1.6727,  2.7003,  1.8013, -0.9100,  1.6807,  1.7393,  1.6672,  2.2910],\n",
      "        [ 2.6973,  1.8930,  1.7619,  2.1471,  1.6707,  2.4349,  2.3083,  2.7153],\n",
      "        [ 2.5276,  2.5356,  2.2228,  2.5534,  1.8641,  1.9760,  1.8562,  2.3351],\n",
      "        [-0.9042,  1.6350,  1.8674,  2.7326,  2.0944,  1.7891,  2.0887,  1.6654],\n",
      "        [-0.9490,  2.3970,  1.6324,  1.7243,  1.5315,  2.9248,  1.5404,  1.7654],\n",
      "        [ 1.6223,  1.7044,  1.9913,  2.1852,  1.7654,  2.4870,  1.6528,  1.6780],\n",
      "        [-0.9099,  1.9736,  1.9323,  2.2520,  1.9182,  2.7927,  1.5517,  2.5761]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.8404,  2.6335,  2.3631, -1.1281,  2.2629,  1.5542,  1.9514, -0.9753],\n",
      "        [ 1.6849,  2.7092,  1.8127, -0.9249,  1.6929,  1.7510,  1.6795,  2.3006],\n",
      "        [ 2.7061,  1.9039,  1.7735,  2.1571,  1.6830,  2.4442,  2.3179,  2.7241],\n",
      "        [ 2.5367,  2.5448,  2.2326,  2.5625,  1.8751,  1.9865,  1.8672,  2.3446],\n",
      "        [-0.9192,  1.6476,  1.8784,  2.7414,  2.1045,  1.8005,  2.0988,  1.6777],\n",
      "        [-0.9634,  2.4064,  1.6450,  1.7362,  1.5454,  2.9334,  1.5542,  1.7770],\n",
      "        [ 1.6350,  1.7164,  2.0017,  2.1950,  1.7769,  2.4962,  1.6652,  1.6902],\n",
      "        [-0.9248,  1.9841,  1.9430,  2.2617,  1.9289,  2.8014,  1.5653,  2.5852]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.8404,  2.6335,  2.3631, -1.1281,  2.2629,  1.5542,  1.9514, -0.9753],\n",
      "        [ 1.6849,  2.7092,  1.8127, -0.9249,  1.6929,  1.7510,  1.6795,  2.3006],\n",
      "        [ 2.7061,  1.9039,  1.7735,  2.1571,  1.6830,  2.4442,  2.3179,  2.7241],\n",
      "        [ 2.5367,  2.5448,  2.2326,  2.5625,  1.8751,  1.9865,  1.8672,  2.3446],\n",
      "        [-0.9192,  1.6476,  1.8784,  2.7414,  2.1045,  1.8005,  2.0988,  1.6777],\n",
      "        [-0.9634,  2.4064,  1.6450,  1.7362,  1.5454,  2.9334,  1.5542,  1.7770],\n",
      "        [ 1.6350,  1.7164,  2.0017,  2.1950,  1.7769,  2.4962,  1.6652,  1.6902],\n",
      "        [-0.9248,  1.9841,  1.9430,  2.2617,  1.9289,  2.8014,  1.5653,  2.5852]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.8491,  2.6425,  2.3726, -1.1408,  2.2726,  1.5679,  1.9620, -0.9896],\n",
      "        [ 1.6971,  2.7180,  1.8240, -0.9399,  1.7050,  1.7627,  1.6918,  2.3102],\n",
      "        [ 2.7150,  1.9147,  1.7850,  2.1670,  1.6952,  2.4535,  2.3274,  2.7330],\n",
      "        [ 2.5459,  2.5539,  2.2423,  2.5716,  1.8861,  1.9970,  1.8783,  2.3541],\n",
      "        [-0.9342,  1.6601,  1.8894,  2.7503,  2.1146,  1.8119,  2.1089,  1.6899],\n",
      "        [-0.9778,  2.4157,  1.6576,  1.7480,  1.5593,  2.9420,  1.5679,  1.7885],\n",
      "        [ 1.6477,  1.7283,  2.0122,  2.2049,  1.7885,  2.5054,  1.6775,  1.7024],\n",
      "        [-0.9398,  1.9946,  1.9536,  2.2713,  1.9397,  2.8102,  1.5789,  2.5942]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.8491,  2.6425,  2.3726, -1.1408,  2.2726,  1.5679,  1.9620, -0.9896],\n",
      "        [ 1.6971,  2.7180,  1.8240, -0.9399,  1.7050,  1.7627,  1.6918,  2.3102],\n",
      "        [ 2.7150,  1.9147,  1.7850,  2.1670,  1.6952,  2.4535,  2.3274,  2.7330],\n",
      "        [ 2.5459,  2.5539,  2.2423,  2.5716,  1.8861,  1.9970,  1.8783,  2.3541],\n",
      "        [-0.9342,  1.6601,  1.8894,  2.7503,  2.1146,  1.8119,  2.1089,  1.6899],\n",
      "        [-0.9778,  2.4157,  1.6576,  1.7480,  1.5593,  2.9420,  1.5679,  1.7885],\n",
      "        [ 1.6477,  1.7283,  2.0122,  2.2049,  1.7885,  2.5054,  1.6775,  1.7024],\n",
      "        [-0.9398,  1.9946,  1.9536,  2.2713,  1.9397,  2.8102,  1.5789,  2.5942]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.8578,  2.6515,  2.3820, -1.1536,  2.2822,  1.5817,  1.9726, -1.0038],\n",
      "        [ 1.7093,  2.7269,  1.8353, -0.9548,  1.7172,  1.7744,  1.7040,  2.3197],\n",
      "        [ 2.7239,  1.9256,  1.7966,  2.1769,  1.7074,  2.4627,  2.3369,  2.7418],\n",
      "        [ 2.5550,  2.5629,  2.2520,  2.5807,  1.8971,  2.0075,  1.8893,  2.3635],\n",
      "        [-0.9492,  1.6727,  1.9003,  2.7591,  2.1247,  1.8233,  2.1190,  1.7022],\n",
      "        [-0.9922,  2.4250,  1.6702,  1.7598,  1.5731,  2.9506,  1.5816,  1.8000],\n",
      "        [ 1.6603,  1.7403,  2.0226,  2.2147,  1.8000,  2.5145,  1.6899,  1.7145],\n",
      "        [-0.9547,  2.0051,  1.9643,  2.2810,  1.9504,  2.8189,  1.5924,  2.6033]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.8578,  2.6515,  2.3820, -1.1536,  2.2822,  1.5817,  1.9726, -1.0038],\n",
      "        [ 1.7093,  2.7269,  1.8353, -0.9548,  1.7172,  1.7744,  1.7040,  2.3197],\n",
      "        [ 2.7239,  1.9256,  1.7966,  2.1769,  1.7074,  2.4627,  2.3369,  2.7418],\n",
      "        [ 2.5550,  2.5629,  2.2520,  2.5807,  1.8971,  2.0075,  1.8893,  2.3635],\n",
      "        [-0.9492,  1.6727,  1.9003,  2.7591,  2.1247,  1.8233,  2.1190,  1.7022],\n",
      "        [-0.9922,  2.4250,  1.6702,  1.7598,  1.5731,  2.9506,  1.5816,  1.8000],\n",
      "        [ 1.6603,  1.7403,  2.0226,  2.2147,  1.8000,  2.5145,  1.6899,  1.7145],\n",
      "        [-0.9547,  2.0051,  1.9643,  2.2810,  1.9504,  2.8189,  1.5924,  2.6033]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.8664,  2.6604,  2.3914, -1.1663,  2.2918,  1.5954,  1.9832, -1.0181],\n",
      "        [ 1.7215,  2.7357,  1.8465, -0.9697,  1.7293,  1.7860,  1.7162,  2.3293],\n",
      "        [ 2.7327,  1.9364,  1.8081,  2.1868,  1.7196,  2.4720,  2.3464,  2.7506],\n",
      "        [ 2.5640,  2.5720,  2.2617,  2.5897,  1.9080,  2.0180,  1.9002,  2.3730],\n",
      "        [-0.9642,  1.6852,  1.9112,  2.7678,  2.1347,  1.8347,  2.1291,  1.7144],\n",
      "        [-1.0066,  2.4344,  1.6827,  1.7715,  1.5870,  2.9591,  1.5953,  1.8115],\n",
      "        [ 1.6730,  1.7522,  2.0330,  2.2245,  1.8115,  2.5237,  1.7023,  1.7267],\n",
      "        [-0.9696,  2.0156,  1.9749,  2.2906,  1.9611,  2.8276,  1.6060,  2.6123]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.8664,  2.6604,  2.3914, -1.1663,  2.2918,  1.5954,  1.9832, -1.0181],\n",
      "        [ 1.7215,  2.7357,  1.8465, -0.9697,  1.7293,  1.7860,  1.7162,  2.3293],\n",
      "        [ 2.7327,  1.9364,  1.8081,  2.1868,  1.7196,  2.4720,  2.3464,  2.7506],\n",
      "        [ 2.5640,  2.5720,  2.2617,  2.5897,  1.9080,  2.0180,  1.9002,  2.3730],\n",
      "        [-0.9642,  1.6852,  1.9112,  2.7678,  2.1347,  1.8347,  2.1291,  1.7144],\n",
      "        [-1.0066,  2.4344,  1.6827,  1.7715,  1.5870,  2.9591,  1.5953,  1.8115],\n",
      "        [ 1.6730,  1.7522,  2.0330,  2.2245,  1.8115,  2.5237,  1.7023,  1.7267],\n",
      "        [-0.9696,  2.0156,  1.9749,  2.2906,  1.9611,  2.8276,  1.6060,  2.6123]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.8751,  2.6693,  2.4008, -1.1790,  2.3014,  1.6090,  1.9938, -1.0323],\n",
      "        [ 1.7337,  2.7445,  1.8578, -0.9846,  1.7414,  1.7977,  1.7285,  2.3388],\n",
      "        [ 2.7415,  1.9472,  1.8196,  2.1967,  1.7318,  2.4812,  2.3559,  2.7594],\n",
      "        [ 2.5731,  2.5811,  2.2714,  2.5987,  1.9190,  2.0284,  1.9112,  2.3824],\n",
      "        [-0.9791,  1.6977,  1.9222,  2.7766,  2.1447,  1.8460,  2.1391,  1.7267],\n",
      "        [-1.0210,  2.4436,  1.6953,  1.7833,  1.6008,  2.9677,  1.6090,  1.8230],\n",
      "        [ 1.6856,  1.7641,  2.0434,  2.2343,  1.8230,  2.5328,  1.7146,  1.7388],\n",
      "        [-0.9845,  2.0261,  1.9856,  2.3002,  1.9718,  2.8363,  1.6195,  2.6213]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.8751,  2.6693,  2.4008, -1.1790,  2.3014,  1.6090,  1.9938, -1.0323],\n",
      "        [ 1.7337,  2.7445,  1.8578, -0.9846,  1.7414,  1.7977,  1.7285,  2.3388],\n",
      "        [ 2.7415,  1.9472,  1.8196,  2.1967,  1.7318,  2.4812,  2.3559,  2.7594],\n",
      "        [ 2.5731,  2.5811,  2.2714,  2.5987,  1.9190,  2.0284,  1.9112,  2.3824],\n",
      "        [-0.9791,  1.6977,  1.9222,  2.7766,  2.1447,  1.8460,  2.1391,  1.7267],\n",
      "        [-1.0210,  2.4436,  1.6953,  1.7833,  1.6008,  2.9677,  1.6090,  1.8230],\n",
      "        [ 1.6856,  1.7641,  2.0434,  2.2343,  1.8230,  2.5328,  1.7146,  1.7388],\n",
      "        [-0.9845,  2.0261,  1.9856,  2.3002,  1.9718,  2.8363,  1.6195,  2.6213]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.8837,  2.6782,  2.4101, -1.1917,  2.3110,  1.6227,  2.0044, -1.0465],\n",
      "        [ 1.7458,  2.7533,  1.8691, -0.9994,  1.7534,  1.8093,  1.7406,  2.3483],\n",
      "        [ 2.7503,  1.9580,  1.8311,  2.2066,  1.7440,  2.4904,  2.3654,  2.7682],\n",
      "        [ 2.5821,  2.5901,  2.2811,  2.6077,  1.9299,  2.0389,  1.9222,  2.3918],\n",
      "        [-0.9941,  1.7102,  1.9331,  2.7853,  2.1548,  1.8573,  2.1492,  1.7389],\n",
      "        [-1.0354,  2.4529,  1.7078,  1.7950,  1.6145,  2.9762,  1.6226,  1.8344],\n",
      "        [ 1.6982,  1.7760,  2.0537,  2.2440,  1.8344,  2.5419,  1.7269,  1.7509],\n",
      "        [-0.9993,  2.0365,  1.9962,  2.3098,  1.9824,  2.8450,  1.6329,  2.6302]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.8837,  2.6782,  2.4101, -1.1917,  2.3110,  1.6227,  2.0044, -1.0465],\n",
      "        [ 1.7458,  2.7533,  1.8691, -0.9994,  1.7534,  1.8093,  1.7406,  2.3483],\n",
      "        [ 2.7503,  1.9580,  1.8311,  2.2066,  1.7440,  2.4904,  2.3654,  2.7682],\n",
      "        [ 2.5821,  2.5901,  2.2811,  2.6077,  1.9299,  2.0389,  1.9222,  2.3918],\n",
      "        [-0.9941,  1.7102,  1.9331,  2.7853,  2.1548,  1.8573,  2.1492,  1.7389],\n",
      "        [-1.0354,  2.4529,  1.7078,  1.7950,  1.6145,  2.9762,  1.6226,  1.8344],\n",
      "        [ 1.6982,  1.7760,  2.0537,  2.2440,  1.8344,  2.5419,  1.7269,  1.7509],\n",
      "        [-0.9993,  2.0365,  1.9962,  2.3098,  1.9824,  2.8450,  1.6329,  2.6302]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.8923,  2.6871,  2.4195, -1.2044,  2.3206,  1.6363,  2.0149, -1.0607],\n",
      "        [ 1.7579,  2.7620,  1.8803, -1.0143,  1.7655,  1.8210,  1.7528,  2.3578],\n",
      "        [ 2.7591,  1.9687,  1.8425,  2.2164,  1.7561,  2.4996,  2.3748,  2.7769],\n",
      "        [ 2.5912,  2.5991,  2.2907,  2.6167,  1.9408,  2.0493,  1.9331,  2.4012],\n",
      "        [-1.0090,  1.7227,  1.9440,  2.7941,  2.1648,  1.8686,  2.1592,  1.7511],\n",
      "        [-1.0497,  2.4622,  1.7203,  1.8068,  1.6283,  2.9847,  1.6362,  1.8459],\n",
      "        [ 1.7108,  1.7879,  2.0641,  2.2538,  1.8459,  2.5510,  1.7392,  1.7630],\n",
      "        [-1.0142,  2.0469,  2.0068,  2.3194,  1.9931,  2.8536,  1.6464,  2.6392]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.8923,  2.6871,  2.4195, -1.2044,  2.3206,  1.6363,  2.0149, -1.0607],\n",
      "        [ 1.7579,  2.7620,  1.8803, -1.0143,  1.7655,  1.8210,  1.7528,  2.3578],\n",
      "        [ 2.7591,  1.9687,  1.8425,  2.2164,  1.7561,  2.4996,  2.3748,  2.7769],\n",
      "        [ 2.5912,  2.5991,  2.2907,  2.6167,  1.9408,  2.0493,  1.9331,  2.4012],\n",
      "        [-1.0090,  1.7227,  1.9440,  2.7941,  2.1648,  1.8686,  2.1592,  1.7511],\n",
      "        [-1.0497,  2.4622,  1.7203,  1.8068,  1.6283,  2.9847,  1.6362,  1.8459],\n",
      "        [ 1.7108,  1.7879,  2.0641,  2.2538,  1.8459,  2.5510,  1.7392,  1.7630],\n",
      "        [-1.0142,  2.0469,  2.0068,  2.3194,  1.9931,  2.8536,  1.6464,  2.6392]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.9009,  2.6959,  2.4288, -1.2171,  2.3301,  1.6499,  2.0255, -1.0749],\n",
      "        [ 1.7701,  2.7708,  1.8915, -1.0291,  1.7775,  1.8326,  1.7650,  2.3672],\n",
      "        [ 2.7678,  1.9795,  1.8540,  2.2263,  1.7682,  2.5087,  2.3842,  2.7856],\n",
      "        [ 2.6002,  2.6081,  2.3003,  2.6257,  1.9517,  2.0597,  1.9440,  2.4105],\n",
      "        [-1.0239,  1.7351,  1.9548,  2.8028,  2.1748,  1.8799,  2.1692,  1.7632],\n",
      "        [-1.0640,  2.4714,  1.7327,  1.8185,  1.6420,  2.9932,  1.6498,  1.8573],\n",
      "        [ 1.7234,  1.7997,  2.0744,  2.2635,  1.8573,  2.5601,  1.7515,  1.7750],\n",
      "        [-1.0290,  2.0573,  2.0173,  2.3289,  2.0037,  2.8622,  1.6598,  2.6481]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.9009,  2.6959,  2.4288, -1.2171,  2.3301,  1.6499,  2.0255, -1.0749],\n",
      "        [ 1.7701,  2.7708,  1.8915, -1.0291,  1.7775,  1.8326,  1.7650,  2.3672],\n",
      "        [ 2.7678,  1.9795,  1.8540,  2.2263,  1.7682,  2.5087,  2.3842,  2.7856],\n",
      "        [ 2.6002,  2.6081,  2.3003,  2.6257,  1.9517,  2.0597,  1.9440,  2.4105],\n",
      "        [-1.0239,  1.7351,  1.9548,  2.8028,  2.1748,  1.8799,  2.1692,  1.7632],\n",
      "        [-1.0640,  2.4714,  1.7327,  1.8185,  1.6420,  2.9932,  1.6498,  1.8573],\n",
      "        [ 1.7234,  1.7997,  2.0744,  2.2635,  1.8573,  2.5601,  1.7515,  1.7750],\n",
      "        [-1.0290,  2.0573,  2.0173,  2.3289,  2.0037,  2.8622,  1.6598,  2.6481]],\n",
      "       requires_grad=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.7100, 0.0000, 0.0000, 0.0000, 0.6336, 0.0000, 0.5644],\n",
       "        [0.7100, 0.0000, 0.0000, 0.0000, 0.7011, 0.0000, 0.7018, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7083, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7139],\n",
       "        [0.0000, 0.7011, 0.0000, 0.0000, 0.0000, 0.7011, 0.7071, 0.0000],\n",
       "        [0.6336, 0.0000, 0.7083, 0.0000, 0.7011, 0.0000, 0.0000, 0.7121],\n",
       "        [0.0000, 0.7018, 0.0000, 0.0000, 0.7071, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5644, 0.0000, 0.0000, 0.7139, 0.0000, 0.7121, 0.0000, 0.0000]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "explainer.train()\n",
    "for epoch in range(100):\n",
    "    explainer.zero_grad()\n",
    "    optimizer.zero_grad()\n",
    "    ypred, adj_atts, sub_adj = explainer.forward()\n",
    "    loss = explainer.criterion(epoch)\n",
    "    pred_label, original_label, neighbors, sub_label, sub_feat, num_hops = explainer.return_stuff()\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    loss.backward(retain_graph=True)\n",
    "    optimizer.step()\n",
    "    mask_density = explainer.mask_density()\n",
    "    single_subgraph_label = sub_label.squeeze()\n",
    "    if epoch % 25 == 0:\n",
    "    # #     explainer.log_mask(epoch)\n",
    "    # #     explainer.log_masked_adj(\n",
    "    # #         node_idx_new, epoch, label=single_subgraph_label\n",
    "    # #     )\n",
    "\n",
    "\n",
    "        explainer.log_adj_grad(\n",
    "            node_idx_new, pred_label, epoch, label=single_subgraph_label)\n",
    "\n",
    "\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "\n",
    "        print(\n",
    "        \"epoch: \",\n",
    "        epoch,\n",
    "        \"; loss: \",\n",
    "        loss.item(),\n",
    "        \"; mask density: \",\n",
    "        mask_density.item(),\n",
    "        \"; pred: \",\n",
    "        ypred,\n",
    "        \"; labels equal: \",\n",
    "        torch.argmax(ypred) == original_label== pred_label,\n",
    "\n",
    "    )\n",
    "\n",
    "\n",
    "adj_atts = torch.sigmoid(adj_atts).squeeze()\n",
    "masked_adj = adj_atts * sub_adj.squeeze()\n",
    "masked_adj"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VISUALIZATION \n",
    "We visualize the output according to how strong the model has imputed the connections between the nodes of the neighborhood to be for the sake of prediction of the node of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.71004957, 0.6336027, 0.56436, 0.70113933, 0.7018184, 0.70832366, 0.71390694, 0.70113397, 0.707074, 0.71207166]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ww/33zq_rh50tx94n81lb4thx0w0000gn/T/ipykernel_5019/2378129677.py:24: MatplotlibDeprecationWarning: Unable to determine Axes to steal space for Colorbar. Using gca(), but will raise in the future. Either provide the *cax* argument to use as the Axes for the Colorbar, provide the *ax* argument to steal space from it, or add *mappable* to an Axes.\n",
      "  cbar = plt.colorbar(sm)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def visualize_result(node_idx, masked_adj, neighbors, data, num_hops):\n",
    "    \"\"\"Visualizes the n-hop neighborhood of a given node.\"\"\"\n",
    "    G = nx.from_pandas_adjacency(pd.DataFrame(masked_adj.detach().numpy()))\n",
    "\n",
    "    labeldict = {}\n",
    "    for i,j in zip(range(len(neighbors)),neighbors):\n",
    "        labeldict[i] = j\n",
    "    #print(labeldict)    \n",
    "    a = nx.get_edge_attributes(G,'weight')\n",
    "    weights = {key : round(a[key], 3) for key in a}\n",
    "\n",
    "    edge_colors = [masked_adj.detach().numpy()[u][v] for u, v in G.edges()]\n",
    "    print(edge_colors)\n",
    "    # draw graph with edge colors\n",
    "    plt.figure()  \n",
    "    plt.title(\"Node {}'s {}-hop neighborhood important nodes\".format(node_idx, num_hops))\n",
    "    pos = nx.circular_layout(G)\n",
    "    nx.draw(G, pos=pos, with_labels=True, edge_color=edge_colors, edge_cmap=plt.cm.Reds,labels = labeldict, node_color = data.y[neighbors], cmap=\"Set2\" )\n",
    "    nx.draw_networkx_edge_labels( G, pos,edge_labels=weights,font_size=8,font_color='red')\n",
    "\n",
    "    # add colorbar legend\n",
    "    sm = plt.cm.ScalarMappable(cmap=plt.cm.Reds, norm=plt.Normalize(vmin=0, vmax=1))\n",
    "    sm.set_array(edge_colors)\n",
    "    cbar = plt.colorbar(sm)\n",
    "    cbar.ax.set_title('Weight')\n",
    "\n",
    "    plt.show()  \n",
    "\n",
    "visualize_result(1, masked_adj, neighbors,data,num_hops)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "from torch_geometric.datasets import TUDataset, Planetoid\n",
    "from torch_geometric.nn import GCNConv, Set2Set, GNNExplainer\n",
    "import torch_geometric.transforms as T\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the dataset\n",
    "dataset = 'cora'\n",
    "path = os.path.join(os.getcwd(), 'data', 'Planetoid')\n",
    "train_dataset = Planetoid(path, dataset, transform=T.NormalizeFeatures())\n",
    "\n",
    "# Since the dataset is comprised of a single huge graph, we extract that graph by indexing 0.\n",
    "data = train_dataset[0]\n",
    "\n",
    "# Since there is only 1 graph, the train/test split is done by masking regions of the graph. We split the last 500+500 nodes as val and test, and use the rest as the training data.\n",
    "data.train_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
    "data.train_mask[:data.num_nodes - 1000] = 1\n",
    "data.val_mask = None\n",
    "data.test_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
    "data.test_mask[data.num_nodes - 500:] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, num_features, dim=16, num_classes=1):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = GCNConv(num_features, dim)\n",
    "        self.conv2 = GCNConv(dim, num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index, data=None):\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 200\n",
    "dim = 16\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = Net(num_features=train_dataset.num_features, dim=dim, num_classes=train_dataset.num_classes).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-3)\n",
    "def test(model, data):\n",
    "    model.eval()\n",
    "    logits, accs = model(data.x, data.edge_index, data), []\n",
    "    for _, mask in data('train_mask', 'test_mask'):\n",
    "        pred = logits[mask].max(1)[1]\n",
    "        acc = pred.eq(data.y[mask]).sum().item() / mask.sum().item()\n",
    "        accs.append(acc)\n",
    "    return accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train_loss:1.582890 Train_acc: 0.4274, Test_acc: 0.4380]: 100%|| 200/200 [00:05<00:00, 34.36it/s]\n"
     ]
    }
   ],
   "source": [
    "loss = 999.0\n",
    "train_acc = 0.0\n",
    "test_acc = 0.0\n",
    "\n",
    "t = trange(epochs, desc=\"Stats: \", position=0)\n",
    "\n",
    "for epoch in t:\n",
    "\n",
    "    model.train()\n",
    "    \n",
    "    loss = 0\n",
    "\n",
    "    data = data.to(device)\n",
    "    optimizer.zero_grad()\n",
    "    log_logits = model(data.x, data.edge_index, data)\n",
    "\n",
    "    # Since the data is a single huge graph, training on the training set is done by masking the nodes that are not in the training set.\n",
    "    loss = F.nll_loss(log_logits[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # validate\n",
    "    train_acc, test_acc = test(model, data)\n",
    "    train_loss = loss\n",
    "    \n",
    "    t.set_description('[Train_loss:{:.6f} Train_acc: {:.4f}, Test_acc: {:.4f}]'.format(loss, train_acc, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Caskroom/miniconda/base/lib/python3.9/site-packages/torch_geometric/deprecation.py:12: UserWarning: 'nn.models.GNNExplainer' is deprecated, use 'explain.Explainer' with 'explain.algorithm.GNNExplainer' instead\n",
      "  warnings.warn(out)\n"
     ]
    }
   ],
   "source": [
    "node_idx = 0\n",
    "x, edge_index = data.x, data.edge_index\n",
    "explainer = GNNExplainer(model)\n",
    "node_feat_mask, edge_mask = explainer.explain_node(node_idx, x, edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.7415, 0.7423, 0.7553,  ..., 0.0000, 0.0000, 0.0000])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(edge_mask)\n",
    "data.edge_index.shape\n",
    "edge_mask[neighbors]\n",
    "neighbors\n",
    "\n",
    "edge_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "tensors used as indices must be long, byte or bool tensors",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m ypred \u001b[39m=\u001b[39m model(data\u001b[39m.\u001b[39;49mx, edge_mask\u001b[39m.\u001b[39;49munsqueeze(\u001b[39m1\u001b[39;49m), data)\n\u001b[1;32m      2\u001b[0m ypred\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[38], line 8\u001b[0m, in \u001b[0;36mNet.forward\u001b[0;34m(self, x, edge_index, data)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x, edge_index, data\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m----> 8\u001b[0m     x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv1(x, edge_index))\n\u001b[1;32m      9\u001b[0m     x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mdropout(x, training\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining)\n\u001b[1;32m     10\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv2(x, edge_index)\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/lib/python3.9/site-packages/torch_geometric/nn/conv/gcn_conv.py:176\u001b[0m, in \u001b[0;36mGCNConv.forward\u001b[0;34m(self, x, edge_index, edge_weight)\u001b[0m\n\u001b[1;32m    174\u001b[0m cache \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cached_edge_index\n\u001b[1;32m    175\u001b[0m \u001b[39mif\u001b[39;00m cache \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 176\u001b[0m     edge_index, edge_weight \u001b[39m=\u001b[39m gcn_norm(  \u001b[39m# yapf: disable\u001b[39;49;00m\n\u001b[1;32m    177\u001b[0m         edge_index, edge_weight, x\u001b[39m.\u001b[39;49msize(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnode_dim),\n\u001b[1;32m    178\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mimproved, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madd_self_loops, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mflow, x\u001b[39m.\u001b[39;49mdtype)\n\u001b[1;32m    179\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcached:\n\u001b[1;32m    180\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cached_edge_index \u001b[39m=\u001b[39m (edge_index, edge_weight)\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/lib/python3.9/site-packages/torch_geometric/nn/conv/gcn_conv.py:61\u001b[0m, in \u001b[0;36mgcn_norm\u001b[0;34m(edge_index, edge_weight, num_nodes, improved, add_self_loops, flow, dtype)\u001b[0m\n\u001b[1;32m     57\u001b[0m     edge_weight \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mones((edge_index\u001b[39m.\u001b[39msize(\u001b[39m1\u001b[39m), ), dtype\u001b[39m=\u001b[39mdtype,\n\u001b[1;32m     58\u001b[0m                              device\u001b[39m=\u001b[39medge_index\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m     60\u001b[0m \u001b[39mif\u001b[39;00m add_self_loops:\n\u001b[0;32m---> 61\u001b[0m     edge_index, tmp_edge_weight \u001b[39m=\u001b[39m add_remaining_self_loops(\n\u001b[1;32m     62\u001b[0m         edge_index, edge_weight, fill_value, num_nodes)\n\u001b[1;32m     63\u001b[0m     \u001b[39massert\u001b[39;00m tmp_edge_weight \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m     edge_weight \u001b[39m=\u001b[39m tmp_edge_weight\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/lib/python3.9/site-packages/torch_geometric/utils/loop.py:298\u001b[0m, in \u001b[0;36madd_remaining_self_loops\u001b[0;34m(edge_index, edge_attr, fill_value, num_nodes)\u001b[0m\n\u001b[1;32m    295\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mNo valid \u001b[39m\u001b[39m'\u001b[39m\u001b[39mfill_value\u001b[39m\u001b[39m'\u001b[39m\u001b[39m provided\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    297\u001b[0m     inv_mask \u001b[39m=\u001b[39m \u001b[39m~\u001b[39mmask\n\u001b[0;32m--> 298\u001b[0m     loop_attr[edge_index[\u001b[39m0\u001b[39m][inv_mask]] \u001b[39m=\u001b[39m edge_attr[inv_mask]\n\u001b[1;32m    300\u001b[0m     edge_attr \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([edge_attr[mask], loop_attr], dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m    302\u001b[0m edge_index \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([edge_index[:, mask], loop_index], dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "\u001b[0;31mIndexError\u001b[0m: tensors used as indices must be long, byte or bool tensors"
     ]
    }
   ],
   "source": [
    "ypred = model(data.x, edge_mask.unsqueeze(1), data)\n",
    "ypred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_mask.to_sparse()\n",
    "indices = edge_mask.to_sparse().indices()\n",
    "# indices\n",
    "#data.edge_index\n",
    "edge_mask.to_sparse().values()\n",
    "i = data.edge_index.t()[indices].squeeze().t()\n",
    "v = edge_mask.to_sparse().values()\n",
    "e = torch.sparse_coo_tensor(i ,v )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "e = torch.sparse_coo_tensor(i ,v )\n",
    "e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in data.edge_index.t():\n",
    "    if 0 in i :\n",
    "        print(i)\n",
    "#data.edge_index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_n_hop_neighbors(edge_index, n, node=None):\n",
    "    \"\"\" \n",
    "    edge_index \n",
    "    n = num hops\n",
    "    node = node_idx\n",
    "    \"\"\"\n",
    "    # create dictionary of node neighborhoods\n",
    "    neighborhoods = {}\n",
    "    for i in range(edge_index.max().item() + 1):\n",
    "        neighborhoods[i] = set()\n",
    "\n",
    "    # find 1-hop neighbors and corresponding edges\n",
    "    edges = []\n",
    "    for j in range(edge_index.shape[1]):\n",
    "        src, dst = edge_index[0, j].item(), edge_index[1, j].item()\n",
    "        neighborhoods[src].add(dst)\n",
    "        neighborhoods[dst].add(src)\n",
    "        edges.append((src, dst))\n",
    "\n",
    "    # find n-hop neighbors for the specified node or all nodes\n",
    "\n",
    "    for k in range(2, n+1):\n",
    "        new_neighbors = set()\n",
    "        for neighbor in neighborhoods[node]:\n",
    "            new_neighbors.update(neighborhoods[neighbor])\n",
    "        neighborhoods[node].update(new_neighbors)\n",
    "    sub_edges = []\n",
    "    for edge in edges:\n",
    "        src, dst = edge\n",
    "        if src in neighborhoods[node] and dst in neighborhoods[node] or src == node or dst == node:\n",
    "            sub_edges.append(edge)\n",
    "            \n",
    "    sub_edges_tensor = torch.tensor([sub_edges[i] for i in range(len(sub_edges))]).t()        \n",
    "\n",
    "    #return {node: sub_edges}, {node: neighborhoods[node]}, sub_edges_tensor\n",
    "    return sub_edges, neighborhoods[node], sub_edges_tensor \n",
    "\n",
    "sub_edges, neighborhoods, sub_edges_tensor = find_n_hop_neighbors(data.edge_index, 2, node=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighborhoods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.edge_index.t()[indices] == sub_edges_tensor.t()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c30f2af5f468e7f5b45bcc30fca5f4886c90d54777aed916ed5f6294dfb24bf2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
