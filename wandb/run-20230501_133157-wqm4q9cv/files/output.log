24
start training
tensor([[ 0.1214, -0.0644,  0.0171, -0.1028],
        [ 0.1214, -0.0644,  0.0171, -0.1028],
        [ 0.1214, -0.0644,  0.0171, -0.1028],
        ...,
        [ 0.1214, -0.0644,  0.0171, -0.1028],
        [ 0.1214, -0.0644,  0.0171, -0.1028],
        [ 0.1214, -0.0644,  0.0171, -0.1028]], grad_fn=<AddBackward0>)
masked_hor: tensor(indices=tensor([[  5757,   5757,   5757,   5757,   5757,   5757,   5757,
                          5757,   5757,   5757,   5757,   5757,   5757,   5757,
                          6860,   6874,   6920,   6976,   7731,   7837,   7857,
                          7905,   7933,   7973],
                       [ 83758, 192782, 224697, 255410, 255424, 255470, 255526,
                        256281, 256387, 256407, 256455, 256483, 256523, 328345,
                         22327,  22327,  22327,  22327,  22327,  22327,  22327,
                         22327,  22327,  22327]]),
       values=tensor([0.8258, 0.8068, 0.7790, 0.0597, 0.0768, 0.0656, 0.0729,
                      0.0631, 0.0751, 0.0690, 0.0702, 0.0745, 0.0664, 0.7362,
                      0.5820, 0.7186, 0.7131, 0.7080, 0.7742, 0.6944, 0.6962,
                      0.7275, 0.6815, 0.7494]),
       size=(8285, 753935), nnz=24, layout=torch.sparse_coo,
       grad_fn=<SparseCooTensorWithDimsAndTensorsBackward0>)
tensor([[ 0.1214, -0.0644,  0.0171, -0.1028],
        [ 0.1214, -0.0644,  0.0171, -0.1028],
        [ 0.1214, -0.0644,  0.0171, -0.1028],
        ...,
        [ 0.1214, -0.0644,  0.0171, -0.1028],
        [ 0.1214, -0.0644,  0.0171, -0.1028],
        [ 0.1214, -0.0644,  0.0171, -0.1028]], grad_fn=<AddBackward0>)
gt_label_node tensor(2, dtype=torch.int32)
gt_label_node <class 'torch.Tensor'>
logit tensor(0.2903, grad_fn=<SelectBackward0>)
mask_without_small tensor([0.8258, 0.8068, 0.7790, 0.5968, 0.7678, 0.6556, 0.7286, 0.6311, 0.7508,
        0.6904, 0.7022, 0.7445, 0.6636, 0.7362, 0.5820, 0.7186, 0.7131, 0.7080,
        0.7742, 0.6944, 0.6962, 0.7275, 0.6815, 0.7494],
       grad_fn=<IndexBackward0>)
pred_loss tensor(1.2367, grad_fn=<MulBackward0>)
size_loss tensor(-0.0360, grad_fn=<MulBackward0>)
size_num_loss 72.0
  0%|                                                                                                                                              | 0/2 [00:00<?, ?it/s]/Users/macoftraopia/Documents/GitHub/RGCN-Explainer/RGCN_stuff/r.py:126: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  adj = torch.tensor(adj)
 50%|███████████████████████████████████████████████████████████████████                                                                   | 1/2 [00:25<00:25, 25.80s/it]
epoch:  0 ; loss:  73.20069122314453 ; pred:  tensor([0.2633, 0.2760, 0.2903, 0.1704], grad_fn=<SoftmaxBackward0>)
tensor([[-0.3786, -0.5644,  0.5171, -0.6028],
        [-0.3786, -0.5644,  0.5171, -0.6028],
        [-0.3786, -0.5644,  0.5171, -0.6028],
        ...,
        [-0.3786, -0.5644,  0.5171, -0.6028],
        [-0.3786, -0.5644,  0.5171, -0.6028],
        [-0.3786, -0.5644,  0.5171, -0.6028]], grad_fn=<AddBackward0>)
masked_hor: tensor(indices=tensor([[  5757,   5757,   5757,   5757,   5757,   5757,   5757,
                          5757,   5757,   5757,   5757,   5757,   5757,   5757,
                          6860,   6874,   6920,   6976,   7731,   7837,   7857,
                          7905,   7933,   7973],
                       [ 83758, 192782, 224697, 255410, 255424, 255470, 255526,
                        256281, 256387, 256407, 256455, 256483, 256523, 328345,
                         22327,  22327,  22327,  22327,  22327,  22327,  22327,
                         22327,  22327,  22327]]),
       values=tensor([0.8866, 0.8732, 0.8532, 0.0473, 0.0845, 0.0536, 0.0816,
                      0.0509, 0.0832, 0.0575, 0.0795, 0.0828, 0.0545, 0.8215,
                      0.4579, 0.8081, 0.8039, 0.7999, 0.8497, 0.7893, 0.7907,
                      0.8148, 0.7792, 0.6446]),
       size=(8285, 753935), nnz=24, layout=torch.sparse_coo,
       grad_fn=<SparseCooTensorWithDimsAndTensorsBackward0>)
tensor([[-0.3786, -0.5644,  0.5171, -0.6028],
        [-0.3786, -0.5644,  0.5171, -0.6028],
        [-0.3786, -0.5644,  0.5171, -0.6028],
        ...,
        [-0.3786, -0.5644,  0.5171, -0.6028],
        [-0.3786, -0.5644,  0.5171, -0.6028],
        [-0.3786, -0.5644,  0.5171, -0.6028]], grad_fn=<AddBackward0>)
gt_label_node tensor(2, dtype=torch.int32)
gt_label_node <class 'torch.Tensor'>
logit tensor(1., grad_fn=<SelectBackward0>)
mask_without_small tensor([0.8866, 0.8732, 0.8532, 0.8450, 0.5358, 0.8157, 0.5092, 0.8324, 0.5749,
        0.7954, 0.8277, 0.5447, 0.8215, 0.8081, 0.8039, 0.7999, 0.8497, 0.7893,
        0.7907, 0.8148, 0.7792, 0.6446], grad_fn=<IndexBackward0>)
pred_loss tensor(-0., grad_fn=<MulBackward0>)
size_loss tensor(-137.4141, grad_fn=<MulBackward0>)

100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:49<00:00, 24.93s/it]
sel masked ver tensor(indices=tensor([[6860, 6874, 6920, 7731, 7837, 7857, 7905, 7933, 7973,
                        5757, 5757, 5757, 5757],
                       [5757, 5757, 5757, 5757, 5757, 5757, 5757, 5757, 5757,
                         908, 2227, 1002, 5230]]),
       values=tensor([0.8866, 0.8732, 0.8532, 0.8450, 0.5358, 0.8157, 0.5092,
                      0.8324, 0.5749, 0.7954, 0.8277, 0.5447, 0.6446]),
       size=(7974, 5758), nnz=13, layout=torch.sparse_coo,
       grad_fn=<SparseCooTensorWithDimsAndTensorsBackward0>)
/usr/local/Caskroom/miniconda/base/lib/python3.9/site-packages/networkx/drawing/nx_pylab.py:433: UserWarning: No data for colormapping provided via 'c'. Parameters 'cmap' will be ignored
  node_collection = ax.scatter(
Counter({'author': 9, 'fax': 1, 'phone': 1, 'name': 1, 'type': 1})
dict index: {}
ypred explain tensor([1.1327e-24, 1.4731e-23, 1.0000e+00, 1.0734e-23],
       grad_fn=<SoftmaxBackward0>)
ypred full tensor([1.3655e-38, 9.0362e-37, 1.0000e+00, 3.1108e-37],
       grad_fn=<SoftmaxBackward0>)
original label [2]
num mislabel 1
num correct 34