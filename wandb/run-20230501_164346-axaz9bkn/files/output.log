24
start training
masked_hor: tensor(indices=tensor([[  5757,   5757,   5757,   5757,   5757,   5757,   5757,
                          5757,   5757,   5757,   5757,   5757,   5757,   5757,
                          6860,   6874,   6920,   6976,   7731,   7837,   7857,
                          7905,   7933,   7973],
                       [ 83758, 192782, 224697, 255410, 255424, 255470, 255526,
                        256281, 256387, 256407, 256455, 256483, 256523, 328345,
                         22327,  22327,  22327,  22327,  22327,  22327,  22327,
                         22327,  22327,  22327]]),
       values=tensor([0.8258, 0.8068, 0.7790, 0.0597, 0.0768, 0.0656, 0.0729,
                      0.0631, 0.0751, 0.0690, 0.0702, 0.0745, 0.0664, 0.7362,
                      0.5820, 0.7186, 0.7131, 0.7080, 0.7742, 0.6944, 0.6962,
                      0.7275, 0.6815, 0.7494]),
       size=(8285, 753935), nnz=24, layout=torch.sparse_coo,
       grad_fn=<SparseCooTensorWithDimsAndTensorsBackward0>)
tensor([0.8258, 0.8068, 0.7790, 0.5968, 0.7678, 0.6556, 0.7286, 0.6311, 0.7508,
        0.6904, 0.7022, 0.7445, 0.6636, 0.7362, 0.5820, 0.7186, 0.7131, 0.7080,
        0.7742, 0.6944, 0.6962, 0.7275, 0.6815, 0.7494],
       grad_fn=<SigmoidBackward0>)
num_high 24
pred_loss tensor(1.2367, grad_fn=<MulBackward0>)
size_loss tensor(-0.0360, grad_fn=<MulBackward0>) tensor(-8.8362, grad_fn=<MulBackward0>)
size_num_loss 1.0
loss_reg tensor(1.7124, grad_fn=<MulBackward0>)
  0%|                                                                                                                | 0/10 [00:00<?, ?it/s]/Users/macoftraopia/Documents/GitHub/RGCN-Explainer/RGCN_stuff/r.py:126: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  adj = torch.tensor(adj)
epoch:  0 ; loss:  -4.923101425170898 ; pred:  tensor([0.2633, 0.2760, 0.2903, 0.1704], grad_fn=<SoftmaxBackward0>)
masked_hor: tensor(indices=tensor([[  5757,   5757,   5757,   5757,   5757,   5757,   5757,
                          5757,   5757,   5757,   5757,   5757,   5757,   5757,
                          6860,   6874,   6920,   6976,   7731,   7837,   7857,
                          7905,   7933,   7973],
                       [ 83758, 192782, 224697, 255410, 255424, 255470, 255526,
                        256281, 256387, 256407, 256455, 256483, 256523, 328345,
                         22327,  22327,  22327,  22327,  22327,  22327,  22327,
                         22327,  22327,  22327]]),
       values=tensor([0.7420, 0.7169, 0.6814, 0.0709, 0.0667, 0.0758, 0.0620,
                      0.0738, 0.0646, 0.0786, 0.0795, 0.0639, 0.0765, 0.6286,
                      0.6966, 0.6076, 0.6012, 0.7999, 0.6752, 0.7893, 0.7907,
                      0.6182, 0.7792, 0.6446]),
       size=(8285, 753935), nnz=24, layout=torch.sparse_coo,
       grad_fn=<SparseCooTensorWithDimsAndTensorsBackward0>)
tensor([0.7420, 0.7169, 0.6814, 0.7093, 0.6673, 0.7583, 0.6195, 0.7382, 0.6463,
        0.7861, 0.7954, 0.6387, 0.7648, 0.6286, 0.6966, 0.6076, 0.6012, 0.7999,
        0.6752, 0.7893, 0.7907, 0.6182, 0.7792, 0.6446],
       grad_fn=<SigmoidBackward0>)
num_high 24
pred_loss tensor(-0., grad_fn=<MulBackward0>)
size_loss tensor(-0.0470, grad_fn=<MulBackward0>) tensor(-8.6760, grad_fn=<MulBackward0>)
size_num_loss 1.0
loss_reg tensor(1.6895, grad_fn=<MulBackward0>)

 20%|████████████████████▊                                                                                   | 2/10 [00:45<03:03, 22.99s/it]
masked_hor: tensor(indices=tensor([[  5757,   5757,   5757,   5757,   5757,   5757,   5757,
                          5757,   5757,   5757,   5757,   5757,   5757,   5757,
                          6860,   6874,   6920,   6976,   7731,   7837,   7857,
                          7905,   7933,   7973],
                       [ 83758, 192782, 224697, 255410, 255424, 255470, 255526,
                        256281, 256387, 256407, 256455, 256483, 256523, 328345,
                         22327,  22327,  22327,  22327,  22327,  22327,  22327,
                         22327,  22327,  22327]]),
       values=tensor([0.6453, 0.6261, 0.6234, 0.0767, 0.0635, 0.0759, 0.0679,
                      0.0769, 0.0659, 0.0735, 0.0738, 0.0688, 0.0734, 0.6762,
                      0.7618, 0.6830, 0.6825, 0.7342, 0.6287, 0.7369, 0.7361,
                      0.6821, 0.7439, 0.6521]),
       size=(8285, 753935), nnz=24, layout=torch.sparse_coo,
       grad_fn=<SparseCooTensorWithDimsAndTensorsBackward0>)
tensor([0.6453, 0.6261, 0.6234, 0.7666, 0.6348, 0.7592, 0.6789, 0.7692, 0.6591,
        0.7352, 0.7384, 0.6876, 0.7336, 0.6762, 0.7618, 0.6830, 0.6825, 0.7342,
        0.6287, 0.7369, 0.7361, 0.6821, 0.7439, 0.6521],
       grad_fn=<SigmoidBackward0>)
num_high 24
pred_loss tensor(-0., grad_fn=<MulBackward0>)
size_loss tensor(-0.0243, grad_fn=<MulBackward0>) tensor(-9.0388, grad_fn=<MulBackward0>)
size_num_loss 1.0

 30%|███████████████████████████████▏                                                                        | 3/10 [01:07<02:36, 22.32s/it]
epoch:  2 ; loss:  -6.385592460632324 ; pred:  tensor([6.6160e-24, 7.5197e-23, 1.0000e+00, 5.3673e-23],
       grad_fn=<SoftmaxBackward0>)
masked_hor: tensor(indices=tensor([[  5757,   5757,   5757,   5757,   5757,   5757,   5757,
                          5757,   5757,   5757,   5757,   5757,   5757,   5757,
                          6860,   6874,   6920,   6976,   7731,   7837,   7857,
                          7905,   7933,   7973],
                       [ 83758, 192782, 224697, 255410, 255424, 255470, 255526,
                        256281, 256387, 256407, 256455, 256483, 256523, 328345,
                         22327,  22327,  22327,  22327,  22327,  22327,  22327,
                         22327,  22327,  22327]]),
       values=tensor([0.6216, 0.6355, 0.6610, 0.0771, 0.0678, 0.0716, 0.0735,
                      0.0748, 0.0708, 0.0658, 0.0656, 0.0730, 0.0671, 0.7283,
                      0.7765, 0.7439, 0.7468, 0.6453, 0.6700, 0.6585, 0.6561,
                      0.7377, 0.6775, 0.7010]),
       size=(8285, 753935), nnz=24, layout=torch.sparse_coo,
       grad_fn=<SparseCooTensorWithDimsAndTensorsBackward0>)
tensor([0.6216, 0.6355, 0.6610, 0.7710, 0.6781, 0.7159, 0.7346, 0.7476, 0.7077,
        0.6582, 0.6562, 0.7297, 0.6707, 0.7283, 0.7765, 0.7439, 0.7468, 0.6453,
        0.6700, 0.6585, 0.6561, 0.7377, 0.6775, 0.7010],
       grad_fn=<SigmoidBackward0>)
num_high 24
pred_loss tensor(-0., grad_fn=<MulBackward0>)
size_loss tensor(-0.0202, grad_fn=<MulBackward0>) tensor(-9.1203, grad_fn=<MulBackward0>)
size_num_loss 1.0

 40%|█████████████████████████████████████████▌                                                              | 4/10 [01:29<02:14, 22.45s/it]
masked_hor: tensor(indices=tensor([[  5757,   5757,   5757,   5757,   5757,   5757,   5757,
                          5757,   5757,   5757,   5757,   5757,   5757,   5757,
                          6860,   6874,   6920,   6976,   7731,   7837,   7857,
                          7905,   7933,   7973],
                       [ 83758, 192782, 224697, 255410, 255424, 255470, 255526,
                        256281, 256387, 256407, 256455, 256483, 256523, 328345,
                         22327,  22327,  22327,  22327,  22327,  22327,  22327,
                         22327,  22327,  22327]]),
       values=tensor([0.6545, 0.6793, 0.7106, 0.0746, 0.0722, 0.0659, 0.0737,
                      0.0702, 0.0727, 0.0634, 0.0628, 0.0717, 0.0643, 0.7301,
                      0.7607, 0.7474, 0.7534, 0.6271, 0.7177, 0.6316, 0.6313,
                      0.7378, 0.6337, 0.7289]),
       size=(8285, 753935), nnz=24, layout=torch.sparse_coo,
       grad_fn=<SparseCooTensorWithDimsAndTensorsBackward0>)
tensor([0.6545, 0.6793, 0.7106, 0.7460, 0.7221, 0.6594, 0.7369, 0.7019, 0.7269,
        0.6339, 0.6281, 0.7170, 0.6433, 0.7301, 0.7607, 0.7474, 0.7534, 0.6271,
        0.7177, 0.6316, 0.6313, 0.7378, 0.6337, 0.7289],
       grad_fn=<SigmoidBackward0>)
num_high 24
pred_loss tensor(-0., grad_fn=<MulBackward0>)
size_loss tensor(-0.0228, grad_fn=<MulBackward0>) tensor(-9.0669, grad_fn=<MulBackward0>)
size_num_loss 1.0
