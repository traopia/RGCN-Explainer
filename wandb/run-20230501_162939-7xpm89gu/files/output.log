24
start training
masked_hor: tensor(indices=tensor([[  5757,   5757,   5757,   5757,   5757,   5757,   5757,
                          5757,   5757,   5757,   5757,   5757,   5757,   5757,
                          6860,   6874,   6920,   6976,   7731,   7837,   7857,
                          7905,   7933,   7973],
                       [ 83758, 192782, 224697, 255410, 255424, 255470, 255526,
                        256281, 256387, 256407, 256455, 256483, 256523, 328345,
                         22327,  22327,  22327,  22327,  22327,  22327,  22327,
                         22327,  22327,  22327]]),
       values=tensor([0.8258, 0.8068, 0.7790, 0.0597, 0.0768, 0.0656, 0.0729,
                      0.0631, 0.0751, 0.0690, 0.0702, 0.0745, 0.0664, 0.7362,
                      0.5820, 0.7186, 0.7131, 0.7080, 0.7742, 0.6944, 0.6962,
                      0.7275, 0.6815, 0.7494]),
       size=(8285, 753935), nnz=24, layout=torch.sparse_coo,
       grad_fn=<SparseCooTensorWithDimsAndTensorsBackward0>)
tensor([0.8258, 0.8068, 0.7790, 0.5968, 0.7678, 0.6556, 0.7286, 0.6311, 0.7508,
        0.6904, 0.7022, 0.7445, 0.6636, 0.7362, 0.5820, 0.7186, 0.7131, 0.7080,
        0.7742, 0.6944, 0.6962, 0.7275, 0.6815, 0.7494],
       grad_fn=<SigmoidBackward0>)
num_high 24
pred_loss tensor(1.2367, grad_fn=<MulBackward0>)
size_loss tensor(213.2379, grad_fn=<MulBackward0>) tensor(-0.0360, grad_fn=<MulBackward0>) tensor(-8.8362, grad_fn=<MulBackward0>)
size_num_loss 72.0
loss_reg tensor(8.5620, grad_fn=<MulBackward0>)
  0%|                                                                                                                | 0/10 [00:00<?, ?it/s]/Users/macoftraopia/Documents/GitHub/RGCN-Explainer/RGCN_stuff/r.py:126: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  adj = torch.tensor(adj)
 10%|██████████▍                                                                                             | 1/10 [00:23<03:35, 23.96s/it]
epoch:  0 ; loss:  286.1644287109375 ; pred:  tensor([0.2633, 0.2760, 0.2903, 0.1704], grad_fn=<SoftmaxBackward0>)
masked_hor: tensor(indices=tensor([[  5757,   5757,   5757,   5757,   5757,   5757,   5757,
                          5757,   5757,   5757,   5757,   5757,   5757,   5757,
                          6860,   6874,   6920,   6976,   7731,   7837,   7857,
                          7905,   7933,   7973],
                       [ 83758, 192782, 224697, 255410, 255424, 255470, 255526,
                        256281, 256387, 256407, 256455, 256483, 256523, 328345,
                         22327,  22327,  22327,  22327,  22327,  22327,  22327,
                         22327,  22327,  22327]]),
       values=tensor([0.7420, 0.7169, 0.6814, 0.0473, 0.0667, 0.0536, 0.0620,
                      0.0509, 0.0646, 0.0575, 0.0588, 0.0639, 0.0545, 0.6286,
                      0.4579, 0.6076, 0.6012, 0.5953, 0.6752, 0.5795, 0.5815,
                      0.6182, 0.5648, 0.6446]),
       size=(8285, 753935), nnz=24, layout=torch.sparse_coo,
       grad_fn=<SparseCooTensorWithDimsAndTensorsBackward0>)
tensor([0.7420, 0.7169, 0.6814, 0.4731, 0.6673, 0.5358, 0.6195, 0.5092, 0.6463,
        0.5749, 0.5885, 0.6387, 0.5447, 0.6286, 0.4579, 0.6076, 0.6012, 0.5953,
        0.6752, 0.5795, 0.5815, 0.6182, 0.5648, 0.6446],
       grad_fn=<SigmoidBackward0>)
num_high 22
pred_loss tensor(-0., grad_fn=<MulBackward0>)
size_loss tensor(93.2380, grad_fn=<MulBackward0>) tensor(-0.0334, grad_fn=<MulBackward0>) tensor(-8.6530, grad_fn=<MulBackward0>)
size_num_loss 84.5

 20%|████████████████████▊                                                                                   | 2/10 [00:47<03:09, 23.64s/it]
masked_hor: tensor(indices=tensor([[  5757,   5757,   5757,   5757,   5757,   5757,   5757,
                          5757,   5757,   5757,   5757,   5757,   5757,   5757,
                          6860,   6874,   6920,   6976,   7731,   7837,   7857,
                          7905,   7933,   7973],
                       [ 83758, 192782, 224697, 255410, 255424, 255470, 255526,
                        256281, 256387, 256407, 256455, 256483, 256523, 328345,
                         22327,  22327,  22327,  22327,  22327,  22327,  22327,
                         22327,  22327,  22327]]),
       values=tensor([0.6356, 0.6057, 0.5646, 0.0353, 0.0549, 0.0412, 0.0497,
                      0.0386, 0.0526, 0.0451, 0.0464, 0.0517, 0.0421, 0.5066,
                      0.3388, 0.4844, 0.4777, 0.4715, 0.5577, 0.4553, 0.4574,
                      0.4954, 0.4405, 0.5239]),
       size=(8285, 753935), nnz=24, layout=torch.sparse_coo,
       grad_fn=<SparseCooTensorWithDimsAndTensorsBackward0>)
tensor([0.6356, 0.6057, 0.5646, 0.3526, 0.5488, 0.4118, 0.4969, 0.3862, 0.5256,
        0.4506, 0.4644, 0.5174, 0.4205, 0.5066, 0.3388, 0.4844, 0.4777, 0.4715,
        0.5577, 0.4553, 0.4574, 0.4954, 0.4405, 0.5239],
       grad_fn=<SigmoidBackward0>)
num_high 9
pred_loss tensor(-0., grad_fn=<MulBackward0>)
size_loss tensor(0., grad_fn=<MulBackward0>) tensor(-0.0185, grad_fn=<MulBackward0>) tensor(-8.5996, grad_fn=<MulBackward0>)
size_num_loss 190.125

 30%|███████████████████████████████▏                                                                        | 3/10 [01:10<02:43, 23.31s/it]
epoch:  2 ; loss:  187.30174255371094 ; pred:  tensor([1.2904e-16, 6.5642e-16, 1.0000e+00, 4.6737e-16],
       grad_fn=<SoftmaxBackward0>)
masked_hor: tensor(indices=tensor([[  5757,   5757,   5757,   5757,   5757,   5757,   5757,
                          5757,   5757,   5757,   5757,   5757,   5757,   5757,
                          6860,   6874,   6920,   6976,   7731,   7837,   7857,
                          7905,   7933,   7973],
                       [ 83758, 192782, 224697, 255410, 255424, 255470, 255526,
                        256281, 256387, 256407, 256455, 256483, 256523, 328345,
                         22327,  22327,  22327,  22327,  22327,  22327,  22327,
                         22327,  22327,  22327]]),
       values=tensor([0.5398, 0.5084, 0.4666, 0.0271, 0.0451, 0.0323, 0.0401,
                      0.0300, 0.0428, 0.0358, 0.0370, 0.0420, 0.0330, 0.4098,
                      0.2593, 0.3889, 0.3826, 0.3769, 0.4596, 0.3619, 0.3639,
                      0.3993, 0.3484, 0.4264]),
       size=(8285, 753935), nnz=24, layout=torch.sparse_coo,
       grad_fn=<SparseCooTensorWithDimsAndTensorsBackward0>)
tensor([0.5398, 0.5084, 0.4666, 0.2711, 0.4508, 0.3227, 0.4007, 0.3001, 0.4281,
        0.3577, 0.3704, 0.4201, 0.3305, 0.4098, 0.2593, 0.3889, 0.3826, 0.3769,
        0.4596, 0.3619, 0.3639, 0.3993, 0.3484, 0.4264],
       grad_fn=<SigmoidBackward0>)
num_high 2
pred_loss tensor(-0., grad_fn=<MulBackward0>)
size_loss tensor(0., grad_fn=<MulBackward0>) tensor(-0.0049, grad_fn=<MulBackward0>) tensor(-8.6802, grad_fn=<MulBackward0>)
size_num_loss 264.5

 40%|█████████████████████████████████████████▌                                                              | 4/10 [01:31<02:15, 22.63s/it]
masked_hor: tensor(indices=tensor([[  5757,   5757,   5757,   5757,   5757,   5757,   5757,
                          5757,   5757,   5757,   5757,   5757,   5757,   5757,
                          6860,   6874,   6920,   6976,   7731,   7837,   7857,
                          7905,   7933,   7973],
                       [ 83758, 192782, 224697, 255410, 255424, 255470, 255526,
                        256281, 256387, 256407, 256455, 256483, 256523, 328345,
                         22327,  22327,  22327,  22327,  22327,  22327,  22327,
                         22327,  22327,  22327]]),
       values=tensor([0.4564, 0.4253, 0.3861, 0.0215, 0.0372, 0.0258, 0.0326,
                      0.0239, 0.0351, 0.0288, 0.0299, 0.0344, 0.0265, 0.3342,
                      0.2047, 0.3158, 0.3102, 0.3052, 0.3797, 0.2921, 0.2938,
                      0.3250, 0.2804, 0.3492]),
       size=(8285, 753935), nnz=24, layout=torch.sparse_coo,
       grad_fn=<SparseCooTensorWithDimsAndTensorsBackward0>)
tensor([0.4564, 0.4253, 0.3861, 0.2145, 0.3715, 0.2582, 0.3262, 0.2390, 0.3508,
        0.2884, 0.2995, 0.3435, 0.2649, 0.3342, 0.2047, 0.3158, 0.3102, 0.3052,
        0.3797, 0.2921, 0.2938, 0.3250, 0.2804, 0.3492],
       grad_fn=<SigmoidBackward0>)
num_high 0
pred_loss tensor(-0., grad_fn=<MulBackward0>)
size_loss tensor(0., grad_fn=<MulBackward0>) tensor(nan, grad_fn=<MulBackward0>) tensor(-8.8186, grad_fn=<MulBackward0>)
size_num_loss 288.0
