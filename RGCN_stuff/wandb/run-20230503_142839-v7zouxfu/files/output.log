
wandb: WARNING Calling wandb.login() after wandb.init() has no effect.
masked_adj tensor([0.7773, 0.7672, 0.7534, 0.6742, 0.7480, 0.6985, 0.7300, 0.6883, 0.7115,
        0.7710, 0.7209, 0.6938, 0.7121, 0.7166, 0.7110, 0.7500, 0.7708, 0.7270,
        0.7182, 0.7421, 0.7113, 0.7576, 0.7510, 0.7717, 0.7624, 0.7628, 0.7463,
        0.7637, 0.7251, 0.7321, 0.7246, 0.7524, 0.6943, 0.7083, 0.7253, 0.7725,
        0.7391, 0.7201, 0.7388, 0.7109, 0.6896, 0.7557, 0.7081, 0.7155, 0.6974,
        0.7816, 0.6985, 0.7184, 0.7071, 0.7140, 0.7330, 0.7442, 0.7184, 0.7603,
        0.7098, 0.7119, 0.6938, 0.7320, 0.7294, 0.7479, 0.7286, 0.7754, 0.6998,
        0.7648, 0.7663, 0.7523, 0.7837, 0.7442, 0.7398, 0.7260, 0.7033, 0.7624,
        0.7266, 0.7442, 0.7325, 0.7418, 0.7455, 0.7144, 0.6713, 0.7115, 0.7313,
        0.7223, 0.6955, 0.7159, 0.7445, 0.7442, 0.7591, 0.7324, 0.7496, 0.7186,
        0.7035, 0.7462, 0.6850, 0.7094, 0.7637, 0.7432, 0.6625, 0.7433, 0.7506,
        0.7318, 0.7471, 0.7457, 0.7574, 0.7443, 0.7565, 0.7127, 0.7561, 0.7225,
        0.7440, 0.7408, 0.7160, 0.7414, 0.7162, 0.7438, 0.7188, 0.7183, 0.7379,
        0.7165, 0.7480], grad_fn=<MulBackward0>)
res: tensor([9.9855e-01, 6.6735e-04, 9.8767e-05, 6.8133e-04],
       grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.7773, 0.7672, 0.7534, 0.6742, 0.7480, 0.6985, 0.7300, 0.6883, 0.7115,
        0.7710, 0.7209, 0.6938, 0.7121, 0.7166, 0.7110, 0.7500, 0.7708, 0.7270,
        0.7182, 0.7421, 0.7113, 0.7576, 0.7510, 0.7717, 0.7624, 0.7628, 0.7463,
        0.7637, 0.7251, 0.7321, 0.7246, 0.7524, 0.6943, 0.7083, 0.7253, 0.7725,
        0.7391, 0.7201, 0.7388, 0.7109, 0.6896, 0.7557, 0.7081, 0.7155, 0.6974,
        0.7816, 0.6985, 0.7184, 0.7071, 0.7140, 0.7330, 0.7442, 0.7184, 0.7603,
        0.7098, 0.7119, 0.6938, 0.7320, 0.7294, 0.7479, 0.7286, 0.7754, 0.6998,
        0.7648, 0.7663, 0.7523, 0.7837, 0.7442, 0.7398, 0.7260, 0.7033, 0.7624,
        0.7266, 0.7442, 0.7325, 0.7418, 0.7455, 0.7144, 0.6713, 0.7115, 0.7313,
        0.7223, 0.6955, 0.7159, 0.7445, 0.7442, 0.7591, 0.7324, 0.7496, 0.7186,
        0.7035, 0.7462, 0.6850, 0.7094, 0.7637, 0.7432, 0.6625, 0.7433, 0.7506,
        0.7318, 0.7471, 0.7457, 0.7574, 0.7443, 0.7565, 0.7127, 0.7561, 0.7225,
        0.7440, 0.7408, 0.7160, 0.7414, 0.7162, 0.7438, 0.7188, 0.7183, 0.7379,
        0.7165, 0.7480], grad_fn=<IndexBackward0>)
num_high 119 len(mask) 119
pred_loss tensor([0.0014], grad_fn=<MulBackward0>)
size_loss tensor(43.5387, grad_fn=<AddBackward0>)
size_num_loss 1.0000008403361345
loss: tensor([45.1200], grad_fn=<AddBackward0>)
0
epoch:  0 ; loss:  45.11996078491211 ; pred:  tensor([9.9855e-01, 6.6735e-04, 9.8767e-05, 6.8133e-04],
       grad_fn=<SoftmaxBackward0>)
masked_adj tensor([0.6791, 0.6666, 0.6495, 0.5565, 0.6429, 0.5842, 0.6211, 0.5725, 0.5993,
        0.6712, 0.6104, 0.5788, 0.6000, 0.6053, 0.5988, 0.6454, 0.6710, 0.6176,
        0.6072, 0.6358, 0.5991, 0.6547, 0.6465, 0.6721, 0.6606, 0.6611, 0.6409,
        0.6622, 0.6154, 0.6237, 0.6148, 0.6483, 0.5794, 0.5956, 0.6156, 0.6732,
        0.6321, 0.6094, 0.6317, 0.5986, 0.5740, 0.6523, 0.5953, 0.6040, 0.5829,
        0.6846, 0.5842, 0.6075, 0.5942, 0.6022, 0.6248, 0.6383, 0.6075, 0.6580,
        0.5974, 0.5998, 0.5789, 0.6236, 0.6205, 0.6428, 0.6195, 0.6768, 0.5858,
        0.6636, 0.6654, 0.6482, 0.6873, 0.6383, 0.6330, 0.6164, 0.5898, 0.6605,
        0.6172, 0.6383, 0.6242, 0.6354, 0.6398, 0.6027, 0.5533, 0.5993, 0.6228,
        0.6121, 0.5808, 0.6045, 0.6387, 0.6383, 0.6565, 0.6240, 0.6448, 0.6077,
        0.5900, 0.6407, 0.5687, 0.5969, 0.6622, 0.6371, 0.5436, 0.6372, 0.6460,
        0.6233, 0.6418, 0.6401, 0.6544, 0.6384, 0.6533, 0.6008, 0.6528, 0.6123,
        0.6381, 0.6342, 0.6046, 0.6349, 0.6048, 0.6378, 0.6079, 0.6074, 0.6307,
        0.6052, 0.6429], grad_fn=<MulBackward0>)
res: tensor([0.9885, 0.0053, 0.0013, 0.0049], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.6791, 0.6666, 0.6495, 0.5565, 0.6429, 0.5842, 0.6211, 0.5725, 0.5993,
        0.6712, 0.6104, 0.5788, 0.6000, 0.6053, 0.5988, 0.6454, 0.6710, 0.6176,
        0.6072, 0.6358, 0.5991, 0.6547, 0.6465, 0.6721, 0.6606, 0.6611, 0.6409,
        0.6622, 0.6154, 0.6237, 0.6148, 0.6483, 0.5794, 0.5956, 0.6156, 0.6732,
        0.6321, 0.6094, 0.6317, 0.5986, 0.5740, 0.6523, 0.5953, 0.6040, 0.5829,
        0.6846, 0.5842, 0.6075, 0.5942, 0.6022, 0.6248, 0.6383, 0.6075, 0.6580,
        0.5974, 0.5998, 0.5789, 0.6236, 0.6205, 0.6428, 0.6195, 0.6768, 0.5858,
        0.6636, 0.6654, 0.6482, 0.6873, 0.6383, 0.6330, 0.6164, 0.5898, 0.6605,
        0.6172, 0.6383, 0.6242, 0.6354, 0.6398, 0.6027, 0.5533, 0.5993, 0.6228,
        0.6121, 0.5808, 0.6045, 0.6387, 0.6383, 0.6565, 0.6240, 0.6448, 0.6077,
        0.5900, 0.6407, 0.5687, 0.5969, 0.6622, 0.6371, 0.5436, 0.6372, 0.6460,
        0.6233, 0.6418, 0.6401, 0.6544, 0.6384, 0.6533, 0.6008, 0.6528, 0.6123,
        0.6381, 0.6342, 0.6046, 0.6349, 0.6048, 0.6378, 0.6079, 0.6074, 0.6307,
        0.6052, 0.6429], grad_fn=<IndexBackward0>)
num_high 119 len(mask) 119
pred_loss tensor([0.0116], grad_fn=<MulBackward0>)
size_loss tensor(37.1080, grad_fn=<AddBackward0>)
size_num_loss 1.0000008403361345
loss: tensor([38.7799], grad_fn=<AddBackward0>)
1
epoch:  1 ; loss:  38.779869079589844 ; pred:  tensor([0.9885, 0.0053, 0.0013, 0.0049], grad_fn=<SoftmaxBackward0>)
masked_adj tensor([0.5629, 0.5489, 0.5300, 0.4335, 0.5229, 0.4612, 0.4996, 0.4494, 0.4767,
        0.5540, 0.4883, 0.4558, 0.4775, 0.4829, 0.4762, 0.5256, 0.5538, 0.4958,
        0.4849, 0.5152, 0.4765, 0.5357, 0.5268, 0.5550, 0.5422, 0.5427, 0.5207,
        0.5440, 0.4935, 0.5023, 0.4928, 0.5287, 0.4564, 0.4729, 0.4938, 0.5562,
        0.5113, 0.4873, 0.5109, 0.4760, 0.4509, 0.5331, 0.4726, 0.4816, 0.4600,
        0.5691, 0.4612, 0.4852, 0.4715, 0.4797, 0.5035, 0.5179, 0.4852, 0.5394,
        0.4747, 0.4772, 0.4558, 0.5021, 0.4989, 0.5228, 0.4978, 0.5603, 0.4629,
        0.5455, 0.5475, 0.5286, 0.5722, 0.5179, 0.5122, 0.4946, 0.4670, 0.5422,
        0.4954, 0.5179, 0.5028, 0.5147, 0.5195, 0.4803, 0.4304, 0.4768, 0.5013,
        0.4901, 0.4579, 0.4821, 0.5183, 0.5180, 0.5378, 0.5027, 0.5250, 0.4855,
        0.4672, 0.5205, 0.4458, 0.4743, 0.5440, 0.5167, 0.4209, 0.5168, 0.5264,
        0.5019, 0.5217, 0.5198, 0.5354, 0.5180, 0.5342, 0.4783, 0.5337, 0.4903,
        0.5177, 0.5135, 0.4823, 0.5143, 0.4825, 0.5174, 0.4857, 0.4851, 0.5100,
        0.4828, 0.5229], grad_fn=<MulBackward0>)
res: tensor([0.9230, 0.0339, 0.0131, 0.0299], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.5629, 0.5489, 0.5300, 0.5229, 0.5540, 0.5256, 0.5538, 0.5152, 0.5357,
        0.5268, 0.5550, 0.5422, 0.5427, 0.5207, 0.5440, 0.5023, 0.5287, 0.5562,
        0.5113, 0.5109, 0.5331, 0.5691, 0.5035, 0.5179, 0.5394, 0.5021, 0.5228,
        0.5603, 0.5455, 0.5475, 0.5286, 0.5722, 0.5179, 0.5122, 0.5422, 0.5179,
        0.5028, 0.5147, 0.5195, 0.5013, 0.5183, 0.5180, 0.5378, 0.5027, 0.5250,
        0.5205, 0.5440, 0.5167, 0.5168, 0.5264, 0.5019, 0.5217, 0.5198, 0.5354,
        0.5180, 0.5342, 0.5337, 0.5177, 0.5135, 0.5143, 0.5174, 0.5100, 0.5229],
       grad_fn=<IndexBackward0>)
num_high 63 len(mask) 119
pred_loss tensor([0.0801], grad_fn=<MulBackward0>)
size_loss tensor(-0.3132, grad_fn=<MulBackward0>)
size_num_loss 0.5294126050420168
loss: tensor([0.9875], grad_fn=<AddBackward0>)
2
epoch:  2 ; loss:  0.9874849915504456 ; pred:  tensor([0.9230, 0.0339, 0.0131, 0.0299], grad_fn=<SoftmaxBackward0>)
masked_adj tensor([0.5467, 0.5035, 0.4365, 0.3516, 0.4150, 0.3734, 0.4049, 0.3640, 0.3859,
        0.5206, 0.3955, 0.3691, 0.3866, 0.3910, 0.3855, 0.4226, 0.5199, 0.4018,
        0.3927, 0.3976, 0.3858, 0.4563, 0.4263, 0.5238, 0.4798, 0.4818, 0.4094,
        0.4871, 0.3998, 0.3800, 0.3993, 0.4322, 0.3695, 0.3828, 0.4000, 0.5274,
        0.3910, 0.3946, 0.3910, 0.3853, 0.3652, 0.4469, 0.3826, 0.3899, 0.3724,
        0.5628, 0.3734, 0.3929, 0.3817, 0.3884, 0.3811, 0.4032, 0.3929, 0.4695,
        0.3843, 0.3864, 0.3691, 0.3798, 0.4044, 0.4148, 0.4034, 0.5394, 0.3756,
        0.4920, 0.4992, 0.4322, 0.5699, 0.4033, 0.3926, 0.4012, 0.3790, 0.4816,
        0.4022, 0.4035, 0.3805, 0.3971, 0.4072, 0.3900, 0.3501, 0.3860, 0.3790,
        0.3979, 0.3730, 0.3926, 0.4041, 0.4046, 0.4677, 0.3804, 0.4228, 0.3957,
        0.3808, 0.4098, 0.3652, 0.3854, 0.4907, 0.4020, 0.3458, 0.4018, 0.4277,
        0.3796, 0.4126, 0.4079, 0.4560, 0.4039, 0.4524, 0.3883, 0.4500, 0.3984,
        0.4032, 0.3949, 0.3915, 0.3962, 0.3915, 0.4025, 0.3943, 0.3938, 0.3921,
        0.3922, 0.4159], grad_fn=<MulBackward0>)
res: tensor([0.7676, 0.0963, 0.0527, 0.0833], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.5467, 0.5035, 0.5206, 0.5199, 0.5238, 0.5274, 0.5628, 0.5394, 0.5699],
       grad_fn=<IndexBackward0>)
num_high 9 len(mask) 119
pred_loss tensor([0.2645], grad_fn=<MulBackward0>)
size_loss tensor(-0.4696, grad_fn=<MulBackward0>)
size_num_loss 0.07563109243697479
loss: tensor([0.5439], grad_fn=<AddBackward0>)
3
epoch:  3 ; loss:  0.5439434051513672 ; pred:  tensor([0.7676, 0.0963, 0.0527, 0.0833], grad_fn=<SoftmaxBackward0>)
masked_adj tensor([0.6035, 0.4265, 0.3694, 0.3069, 0.3406, 0.3209, 0.3422, 0.3148, 0.3292,
        0.4431, 0.3357, 0.3181, 0.3297, 0.3327, 0.3289, 0.3506, 0.4424, 0.3400,
        0.3338, 0.3186, 0.3291, 0.3972, 0.3555, 0.4473, 0.4311, 0.4340, 0.3334,
        0.4433, 0.3387, 0.2989, 0.3383, 0.3636, 0.3184, 0.3271, 0.3388, 0.4547,
        0.3108, 0.3351, 0.3162, 0.3288, 0.3156, 0.3839, 0.3270, 0.3319, 0.3202,
        0.6294, 0.3209, 0.3340, 0.3264, 0.3309, 0.3001, 0.3255, 0.3340, 0.4161,
        0.3281, 0.3295, 0.3181, 0.2988, 0.3418, 0.3403, 0.3412, 0.5683, 0.3252,
        0.4492, 0.4601, 0.3645, 0.6376, 0.3268, 0.3143, 0.3405, 0.3276, 0.4375,
        0.3427, 0.3276, 0.3007, 0.3197, 0.3322, 0.3354, 0.3089, 0.3294, 0.2978,
        0.3403, 0.3271, 0.3399, 0.3270, 0.3329, 0.4207, 0.3038, 0.3559, 0.3435,
        0.3330, 0.3363, 0.3270, 0.3328, 0.4548, 0.3308, 0.3136, 0.3286, 0.3647,
        0.3006, 0.3400, 0.3334, 0.3987, 0.3290, 0.3950, 0.3344, 0.3909, 0.3414,
        0.3276, 0.3173, 0.3361, 0.3188, 0.3354, 0.3273, 0.3379, 0.3374, 0.3381,
        0.3373, 0.3447], grad_fn=<MulBackward0>)
res: tensor([0.6373, 0.1449, 0.0933, 0.1245], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.6035, 0.6294, 0.5683, 0.6376], grad_fn=<IndexBackward0>)
num_high 4 len(mask) 119
pred_loss tensor([0.4505], grad_fn=<MulBackward0>)
size_loss tensor(-0.9725, grad_fn=<MulBackward0>)
size_num_loss 0.033614285714285715
loss: tensor([0.1541], grad_fn=<AddBackward0>)
4
epoch:  4 ; loss:  0.15408629179000854 ; pred:  tensor([0.6373, 0.1449, 0.0933, 0.1245], grad_fn=<SoftmaxBackward0>)
masked_adj tensor([0.5809, 0.3648, 0.3288, 0.2907, 0.2978, 0.2972, 0.3082, 0.2942, 0.3014,
        0.3811, 0.3047, 0.2958, 0.3016, 0.3031, 0.3012, 0.3085, 0.3803, 0.3071,
        0.3037, 0.2743, 0.3013, 0.3592, 0.3138, 0.3862, 0.3972, 0.4005, 0.2901,
        0.4141, 0.3063, 0.2529, 0.3061, 0.3224, 0.2960, 0.3003, 0.3064, 0.3966,
        0.2658, 0.3044, 0.2800, 0.3012, 0.2946, 0.3445, 0.3002, 0.3028, 0.2969,
        0.7075, 0.2972, 0.3038, 0.2999, 0.3022, 0.2542, 0.2817, 0.3038, 0.3803,
        0.3008, 0.3015, 0.2958, 0.2527, 0.3080, 0.2975, 0.3077, 0.5023, 0.3044,
        0.4180, 0.4316, 0.3252, 0.7143, 0.2845, 0.2719, 0.3084, 0.3064, 0.4113,
        0.3129, 0.2866, 0.2568, 0.2779, 0.2915, 0.3108, 0.2968, 0.3018, 0.2513,
        0.3125, 0.3116, 0.3177, 0.2836, 0.2975, 0.3963, 0.2643, 0.3219, 0.3229,
        0.3158, 0.2961, 0.3201, 0.3098, 0.4371, 0.2966, 0.3118, 0.2915, 0.3342,
        0.2576, 0.3009, 0.2932, 0.3640, 0.2891, 0.3625, 0.3108, 0.3567, 0.3144,
        0.2870, 0.2756, 0.3108, 0.2771, 0.3091, 0.2873, 0.3113, 0.3107, 0.3300,
        0.3130, 0.3066], grad_fn=<MulBackward0>)
res: tensor([0.5795, 0.1659, 0.1127, 0.1419], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.5809, 0.7075, 0.5023, 0.7143], grad_fn=<IndexBackward0>)
num_high 4 len(mask) 119
pred_loss tensor([0.5456], grad_fn=<MulBackward0>)
size_loss tensor(-10.5914, grad_fn=<MulBackward0>)
size_num_loss 0.033614285714285715
loss: tensor([-9.3930], grad_fn=<AddBackward0>)
5
epoch:  5 ; loss:  -9.392990112304688 ; pred:  tensor([0.5795, 0.1659, 0.1127, 0.1419], grad_fn=<SoftmaxBackward0>)
masked_adj tensor([0.5138, 0.3165, 0.3104, 0.2950, 0.2800, 0.2950, 0.2969, 0.2948, 0.2955,
        0.3335, 0.2961, 0.2949, 0.2955, 0.2958, 0.2955, 0.2906, 0.3325, 0.2966,
        0.2959, 0.2560, 0.2955, 0.3401, 0.2958, 0.3397, 0.3776, 0.3809, 0.2723,
        0.3987, 0.2965, 0.2322, 0.2964, 0.3043, 0.2949, 0.2953, 0.2965, 0.3529,
        0.2470, 0.2960, 0.2710, 0.2954, 0.2948, 0.3258, 0.2953, 0.2957, 0.2949,
        0.7726, 0.2950, 0.2959, 0.2953, 0.2956, 0.2338, 0.2637, 0.2959, 0.3608,
        0.2954, 0.2955, 0.2949, 0.2320, 0.2969, 0.2797, 0.2968, 0.4230, 0.3057,
        0.3984, 0.4137, 0.3091, 0.7810, 0.2682, 0.2559, 0.2987, 0.3077, 0.4015,
        0.3061, 0.2717, 0.2386, 0.2625, 0.2767, 0.3089, 0.3056, 0.2962, 0.2301,
        0.3076, 0.3181, 0.3179, 0.2657, 0.2881, 0.3907, 0.2506, 0.3129, 0.3254,
        0.3209, 0.2810, 0.3351, 0.3085, 0.4351, 0.2886, 0.3312, 0.2804, 0.3287,
        0.2401, 0.2876, 0.2789, 0.3493, 0.2753, 0.3512, 0.3100, 0.3436, 0.3101,
        0.2724, 0.2600, 0.3083, 0.2618, 0.3055, 0.2733, 0.3072, 0.3065, 0.3507,
        0.3115, 0.2941], grad_fn=<MulBackward0>)
res: tensor([0.5755, 0.1682, 0.1130, 0.1433], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.5138, 0.7726, 0.7810], grad_fn=<IndexBackward0>)
num_high 3 len(mask) 119
pred_loss tensor([0.5525], grad_fn=<MulBackward0>)
size_loss tensor(-23.0783, grad_fn=<MulBackward0>)
size_num_loss 0.0252109243697479
loss: tensor([0.5525], grad_fn=<MulBackward0>)
6
epoch:  6 ; loss:  0.5525031089782715 ; pred:  tensor([0.5755, 0.1682, 0.1130, 0.1433], grad_fn=<SoftmaxBackward0>)
masked_adj tensor([0.4547, 0.2793, 0.3101, 0.3151, 0.2813, 0.3092, 0.3032, 0.3116, 0.3066,
        0.2984, 0.3048, 0.3103, 0.3064, 0.3056, 0.3066, 0.2915, 0.2972, 0.3037,
        0.3053, 0.2569, 0.3066, 0.3371, 0.2964, 0.3061, 0.3710, 0.3740, 0.2736,
        0.3957, 0.3040, 0.2299, 0.3041, 0.3044, 0.3102, 0.3072, 0.3040, 0.3226,
        0.2471, 0.3049, 0.2815, 0.3067, 0.3113, 0.3241, 0.3072, 0.3058, 0.3095,
        0.8202, 0.3092, 0.3052, 0.3074, 0.3061, 0.2318, 0.2649, 0.3052, 0.3558,
        0.3069, 0.3065, 0.3103, 0.2296, 0.3033, 0.2809, 0.3035, 0.3569, 0.3239,
        0.3898, 0.4060, 0.3115, 0.8290, 0.2712, 0.2592, 0.3065, 0.3262, 0.4058,
        0.3171, 0.2762, 0.2388, 0.2664, 0.2813, 0.3243, 0.3304, 0.3076, 0.2271,
        0.3201, 0.3414, 0.3351, 0.2671, 0.2979, 0.4001, 0.2554, 0.3231, 0.3455,
        0.3429, 0.2846, 0.3662, 0.3238, 0.4458, 0.2998, 0.3664, 0.2885, 0.3421,
        0.2408, 0.2937, 0.2840, 0.3511, 0.2809, 0.3569, 0.3265, 0.3478, 0.3232,
        0.2770, 0.2636, 0.3233, 0.2659, 0.3192, 0.2786, 0.3204, 0.3195, 0.3919,
        0.3273, 0.3008], grad_fn=<MulBackward0>)
res: tensor([0.6112, 0.1567, 0.0993, 0.1328], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.8202, 0.8290], grad_fn=<IndexBackward0>)
num_high 2 len(mask) 119
pred_loss tensor([0.4924], grad_fn=<MulBackward0>)
size_loss tensor(-0.0392, grad_fn=<MulBackward0>)
size_num_loss 0.016807563025210085
loss: tensor([0.4924], grad_fn=<MulBackward0>)
7
epoch:  7 ; loss:  0.4923798441886902 ; pred:  tensor([0.6112, 0.1567, 0.0993, 0.1328], grad_fn=<SoftmaxBackward0>)
masked_adj tensor([0.4037, 0.2511, 0.3240, 0.3473, 0.2973, 0.3364, 0.3234, 0.3409, 0.3309,
        0.2737, 0.3270, 0.3384, 0.3306, 0.3287, 0.3310, 0.3071, 0.2722, 0.3245,
        0.3281, 0.2726, 0.3309, 0.3473, 0.3116, 0.2835, 0.3754, 0.3780, 0.2899,
        0.4030, 0.3253, 0.2414, 0.3255, 0.3189, 0.3382, 0.3322, 0.3252, 0.3037,
        0.2618, 0.3273, 0.3070, 0.3311, 0.3403, 0.3362, 0.3323, 0.3292, 0.3369,
        0.8549, 0.3364, 0.3280, 0.3327, 0.3298, 0.2438, 0.2810, 0.3280, 0.3629,
        0.3316, 0.3307, 0.3384, 0.2411, 0.3236, 0.2970, 0.3239, 0.3028, 0.3552,
        0.3909, 0.4072, 0.3285, 0.8638, 0.2891, 0.2772, 0.3281, 0.3578, 0.4212,
        0.3419, 0.2958, 0.2532, 0.2853, 0.3009, 0.3531, 0.3674, 0.3322, 0.2379,
        0.3462, 0.3775, 0.3655, 0.2832, 0.3224, 0.4212, 0.2745, 0.3480, 0.3788,
        0.3779, 0.3029, 0.4092, 0.3519, 0.4660, 0.3258, 0.4134, 0.3114, 0.3701,
        0.2555, 0.3148, 0.3041, 0.3662, 0.3014, 0.3760, 0.3562, 0.3653, 0.3497,
        0.2965, 0.2820, 0.3519, 0.2849, 0.3465, 0.2986, 0.3468, 0.3458, 0.4478,
        0.3565, 0.3222], grad_fn=<MulBackward0>)
res: tensor([0.6785, 0.1330, 0.0763, 0.1122], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.8549, 0.8638], grad_fn=<IndexBackward0>)
num_high 2 len(mask) 119
pred_loss tensor([0.3879], grad_fn=<MulBackward0>)
size_loss tensor(-0.0389, grad_fn=<MulBackward0>)
size_num_loss 0.016807563025210085
loss: tensor([0.3879], grad_fn=<MulBackward0>)
8
epoch:  8 ; loss:  0.3878600001335144 ; pred:  tensor([0.6785, 0.1330, 0.0763, 0.1122], grad_fn=<SoftmaxBackward0>)
masked_adj tensor([0.3604, 0.2300, 0.3492, 0.3886, 0.3253, 0.3734, 0.3544, 0.3797, 0.3655,
        0.2577, 0.3598, 0.3763, 0.3651, 0.3624, 0.3657, 0.3344, 0.2557, 0.3561,
        0.3614, 0.3003, 0.3656, 0.3679, 0.3385, 0.2700, 0.3891, 0.3909, 0.3181,
        0.4183, 0.3573, 0.2646, 0.3576, 0.3449, 0.3760, 0.3674, 0.3571, 0.2946,
        0.2885, 0.3603, 0.3444, 0.3658, 0.3789, 0.3593, 0.3675, 0.3631, 0.3741,
        0.8805, 0.3734, 0.3613, 0.3681, 0.3640, 0.2674, 0.3092, 0.3613, 0.3798,
        0.3665, 0.3652, 0.3763, 0.2641, 0.3547, 0.3250, 0.3552, 0.2591, 0.3964,
        0.4002, 0.4160, 0.3569, 0.8891, 0.3190, 0.3075, 0.3607, 0.3992, 0.4447,
        0.3775, 0.3275, 0.2795, 0.3163, 0.3326, 0.3919, 0.4130, 0.3671, 0.2602,
        0.3826, 0.4228, 0.4057, 0.3116, 0.3587, 0.4506, 0.3054, 0.3844, 0.4216,
        0.4222, 0.3330, 0.4597, 0.3899, 0.4926, 0.3635, 0.4676, 0.3462, 0.4093,
        0.2818, 0.3478, 0.3362, 0.3915, 0.3338, 0.4049, 0.3958, 0.3931, 0.3865,
        0.3279, 0.3123, 0.3908, 0.3160, 0.3842, 0.3306, 0.3834, 0.3823, 0.5126,
        0.3956, 0.3552], grad_fn=<MulBackward0>)
res: tensor([0.7654, 0.1003, 0.0501, 0.0842], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.8805, 0.8891, 0.5126], grad_fn=<IndexBackward0>)
num_high 3 len(mask) 119
pred_loss tensor([0.2674], grad_fn=<MulBackward0>)
size_loss tensor(-46.1969, grad_fn=<MulBackward0>)
size_num_loss 0.0252109243697479
loss: tensor([0.2674], grad_fn=<MulBackward0>)
9
epoch:  9 ; loss:  0.26737329363822937 ; pred:  tensor([0.7654, 0.1003, 0.0501, 0.0842], grad_fn=<SoftmaxBackward0>)
Finished Training
sel masked ver tensor(indices=tensor([[6881, 5502, 5678],
                       [5357, 5678, 5939]]),
       values=tensor([0.8805, 0.8891, 0.5126]),
       size=(6882, 5940), nnz=3, layout=torch.sparse_coo,
       grad_fn=<SparseCooTensorWithDimsAndTensorsBackward0>)
Counter({'member': 1, 'worksAtProject': 1, 'isAbout': 1})
dict index: {}
node_idx 5678
 node original label [0]
 node predicted label explain 0
 node prediction probability explain tensor([0.7654, 0.1003, 0.0501, 0.0842], grad_fn=<SoftmaxBackward0>)
 node predicted label full 0 most important relations  {'member': 1, 'worksAtProject': 1, 'isAbout': 1, 'label': 0, 'node_idx': '5678'}
 final masks and lenght tensor(indices=tensor([[ 23451,  23458,  23551,  23567,  23615,  23638,  23670,
                         23963,  24431,  24481,  24538,  24580,  24582,  24583,
                         24584,  24585,  39077,  39079,  46927,  46927,  63352,
                         63403,  63408,  63426,  63445,  63489,  81452,  88528,
                        114586, 114592, 114593, 114702, 115616, 115715, 115717,
                        115718, 115719, 115720, 129953, 146782, 146782, 146784,
                        146784, 146784, 146784, 147726, 147732, 147826, 147842,
                        147890, 147913, 147945, 148238, 148238, 148756, 148855,
                        148857, 148858, 148859, 148860, 154487, 154538, 154543,
                        154561, 154580, 154624, 179487, 179922, 179924, 196233,
                        229373, 237658, 246202, 246204, 246204, 246204, 246204,
                        246204, 246204, 246204, 246204, 246204, 254228, 254228,
                        254228, 254228, 254228, 254228, 254228, 254228, 254228,
                        254228, 254228, 254228, 254228, 254228, 254228, 254228,
                        254228, 262337, 262337, 262337, 262337, 262337, 262337,
                        262337, 262337, 262337, 262337, 262337, 262337, 262337,
                        262337, 262337, 262337, 262337, 303938, 328793, 328793],
                       [  5678,   5678,   5678,   5678,   5678,   5678,   5678,
                          5678,   5678,   5678,   5678,   5678,   5678,   5678,
                          5678,   5678,   5502,   5502,   5937,   5939,   5939,
                          5939,   5939,   5937,   5939,   5937,   5678,     22,
                          5939,   5939,   5937,   5939,   5939,   5939,   5939,
                          5939,   5939,   5939,    117,   5431,   5494,   5357,
                          5408,   5413,   5450,   5357,   5450,   5450,   5450,
                          5450,   5450,   5450,   5357,   5450,   5450,   5450,
                          5450,   5450,   5450,   5450,   5678,   5678,   5678,
                          5678,   5678,   5678,   5678,   5678,   5678,   2215,
                            35,   5535,   6888,   6881,   6887,   6997,   7911,
                          8010,   8012,   8013,   8014,   8015,   6881,   6887,
                          6888,   6981,   6997,   7045,   7068,   7100,   7393,
                          7861,   7911,   7968,   8010,   8012,   8013,   8014,
                          8015,   6881,   6887,   6888,   6981,   6997,   7045,
                          7068,   7100,   7393,   7861,   7911,   7968,   8010,
                          8012,   8013,   8014,   8015,   5939,   5230,   5231]]),
       values=tensor([0.3604, 0.2300, 0.3492, 0.3886, 0.3253, 0.3734, 0.3544,
                      0.3797, 0.3655, 0.2577, 0.3598, 0.3763, 0.3651, 0.3624,
                      0.3657, 0.3344, 0.2557, 0.3561, 0.3614, 0.3003, 0.3656,
                      0.3679, 0.3385, 0.2700, 0.3891, 0.3909, 0.3181, 0.4183,
                      0.3573, 0.2646, 0.3576, 0.3449, 0.3760, 0.3674, 0.3571,
                      0.2946, 0.2885, 0.3603, 0.3444, 0.3658, 0.3789, 0.3593,
                      0.3675, 0.3631, 0.3741, 0.8805, 0.3734, 0.3613, 0.3681,
                      0.3640, 0.2674, 0.3092, 0.3613, 0.3798, 0.3665, 0.3652,
                      0.3763, 0.2641, 0.3547, 0.3250, 0.3552, 0.2591, 0.3964,
                      0.4002, 0.4160, 0.3569, 0.8891, 0.3190, 0.3075, 0.3607,
                      0.3992, 0.4447, 0.3775, 0.3275, 0.2795, 0.3163, 0.3326,
                      0.3919, 0.4130, 0.3671, 0.2602, 0.3826, 0.4228, 0.4057,
                      0.3116, 0.3587, 0.4506, 0.3054, 0.3844, 0.4216, 0.4222,
                      0.3330, 0.4597, 0.3899, 0.4926, 0.3635, 0.4676, 0.3462,
                      0.4093, 0.2818, 0.3478, 0.3362, 0.3915, 0.3338, 0.4049,
                      0.3958, 0.3931, 0.3865, 0.3279, 0.3123, 0.3908, 0.3160,
                      0.3842, 0.3306, 0.3834, 0.3823, 0.5126, 0.3956, 0.3552]),
       size=(753935, 8285), nnz=119, layout=torch.sparse_coo,
       grad_fn=<SparseCooTensorWithDimsAndTensorsBackward0>) 3 ---------------------------------------------------------------
r_exp.py:102: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  hor_graph, ver_graph = torch.tensor(sub_hor_graph, dtype=torch.float), torch.tensor(sub_ver_graph, dtype=torch.float)
wandb: WARNING Calling wandb.login() after wandb.init() has no effect.
node label: 0
5
masked_adj tensor([1.5299, 0.7444, 0.7550, 1.5091, 0.5986, 0.7110, 0.8985],
       grad_fn=<MulBackward0>)
res: tensor([0.3776, 0.2412, 0.2576, 0.1236], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.7649, 0.7444, 0.7550, 0.7546, 0.5986, 0.7110, 0.8985],
       grad_fn=<IndexBackward0>)
num_high 7 len(mask) 7
pred_loss tensor([0.9740], grad_fn=<MulBackward0>)
size_loss tensor(2.5357, grad_fn=<AddBackward0>)
size_num_loss 1.0000142857142857
loss: tensor([5.0570], grad_fn=<AddBackward0>)
0
epoch:  0 ; loss:  5.056957244873047 ; pred:  tensor([0.3776, 0.2412, 0.2576, 0.1236], grad_fn=<SoftmaxBackward0>)
masked_adj tensor([1.3275, 0.6385, 0.6514, 1.3019, 0.4750, 0.5988, 0.8429],
       grad_fn=<MulBackward0>)
res: tensor([0.3623, 0.2385, 0.2622, 0.1369], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.6637, 0.6385, 0.6514, 0.6509, 0.5988, 0.8429],
       grad_fn=<IndexBackward0>)
num_high 6 len(mask) 7
pred_loss tensor([1.0154], grad_fn=<MulBackward0>)
size_loss tensor(-7.3215, grad_fn=<MulBackward0>)
size_num_loss 0.8571571428571428
loss: tensor([-4.8224], grad_fn=<AddBackward0>)
1
epoch:  1 ; loss:  -4.822382926940918 ; pred:  tensor([0.3623, 0.2385, 0.2622, 0.1369], grad_fn=<SoftmaxBackward0>)
masked_adj tensor([1.1282, 0.5451, 0.5578, 1.1114, 0.4044, 0.5050, 0.8855],
       grad_fn=<MulBackward0>)
res: tensor([0.3545, 0.2387, 0.2568, 0.1499], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.5641, 0.5451, 0.5578, 0.5557, 0.5050, 0.8855],
       grad_fn=<IndexBackward0>)
num_high 6 len(mask) 7
pred_loss tensor([1.0370], grad_fn=<MulBackward0>)
size_loss tensor(-19.7092, grad_fn=<MulBackward0>)
size_num_loss 0.8571571428571428
loss: tensor([-17.1763], grad_fn=<AddBackward0>)
2
epoch:  2 ; loss:  -17.176334381103516 ; pred:  tensor([0.3545, 0.2387, 0.2568, 0.1499], grad_fn=<SoftmaxBackward0>)
masked_adj tensor([0.9325, 0.4387, 0.4533, 0.9017, 0.3675, 0.3985, 0.9221],
       grad_fn=<MulBackward0>)
res: tensor([0.3457, 0.2420, 0.2469, 0.1654], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.9221], grad_fn=<IndexBackward0>)
num_high 1 len(mask) 7
pred_loss tensor([1.0621], grad_fn=<MulBackward0>)
size_loss tensor(nan, grad_fn=<MulBackward0>)
size_num_loss 0.14287142857142857
loss: tensor([1.0621], grad_fn=<MulBackward0>)
3
epoch:  3 ; loss:  1.0621469020843506 ; pred:  tensor([0.3457, 0.2420, 0.2469, 0.1654], grad_fn=<SoftmaxBackward0>)
masked_adj tensor([0.7757, 0.3556, 0.3712, 0.7353, 0.3586, 0.3178, 0.9435],
       grad_fn=<MulBackward0>)
res: tensor([0.3386, 0.2459, 0.2374, 0.1781], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.9435], grad_fn=<IndexBackward0>)
num_high 1 len(mask) 7
pred_loss tensor([1.0829], grad_fn=<MulBackward0>)
size_loss tensor(nan, grad_fn=<MulBackward0>)
size_num_loss 0.14287142857142857
loss: tensor([1.0829], grad_fn=<MulBackward0>)
4
epoch:  4 ; loss:  1.082898497581482 ; pred:  tensor([0.3386, 0.2459, 0.2374, 0.1781], grad_fn=<SoftmaxBackward0>)
masked_adj tensor([0.6529, 0.2921, 0.3083, 0.6065, 0.3695, 0.2573, 0.9569],
       grad_fn=<MulBackward0>)
res: tensor([0.3333, 0.2501, 0.2285, 0.1882], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.9569], grad_fn=<IndexBackward0>)
num_high 1 len(mask) 7
pred_loss tensor([1.0988], grad_fn=<MulBackward0>)
size_loss tensor(nan, grad_fn=<MulBackward0>)
size_num_loss 0.14287142857142857
loss: tensor([1.0988], grad_fn=<MulBackward0>)
5
epoch:  5 ; loss:  1.0988291501998901 ; pred:  tensor([0.3333, 0.2501, 0.2285, 0.1882], grad_fn=<SoftmaxBackward0>)
masked_adj tensor([0.5576, 0.2438, 0.2604, 0.5077, 0.3947, 0.2121, 0.9658],
       grad_fn=<MulBackward0>)
res: tensor([0.3294, 0.2545, 0.2203, 0.1958], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.9658], grad_fn=<IndexBackward0>)
num_high 1 len(mask) 7
pred_loss tensor([1.1105], grad_fn=<MulBackward0>)
size_loss tensor(nan, grad_fn=<MulBackward0>)
size_num_loss 0.14287142857142857
loss: tensor([1.1105], grad_fn=<MulBackward0>)
6
epoch:  6 ; loss:  1.1104851961135864 ; pred:  tensor([0.3294, 0.2545, 0.2203, 0.1958], grad_fn=<SoftmaxBackward0>)
masked_adj tensor([0.4840, 0.2072, 0.2241, 0.4322, 0.4298, 0.1780, 0.9719],
       grad_fn=<MulBackward0>)
res: tensor([0.3267, 0.2591, 0.2127, 0.2015], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.9719], grad_fn=<IndexBackward0>)
num_high 1 len(mask) 7
pred_loss tensor([1.1187], grad_fn=<MulBackward0>)
size_loss tensor(nan, grad_fn=<MulBackward0>)
size_num_loss 0.14287142857142857
loss: tensor([1.1187], grad_fn=<MulBackward0>)
7
epoch:  7 ; loss:  1.1187057495117188 ; pred:  tensor([0.3267, 0.2591, 0.2127, 0.2015], grad_fn=<SoftmaxBackward0>)
masked_adj tensor([0.4273, 0.1793, 0.1966, 0.3744, 0.4709, 0.1521, 0.9763],
       grad_fn=<MulBackward0>)
res: tensor([0.3249, 0.2637, 0.2057, 0.2057], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.9763], grad_fn=<IndexBackward0>)
num_high 1 len(mask) 7
pred_loss tensor([1.1244], grad_fn=<MulBackward0>)
size_loss tensor(nan, grad_fn=<MulBackward0>)
size_num_loss 0.14287142857142857
loss: tensor([1.1244], grad_fn=<MulBackward0>)
8
epoch:  8 ; loss:  1.124354600906372 ; pred:  tensor([0.3249, 0.2637, 0.2057, 0.2057], grad_fn=<SoftmaxBackward0>)
masked_adj tensor([0.3837, 0.1579, 0.1756, 0.3300, 0.5135, 0.1322, 0.9795],
       grad_fn=<MulBackward0>)
res: tensor([0.3236, 0.2682, 0.1995, 0.2087], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.5135, 0.9795], grad_fn=<IndexBackward0>)
num_high 2 len(mask) 7
pred_loss tensor([1.1282], grad_fn=<MulBackward0>)
size_loss tensor(-108.5509, grad_fn=<MulBackward0>)
size_num_loss 0.28572857142857144
loss: tensor([1.1282], grad_fn=<MulBackward0>)
9
epoch:  9 ; loss:  1.1282082796096802 ; pred:  tensor([0.3236, 0.2682, 0.1995, 0.2087], grad_fn=<SoftmaxBackward0>)
Finished Training
sel masked ver tensor(indices=tensor([[5724, 5724, 5724],
                       [   0,    0, 5230]]),
       values=tensor([0.6318, 2.0541, 0.9795]),
       size=(5725, 5231), nnz=3, layout=torch.sparse_coo,
       grad_fn=<SparseCooTensorWithDimsAndTensorsBackward0>)
Counter({'fax': 1, 'phone': 1, 'type': 1})
dict index: {}
node_idx 5724
 node original label [0]
 node predicted label explain 0
 node prediction probability explain tensor([0.3380, 0.3121, 0.1625, 0.1875], grad_fn=<SoftmaxBackward0>)
 node predicted label full 0 most important relations  {'fax': 1, 'phone': 1, 'type': 1, 'label': 0, 'node_idx': '5724'}
 final masks and lenght tensor(indices=tensor([[ 23897,  88574, 129999, 196279, 229419, 254274, 328839],
                       [  5724,      0,   3162,   4552,      0,   7327,   5230]]),
       values=tensor([0.1919, 0.6318, 0.1756, 0.1650, 2.0541, 0.1322, 0.9795]),
       size=(753935, 8285), nnz=7, layout=torch.sparse_coo,
       grad_fn=<SparseCooTensorWithDimsAndTensorsBackward0>) 3 ---------------------------------------------------------------
wandb: WARNING Calling wandb.login() after wandb.init() has no effect.
node label: 0
4
masked_adj tensor([1.5416, 0.7468, 0.7592, 1.5174, 0.5720], grad_fn=<MulBackward0>)
res: tensor([0.3628, 0.2503, 0.2652, 0.1217], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.7708, 0.7468, 0.7592, 0.7587, 0.5720], grad_fn=<IndexBackward0>)
num_high 5 len(mask) 5
pred_loss tensor([1.0140], grad_fn=<MulBackward0>)
size_loss tensor(1.7331, grad_fn=<AddBackward0>)
size_num_loss 1.00002
loss: tensor([4.3254], grad_fn=<AddBackward0>)
0
epoch:  0 ; loss:  4.3254170417785645 ; pred:  tensor([0.3628, 0.2503, 0.2652, 0.1217], grad_fn=<SoftmaxBackward0>)
masked_adj tensor([1.3421, 0.6414, 0.6566, 1.3121, 0.4477], grad_fn=<MulBackward0>)
res: tensor([0.3516, 0.2496, 0.2665, 0.1323], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.6711, 0.6414, 0.6566, 0.6560], grad_fn=<IndexBackward0>)
num_high 4 len(mask) 5
pred_loss tensor([1.0453], grad_fn=<MulBackward0>)
size_loss tensor(-0.1465, grad_fn=<MulBackward0>)
size_num_loss 0.80002
loss: tensor([2.3509], grad_fn=<AddBackward0>)
1
epoch:  1 ; loss:  2.3509373664855957 ; pred:  tensor([0.3516, 0.2496, 0.2665, 0.1323], grad_fn=<SoftmaxBackward0>)
masked_adj tensor([1.4881, 0.5485, 0.5740, 1.0935, 0.3733], grad_fn=<MulBackward0>)
res: tensor([0.3453, 0.2463, 0.2750, 0.1334], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.7441, 0.5485, 0.5740, 0.5468], grad_fn=<IndexBackward0>)
num_high 4 len(mask) 5
pred_loss tensor([1.0633], grad_fn=<MulBackward0>)
size_loss tensor(-8.9563, grad_fn=<MulBackward0>)
size_num_loss 0.80002
loss: tensor([-6.4352], grad_fn=<AddBackward0>)
2
epoch:  2 ; loss:  -6.4351911544799805 ; pred:  tensor([0.3453, 0.2463, 0.2750, 0.1334], grad_fn=<SoftmaxBackward0>)
masked_adj tensor([1.6098, 0.4531, 0.4927, 0.9318, 0.3286], grad_fn=<MulBackward0>)
res: tensor([0.3400, 0.2456, 0.2790, 0.1354], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.8049], grad_fn=<IndexBackward0>)
num_high 1 len(mask) 5
pred_loss tensor([1.0789], grad_fn=<MulBackward0>)
size_loss tensor(nan, grad_fn=<MulBackward0>)
size_num_loss 0.20002
loss: tensor([1.0789], grad_fn=<MulBackward0>)
3
epoch:  3 ; loss:  1.078879475593567 ; pred:  tensor([0.3400, 0.2456, 0.2790, 0.1354], grad_fn=<SoftmaxBackward0>)
masked_adj tensor([1.6915, 0.3776, 0.4258, 0.8022, 0.3108], grad_fn=<MulBackward0>)
res: tensor([0.3359, 0.2451, 0.2813, 0.1377], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.8457], grad_fn=<IndexBackward0>)
num_high 1 len(mask) 5
pred_loss tensor([1.0908], grad_fn=<MulBackward0>)
size_loss tensor(nan, grad_fn=<MulBackward0>)
size_num_loss 0.20002
loss: tensor([1.0908], grad_fn=<MulBackward0>)
4
epoch:  4 ; loss:  1.0908008813858032 ; pred:  tensor([0.3359, 0.2451, 0.2813, 0.1377], grad_fn=<SoftmaxBackward0>)
masked_adj tensor([1.7486, 0.3186, 0.3713, 0.6985, 0.3122], grad_fn=<MulBackward0>)
res: tensor([0.3330, 0.2445, 0.2825, 0.1400], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.8743], grad_fn=<IndexBackward0>)
num_high 1 len(mask) 5
pred_loss tensor([1.0996], grad_fn=<MulBackward0>)
size_loss tensor(nan, grad_fn=<MulBackward0>)
size_num_loss 0.20002
loss: tensor([1.0996], grad_fn=<MulBackward0>)
5
epoch:  5 ; loss:  1.0996043682098389 ; pred:  tensor([0.3330, 0.2445, 0.2825, 0.1400], grad_fn=<SoftmaxBackward0>)
masked_adj tensor([1.7898, 0.2726, 0.3271, 0.6155, 0.3281], grad_fn=<MulBackward0>)
res: tensor([0.3309, 0.2440, 0.2829, 0.1422], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.8949], grad_fn=<IndexBackward0>)
num_high 1 len(mask) 5
pred_loss tensor([1.1059], grad_fn=<MulBackward0>)
size_loss tensor(nan, grad_fn=<MulBackward0>)
size_num_loss 0.20002
loss: tensor([1.1059], grad_fn=<MulBackward0>)
6
epoch:  6 ; loss:  1.1058752536773682 ; pred:  tensor([0.3309, 0.2440, 0.2829, 0.1422], grad_fn=<SoftmaxBackward0>)
masked_adj tensor([1.8204, 0.2367, 0.2914, 0.5488, 0.3552], grad_fn=<MulBackward0>)
res: tensor([0.3295, 0.2435, 0.2827, 0.1442], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.9102], grad_fn=<IndexBackward0>)
num_high 1 len(mask) 5
pred_loss tensor([1.1101], grad_fn=<MulBackward0>)
size_loss tensor(nan, grad_fn=<MulBackward0>)
size_num_loss 0.20002
loss: tensor([1.1101], grad_fn=<MulBackward0>)
7
epoch:  7 ; loss:  1.1101433038711548 ; pred:  tensor([0.3295, 0.2435, 0.2827, 0.1442], grad_fn=<SoftmaxBackward0>)
masked_adj tensor([1.8434, 0.2085, 0.2626, 0.4950, 0.3908], grad_fn=<MulBackward0>)
res: tensor([0.3286, 0.2431, 0.2821, 0.1461], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.9217], grad_fn=<IndexBackward0>)
num_high 1 len(mask) 5
pred_loss tensor([1.1129], grad_fn=<MulBackward0>)
size_loss tensor(nan, grad_fn=<MulBackward0>)
size_num_loss 0.20002
loss: tensor([1.1129], grad_fn=<MulBackward0>)
8
epoch:  8 ; loss:  1.1128616333007812 ; pred:  tensor([0.3286, 0.2431, 0.2821, 0.1461], grad_fn=<SoftmaxBackward0>)
masked_adj tensor([1.8612, 0.1863, 0.2394, 0.4516, 0.4322], grad_fn=<MulBackward0>)
res: tensor([0.3281, 0.2427, 0.2813, 0.1479], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.9306], grad_fn=<IndexBackward0>)
num_high 1 len(mask) 5
pred_loss tensor([1.1144], grad_fn=<MulBackward0>)
size_loss tensor(nan, grad_fn=<MulBackward0>)
size_num_loss 0.20002
loss: tensor([1.1144], grad_fn=<MulBackward0>)
9
epoch:  9 ; loss:  1.1144241094589233 ; pred:  tensor([0.3281, 0.2427, 0.2813, 0.1479], grad_fn=<SoftmaxBackward0>)
Finished Training
sel masked ver tensor(indices=tensor([[5699, 5699],
                       [   0,    0]]),
       values=tensor([3.7224, 0.9032]),
       size=(5700, 1), nnz=2, layout=torch.sparse_coo,
       grad_fn=<SparseCooTensorWithDimsAndTensorsBackward0>)
Counter({'fax': 1, 'phone': 1})
dict index: {}
node_idx 5699
 node original label [0]
 node predicted label explain 0
 node prediction probability explain tensor([0.3564, 0.2532, 0.2911, 0.0992], grad_fn=<SoftmaxBackward0>)
 node predicted label full 0 most important relations  {'fax': 1, 'phone': 1, 'label': 0, 'node_idx': '5699'}
 final masks and lenght tensor(indices=tensor([[ 88549, 129974, 196254, 229394, 328814],
                       [     0,   3162,   3153,      0,   5230]]),
       values=tensor([3.7224, 0.1863, 0.2394, 0.9032, 0.4322]),
       size=(753935, 8285), nnz=5, layout=torch.sparse_coo,
       grad_fn=<SparseCooTensorWithDimsAndTensorsBackward0>) 2 ---------------------------------------------------------------
wandb: WARNING Calling wandb.login() after wandb.init() has no effect.
node label: 0
3
masked_adj tensor([1.5505, 0.7486, 1.5248, 0.7619], grad_fn=<MulBackward0>)
res: tensor([0.3141, 0.3214, 0.2250, 0.1395], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.7752, 0.7486, 0.7624, 0.7619], grad_fn=<IndexBackward0>)
num_high 4 len(mask) 4
pred_loss tensor([1.1581], grad_fn=<MulBackward0>)
size_loss tensor(1.5228, grad_fn=<AddBackward0>)
size_num_loss 1.000025
loss: tensor([4.2295], grad_fn=<AddBackward0>)
0
epoch:  0 ; loss:  4.229499816894531 ; pred:  tensor([0.3141, 0.3214, 0.2250, 0.1395], grad_fn=<SoftmaxBackward0>)
masked_adj tensor([1.3532, 0.6436, 1.3211, 0.6599], grad_fn=<MulBackward0>)
res: tensor([0.3114, 0.3095, 0.2298, 0.1494], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.6766, 0.6436, 0.6606, 0.6599], grad_fn=<IndexBackward0>)
num_high 4 len(mask) 4
pred_loss tensor([1.1667], grad_fn=<MulBackward0>)
size_loss tensor(1.3185, grad_fn=<AddBackward0>)
size_num_loss 1.000025
loss: tensor([4.1259], grad_fn=<AddBackward0>)
1
epoch:  1 ; loss:  4.125886917114258 ; pred:  tensor([0.3114, 0.3095, 0.2298, 0.1494], grad_fn=<SoftmaxBackward0>)
masked_adj tensor([1.1218, 0.5228, 1.0840, 0.5411], grad_fn=<MulBackward0>)
res: tensor([0.3077, 0.2957, 0.2351, 0.1615], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.5609, 0.5228, 0.5420, 0.5411], grad_fn=<IndexBackward0>)
num_high 4 len(mask) 4
pred_loss tensor([1.1785], grad_fn=<MulBackward0>)
size_loss tensor(1.0810, grad_fn=<AddBackward0>)
size_num_loss 1.000025
loss: tensor([3.9488], grad_fn=<AddBackward0>)
2
epoch:  2 ; loss:  3.948819398880005 ; pred:  tensor([0.3077, 0.2957, 0.2351, 0.1615], grad_fn=<SoftmaxBackward0>)
masked_adj tensor([0.8816, 0.3993, 0.8390, 0.4182], grad_fn=<MulBackward0>)
res: tensor([0.3034, 0.2816, 0.2403, 0.1747], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([], grad_fn=<IndexBackward0>)
num_high 0 len(mask) 4
pred_loss tensor([1.1928], grad_fn=<MulBackward0>)
size_loss tensor(nan, grad_fn=<MulBackward0>)
size_num_loss 2.5e-05
loss: tensor([1.1928], grad_fn=<MulBackward0>)
3
epoch:  3 ; loss:  1.1927703619003296 ; pred:  tensor([0.3034, 0.2816, 0.2403, 0.1747], grad_fn=<SoftmaxBackward0>)
masked_adj tensor([0.7337, 0.3102, 0.6849, 0.3372], grad_fn=<MulBackward0>)
res: tensor([0.3007, 0.2725, 0.2430, 0.1838], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([], grad_fn=<IndexBackward0>)
num_high 0 len(mask) 4
pred_loss tensor([1.2017], grad_fn=<MulBackward0>)
size_loss tensor(nan, grad_fn=<MulBackward0>)
size_num_loss 2.5e-05
loss: tensor([1.2017], grad_fn=<MulBackward0>)
4
epoch:  4 ; loss:  1.2017062902450562 ; pred:  tensor([0.3007, 0.2725, 0.2430, 0.1838], grad_fn=<SoftmaxBackward0>)
masked_adj tensor([0.6723, 0.2549, 0.6126, 0.2954], grad_fn=<MulBackward0>)
res: tensor([0.2998, 0.2680, 0.2438, 0.1885], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([], grad_fn=<IndexBackward0>)
num_high 0 len(mask) 4
pred_loss tensor([1.2048], grad_fn=<MulBackward0>)
size_loss tensor(nan, grad_fn=<MulBackward0>)
size_num_loss 2.5e-05
loss: tensor([1.2048], grad_fn=<MulBackward0>)
5
epoch:  5 ; loss:  1.2047746181488037 ; pred:  tensor([0.2998, 0.2680, 0.2438, 0.1885], grad_fn=<SoftmaxBackward0>)
masked_adj tensor([0.6730, 0.2262, 0.5990, 0.2827], grad_fn=<MulBackward0>)
res: tensor([0.3002, 0.2666, 0.2434, 0.1898], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([], grad_fn=<IndexBackward0>)
num_high 0 len(mask) 4
pred_loss tensor([1.2034], grad_fn=<MulBackward0>)
size_loss tensor(nan, grad_fn=<MulBackward0>)
size_num_loss 2.5e-05
loss: tensor([1.2034], grad_fn=<MulBackward0>)
6
epoch:  6 ; loss:  1.2033932209014893 ; pred:  tensor([0.3002, 0.2666, 0.2434, 0.1898], grad_fn=<SoftmaxBackward0>)
masked_adj tensor([0.7172, 0.2169, 0.6273, 0.2908], grad_fn=<MulBackward0>)
res: tensor([0.3015, 0.2676, 0.2422, 0.1886], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([], grad_fn=<IndexBackward0>)
num_high 0 len(mask) 4
pred_loss tensor([1.1988], grad_fn=<MulBackward0>)
size_loss tensor(nan, grad_fn=<MulBackward0>)
size_num_loss 2.5e-05
loss: tensor([1.1988], grad_fn=<MulBackward0>)
7
epoch:  7 ; loss:  1.1988232135772705 ; pred:  tensor([0.3015, 0.2676, 0.2422, 0.1886], grad_fn=<SoftmaxBackward0>)
masked_adj tensor([0.7931, 0.2220, 0.6873, 0.3148], grad_fn=<MulBackward0>)
res: tensor([0.3036, 0.2704, 0.2403, 0.1857], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([], grad_fn=<IndexBackward0>)
num_high 0 len(mask) 4
pred_loss tensor([1.1920], grad_fn=<MulBackward0>)
size_loss tensor(nan, grad_fn=<MulBackward0>)
size_num_loss 2.5e-05
loss: tensor([1.1920], grad_fn=<MulBackward0>)
8
epoch:  8 ; loss:  1.1919578313827515 ; pred:  tensor([0.3036, 0.2704, 0.2403, 0.1857], grad_fn=<SoftmaxBackward0>)
masked_adj tensor([0.8909, 0.2389, 0.7716, 0.3514], grad_fn=<MulBackward0>)
res: tensor([0.3062, 0.2745, 0.2378, 0.1816], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([], grad_fn=<IndexBackward0>)
num_high 0 len(mask) 4
pred_loss tensor([1.1836], grad_fn=<MulBackward0>)
size_loss tensor(nan, grad_fn=<MulBackward0>)
size_num_loss 2.5e-05
loss: tensor([1.1836], grad_fn=<MulBackward0>)
9
epoch:  9 ; loss:  1.183628797531128 ; pred:  tensor([0.3062, 0.2745, 0.2378, 0.1816], grad_fn=<SoftmaxBackward0>)
Finished Training
sel masked ver tensor(indices=tensor([[5688, 5688],
                       [   0,    0]]),
       values=tensor([1.7818, 1.5432]),
       size=(5689, 1), nnz=2, layout=torch.sparse_coo,
       grad_fn=<SparseCooTensorWithDimsAndTensorsBackward0>)
Counter({'fax': 1, 'phone': 1})
dict index: {}
node_idx 5688
 node original label [0]
 node predicted label explain 0
 node prediction probability explain tensor([0.3277, 0.3081, 0.2160, 0.1482], grad_fn=<SoftmaxBackward0>)
 node predicted label full 0 most important relations  {'fax': 1, 'phone': 1, 'label': 0, 'node_idx': '5688'}
 final masks and lenght tensor(indices=tensor([[ 88538, 196243, 229383, 328803],
                       [     0,   1579,      0,   5230]]),
       values=tensor([1.7818, 0.2389, 1.5432, 0.3514]),
       size=(753935, 8285), nnz=4, layout=torch.sparse_coo,
       grad_fn=<SparseCooTensorWithDimsAndTensorsBackward0>) 2 ---------------------------------------------------------------
wandb: WARNING Calling wandb.login() after wandb.init() has no effect.
node label: 0
4
masked_adj tensor([1.5350, 0.7454, 1.5137, 0.7564, 0.5870, 0.7094],
       grad_fn=<MulBackward0>)
res: tensor([0.2956, 0.3296, 0.2312, 0.1436], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.7675, 0.7454, 0.7568, 0.7564, 0.5870, 0.7094],
       grad_fn=<IndexBackward0>)
num_high 6 len(mask) 6
pred_loss tensor([1.2188], grad_fn=<MulBackward0>)
size_loss tensor(2.1145, grad_fn=<AddBackward0>)
size_num_loss 1.0000166666666666
loss: tensor([4.9167], grad_fn=<AddBackward0>)
0
epoch:  0 ; loss:  4.916685104370117 ; pred:  tensor([0.2956, 0.3296, 0.2312, 0.1436], grad_fn=<SoftmaxBackward0>)
masked_adj tensor([1.3339, 0.6398, 1.3074, 0.6532, 0.4630, 0.5969],
       grad_fn=<MulBackward0>)
res: tensor([0.2961, 0.3155, 0.2352, 0.1532], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.6669, 0.6398, 0.6537, 0.6532, 0.5969], grad_fn=<IndexBackward0>)
num_high 5 len(mask) 6
pred_loss tensor([1.2169], grad_fn=<MulBackward0>)
size_loss tensor(-0.7315, grad_fn=<MulBackward0>)
size_num_loss 0.8333499999999999
loss: tensor([1.9764], grad_fn=<AddBackward0>)
1
epoch:  1 ; loss:  1.9763569831848145 ; pred:  tensor([0.2961, 0.3155, 0.2352, 0.1532], grad_fn=<SoftmaxBackward0>)
masked_adj tensor([1.4811, 0.5237, 1.4468, 0.7245, 0.3772, 0.5026],
       grad_fn=<MulBackward0>)
res: tensor([0.2936, 0.3234, 0.2286, 0.1544], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.7405, 0.5237, 0.7234, 0.7245, 0.5026], grad_fn=<IndexBackward0>)
num_high 5 len(mask) 6
pred_loss tensor([1.2255], grad_fn=<MulBackward0>)
size_loss tensor(-14.1352, grad_fn=<MulBackward0>)
size_num_loss 0.8333499999999999
loss: tensor([-11.4432], grad_fn=<AddBackward0>)
2
epoch:  2 ; loss:  -11.44321060180664 ; pred:  tensor([0.2936, 0.3234, 0.2286, 0.1544], grad_fn=<SoftmaxBackward0>)
masked_adj tensor([1.6145, 0.4422, 1.5771, 0.7894, 0.3176, 0.4056],
       grad_fn=<MulBackward0>)
res: tensor([0.2912, 0.3310, 0.2241, 0.1538], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.8072, 0.7885, 0.7894], grad_fn=<IndexBackward0>)
num_high 3 len(mask) 6
pred_loss tensor([1.2338], grad_fn=<MulBackward0>)
size_loss tensor(-0.1113, grad_fn=<MulBackward0>)
size_num_loss 0.5000166666666667
loss: tensor([1.2338], grad_fn=<MulBackward0>)
3
epoch:  3 ; loss:  1.2337738275527954 ; pred:  tensor([0.2912, 0.3310, 0.2241, 0.1538], grad_fn=<SoftmaxBackward0>)
masked_adj tensor([1.7019, 0.3777, 1.6645, 0.8331, 0.2845, 0.3311],
       grad_fn=<MulBackward0>)
res: tensor([0.2890, 0.3362, 0.2210, 0.1539], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.8509, 0.8322, 0.8331], grad_fn=<IndexBackward0>)
num_high 3 len(mask) 6
pred_loss tensor([1.2415], grad_fn=<MulBackward0>)
size_loss tensor(-0.1115, grad_fn=<MulBackward0>)
size_num_loss 0.5000166666666667
loss: tensor([1.2415], grad_fn=<MulBackward0>)
4
epoch:  4 ; loss:  1.2414603233337402 ; pred:  tensor([0.2890, 0.3362, 0.2210, 0.1539], grad_fn=<SoftmaxBackward0>)
masked_adj tensor([1.7613, 0.3266, 1.7251, 0.8635, 0.2716, 0.2743],
       grad_fn=<MulBackward0>)
res: tensor([0.2869, 0.3400, 0.2188, 0.1543], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.8807, 0.8626, 0.8635], grad_fn=<IndexBackward0>)
num_high 3 len(mask) 6
pred_loss tensor([1.2486], grad_fn=<MulBackward0>)
size_loss tensor(-0.1041, grad_fn=<MulBackward0>)
size_num_loss 0.5000166666666667
loss: tensor([1.2486], grad_fn=<MulBackward0>)
5
epoch:  5 ; loss:  1.248625636100769 ; pred:  tensor([0.2869, 0.3400, 0.2188, 0.1543], grad_fn=<SoftmaxBackward0>)
masked_adj tensor([1.8031, 0.2859, 1.7685, 0.8852, 0.2739, 0.2307],
       grad_fn=<MulBackward0>)
res: tensor([0.2850, 0.3429, 0.2172, 0.1549], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.9016, 0.8842, 0.8852], grad_fn=<IndexBackward0>)
num_high 3 len(mask) 6
pred_loss tensor([1.2554], grad_fn=<MulBackward0>)
size_loss tensor(-0.0948, grad_fn=<MulBackward0>)
size_num_loss 0.5000166666666667
loss: tensor([1.2554], grad_fn=<MulBackward0>)
6
epoch:  6 ; loss:  1.2553852796554565 ; pred:  tensor([0.2850, 0.3429, 0.2172, 0.1549], grad_fn=<SoftmaxBackward0>)
masked_adj tensor([1.8333, 0.2533, 1.8001, 0.9011, 0.2883, 0.1971],
       grad_fn=<MulBackward0>)
res: tensor([0.2831, 0.3452, 0.2161, 0.1555], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.9166, 0.9001, 0.9011], grad_fn=<IndexBackward0>)
num_high 3 len(mask) 6
pred_loss tensor([1.2619], grad_fn=<MulBackward0>)
size_loss tensor(-0.0862, grad_fn=<MulBackward0>)
size_num_loss 0.5000166666666667
loss: tensor([1.2619], grad_fn=<MulBackward0>)
7
epoch:  7 ; loss:  1.2618666887283325 ; pred:  tensor([0.2831, 0.3452, 0.2161, 0.1555], grad_fn=<SoftmaxBackward0>)
masked_adj tensor([1.8555, 0.2271, 1.8236, 0.9129, 0.3123, 0.1709],
       grad_fn=<MulBackward0>)
res: tensor([0.2813, 0.3471, 0.2153, 0.1562], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.9277, 0.9118, 0.9129], grad_fn=<IndexBackward0>)
num_high 3 len(mask) 6
pred_loss tensor([1.2682], grad_fn=<MulBackward0>)
size_loss tensor(-0.0789, grad_fn=<MulBackward0>)
size_num_loss 0.5000166666666667
loss: tensor([1.2682], grad_fn=<MulBackward0>)
8
epoch:  8 ; loss:  1.268173098564148 ; pred:  tensor([0.2813, 0.3471, 0.2153, 0.1562], grad_fn=<SoftmaxBackward0>)
masked_adj tensor([1.8721, 0.2059, 1.8413, 0.9219, 0.3435, 0.1504],
       grad_fn=<MulBackward0>)
res: tensor([0.2796, 0.3488, 0.2148, 0.1568], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.9360, 0.9207, 0.9219], grad_fn=<IndexBackward0>)
num_high 3 len(mask) 6
pred_loss tensor([1.2743], grad_fn=<MulBackward0>)
size_loss tensor(-0.0732, grad_fn=<MulBackward0>)
size_num_loss 0.5000166666666667
loss: tensor([1.2743], grad_fn=<MulBackward0>)
9
epoch:  9 ; loss:  1.2743436098098755 ; pred:  tensor([0.2796, 0.3488, 0.2148, 0.1568], grad_fn=<SoftmaxBackward0>)
Finished Training
sel masked ver tensor(indices=tensor([[7996, 5702, 5702, 5702],
                       [5702,    0, 3466,    0]]),
       values=tensor([0.9360, 0.8234, 0.9207, 3.6875]),
       size=(7997, 5703), nnz=4, layout=torch.sparse_coo,
       grad_fn=<SparseCooTensorWithDimsAndTensorsBackward0>)
Counter({'fax': 1, 'phone': 1, 'name': 1, 'author': 1})
dict index: {}
wandb: WARNING Calling wandb.login() after wandb.init() has no effect.
node_idx 5702
 node original label [0]
 node predicted label explain 1
 node prediction probability explain tensor([0.2891, 0.4412, 0.1413, 0.1284], grad_fn=<SoftmaxBackward0>)
 node predicted label full 0 most important relations  {'fax': 1, 'phone': 1, 'name': 1, 'author': 1, 'label': 0, 'node_idx': '5702'}
 final masks and lenght tensor(indices=tensor([[ 24566,  88552, 196257, 229397, 254252, 328817],
                       [  5702,      0,   3466,      0,   7996,   5230]]),
       values=tensor([0.9360, 0.8234, 0.9207, 3.6875, 0.3435, 0.1504]),
       size=(753935, 8285), nnz=6, layout=torch.sparse_coo,
       grad_fn=<SparseCooTensorWithDimsAndTensorsBackward0>) 4 ---------------------------------------------------------------
node label: 0
3
masked_adj tensor([1.5505, 0.7486, 1.5248, 0.7619], grad_fn=<MulBackward0>)
res: tensor([0.3141, 0.3214, 0.2250, 0.1395], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.7752, 0.7486, 0.7624, 0.7619], grad_fn=<IndexBackward0>)
num_high 4 len(mask) 4
pred_loss tensor([1.1581], grad_fn=<MulBackward0>)
size_loss tensor(1.5228, grad_fn=<AddBackward0>)
size_num_loss 1.000025
loss: tensor([4.2295], grad_fn=<AddBackward0>)
0
epoch:  0 ; loss:  4.229499816894531 ; pred:  tensor([0.3141, 0.3214, 0.2250, 0.1395], grad_fn=<SoftmaxBackward0>)
masked_adj tensor([1.3532, 0.6436, 1.3211, 0.6599], grad_fn=<MulBackward0>)
res: tensor([0.3114, 0.3095, 0.2298, 0.1494], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.6766, 0.6436, 0.6606, 0.6599], grad_fn=<IndexBackward0>)
num_high 4 len(mask) 4
pred_loss tensor([1.1667], grad_fn=<MulBackward0>)
size_loss tensor(1.3185, grad_fn=<AddBackward0>)
size_num_loss 1.000025
loss: tensor([4.1259], grad_fn=<AddBackward0>)
1
epoch:  1 ; loss:  4.125886917114258 ; pred:  tensor([0.3114, 0.3095, 0.2298, 0.1494], grad_fn=<SoftmaxBackward0>)
masked_adj tensor([1.1218, 0.5228, 1.0840, 0.5411], grad_fn=<MulBackward0>)
res: tensor([0.3077, 0.2957, 0.2351, 0.1615], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.5609, 0.5228, 0.5420, 0.5411], grad_fn=<IndexBackward0>)
num_high 4 len(mask) 4
pred_loss tensor([1.1785], grad_fn=<MulBackward0>)
size_loss tensor(1.0810, grad_fn=<AddBackward0>)
size_num_loss 1.000025
loss: tensor([3.9488], grad_fn=<AddBackward0>)
2
epoch:  2 ; loss:  3.948819398880005 ; pred:  tensor([0.3077, 0.2957, 0.2351, 0.1615], grad_fn=<SoftmaxBackward0>)
masked_adj tensor([0.8816, 0.3993, 0.8390, 0.4182], grad_fn=<MulBackward0>)
res: tensor([0.3034, 0.2816, 0.2403, 0.1747], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([], grad_fn=<IndexBackward0>)
num_high 0 len(mask) 4
pred_loss tensor([1.1928], grad_fn=<MulBackward0>)
size_loss tensor(nan, grad_fn=<MulBackward0>)
size_num_loss 2.5e-05
loss: tensor([1.1928], grad_fn=<MulBackward0>)
3
epoch:  3 ; loss:  1.1927703619003296 ; pred:  tensor([0.3034, 0.2816, 0.2403, 0.1747], grad_fn=<SoftmaxBackward0>)
masked_adj tensor([0.7337, 0.3102, 0.6849, 0.3372], grad_fn=<MulBackward0>)
res: tensor([0.3007, 0.2725, 0.2430, 0.1838], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([], grad_fn=<IndexBackward0>)
num_high 0 len(mask) 4
pred_loss tensor([1.2017], grad_fn=<MulBackward0>)
size_loss tensor(nan, grad_fn=<MulBackward0>)
size_num_loss 2.5e-05
loss: tensor([1.2017], grad_fn=<MulBackward0>)
4
epoch:  4 ; loss:  1.2017062902450562 ; pred:  tensor([0.3007, 0.2725, 0.2430, 0.1838], grad_fn=<SoftmaxBackward0>)
masked_adj tensor([0.6723, 0.2549, 0.6126, 0.2954], grad_fn=<MulBackward0>)
res: tensor([0.2998, 0.2680, 0.2438, 0.1885], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([], grad_fn=<IndexBackward0>)
num_high 0 len(mask) 4
pred_loss tensor([1.2048], grad_fn=<MulBackward0>)
size_loss tensor(nan, grad_fn=<MulBackward0>)
size_num_loss 2.5e-05
loss: tensor([1.2048], grad_fn=<MulBackward0>)
5
epoch:  5 ; loss:  1.2047746181488037 ; pred:  tensor([0.2998, 0.2680, 0.2438, 0.1885], grad_fn=<SoftmaxBackward0>)
masked_adj tensor([0.6730, 0.2262, 0.5990, 0.2827], grad_fn=<MulBackward0>)
res: tensor([0.3002, 0.2666, 0.2434, 0.1898], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([], grad_fn=<IndexBackward0>)
num_high 0 len(mask) 4
pred_loss tensor([1.2034], grad_fn=<MulBackward0>)
size_loss tensor(nan, grad_fn=<MulBackward0>)
size_num_loss 2.5e-05
loss: tensor([1.2034], grad_fn=<MulBackward0>)
6
epoch:  6 ; loss:  1.2033932209014893 ; pred:  tensor([0.3002, 0.2666, 0.2434, 0.1898], grad_fn=<SoftmaxBackward0>)
masked_adj tensor([0.7172, 0.2169, 0.6273, 0.2908], grad_fn=<MulBackward0>)
res: tensor([0.3015, 0.2676, 0.2422, 0.1886], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([], grad_fn=<IndexBackward0>)
num_high 0 len(mask) 4
pred_loss tensor([1.1988], grad_fn=<MulBackward0>)
size_loss tensor(nan, grad_fn=<MulBackward0>)
size_num_loss 2.5e-05
loss: tensor([1.1988], grad_fn=<MulBackward0>)
7
epoch:  7 ; loss:  1.1988232135772705 ; pred:  tensor([0.3015, 0.2676, 0.2422, 0.1886], grad_fn=<SoftmaxBackward0>)
masked_adj tensor([0.7931, 0.2220, 0.6873, 0.3148], grad_fn=<MulBackward0>)
res: tensor([0.3036, 0.2704, 0.2403, 0.1857], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([], grad_fn=<IndexBackward0>)
num_high 0 len(mask) 4
pred_loss tensor([1.1920], grad_fn=<MulBackward0>)
size_loss tensor(nan, grad_fn=<MulBackward0>)
size_num_loss 2.5e-05
loss: tensor([1.1920], grad_fn=<MulBackward0>)
8
epoch:  8 ; loss:  1.1919578313827515 ; pred:  tensor([0.3036, 0.2704, 0.2403, 0.1857], grad_fn=<SoftmaxBackward0>)
masked_adj tensor([0.8909, 0.2389, 0.7716, 0.3514], grad_fn=<MulBackward0>)
res: tensor([0.3062, 0.2745, 0.2378, 0.1816], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([], grad_fn=<IndexBackward0>)
num_high 0 len(mask) 4
pred_loss tensor([1.1836], grad_fn=<MulBackward0>)
size_loss tensor(nan, grad_fn=<MulBackward0>)
size_num_loss 2.5e-05
loss: tensor([1.1836], grad_fn=<MulBackward0>)
9
epoch:  9 ; loss:  1.183628797531128 ; pred:  tensor([0.3062, 0.2745, 0.2378, 0.1816], grad_fn=<SoftmaxBackward0>)
Finished Training
sel masked ver tensor(indices=tensor([[5714, 5714],
                       [   0,    0]]),
       values=tensor([1.7818, 1.5432]),
       size=(5715, 1), nnz=2, layout=torch.sparse_coo,
       grad_fn=<SparseCooTensorWithDimsAndTensorsBackward0>)
Counter({'fax': 1, 'phone': 1})
dict index: {}
node_idx 5714
 node original label [0]
 node predicted label explain 0
 node prediction probability explain tensor([0.3277, 0.3081, 0.2160, 0.1482], grad_fn=<SoftmaxBackward0>)
 node predicted label full 0 most important relations  {'fax': 1, 'phone': 1, 'label': 0, 'node_idx': '5714'}
 final masks and lenght tensor(indices=tensor([[ 88564, 196269, 229409, 328829],
                       [     0,   4560,      0,   5230]]),
       values=tensor([1.7818, 0.2389, 1.5432, 0.3514]),
       size=(753935, 8285), nnz=4, layout=torch.sparse_coo,
       grad_fn=<SparseCooTensorWithDimsAndTensorsBackward0>) 2 ---------------------------------------------------------------
wandb: WARNING Calling wandb.login() after wandb.init() has no effect.
node label: 0
4
masked_adj tensor([1.5416, 0.7468, 0.7592, 1.5174, 0.5720], grad_fn=<MulBackward0>)
res: tensor([0.3628, 0.2503, 0.2652, 0.1217], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.7708, 0.7468, 0.7592, 0.7587, 0.5720], grad_fn=<IndexBackward0>)
num_high 5 len(mask) 5
pred_loss tensor([1.0140], grad_fn=<MulBackward0>)
size_loss tensor(1.7331, grad_fn=<AddBackward0>)
size_num_loss 1.00002
loss: tensor([4.3254], grad_fn=<AddBackward0>)
0
epoch:  0 ; loss:  4.3254170417785645 ; pred:  tensor([0.3628, 0.2503, 0.2652, 0.1217], grad_fn=<SoftmaxBackward0>)
masked_adj tensor([1.3421, 0.6414, 0.6566, 1.3121, 0.4477], grad_fn=<MulBackward0>)
res: tensor([0.3516, 0.2496, 0.2665, 0.1323], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.6711, 0.6414, 0.6566, 0.6560], grad_fn=<IndexBackward0>)
num_high 4 len(mask) 5
pred_loss tensor([1.0453], grad_fn=<MulBackward0>)
size_loss tensor(-0.1465, grad_fn=<MulBackward0>)
size_num_loss 0.80002
loss: tensor([2.3509], grad_fn=<AddBackward0>)
1
epoch:  1 ; loss:  2.3509373664855957 ; pred:  tensor([0.3516, 0.2496, 0.2665, 0.1323], grad_fn=<SoftmaxBackward0>)
masked_adj tensor([1.4881, 0.5485, 0.5740, 1.0935, 0.3733], grad_fn=<MulBackward0>)
res: tensor([0.3453, 0.2463, 0.2750, 0.1334], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.7441, 0.5485, 0.5740, 0.5468], grad_fn=<IndexBackward0>)
num_high 4 len(mask) 5
pred_loss tensor([1.0633], grad_fn=<MulBackward0>)
size_loss tensor(-8.9563, grad_fn=<MulBackward0>)
size_num_loss 0.80002
loss: tensor([-6.4352], grad_fn=<AddBackward0>)
2
epoch:  2 ; loss:  -6.4351911544799805 ; pred:  tensor([0.3453, 0.2463, 0.2750, 0.1334], grad_fn=<SoftmaxBackward0>)
masked_adj tensor([1.6098, 0.4531, 0.4927, 0.9318, 0.3286], grad_fn=<MulBackward0>)
res: tensor([0.3400, 0.2456, 0.2790, 0.1354], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.8049], grad_fn=<IndexBackward0>)
num_high 1 len(mask) 5
pred_loss tensor([1.0789], grad_fn=<MulBackward0>)
size_loss tensor(nan, grad_fn=<MulBackward0>)
size_num_loss 0.20002
loss: tensor([1.0789], grad_fn=<MulBackward0>)
3
epoch:  3 ; loss:  1.078879475593567 ; pred:  tensor([0.3400, 0.2456, 0.2790, 0.1354], grad_fn=<SoftmaxBackward0>)
masked_adj tensor([1.6915, 0.3776, 0.4258, 0.8022, 0.3108], grad_fn=<MulBackward0>)
res: tensor([0.3359, 0.2451, 0.2813, 0.1377], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.8457], grad_fn=<IndexBackward0>)
num_high 1 len(mask) 5
pred_loss tensor([1.0908], grad_fn=<MulBackward0>)
size_loss tensor(nan, grad_fn=<MulBackward0>)
size_num_loss 0.20002
loss: tensor([1.0908], grad_fn=<MulBackward0>)
4
epoch:  4 ; loss:  1.0908008813858032 ; pred:  tensor([0.3359, 0.2451, 0.2813, 0.1377], grad_fn=<SoftmaxBackward0>)
masked_adj tensor([1.7486, 0.3186, 0.3713, 0.6985, 0.3122], grad_fn=<MulBackward0>)
res: tensor([0.3330, 0.2445, 0.2825, 0.1400], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.8743], grad_fn=<IndexBackward0>)
num_high 1 len(mask) 5
pred_loss tensor([1.0996], grad_fn=<MulBackward0>)
size_loss tensor(nan, grad_fn=<MulBackward0>)
size_num_loss 0.20002
loss: tensor([1.0996], grad_fn=<MulBackward0>)
5
epoch:  5 ; loss:  1.0996043682098389 ; pred:  tensor([0.3330, 0.2445, 0.2825, 0.1400], grad_fn=<SoftmaxBackward0>)
masked_adj tensor([1.7898, 0.2726, 0.3271, 0.6155, 0.3281], grad_fn=<MulBackward0>)
res: tensor([0.3309, 0.2440, 0.2829, 0.1422], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.8949], grad_fn=<IndexBackward0>)
num_high 1 len(mask) 5
pred_loss tensor([1.1059], grad_fn=<MulBackward0>)
size_loss tensor(nan, grad_fn=<MulBackward0>)
size_num_loss 0.20002
loss: tensor([1.1059], grad_fn=<MulBackward0>)
6
epoch:  6 ; loss:  1.1058752536773682 ; pred:  tensor([0.3309, 0.2440, 0.2829, 0.1422], grad_fn=<SoftmaxBackward0>)
masked_adj tensor([1.8204, 0.2367, 0.2914, 0.5488, 0.3552], grad_fn=<MulBackward0>)
res: tensor([0.3295, 0.2435, 0.2827, 0.1442], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.9102], grad_fn=<IndexBackward0>)
num_high 1 len(mask) 5
pred_loss tensor([1.1101], grad_fn=<MulBackward0>)
size_loss tensor(nan, grad_fn=<MulBackward0>)
size_num_loss 0.20002
loss: tensor([1.1101], grad_fn=<MulBackward0>)
7
epoch:  7 ; loss:  1.1101433038711548 ; pred:  tensor([0.3295, 0.2435, 0.2827, 0.1442], grad_fn=<SoftmaxBackward0>)
masked_adj tensor([1.8434, 0.2085, 0.2626, 0.4950, 0.3908], grad_fn=<MulBackward0>)
res: tensor([0.3286, 0.2431, 0.2821, 0.1461], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.9217], grad_fn=<IndexBackward0>)
num_high 1 len(mask) 5
pred_loss tensor([1.1129], grad_fn=<MulBackward0>)
size_loss tensor(nan, grad_fn=<MulBackward0>)
size_num_loss 0.20002
loss: tensor([1.1129], grad_fn=<MulBackward0>)
8
epoch:  8 ; loss:  1.1128616333007812 ; pred:  tensor([0.3286, 0.2431, 0.2821, 0.1461], grad_fn=<SoftmaxBackward0>)
masked_adj tensor([1.8612, 0.1863, 0.2394, 0.4516, 0.4322], grad_fn=<MulBackward0>)
res: tensor([0.3281, 0.2427, 0.2813, 0.1479], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.9306], grad_fn=<IndexBackward0>)
num_high 1 len(mask) 5
pred_loss tensor([1.1144], grad_fn=<MulBackward0>)
size_loss tensor(nan, grad_fn=<MulBackward0>)
size_num_loss 0.20002
loss: tensor([1.1144], grad_fn=<MulBackward0>)
9
epoch:  9 ; loss:  1.1144241094589233 ; pred:  tensor([0.3281, 0.2427, 0.2813, 0.1479], grad_fn=<SoftmaxBackward0>)
Finished Training
sel masked ver tensor(indices=tensor([[5708, 5708],
                       [   0,    0]]),
       values=tensor([3.7224, 0.9032]),
       size=(5709, 1), nnz=2, layout=torch.sparse_coo,
       grad_fn=<SparseCooTensorWithDimsAndTensorsBackward0>)
Counter({'fax': 1, 'phone': 1})
dict index: {}
node_idx 5708
 node original label [0]
 node predicted label explain 0
 node prediction probability explain tensor([0.3564, 0.2532, 0.2911, 0.0992], grad_fn=<SoftmaxBackward0>)
 node predicted label full 0 most important relations  {'fax': 1, 'phone': 1, 'label': 0, 'node_idx': '5708'}
 final masks and lenght tensor(indices=tensor([[ 88558, 129983, 196263, 229403, 328823],
                       [     0,   3162,   4855,      0,   5230]]),
       values=tensor([3.7224, 0.1863, 0.2394, 0.9032, 0.4322]),
       size=(753935, 8285), nnz=5, layout=torch.sparse_coo,
       grad_fn=<SparseCooTensorWithDimsAndTensorsBackward0>) 2 ---------------------------------------------------------------
wandb: WARNING Calling wandb.login() after wandb.init() has no effect.
node label: 0
18
masked_adj tensor([0.8011, 0.7864, 0.7656, 0.6388, 0.7574, 0.6787, 0.7293, 0.6621, 0.6998,
        0.7919, 0.7150, 0.6712, 0.7009, 0.7080, 1.3982, 0.7605, 0.7917, 1.4492,
        0.7106, 0.7483, 0.6996, 0.7721, 0.7620, 0.7930, 0.7792, 0.7798, 0.7548,
        0.7812, 0.7217, 0.7327, 0.7208, 0.7641, 0.6720, 0.6947, 0.7220, 0.7942,
        0.7437, 0.7137, 0.7431, 0.6989, 0.6642, 0.7691, 0.6943, 0.7063, 0.6770,
        0.8074, 0.6787, 0.7110], grad_fn=<MulBackward0>)
res: tensor([0.6735, 0.1458, 0.0849, 0.0958], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.8011, 0.7864, 0.7656, 0.6388, 0.7574, 0.6787, 0.7293, 0.6621, 0.6998,
        0.7919, 0.7150, 0.6712, 0.7009, 0.7080, 0.6991, 0.7605, 0.7917, 0.7246,
        0.7106, 0.7483, 0.6996, 0.7721, 0.7620, 0.7930, 0.7792, 0.7798, 0.7548,
        0.7812, 0.7217, 0.7327, 0.7208, 0.7641, 0.6720, 0.6947, 0.7220, 0.7942,
        0.7437, 0.7137, 0.7431, 0.6989, 0.6642, 0.7691, 0.6943, 0.7063, 0.6770,
        0.8074, 0.6787, 0.7110], grad_fn=<IndexBackward0>)
num_high 48 len(mask) 48
pred_loss tensor([0.3952], grad_fn=<MulBackward0>)
size_loss tensor(17.5274, grad_fn=<AddBackward0>)
size_num_loss 1.0000020833333334
loss: tensor([19.5000], grad_fn=<AddBackward0>)
0
epoch:  0 ; loss:  19.50002098083496 ; pred:  tensor([0.6735, 0.1458, 0.0849, 0.0958], grad_fn=<SoftmaxBackward0>)
masked_adj tensor([0.7096, 0.6907, 0.6646, 0.5175, 0.6544, 0.5617, 0.6204, 0.5430, 0.5858,
        0.6977, 0.6035, 0.5532, 0.5870, 0.5953, 1.1699, 0.6583, 0.6975, 1.2295,
        0.5983, 0.6433, 0.5855, 0.6726, 0.6600, 0.6991, 0.6816, 0.6824, 0.6513,
        0.6841, 0.6113, 0.6245, 0.6103, 0.6627, 0.5541, 0.5799, 0.6117, 0.7007,
        0.6376, 0.6019, 0.6370, 0.5846, 0.5454, 0.6689, 0.5794, 0.5932, 0.5597,
        0.7177, 0.5617, 0.5988], grad_fn=<MulBackward0>)
res: tensor([0.5631, 0.1865, 0.1265, 0.1238], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.7096, 0.6907, 0.6646, 0.5175, 0.6544, 0.5617, 0.6204, 0.5430, 0.5858,
        0.6977, 0.6035, 0.5532, 0.5870, 0.5953, 0.5849, 0.6583, 0.6975, 0.6148,
        0.5983, 0.6433, 0.5855, 0.6726, 0.6600, 0.6991, 0.6816, 0.6824, 0.6513,
        0.6841, 0.6113, 0.6245, 0.6103, 0.6627, 0.5541, 0.5799, 0.6117, 0.7007,
        0.6376, 0.6019, 0.6370, 0.5846, 0.5454, 0.6689, 0.5794, 0.5932, 0.5597,
        0.7177, 0.5617, 0.5988], grad_fn=<IndexBackward0>)
num_high 48 len(mask) 48
pred_loss tensor([0.5742], grad_fn=<MulBackward0>)
size_loss tensor(14.9424, grad_fn=<AddBackward0>)
size_num_loss 1.0000020833333334
loss: tensor([17.1732], grad_fn=<AddBackward0>)
1
epoch:  1 ; loss:  17.17316246032715 ; pred:  tensor([0.5631, 0.1865, 0.1265, 0.1238], grad_fn=<SoftmaxBackward0>)
masked_adj tensor([0.5979, 0.5761, 0.5467, 0.3956, 0.5354, 0.4385, 0.4987, 0.4201, 0.4627,
        0.5841, 0.4810, 0.4301, 0.4644, 0.4725, 0.9238, 0.5397, 0.5838, 0.9879,
        0.4756, 0.5233, 0.4624, 0.5556, 0.5416, 0.5857, 0.5657, 0.5666, 0.5320,
        0.5685, 0.4891, 0.5031, 0.4881, 0.5447, 0.4314, 0.4568, 0.4897, 0.5875,
        0.5178, 0.4796, 0.5170, 0.4621, 0.4230, 0.5520, 0.4567, 0.4709, 0.4372,
        0.6078, 0.4497, 0.4762], grad_fn=<MulBackward0>)
res: tensor([0.4573, 0.2205, 0.1723, 0.1499], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.5979, 0.5761, 0.5467, 0.5354, 0.5841, 0.5397, 0.5838, 0.5233, 0.5556,
        0.5416, 0.5857, 0.5657, 0.5666, 0.5320, 0.5685, 0.5031, 0.5447, 0.5875,
        0.5178, 0.5170, 0.5520, 0.6078], grad_fn=<IndexBackward0>)
num_high 22 len(mask) 48
pred_loss tensor([0.7825], grad_fn=<MulBackward0>)
size_loss tensor(-0.8193, grad_fn=<MulBackward0>)
size_num_loss 0.45833541666666666
loss: tensor([1.1087], grad_fn=<AddBackward0>)
2
epoch:  2 ; loss:  1.1086984872817993 ; pred:  tensor([0.4573, 0.2205, 0.1723, 0.1499], grad_fn=<SoftmaxBackward0>)
masked_adj tensor([0.6438, 0.5929, 0.4227, 0.3220, 0.4209, 0.3550, 0.4042, 0.3406, 0.3744,
        0.6169, 0.3893, 0.3484, 0.3840, 0.3823, 0.7474, 0.4210, 0.6162, 0.8377,
        0.3849, 0.4181, 0.3741, 0.4516, 0.4211, 0.6207, 0.5371, 0.5434, 0.4205,
        0.5562, 0.3961, 0.4069, 0.3953, 0.4218, 0.3572, 0.3700, 0.3997, 0.6250,
        0.4171, 0.3932, 0.4167, 0.3820, 0.3538, 0.4347, 0.3775, 0.3892, 0.3644,
        0.6609, 0.4580, 0.3870], grad_fn=<MulBackward0>)
res: tensor([0.4165, 0.2320, 0.1959, 0.1557], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.6438, 0.5929, 0.6169, 0.6162, 0.6207, 0.5371, 0.5434, 0.5562, 0.6250,
        0.6609], grad_fn=<IndexBackward0>)
num_high 10 len(mask) 48
pred_loss tensor([0.8759], grad_fn=<MulBackward0>)
size_loss tensor(-1.8141, grad_fn=<MulBackward0>)
size_num_loss 0.20833541666666666
loss: tensor([-0.0614], grad_fn=<AddBackward0>)
3
epoch:  3 ; loss:  -0.061437249183654785 ; pred:  tensor([0.4165, 0.2320, 0.1959, 0.1557], grad_fn=<SoftmaxBackward0>)
masked_adj tensor([0.7156, 0.5543, 0.3329, 0.2869, 0.3361, 0.3080, 0.3415, 0.2987, 0.3209,
        0.6834, 0.3311, 0.3036, 0.3422, 0.3263, 0.6408, 0.3339, 0.6819, 0.7712,
        0.3281, 0.3394, 0.3207, 0.3739, 0.3329, 0.6895, 0.4627, 0.4693, 0.3377,
        0.4831, 0.3358, 0.3346, 0.3352, 0.3335, 0.3236, 0.3188, 0.3443, 0.6954,
        0.3441, 0.3434, 0.3428, 0.3401, 0.3251, 0.3559, 0.3360, 0.3439, 0.3330,
        0.7324, 0.5074, 0.3325], grad_fn=<MulBackward0>)
res: tensor([0.4054, 0.2333, 0.2066, 0.1546], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.7156, 0.5543, 0.6834, 0.6819, 0.6895, 0.6954, 0.7324, 0.5074],
       grad_fn=<IndexBackward0>)
num_high 8 len(mask) 48
pred_loss tensor([0.9028], grad_fn=<MulBackward0>)
size_loss tensor(-6.5533, grad_fn=<MulBackward0>)
size_num_loss 0.16666875
loss: tensor([-4.8457], grad_fn=<AddBackward0>)
4
epoch:  4 ; loss:  -4.845659255981445 ; pred:  tensor([0.4054, 0.2333, 0.2066, 0.1546], grad_fn=<SoftmaxBackward0>)
masked_adj tensor([0.7869, 0.4834, 0.2761, 0.2788, 0.2783, 0.2891, 0.3073, 0.2844, 0.2959,
        0.7565, 0.3015, 0.2869, 0.3296, 0.2988, 0.5913, 0.2758, 0.7550, 0.7657,
        0.2998, 0.2833, 0.2958, 0.3243, 0.2750, 0.7625, 0.4012, 0.4078, 0.2802,
        0.4219, 0.3041, 0.2818, 0.3038, 0.2779, 0.3194, 0.2965, 0.3185, 0.7682,
        0.2939, 0.3238, 0.2904, 0.3269, 0.3241, 0.3119, 0.3227, 0.3263, 0.3313,
        0.8019, 0.4404, 0.3068], grad_fn=<MulBackward0>)
res: tensor([0.3857, 0.2350, 0.2243, 0.1550], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.7869, 0.7565, 0.7550, 0.7625, 0.7682, 0.8019],
       grad_fn=<IndexBackward0>)
num_high 6 len(mask) 48
pred_loss tensor([0.9526], grad_fn=<MulBackward0>)
size_loss tensor(-0.3495, grad_fn=<MulBackward0>)
size_num_loss 0.12500208333333332
loss: tensor([1.3378], grad_fn=<AddBackward0>)
5
epoch:  5 ; loss:  1.3378459215164185 ; pred:  tensor([0.3857, 0.2350, 0.2243, 0.1550], grad_fn=<SoftmaxBackward0>)
masked_adj tensor([0.8437, 0.4228, 0.2453, 0.2900, 0.2417, 0.2909, 0.2956, 0.2902, 0.2923,
        0.7770, 0.2938, 0.2906, 0.3381, 0.2931, 0.5846, 0.2407, 0.7710, 0.8037,
        0.2934, 0.2451, 0.2923, 0.2991, 0.2408, 0.7987, 0.3511, 0.3576, 0.2428,
        0.3717, 0.2946, 0.2440, 0.2945, 0.2477, 0.3355, 0.2955, 0.3150, 0.8144,
        0.2609, 0.3262, 0.2546, 0.3347, 0.3427, 0.2944, 0.3303, 0.3292, 0.3512,
        0.8584, 0.3851, 0.3029], grad_fn=<MulBackward0>)
res: tensor([0.3770, 0.2328, 0.2359, 0.1543], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.8437, 0.7770, 0.7710, 0.7987, 0.8144, 0.8584],
       grad_fn=<IndexBackward0>)
num_high 6 len(mask) 48
pred_loss tensor([0.9754], grad_fn=<MulBackward0>)
size_loss tensor(-1.2455, grad_fn=<MulBackward0>)
size_num_loss 0.12500208333333332
loss: tensor([0.4482], grad_fn=<AddBackward0>)
6
epoch:  6 ; loss:  0.4482245445251465 ; pred:  tensor([0.3770, 0.2328, 0.2359, 0.1543], grad_fn=<SoftmaxBackward0>)
masked_adj tensor([0.8895, 0.3719, 0.2336, 0.3156, 0.2210, 0.3082, 0.3012, 0.3111, 0.3050,
        0.7555, 0.3029, 0.3095, 0.3625, 0.3039, 0.6102, 0.2227, 0.7425, 0.8730,
        0.3035, 0.2205, 0.3050, 0.2932, 0.2243, 0.8119, 0.3108, 0.3171, 0.2206,
        0.3311, 0.3021, 0.2176, 0.3022, 0.2361, 0.3663, 0.3106, 0.3282, 0.8497,
        0.2408, 0.3447, 0.2313, 0.3583, 0.3760, 0.2967, 0.3537, 0.3478, 0.3875,
        0.9023, 0.3400, 0.3155], grad_fn=<MulBackward0>)
res: tensor([0.3763, 0.2280, 0.2430, 0.1527], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.8895, 0.7555, 0.7425, 0.8119, 0.8497, 0.9023],
       grad_fn=<IndexBackward0>)
num_high 6 len(mask) 48
pred_loss tensor([0.9773], grad_fn=<MulBackward0>)
size_loss tensor(-4.5117, grad_fn=<MulBackward0>)
size_num_loss 0.12500208333333332
loss: tensor([-2.8209], grad_fn=<AddBackward0>)
7
epoch:  7 ; loss:  -2.8208866119384766 ; pred:  tensor([0.3763, 0.2280, 0.2430, 0.1527], grad_fn=<SoftmaxBackward0>)
masked_adj tensor([0.9251, 0.3296, 0.2364, 0.3524, 0.2125, 0.3377, 0.3204, 0.3437, 0.3304,
        0.7030, 0.3252, 0.3404, 0.3987, 0.3276, 0.6613, 0.2177, 0.6848, 0.9635,
        0.3267, 0.2062, 0.3305, 0.3025, 0.2212, 0.8090, 0.2786, 0.2847, 0.2100,
        0.2986, 0.3230, 0.1999, 0.3233, 0.2385, 0.4077, 0.3382, 0.3541, 0.8842,
        0.2305, 0.3755, 0.2177, 0.3942, 0.4200, 0.3142, 0.3896, 0.3787, 0.4360,
        0.9347, 0.3034, 0.3412], grad_fn=<MulBackward0>)
res: tensor([0.3817, 0.2214, 0.2464, 0.1505], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.9251, 0.7030, 0.6848, 0.8090, 0.8842, 0.9347],
       grad_fn=<IndexBackward0>)
num_high 6 len(mask) 48
pred_loss tensor([0.9632], grad_fn=<MulBackward0>)
size_loss tensor(-12.0665, grad_fn=<MulBackward0>)
size_num_loss 0.12500208333333332
loss: tensor([-10.3871], grad_fn=<AddBackward0>)
8
epoch:  8 ; loss:  -10.387115478515625 ; pred:  tensor([0.3817, 0.2214, 0.2464, 0.1505], grad_fn=<SoftmaxBackward0>)
masked_adj tensor([0.9509, 0.2947, 0.2509, 0.3972, 0.2137, 0.3765, 0.3504, 0.3851, 0.3656,
        0.6294, 0.3578, 0.3804, 0.4429, 0.3614, 0.7320, 0.2230, 0.6074, 1.0648,
        0.3601, 0.2002, 0.3658, 0.3240, 0.2288, 0.7925, 0.2529, 0.2589, 0.2085,
        0.2730, 0.3544, 0.1891, 0.3548, 0.2522, 0.4552, 0.3753, 0.3897, 0.9163,
        0.2281, 0.4148, 0.2118, 0.4387, 0.4708, 0.3441, 0.4347, 0.4187, 0.4921,
        0.9574, 0.2737, 0.3769], grad_fn=<MulBackward0>)
res: tensor([0.3917, 0.2137, 0.2464, 0.1481], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.9509, 0.6294, 0.6074, 0.5324, 0.7925, 0.9163, 0.9574],
       grad_fn=<IndexBackward0>)
num_high 7 len(mask) 48
pred_loss tensor([0.9372], grad_fn=<MulBackward0>)
size_loss tensor(-32.0669, grad_fn=<MulBackward0>)
size_num_loss 0.14583541666666666
loss: tensor([-30.3866], grad_fn=<AddBackward0>)
9
epoch:  9 ; loss:  -30.386577606201172 ; pred:  tensor([0.3917, 0.2137, 0.2464, 0.1481], grad_fn=<SoftmaxBackward0>)
Finished Training
sel masked ver tensor(indices=tensor([[7250, 5486, 5843, 7688, 5843, 5990, 5843, 5843, 5843],
                       [5843, 5990,    0, 5990, 5342, 5486, 3118,    0, 7930]]),
       values=tensor([0.9509, 0.6294, 1.7718, 0.6074, 0.5324, 0.7925, 0.9163,
                      0.9124, 0.9574]),
       size=(7689, 7931), nnz=9, layout=torch.sparse_coo,
       grad_fn=<SparseCooTensorWithDimsAndTensorsBackward0>)
Counter({'dealtWithIn': 1, 'fax': 1, 'phone': 1, 'name': 1, 'homepage': 1, 'publication': 1, 'isAbout': 1, 'author': 1, 'hasProject': 1})
dict index: {}
node_idx 5843
 node original label [0]
 node predicted label explain 0
 node prediction probability explain tensor([0.4124, 0.2272, 0.2385, 0.1219], grad_fn=<SoftmaxBackward0>)
 node predicted label full 0 most important relations  {'dealtWithIn': 1, 'fax': 1, 'phone': 1, 'name': 1, 'homepage': 1, 'publication': 1, 'isAbout': 1, 'author': 1, 'hasProject': 1, 'label': 0, 'node_idx': '5843'}
 final masks and lenght tensor(indices=tensor([[ 23820,  23954,  24202,  24258,  24500,  63345,  63451,
                         63480,  63481,  63481,  63488,  63488,  88693, 115089,
                        115337, 115393, 115393, 130118, 146768, 146768, 146768,
                        146835, 146835, 146835, 146835, 148775, 154480, 154568,
                        154586, 154611, 154615, 154616, 154623, 179908, 179975,
                        196398, 229538, 246188, 246188, 246255, 246255, 254393,
                        254393, 254393, 254393, 254393, 304103, 328958],
                       [  5843,   5843,   5843,   5843,   5843,   5990,   5923,
                          5990,   5923,   5990,   5923,   5990,      0,   5990,
                          5923,   5923,   5990,   5342,   5456,   5486,   5493,
                          5350,   5485,   5486,   5493,   5350,   5843,   5843,
                          5843,   5843,   5843,   5843,   5843,   5843,   5843,
                          3118,      0,   7632,   7688,   7384,   7688,   7250,
                          7384,   7632,   7688,   7930,   5990,   5230]]),
       values=tensor([0.9509, 0.2947, 0.2509, 0.3972, 0.2137, 0.3765, 0.3504,
                      0.3851, 0.3656, 0.6294, 0.3578, 0.3804, 1.7718, 0.3614,
                      0.3660, 0.2230, 0.6074, 0.5324, 0.3601, 0.2002, 0.3658,
                      0.3240, 0.2288, 0.7925, 0.2529, 0.2589, 0.2085, 0.2730,
                      0.3544, 0.1891, 0.3548, 0.2522, 0.4552, 0.3753, 0.3897,
                      0.9163, 0.9124, 0.4148, 0.2118, 0.4387, 0.4708, 0.3441,
                      0.4347, 0.4187, 0.4921, 0.9574, 0.2737, 0.3769]),
       size=(753935, 8285), nnz=48, layout=torch.sparse_coo,
       grad_fn=<SparseCooTensorWithDimsAndTensorsBackward0>) 9 ---------------------------------------------------------------
wandb: WARNING Calling wandb.login() after wandb.init() has no effect.
node label: 0
5
masked_adj tensor([0.7708, 0.7468, 0.7592, 0.7587, 0.5720], grad_fn=<MulBackward0>)
res: tensor([0.3141, 0.2673, 0.2520, 0.1666], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.7708, 0.7468, 0.7592, 0.7587, 0.5720], grad_fn=<IndexBackward0>)
num_high 5 len(mask) 5
pred_loss tensor([1.1581], grad_fn=<MulBackward0>)
size_loss tensor(1.7331, grad_fn=<AddBackward0>)
size_num_loss 1.00002
loss: tensor([4.4695], grad_fn=<AddBackward0>)
0
epoch:  0 ; loss:  4.4695234298706055 ; pred:  tensor([0.3141, 0.2673, 0.2520, 0.1666], grad_fn=<SoftmaxBackward0>)
masked_adj tensor([0.6711, 0.6414, 0.6566, 0.6560, 0.4477], grad_fn=<MulBackward0>)
res: tensor([0.3098, 0.2630, 0.2541, 0.1732], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.6711, 0.6414, 0.6566, 0.6560], grad_fn=<IndexBackward0>)
num_high 4 len(mask) 5
pred_loss tensor([1.1720], grad_fn=<MulBackward0>)
size_loss tensor(-0.1465, grad_fn=<MulBackward0>)
size_num_loss 0.80002
loss: tensor([2.4776], grad_fn=<AddBackward0>)
1
epoch:  1 ; loss:  2.477573871612549 ; pred:  tensor([0.3098, 0.2630, 0.2541, 0.1732], grad_fn=<SoftmaxBackward0>)
masked_adj tensor([0.7439, 0.5469, 0.5987, 0.5464, 0.3731], grad_fn=<MulBackward0>)
res: tensor([0.3086, 0.2608, 0.2564, 0.1741], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.7439, 0.5469, 0.5987, 0.5464], grad_fn=<IndexBackward0>)
num_high 4 len(mask) 5
pred_loss tensor([1.1756], grad_fn=<MulBackward0>)
size_loss tensor(-8.6963, grad_fn=<MulBackward0>)
size_num_loss 0.80002
loss: tensor([-6.0646], grad_fn=<AddBackward0>)
2
epoch:  2 ; loss:  -6.06458044052124 ; pred:  tensor([0.3086, 0.2608, 0.2564, 0.1741], grad_fn=<SoftmaxBackward0>)
masked_adj tensor([0.8049, 0.4524, 0.5168, 0.4657, 0.3279], grad_fn=<MulBackward0>)
res: tensor([0.3082, 0.2569, 0.2588, 0.1761], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.8049, 0.5168], grad_fn=<IndexBackward0>)
num_high 2 len(mask) 5
pred_loss tensor([1.1771], grad_fn=<MulBackward0>)
size_loss tensor(-41.5177, grad_fn=<MulBackward0>)
size_num_loss 0.40002000000000004
loss: tensor([1.1771], grad_fn=<MulBackward0>)
3
epoch:  3 ; loss:  1.1771425008773804 ; pred:  tensor([0.3082, 0.2569, 0.2588, 0.1761], grad_fn=<SoftmaxBackward0>)
masked_adj tensor([0.8459, 0.3773, 0.4490, 0.4009, 0.3096], grad_fn=<MulBackward0>)
res: tensor([0.3078, 0.2538, 0.2601, 0.1782], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.8459], grad_fn=<IndexBackward0>)
num_high 1 len(mask) 5
pred_loss tensor([1.1782], grad_fn=<MulBackward0>)
size_loss tensor(nan, grad_fn=<MulBackward0>)
size_num_loss 0.20002
loss: tensor([1.1782], grad_fn=<MulBackward0>)
4
epoch:  4 ; loss:  1.178208589553833 ; pred:  tensor([0.3078, 0.2538, 0.2601, 0.1782], grad_fn=<SoftmaxBackward0>)
masked_adj tensor([0.8745, 0.3180, 0.3944, 0.3491, 0.3104], grad_fn=<MulBackward0>)
res: tensor([0.3077, 0.2514, 0.2607, 0.1803], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.8745], grad_fn=<IndexBackward0>)
num_high 1 len(mask) 5
pred_loss tensor([1.1788], grad_fn=<MulBackward0>)
size_loss tensor(nan, grad_fn=<MulBackward0>)
size_num_loss 0.20002
loss: tensor([1.1788], grad_fn=<MulBackward0>)
5
epoch:  5 ; loss:  1.1787567138671875 ; pred:  tensor([0.3077, 0.2514, 0.2607, 0.1803], grad_fn=<SoftmaxBackward0>)
masked_adj tensor([0.8951, 0.2715, 0.3512, 0.3074, 0.3257], grad_fn=<MulBackward0>)
res: tensor([0.3077, 0.2495, 0.2606, 0.1823], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.8951], grad_fn=<IndexBackward0>)
num_high 1 len(mask) 5
pred_loss tensor([1.1788], grad_fn=<MulBackward0>)
size_loss tensor(nan, grad_fn=<MulBackward0>)
size_num_loss 0.20002
loss: tensor([1.1788], grad_fn=<MulBackward0>)
6
epoch:  6 ; loss:  1.1787773370742798 ; pred:  tensor([0.3077, 0.2495, 0.2606, 0.1823], grad_fn=<SoftmaxBackward0>)
masked_adj tensor([0.9103, 0.2349, 0.3178, 0.2740, 0.3522], grad_fn=<MulBackward0>)
res: tensor([0.3078, 0.2480, 0.2601, 0.1841], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.9103], grad_fn=<IndexBackward0>)
num_high 1 len(mask) 5
pred_loss tensor([1.1783], grad_fn=<MulBackward0>)
size_loss tensor(nan, grad_fn=<MulBackward0>)
size_num_loss 0.20002
loss: tensor([1.1783], grad_fn=<MulBackward0>)
7
epoch:  7 ; loss:  1.1782913208007812 ; pred:  tensor([0.3078, 0.2480, 0.2601, 0.1841], grad_fn=<SoftmaxBackward0>)
masked_adj tensor([0.9218, 0.2060, 0.2926, 0.2469, 0.3873], grad_fn=<MulBackward0>)
res: tensor([0.3081, 0.2469, 0.2592, 0.1858], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.9218], grad_fn=<IndexBackward0>)
num_high 1 len(mask) 5
pred_loss tensor([1.1774], grad_fn=<MulBackward0>)
size_loss tensor(nan, grad_fn=<MulBackward0>)
size_num_loss 0.20002
loss: tensor([1.1774], grad_fn=<MulBackward0>)
8
epoch:  8 ; loss:  1.1773550510406494 ; pred:  tensor([0.3081, 0.2469, 0.2592, 0.1858], grad_fn=<SoftmaxBackward0>)
masked_adj tensor([0.9307, 0.1831, 0.2741, 0.2250, 0.4281], grad_fn=<MulBackward0>)
res: tensor([0.3085, 0.2462, 0.2580, 0.1873], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.9307], grad_fn=<IndexBackward0>)
num_high 1 len(mask) 5
pred_loss tensor([1.1761], grad_fn=<MulBackward0>)
size_loss tensor(nan, grad_fn=<MulBackward0>)
size_num_loss 0.20002
loss: tensor([1.1761], grad_fn=<MulBackward0>)
9
epoch:  9 ; loss:  1.1760623455047607 ; pred:  tensor([0.3085, 0.2462, 0.2580, 0.1873], grad_fn=<SoftmaxBackward0>)
Finished Training
sel masked ver tensor(indices=tensor([[5873],
                       [  81]]),
       values=tensor([0.9307]),
       size=(5874, 82), nnz=1, layout=torch.sparse_coo,
       grad_fn=<SparseCooTensorWithDimsAndTensorsBackward0>)
Counter({'fax': 1})
dict index: {}
node_idx 5873
 node original label [0]
 node predicted label explain 0
 node prediction probability explain tensor([0.3085, 0.2462, 0.2580, 0.1873], grad_fn=<SoftmaxBackward0>)
 node predicted label full 0 most important relations  {'fax': 1, 'label': 0, 'node_idx': '5873'}
 final masks and lenght tensor(indices=tensor([[ 88723, 196428, 229568, 237853, 328988],
                       [    81,   2258,     34,   5648,   5230]]),
       values=tensor([0.9307, 0.1831, 0.2741, 0.2250, 0.4281]),
       size=(753935, 8285), nnz=5, layout=torch.sparse_coo,
       grad_fn=<SparseCooTensorWithDimsAndTensorsBackward0>) 1 ---------------------------------------------------------------
wandb: WARNING Calling wandb.login() after wandb.init() has no effect.
node label: 0
4
masked_adj tensor([1.5416, 0.7468, 0.7592, 1.5174, 0.5720], grad_fn=<MulBackward0>)
res: tensor([0.3628, 0.2503, 0.2652, 0.1217], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.7708, 0.7468, 0.7592, 0.7587, 0.5720], grad_fn=<IndexBackward0>)
num_high 5 len(mask) 5
pred_loss tensor([1.0140], grad_fn=<MulBackward0>)
size_loss tensor(1.7331, grad_fn=<AddBackward0>)
size_num_loss 1.00002
loss: tensor([4.3254], grad_fn=<AddBackward0>)
0
epoch:  0 ; loss:  4.3254170417785645 ; pred:  tensor([0.3628, 0.2503, 0.2652, 0.1217], grad_fn=<SoftmaxBackward0>)
masked_adj tensor([1.3421, 0.6414, 0.6566, 1.3121, 0.4477], grad_fn=<MulBackward0>)
res: tensor([0.3516, 0.2496, 0.2665, 0.1323], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.6711, 0.6414, 0.6566, 0.6560], grad_fn=<IndexBackward0>)
num_high 4 len(mask) 5
pred_loss tensor([1.0453], grad_fn=<MulBackward0>)
size_loss tensor(-0.1465, grad_fn=<MulBackward0>)
size_num_loss 0.80002
loss: tensor([2.3509], grad_fn=<AddBackward0>)
1
epoch:  1 ; loss:  2.3509373664855957 ; pred:  tensor([0.3516, 0.2496, 0.2665, 0.1323], grad_fn=<SoftmaxBackward0>)
masked_adj tensor([1.4881, 0.5485, 0.5740, 1.0935, 0.3733], grad_fn=<MulBackward0>)
res: tensor([0.3453, 0.2463, 0.2750, 0.1334], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.7441, 0.5485, 0.5740, 0.5468], grad_fn=<IndexBackward0>)
num_high 4 len(mask) 5
pred_loss tensor([1.0633], grad_fn=<MulBackward0>)
size_loss tensor(-8.9563, grad_fn=<MulBackward0>)
size_num_loss 0.80002
loss: tensor([-6.4352], grad_fn=<AddBackward0>)
2
epoch:  2 ; loss:  -6.4351911544799805 ; pred:  tensor([0.3453, 0.2463, 0.2750, 0.1334], grad_fn=<SoftmaxBackward0>)
masked_adj tensor([1.6098, 0.4531, 0.4927, 0.9318, 0.3286], grad_fn=<MulBackward0>)
res: tensor([0.3400, 0.2456, 0.2790, 0.1354], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.8049], grad_fn=<IndexBackward0>)
num_high 1 len(mask) 5
pred_loss tensor([1.0789], grad_fn=<MulBackward0>)
size_loss tensor(nan, grad_fn=<MulBackward0>)
size_num_loss 0.20002
loss: tensor([1.0789], grad_fn=<MulBackward0>)
3
epoch:  3 ; loss:  1.078879475593567 ; pred:  tensor([0.3400, 0.2456, 0.2790, 0.1354], grad_fn=<SoftmaxBackward0>)
masked_adj tensor([1.6915, 0.3776, 0.4258, 0.8022, 0.3108], grad_fn=<MulBackward0>)
res: tensor([0.3359, 0.2451, 0.2813, 0.1377], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.8457], grad_fn=<IndexBackward0>)
num_high 1 len(mask) 5
pred_loss tensor([1.0908], grad_fn=<MulBackward0>)
size_loss tensor(nan, grad_fn=<MulBackward0>)
size_num_loss 0.20002
loss: tensor([1.0908], grad_fn=<MulBackward0>)
4
epoch:  4 ; loss:  1.0908008813858032 ; pred:  tensor([0.3359, 0.2451, 0.2813, 0.1377], grad_fn=<SoftmaxBackward0>)
masked_adj tensor([1.7486, 0.3186, 0.3713, 0.6985, 0.3122], grad_fn=<MulBackward0>)
res: tensor([0.3330, 0.2445, 0.2825, 0.1400], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.8743], grad_fn=<IndexBackward0>)
num_high 1 len(mask) 5
pred_loss tensor([1.0996], grad_fn=<MulBackward0>)
size_loss tensor(nan, grad_fn=<MulBackward0>)
size_num_loss 0.20002
loss: tensor([1.0996], grad_fn=<MulBackward0>)
5
epoch:  5 ; loss:  1.0996043682098389 ; pred:  tensor([0.3330, 0.2445, 0.2825, 0.1400], grad_fn=<SoftmaxBackward0>)
masked_adj tensor([1.7898, 0.2726, 0.3271, 0.6155, 0.3281], grad_fn=<MulBackward0>)
res: tensor([0.3309, 0.2440, 0.2829, 0.1422], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.8949], grad_fn=<IndexBackward0>)
num_high 1 len(mask) 5
pred_loss tensor([1.1059], grad_fn=<MulBackward0>)
size_loss tensor(nan, grad_fn=<MulBackward0>)
size_num_loss 0.20002
loss: tensor([1.1059], grad_fn=<MulBackward0>)
6
epoch:  6 ; loss:  1.1058752536773682 ; pred:  tensor([0.3309, 0.2440, 0.2829, 0.1422], grad_fn=<SoftmaxBackward0>)
masked_adj tensor([1.8204, 0.2367, 0.2914, 0.5488, 0.3552], grad_fn=<MulBackward0>)
res: tensor([0.3295, 0.2435, 0.2827, 0.1442], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.9102], grad_fn=<IndexBackward0>)
num_high 1 len(mask) 5
pred_loss tensor([1.1101], grad_fn=<MulBackward0>)
size_loss tensor(nan, grad_fn=<MulBackward0>)
size_num_loss 0.20002
loss: tensor([1.1101], grad_fn=<MulBackward0>)
7
epoch:  7 ; loss:  1.1101433038711548 ; pred:  tensor([0.3295, 0.2435, 0.2827, 0.1442], grad_fn=<SoftmaxBackward0>)
masked_adj tensor([1.8434, 0.2085, 0.2626, 0.4950, 0.3908], grad_fn=<MulBackward0>)
res: tensor([0.3286, 0.2431, 0.2821, 0.1461], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.9217], grad_fn=<IndexBackward0>)
num_high 1 len(mask) 5
pred_loss tensor([1.1129], grad_fn=<MulBackward0>)
size_loss tensor(nan, grad_fn=<MulBackward0>)
size_num_loss 0.20002
loss: tensor([1.1129], grad_fn=<MulBackward0>)
8
epoch:  8 ; loss:  1.1128616333007812 ; pred:  tensor([0.3286, 0.2431, 0.2821, 0.1461], grad_fn=<SoftmaxBackward0>)
masked_adj tensor([1.8612, 0.1863, 0.2394, 0.4516, 0.4322], grad_fn=<MulBackward0>)
res: tensor([0.3281, 0.2427, 0.2813, 0.1479], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.9306], grad_fn=<IndexBackward0>)
num_high 1 len(mask) 5
pred_loss tensor([1.1144], grad_fn=<MulBackward0>)
size_loss tensor(nan, grad_fn=<MulBackward0>)
size_num_loss 0.20002
loss: tensor([1.1144], grad_fn=<MulBackward0>)
9
epoch:  9 ; loss:  1.1144241094589233 ; pred:  tensor([0.3281, 0.2427, 0.2813, 0.1479], grad_fn=<SoftmaxBackward0>)
Finished Training
sel masked ver tensor(indices=tensor([[5697, 5697],
                       [   0,    0]]),
       values=tensor([3.7224, 0.9032]),
       size=(5698, 1), nnz=2, layout=torch.sparse_coo,
       grad_fn=<SparseCooTensorWithDimsAndTensorsBackward0>)
Counter({'fax': 1, 'phone': 1})
dict index: {}
node_idx 5697
 node original label [0]
 node predicted label explain 0
 node prediction probability explain tensor([0.3564, 0.2532, 0.2911, 0.0992], grad_fn=<SoftmaxBackward0>)
 node predicted label full 0 most important relations  {'fax': 1, 'phone': 1, 'label': 0, 'node_idx': '5697'}
 final masks and lenght tensor(indices=tensor([[ 88547, 129972, 196252, 229392, 328812],
                       [     0,   3162,   4568,      0,   5230]]),
       values=tensor([3.7224, 0.1863, 0.2394, 0.9032, 0.4322]),
       size=(753935, 8285), nnz=5, layout=torch.sparse_coo,
       grad_fn=<SparseCooTensorWithDimsAndTensorsBackward0>) 2 ---------------------------------------------------------------
node label: 0
12
masked_adj tensor([0.8431, 0.8214, 0.7889, 0.5636, 0.7755, 0.6373, 0.7281, 0.6065, 0.6757,
        0.8296, 0.7029, 0.6233, 0.6776, 0.6904, 0.6744, 0.7807],
       grad_fn=<MulBackward0>)
wandb: WARNING Calling wandb.login() after wandb.init() has no effect.
res: tensor([0.3829, 0.2449, 0.2079, 0.1644], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.8431, 0.8214, 0.7889, 0.5636, 0.7755, 0.6373, 0.7281, 0.6065, 0.6757,
        0.8296, 0.7029, 0.6233, 0.6776, 0.6904, 0.6744, 0.7807],
       grad_fn=<IndexBackward0>)
num_high 16 len(mask) 16
pred_loss tensor([0.9601], grad_fn=<MulBackward0>)
size_loss tensor(5.6369, grad_fn=<AddBackward0>)
size_num_loss 1.00000625
loss: tensor([8.1787], grad_fn=<AddBackward0>)
0
epoch:  0 ; loss:  8.178747177124023 ; pred:  tensor([0.3829, 0.2449, 0.2079, 0.1644], grad_fn=<SoftmaxBackward0>)
masked_adj tensor([0.7652, 0.7361, 0.6939, 0.4392, 0.6770, 0.5159, 0.6189, 0.4832, 0.5583,
        0.7470, 0.5893, 0.5009, 0.5604, 0.5750, 0.5568, 0.6834],
       grad_fn=<MulBackward0>)
res: tensor([0.3483, 0.2542, 0.2201, 0.1775], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.7652, 0.7361, 0.6939, 0.6770, 0.5159, 0.6189, 0.5583, 0.7470, 0.5893,
        0.5009, 0.5604, 0.5750, 0.5568, 0.6834], grad_fn=<IndexBackward0>)
num_high 14 len(mask) 16
pred_loss tensor([1.0547], grad_fn=<MulBackward0>)
size_loss tensor(-7.8530, grad_fn=<MulBackward0>)
size_num_loss 0.87500625
loss: tensor([-5.2731], grad_fn=<AddBackward0>)
1
epoch:  1 ; loss:  -5.273069858551025 ; pred:  tensor([0.3483, 0.2542, 0.2201, 0.1775], grad_fn=<SoftmaxBackward0>)
masked_adj tensor([0.8227, 0.7985, 0.7613, 0.3686, 0.7453, 0.4200, 0.4996, 0.4035, 0.4595,
        0.8078, 0.4882, 0.4061, 0.4631, 0.4767, 0.4599, 0.7526],
       grad_fn=<MulBackward0>)
res: tensor([0.3373, 0.2619, 0.2183, 0.1824], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.8227, 0.7985, 0.7613, 0.7453, 0.8078, 0.7526],
       grad_fn=<IndexBackward0>)
num_high 6 len(mask) 16
pred_loss tensor([1.0867], grad_fn=<MulBackward0>)
size_loss tensor(-1.0467, grad_fn=<MulBackward0>)
size_num_loss 0.37500625
loss: tensor([1.0382], grad_fn=<AddBackward0>)
2
epoch:  2 ; loss:  1.0382015705108643 ; pred:  tensor([0.3373, 0.2619, 0.2183, 0.1824], grad_fn=<SoftmaxBackward0>)
masked_adj tensor([0.8738, 0.8495, 0.7656, 0.3350, 0.7165, 0.3499, 0.4067, 0.3583, 0.3848,
        0.8600, 0.4116, 0.3375, 0.3907, 0.4031, 0.3885, 0.7383],
       grad_fn=<MulBackward0>)
res: tensor([0.3274, 0.2680, 0.2167, 0.1879], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.8738, 0.8495, 0.7656, 0.7165, 0.8600, 0.7383],
       grad_fn=<IndexBackward0>)
num_high 6 len(mask) 16
pred_loss tensor([1.1165], grad_fn=<MulBackward0>)
size_loss tensor(-4.6926, grad_fn=<MulBackward0>)
size_num_loss 0.37500625
loss: tensor([-2.6053], grad_fn=<AddBackward0>)
3
epoch:  3 ; loss:  -2.605346202850342 ; pred:  tensor([0.3274, 0.2680, 0.2167, 0.1879], grad_fn=<SoftmaxBackward0>)
masked_adj tensor([0.9145, 0.8942, 0.7295, 0.3276, 0.6486, 0.2976, 0.3407, 0.3403, 0.3278,
        0.9035, 0.3542, 0.2866, 0.3360, 0.3476, 0.3351, 0.6804],
       grad_fn=<MulBackward0>)
res: tensor([0.3192, 0.2726, 0.2154, 0.1928], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.9145, 0.8942, 0.7295, 0.6486, 0.9035, 0.6804],
       grad_fn=<IndexBackward0>)
num_high 6 len(mask) 16
pred_loss tensor([1.1419], grad_fn=<MulBackward0>)
size_loss tensor(-14.9540, grad_fn=<MulBackward0>)
size_num_loss 0.37500625
loss: tensor([-12.8664], grad_fn=<AddBackward0>)
4
epoch:  4 ; loss:  -12.866355895996094 ; pred:  tensor([0.3192, 0.2726, 0.2154, 0.1928], grad_fn=<SoftmaxBackward0>)
masked_adj tensor([0.9441, 0.9290, 0.6628, 0.3388, 0.5608, 0.2580, 0.2981, 0.3427, 0.2841,
        0.9362, 0.3117, 0.2481, 0.2946, 0.3057, 0.2950, 0.5989],
       grad_fn=<MulBackward0>)
res: tensor([0.3124, 0.2758, 0.2145, 0.1973], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.9441, 0.9290, 0.6628, 0.5608, 0.9362, 0.5989],
       grad_fn=<IndexBackward0>)
num_high 6 len(mask) 16
pred_loss tensor([1.1634], grad_fn=<MulBackward0>)
size_loss tensor(-33.5420, grad_fn=<MulBackward0>)
size_num_loss 0.37500625
loss: tensor([-31.4551], grad_fn=<AddBackward0>)
5
epoch:  5 ; loss:  -31.455055236816406 ; pred:  tensor([0.3124, 0.2758, 0.2145, 0.1973], grad_fn=<SoftmaxBackward0>)
masked_adj tensor([0.9643, 0.9539, 0.5764, 0.3636, 0.4615, 0.2278, 0.2742, 0.3602, 0.2506,
        0.9590, 0.2807, 0.2188, 0.2631, 0.2742, 0.2647, 0.5031],
       grad_fn=<MulBackward0>)
res: tensor([0.3065, 0.2780, 0.2141, 0.2013], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.9643, 0.9539, 0.5764, 0.9590, 0.5031], grad_fn=<IndexBackward0>)
num_high 5 len(mask) 16
pred_loss tensor([1.1824], grad_fn=<MulBackward0>)
size_loss tensor(-53.4343, grad_fn=<MulBackward0>)
size_num_loss 0.31250625
loss: tensor([-51.4123], grad_fn=<AddBackward0>)
6
epoch:  6 ; loss:  -51.412315368652344 ; pred:  tensor([0.3065, 0.2780, 0.2141, 0.2013], grad_fn=<SoftmaxBackward0>)
masked_adj tensor([0.9775, 0.9707, 0.4865, 0.3979, 0.3770, 0.2046, 0.2651, 0.3888, 0.2250,
        0.9741, 0.2586, 0.1962, 0.2392, 0.2506, 0.2418, 0.4074],
       grad_fn=<MulBackward0>)
res: tensor([0.3019, 0.2795, 0.2142, 0.2044], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.9775, 0.9707, 0.9741], grad_fn=<IndexBackward0>)
num_high 3 len(mask) 16
pred_loss tensor([1.1976], grad_fn=<MulBackward0>)
size_loss tensor(-0.0115, grad_fn=<MulBackward0>)
size_num_loss 0.18750625
loss: tensor([1.1976], grad_fn=<MulBackward0>)
7
epoch:  7 ; loss:  1.1976404190063477 ; pred:  tensor([0.3019, 0.2795, 0.2142, 0.2044], grad_fn=<SoftmaxBackward0>)
masked_adj tensor([0.9848, 0.9802, 0.4078, 0.4394, 0.3082, 0.1868, 0.2693, 0.4263, 0.2057,
        0.9826, 0.2441, 0.1788, 0.2214, 0.2336, 0.2249, 0.3284],
       grad_fn=<MulBackward0>)
res: tensor([0.2985, 0.2803, 0.2144, 0.2067], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.9848, 0.9802, 0.9826], grad_fn=<IndexBackward0>)
num_high 3 len(mask) 16
pred_loss tensor([1.2088], grad_fn=<MulBackward0>)
size_loss tensor(-0.0054, grad_fn=<MulBackward0>)
size_num_loss 0.18750625
loss: tensor([1.2088], grad_fn=<MulBackward0>)
8
epoch:  8 ; loss:  1.2088221311569214 ; pred:  tensor([0.2985, 0.2803, 0.2144, 0.2067], grad_fn=<SoftmaxBackward0>)
masked_adj tensor([0.9892, 0.9859, 0.3416, 0.4831, 0.2537, 0.1732, 0.2845, 0.4680, 0.1914,
        0.9876, 0.2357, 0.1654, 0.2085, 0.2218, 0.2126, 0.2656],
       grad_fn=<MulBackward0>)
res: tensor([0.2962, 0.2807, 0.2146, 0.2086], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.9892, 0.9859, 0.9876], grad_fn=<IndexBackward0>)
num_high 3 len(mask) 16
pred_loss tensor([1.2169], grad_fn=<MulBackward0>)
size_loss tensor(-0.0028, grad_fn=<MulBackward0>)
size_num_loss 0.18750625
loss: tensor([1.2169], grad_fn=<MulBackward0>)
9
epoch:  9 ; loss:  1.2168501615524292 ; pred:  tensor([0.2962, 0.2807, 0.2146, 0.2086], grad_fn=<SoftmaxBackward0>)
Finished Training
sel masked ver tensor(indices=tensor([[6841, 7733, 5783],
                       [5783, 5783,   62]]),
       values=tensor([0.9892, 0.9859, 0.9876]),
       size=(7734, 5784), nnz=3, layout=torch.sparse_coo,
       grad_fn=<SparseCooTensorWithDimsAndTensorsBackward0>)
Counter({'author': 2, 'phone': 1})
dict index: {}
node_idx 5783
 node original label [0]
 node predicted label explain 0
 node prediction probability explain tensor([0.2962, 0.2807, 0.2146, 0.2086], grad_fn=<SoftmaxBackward0>)
 node predicted label full 0 most important relations  {'phone': 1, 'author': 2, 'label': 0, 'node_idx': '5783'}
 final masks and lenght tensor(indices=tensor([[ 23411,  24303,  24304,  24306,  88633, 154486, 154544,
                        154562, 196338, 229478, 237763, 254333, 254333, 254333,
                        254333, 328898],
                       [  5783,   5783,   5783,   5783,     58,   5783,   5783,
                          5783,   1288,     62,   5583,   6841,   7733,   7734,
                          7736,   5230]]),
       values=tensor([0.9892, 0.9859, 0.3416, 0.4831, 0.2537, 0.1732, 0.2845,
                      0.4680, 0.1914, 0.9876, 0.2357, 0.1654, 0.2085, 0.2218,
                      0.2126, 0.2656]),
       size=(753935, 8285), nnz=16, layout=torch.sparse_coo,
       grad_fn=<SparseCooTensorWithDimsAndTensorsBackward0>) 3 ---------------------------------------------------------------
node label: 0
4
masked_adj tensor([1.5416, 0.7468, 0.7592, 1.5174, 0.5720], grad_fn=<MulBackward0>)
res: tensor([0.3628, 0.2503, 0.2652, 0.1217], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.7708, 0.7468, 0.7592, 0.7587, 0.5720], grad_fn=<IndexBackward0>)
num_high 5 len(mask) 5
pred_loss tensor([1.0140], grad_fn=<MulBackward0>)
size_loss tensor(1.7331, grad_fn=<AddBackward0>)
size_num_loss 1.00002
loss: tensor([4.3254], grad_fn=<AddBackward0>)
wandb: WARNING Calling wandb.login() after wandb.init() has no effect.
0
epoch:  0 ; loss:  4.3254170417785645 ; pred:  tensor([0.3628, 0.2503, 0.2652, 0.1217], grad_fn=<SoftmaxBackward0>)
masked_adj tensor([1.3421, 0.6414, 0.6566, 1.3121, 0.4477], grad_fn=<MulBackward0>)
res: tensor([0.3516, 0.2496, 0.2665, 0.1323], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.6711, 0.6414, 0.6566, 0.6560], grad_fn=<IndexBackward0>)
num_high 4 len(mask) 5
pred_loss tensor([1.0453], grad_fn=<MulBackward0>)
size_loss tensor(-0.1465, grad_fn=<MulBackward0>)
size_num_loss 0.80002
loss: tensor([2.3509], grad_fn=<AddBackward0>)
1
epoch:  1 ; loss:  2.3509373664855957 ; pred:  tensor([0.3516, 0.2496, 0.2665, 0.1323], grad_fn=<SoftmaxBackward0>)
masked_adj tensor([1.4881, 0.5485, 0.5740, 1.0935, 0.3733], grad_fn=<MulBackward0>)
res: tensor([0.3453, 0.2463, 0.2750, 0.1334], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.7441, 0.5485, 0.5740, 0.5468], grad_fn=<IndexBackward0>)
num_high 4 len(mask) 5
pred_loss tensor([1.0633], grad_fn=<MulBackward0>)
size_loss tensor(-8.9563, grad_fn=<MulBackward0>)
size_num_loss 0.80002
loss: tensor([-6.4352], grad_fn=<AddBackward0>)
2
epoch:  2 ; loss:  -6.4351911544799805 ; pred:  tensor([0.3453, 0.2463, 0.2750, 0.1334], grad_fn=<SoftmaxBackward0>)
masked_adj tensor([1.6098, 0.4531, 0.4927, 0.9318, 0.3286], grad_fn=<MulBackward0>)
res: tensor([0.3400, 0.2456, 0.2790, 0.1354], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.8049], grad_fn=<IndexBackward0>)
num_high 1 len(mask) 5
pred_loss tensor([1.0789], grad_fn=<MulBackward0>)
size_loss tensor(nan, grad_fn=<MulBackward0>)
size_num_loss 0.20002
loss: tensor([1.0789], grad_fn=<MulBackward0>)
3
epoch:  3 ; loss:  1.078879475593567 ; pred:  tensor([0.3400, 0.2456, 0.2790, 0.1354], grad_fn=<SoftmaxBackward0>)
masked_adj tensor([1.6915, 0.3776, 0.4258, 0.8022, 0.3108], grad_fn=<MulBackward0>)
res: tensor([0.3359, 0.2451, 0.2813, 0.1377], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.8457], grad_fn=<IndexBackward0>)
num_high 1 len(mask) 5
pred_loss tensor([1.0908], grad_fn=<MulBackward0>)
size_loss tensor(nan, grad_fn=<MulBackward0>)
size_num_loss 0.20002
loss: tensor([1.0908], grad_fn=<MulBackward0>)
4
epoch:  4 ; loss:  1.0908008813858032 ; pred:  tensor([0.3359, 0.2451, 0.2813, 0.1377], grad_fn=<SoftmaxBackward0>)
masked_adj tensor([1.7486, 0.3186, 0.3713, 0.6985, 0.3122], grad_fn=<MulBackward0>)
res: tensor([0.3330, 0.2445, 0.2825, 0.1400], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.8743], grad_fn=<IndexBackward0>)
num_high 1 len(mask) 5
pred_loss tensor([1.0996], grad_fn=<MulBackward0>)
size_loss tensor(nan, grad_fn=<MulBackward0>)
size_num_loss 0.20002
loss: tensor([1.0996], grad_fn=<MulBackward0>)
5
epoch:  5 ; loss:  1.0996043682098389 ; pred:  tensor([0.3330, 0.2445, 0.2825, 0.1400], grad_fn=<SoftmaxBackward0>)
masked_adj tensor([1.7898, 0.2726, 0.3271, 0.6155, 0.3281], grad_fn=<MulBackward0>)
res: tensor([0.3309, 0.2440, 0.2829, 0.1422], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.8949], grad_fn=<IndexBackward0>)
num_high 1 len(mask) 5
pred_loss tensor([1.1059], grad_fn=<MulBackward0>)
size_loss tensor(nan, grad_fn=<MulBackward0>)
size_num_loss 0.20002
loss: tensor([1.1059], grad_fn=<MulBackward0>)
6
epoch:  6 ; loss:  1.1058752536773682 ; pred:  tensor([0.3309, 0.2440, 0.2829, 0.1422], grad_fn=<SoftmaxBackward0>)
masked_adj tensor([1.8204, 0.2367, 0.2914, 0.5488, 0.3552], grad_fn=<MulBackward0>)
res: tensor([0.3295, 0.2435, 0.2827, 0.1442], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.9102], grad_fn=<IndexBackward0>)
num_high 1 len(mask) 5
pred_loss tensor([1.1101], grad_fn=<MulBackward0>)
size_loss tensor(nan, grad_fn=<MulBackward0>)
size_num_loss 0.20002
loss: tensor([1.1101], grad_fn=<MulBackward0>)
7
epoch:  7 ; loss:  1.1101433038711548 ; pred:  tensor([0.3295, 0.2435, 0.2827, 0.1442], grad_fn=<SoftmaxBackward0>)
masked_adj tensor([1.8434, 0.2085, 0.2626, 0.4950, 0.3908], grad_fn=<MulBackward0>)
res: tensor([0.3286, 0.2431, 0.2821, 0.1461], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.9217], grad_fn=<IndexBackward0>)
num_high 1 len(mask) 5
pred_loss tensor([1.1129], grad_fn=<MulBackward0>)
size_loss tensor(nan, grad_fn=<MulBackward0>)
size_num_loss 0.20002
loss: tensor([1.1129], grad_fn=<MulBackward0>)
8
epoch:  8 ; loss:  1.1128616333007812 ; pred:  tensor([0.3286, 0.2431, 0.2821, 0.1461], grad_fn=<SoftmaxBackward0>)
masked_adj tensor([1.8612, 0.1863, 0.2394, 0.4516, 0.4322], grad_fn=<MulBackward0>)
res: tensor([0.3281, 0.2427, 0.2813, 0.1479], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.9306], grad_fn=<IndexBackward0>)
num_high 1 len(mask) 5
pred_loss tensor([1.1144], grad_fn=<MulBackward0>)
size_loss tensor(nan, grad_fn=<MulBackward0>)
size_num_loss 0.20002
loss: tensor([1.1144], grad_fn=<MulBackward0>)
9
epoch:  9 ; loss:  1.1144241094589233 ; pred:  tensor([0.3281, 0.2427, 0.2813, 0.1479], grad_fn=<SoftmaxBackward0>)
Finished Training
sel masked ver tensor(indices=tensor([[5701, 5701],
                       [   0,    0]]),
       values=tensor([3.7224, 0.9032]),
       size=(5702, 1), nnz=2, layout=torch.sparse_coo,
       grad_fn=<SparseCooTensorWithDimsAndTensorsBackward0>)
Counter({'fax': 1, 'phone': 1})
dict index: {}
node_idx 5701
 node original label [0]
 node predicted label explain 0
 node prediction probability explain tensor([0.3564, 0.2532, 0.2911, 0.0992], grad_fn=<SoftmaxBackward0>)
 node predicted label full 0 most important relations  {'fax': 1, 'phone': 1, 'label': 0, 'node_idx': '5701'}
 final masks and lenght tensor(indices=tensor([[ 88551, 129976, 196256, 229396, 328816],
                       [     0,   3162,   3957,      0,   5230]]),
       values=tensor([3.7224, 0.1863, 0.2394, 0.9032, 0.4322]),
       size=(753935, 8285), nnz=5, layout=torch.sparse_coo,
       grad_fn=<SparseCooTensorWithDimsAndTensorsBackward0>) 2 ---------------------------------------------------------------
node label: 0
29
masked_adj tensor([0.7788, 0.7685, 0.7542, 0.6720, 0.7486, 0.6973, 0.7299, 0.6867, 0.7108,
        0.7723, 0.7206, 0.6924, 0.7114, 0.7160, 0.7103, 0.7507, 0.7721, 0.7268,
        0.7177, 0.7425, 0.7106, 0.7585, 0.7517, 0.7730, 0.7635, 0.7639, 0.7469,
        0.7648, 0.7249, 0.7322, 0.7244, 0.7531, 0.6930, 0.7075, 0.7251, 0.7739,
        0.7394, 0.7197, 0.7391, 0.7101, 0.6880, 0.7565, 0.7072, 0.7149, 0.6961,
        0.7833, 0.6973, 0.7180, 0.7063, 0.7133, 0.7331, 0.7447, 0.7180, 0.7613,
        0.7090, 0.7112, 0.6925, 0.7320, 0.7294, 0.7485, 0.7285, 0.7769, 0.6987,
        0.7660, 0.7675, 0.7531, 0.7855, 0.7446, 0.7401, 0.7258, 0.7023, 0.7634,
        0.7265, 0.7447, 0.7326, 0.7422, 0.7460, 0.7138, 0.6690, 0.7108, 0.7313,
        0.7220, 0.6942, 0.7153, 0.7450, 0.7447, 0.7601, 0.7324, 0.7502, 0.7182,
        0.7025, 0.7467, 0.6833, 0.7087, 0.7648, 0.7417, 0.6909, 0.7283, 0.7150,
        0.7435, 0.7498, 0.7335, 0.7207, 0.7635, 0.7307, 0.7374, 0.7345, 0.7507,
        0.7590, 0.7399, 0.7496], grad_fn=<MulBackward0>)
wandb: WARNING Calling wandb.login() after wandb.init() has no effect.
res: tensor([0.9742, 0.0095, 0.0013, 0.0150], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.7788, 0.7685, 0.7542, 0.6720, 0.7486, 0.6973, 0.7299, 0.6867, 0.7108,
        0.7723, 0.7206, 0.6924, 0.7114, 0.7160, 0.7103, 0.7507, 0.7721, 0.7268,
        0.7177, 0.7425, 0.7106, 0.7585, 0.7517, 0.7730, 0.7635, 0.7639, 0.7469,
        0.7648, 0.7249, 0.7322, 0.7244, 0.7531, 0.6930, 0.7075, 0.7251, 0.7739,
        0.7394, 0.7197, 0.7391, 0.7101, 0.6880, 0.7565, 0.7072, 0.7149, 0.6961,
        0.7833, 0.6973, 0.7180, 0.7063, 0.7133, 0.7331, 0.7447, 0.7180, 0.7613,
        0.7090, 0.7112, 0.6925, 0.7320, 0.7294, 0.7485, 0.7285, 0.7769, 0.6987,
        0.7660, 0.7675, 0.7531, 0.7855, 0.7446, 0.7401, 0.7258, 0.7023, 0.7634,
        0.7265, 0.7447, 0.7326, 0.7422, 0.7460, 0.7138, 0.6690, 0.7108, 0.7313,
        0.7220, 0.6942, 0.7153, 0.7450, 0.7447, 0.7601, 0.7324, 0.7502, 0.7182,
        0.7025, 0.7467, 0.6833, 0.7087, 0.7648, 0.7417, 0.6909, 0.7283, 0.7150,
        0.7435, 0.7498, 0.7335, 0.7207, 0.7635, 0.7307, 0.7374, 0.7345, 0.7507,
        0.7590, 0.7399, 0.7496], grad_fn=<IndexBackward0>)
num_high 111 len(mask) 111
pred_loss tensor([0.0262], grad_fn=<MulBackward0>)
size_loss tensor(40.6239, grad_fn=<AddBackward0>)
size_num_loss 1.000000900900901
loss: tensor([42.2296], grad_fn=<AddBackward0>)
0
epoch:  0 ; loss:  42.229557037353516 ; pred:  tensor([0.9742, 0.0095, 0.0013, 0.0150], grad_fn=<SoftmaxBackward0>)
masked_adj tensor([0.6811, 0.6681, 0.6504, 0.5541, 0.6436, 0.5828, 0.6211, 0.5707, 0.5985,
        0.6729, 0.6100, 0.5773, 0.5992, 0.6047, 0.5979, 0.6462, 0.6727, 0.6174,
        0.6066, 0.6362, 0.5983, 0.6558, 0.6474, 0.6738, 0.6619, 0.6624, 0.6415,
        0.6636, 0.6151, 0.6238, 0.6145, 0.6492, 0.5779, 0.5946, 0.6154, 0.6749,
        0.6325, 0.6090, 0.6321, 0.5977, 0.5722, 0.6533, 0.5943, 0.6033, 0.5815,
        0.6867, 0.5828, 0.6069, 0.5932, 0.6015, 0.6249, 0.6389, 0.6069, 0.6592,
        0.5965, 0.5990, 0.5773, 0.6236, 0.6205, 0.6435, 0.6194, 0.6787, 0.5844,
        0.6650, 0.6669, 0.6491, 0.6895, 0.6388, 0.6333, 0.6162, 0.5887, 0.6618,
        0.6170, 0.6388, 0.6242, 0.6358, 0.6404, 0.6020, 0.5508, 0.5985, 0.6228,
        0.6117, 0.5793, 0.6038, 0.6392, 0.6389, 0.6577, 0.6241, 0.6456, 0.6072,
        0.5888, 0.6413, 0.5668, 0.5960, 0.6636, 0.6353, 0.5755, 0.6192, 0.6034,
        0.6374, 0.6451, 0.6253, 0.6101, 0.6619, 0.6221, 0.6300, 0.6266, 0.6462,
        0.6563, 0.6331, 0.6449], grad_fn=<MulBackward0>)
res: tensor([0.9156, 0.0334, 0.0075, 0.0436], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.6811, 0.6681, 0.6504, 0.5541, 0.6436, 0.5828, 0.6211, 0.5707, 0.5985,
        0.6729, 0.6100, 0.5773, 0.5992, 0.6047, 0.5979, 0.6462, 0.6727, 0.6174,
        0.6066, 0.6362, 0.5983, 0.6558, 0.6474, 0.6738, 0.6619, 0.6624, 0.6415,
        0.6636, 0.6151, 0.6238, 0.6145, 0.6492, 0.5779, 0.5946, 0.6154, 0.6749,
        0.6325, 0.6090, 0.6321, 0.5977, 0.5722, 0.6533, 0.5943, 0.6033, 0.5815,
        0.6867, 0.5828, 0.6069, 0.5932, 0.6015, 0.6249, 0.6389, 0.6069, 0.6592,
        0.5965, 0.5990, 0.5773, 0.6236, 0.6205, 0.6435, 0.6194, 0.6787, 0.5844,
        0.6650, 0.6669, 0.6491, 0.6895, 0.6388, 0.6333, 0.6162, 0.5887, 0.6618,
        0.6170, 0.6388, 0.6242, 0.6358, 0.6404, 0.6020, 0.5508, 0.5985, 0.6228,
        0.6117, 0.5793, 0.6038, 0.6392, 0.6389, 0.6577, 0.6241, 0.6456, 0.6072,
        0.5888, 0.6413, 0.5668, 0.5960, 0.6636, 0.6353, 0.5755, 0.6192, 0.6034,
        0.6374, 0.6451, 0.6253, 0.6101, 0.6619, 0.6221, 0.6300, 0.6266, 0.6462,
        0.6563, 0.6331, 0.6449], grad_fn=<IndexBackward0>)
num_high 111 len(mask) 111
pred_loss tensor([0.0882], grad_fn=<MulBackward0>)
size_loss tensor(34.6293, grad_fn=<AddBackward0>)
size_num_loss 1.000000900900901
loss: tensor([36.3775], grad_fn=<AddBackward0>)
1
epoch:  1 ; loss:  36.37746810913086 ; pred:  tensor([0.9156, 0.0334, 0.0075, 0.0436], grad_fn=<SoftmaxBackward0>)
masked_adj tensor([0.5651, 0.5505, 0.5310, 0.4312, 0.5236, 0.4598, 0.4995, 0.4476, 0.4759,
        0.5559, 0.4879, 0.4542, 0.4767, 0.4823, 0.4753, 0.5264, 0.5557, 0.4956,
        0.4843, 0.5157, 0.4757, 0.5370, 0.5277, 0.5569, 0.5436, 0.5442, 0.5214,
        0.5455, 0.4932, 0.5025, 0.4926, 0.5297, 0.4549, 0.4719, 0.4935, 0.5582,
        0.5116, 0.4868, 0.5112, 0.4751, 0.4491, 0.5342, 0.4716, 0.4809, 0.4585,
        0.5715, 0.4598, 0.4847, 0.4705, 0.4790, 0.5036, 0.5185, 0.4847, 0.5407,
        0.4738, 0.4764, 0.4542, 0.5022, 0.4988, 0.5235, 0.4977, 0.5625, 0.4615,
        0.5472, 0.5493, 0.5297, 0.5748, 0.5185, 0.5126, 0.4944, 0.4658, 0.5437,
        0.4952, 0.5185, 0.5031, 0.5153, 0.5203, 0.4797, 0.4280, 0.4760, 0.5014,
        0.4898, 0.4565, 0.4816, 0.5188, 0.5188, 0.5394, 0.5029, 0.5262, 0.4853,
        0.4664, 0.5214, 0.4437, 0.4734, 0.5457, 0.5148, 0.4527, 0.4977, 0.4809,
        0.5171, 0.5254, 0.5042, 0.4879, 0.5436, 0.5007, 0.5089, 0.5056, 0.5270,
        0.5396, 0.5125, 0.5250], grad_fn=<MulBackward0>)
res: tensor([0.7683, 0.0942, 0.0334, 0.1041], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.5651, 0.5505, 0.5310, 0.5236, 0.5559, 0.5264, 0.5557, 0.5157, 0.5370,
        0.5277, 0.5569, 0.5436, 0.5442, 0.5214, 0.5455, 0.5025, 0.5297, 0.5582,
        0.5116, 0.5112, 0.5342, 0.5715, 0.5036, 0.5185, 0.5407, 0.5022, 0.5235,
        0.5625, 0.5472, 0.5493, 0.5297, 0.5748, 0.5185, 0.5126, 0.5437, 0.5185,
        0.5031, 0.5153, 0.5203, 0.5014, 0.5188, 0.5188, 0.5394, 0.5029, 0.5262,
        0.5214, 0.5457, 0.5148, 0.5171, 0.5254, 0.5042, 0.5436, 0.5007, 0.5089,
        0.5056, 0.5270, 0.5396, 0.5125, 0.5250], grad_fn=<IndexBackward0>)
num_high 59 len(mask) 111
pred_loss tensor([0.2635], grad_fn=<MulBackward0>)
size_loss tensor(-0.3748, grad_fn=<MulBackward0>)
size_num_loss 0.5315324324324324
loss: tensor([1.1112], grad_fn=<AddBackward0>)
2
epoch:  2 ; loss:  1.1112380027770996 ; pred:  tensor([0.7683, 0.0942, 0.0334, 0.1041], grad_fn=<SoftmaxBackward0>)
masked_adj tensor([0.5550, 0.5102, 0.4375, 0.3497, 0.4145, 0.3722, 0.4049, 0.3625, 0.3852,
        0.5282, 0.3951, 0.3678, 0.3859, 0.3905, 0.3848, 0.4225, 0.5275, 0.4016,
        0.3922, 0.3967, 0.3851, 0.4592, 0.4264, 0.5315, 0.4848, 0.4870, 0.4087,
        0.4918, 0.3996, 0.3802, 0.3990, 0.4329, 0.3683, 0.3820, 0.3998, 0.5353,
        0.3903, 0.3942, 0.3897, 0.3846, 0.3637, 0.4489, 0.3817, 0.3893, 0.3712,
        0.5712, 0.3722, 0.3925, 0.3808, 0.3878, 0.3811, 0.4023, 0.3925, 0.4737,
        0.3835, 0.3857, 0.3678, 0.3799, 0.4043, 0.4142, 0.4033, 0.5518, 0.3738,
        0.4998, 0.5097, 0.4364, 0.5808, 0.4025, 0.3920, 0.4012, 0.3778, 0.4884,
        0.4010, 0.4029, 0.3808, 0.3968, 0.4073, 0.3908, 0.3501, 0.3879, 0.3792,
        0.3991, 0.3735, 0.3936, 0.4025, 0.4059, 0.4778, 0.3806, 0.4272, 0.4005,
        0.3849, 0.4113, 0.3595, 0.3851, 0.4992, 0.3959, 0.3701, 0.4065, 0.3883,
        0.4006, 0.4215, 0.3818, 0.3939, 0.4834, 0.3787, 0.3866, 0.3833, 0.4333,
        0.5121, 0.3924, 0.4177], grad_fn=<MulBackward0>)
res: tensor([0.6358, 0.1467, 0.0679, 0.1496], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.5550, 0.5102, 0.5282, 0.5275, 0.5315, 0.5353, 0.5712, 0.5518, 0.5097,
        0.5808, 0.5121], grad_fn=<IndexBackward0>)
num_high 11 len(mask) 111
pred_loss tensor([0.4528], grad_fn=<MulBackward0>)
size_loss tensor(-0.5917, grad_fn=<MulBackward0>)
size_num_loss 0.0991
loss: tensor([0.6336], grad_fn=<AddBackward0>)
3
epoch:  3 ; loss:  0.6335963606834412 ; pred:  tensor([0.6358, 0.1467, 0.0679, 0.1496], grad_fn=<SoftmaxBackward0>)
masked_adj tensor([0.6154, 0.4324, 0.3703, 0.3057, 0.3392, 0.3201, 0.3422, 0.3138, 0.3287,
        0.4559, 0.3354, 0.3172, 0.3292, 0.3323, 0.3284, 0.3498, 0.4543, 0.3399,
        0.3335, 0.3167, 0.3286, 0.4010, 0.3551, 0.4673, 0.4384, 0.4416, 0.3316,
        0.4487, 0.3385, 0.3009, 0.3381, 0.3639, 0.3175, 0.3266, 0.3387, 0.4928,
        0.3092, 0.3348, 0.3085, 0.3283, 0.3146, 0.3862, 0.3264, 0.3315, 0.3194,
        0.6381, 0.3201, 0.3336, 0.3258, 0.3305, 0.2996, 0.3236, 0.3336, 0.4219,
        0.3276, 0.3290, 0.3172, 0.2984, 0.3418, 0.3388, 0.3411, 0.6096, 0.3216,
        0.4623, 0.4321, 0.3735, 0.6489, 0.3249, 0.3126, 0.3411, 0.3254, 0.4475,
        0.3385, 0.3267, 0.3044, 0.3201, 0.3333, 0.3380, 0.3116, 0.3360, 0.2997,
        0.3436, 0.3285, 0.3417, 0.3225, 0.3364, 0.4379, 0.3018, 0.3645, 0.3540,
        0.3425, 0.3404, 0.3114, 0.3316, 0.4654, 0.3186, 0.3257, 0.3499, 0.3288,
        0.3257, 0.3525, 0.3032, 0.3321, 0.4350, 0.3009, 0.3037, 0.3056, 0.3805,
        0.4364, 0.3159, 0.3427], grad_fn=<MulBackward0>)
res: tensor([0.5379, 0.1819, 0.1010, 0.1791], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.6154, 0.6381, 0.6096, 0.6489], grad_fn=<IndexBackward0>)
num_high 4 len(mask) 111
pred_loss tensor([0.6200], grad_fn=<MulBackward0>)
size_loss tensor(-0.3459, grad_fn=<MulBackward0>)
size_num_loss 0.036036936936936936
loss: tensor([0.9525], grad_fn=<AddBackward0>)
4
epoch:  4 ; loss:  0.9524514675140381 ; pred:  tensor([0.5379, 0.1819, 0.1010, 0.1791], grad_fn=<SoftmaxBackward0>)
masked_adj tensor([0.5767, 0.3702, 0.3294, 0.2900, 0.2958, 0.2967, 0.3082, 0.2937, 0.3011,
        0.3981, 0.3046, 0.2953, 0.3013, 0.3029, 0.3009, 0.3072, 0.3957, 0.3070,
        0.3035, 0.2716, 0.3010, 0.3632, 0.3130, 0.4157, 0.4055, 0.4092, 0.2877,
        0.4174, 0.3062, 0.2573, 0.3060, 0.3224, 0.2955, 0.2999, 0.3063, 0.4575,
        0.2634, 0.3042, 0.2626, 0.3008, 0.2941, 0.3468, 0.2999, 0.3025, 0.2964,
        0.7162, 0.2967, 0.3036, 0.2996, 0.3019, 0.2527, 0.2790, 0.3036, 0.3867,
        0.3005, 0.3012, 0.2953, 0.2514, 0.3080, 0.2955, 0.3076, 0.5584, 0.2982,
        0.4357, 0.3707, 0.3394, 0.7265, 0.2820, 0.2691, 0.3099, 0.3021, 0.4212,
        0.3039, 0.2857, 0.2648, 0.2793, 0.2938, 0.3147, 0.3019, 0.3136, 0.2553,
        0.3177, 0.3126, 0.3192, 0.2764, 0.3033, 0.4184, 0.2584, 0.3333, 0.3375,
        0.3296, 0.3032, 0.2910, 0.3065, 0.4439, 0.2768, 0.3106, 0.3229, 0.2978,
        0.2865, 0.3154, 0.2606, 0.2989, 0.3999, 0.2590, 0.2554, 0.2647, 0.3628,
        0.3846, 0.2762, 0.2991], grad_fn=<MulBackward0>)
res: tensor([0.4876, 0.1989, 0.1208, 0.1926], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.5767, 0.7162, 0.5584, 0.7265], grad_fn=<IndexBackward0>)
num_high 4 len(mask) 111
pred_loss tensor([0.7182], grad_fn=<MulBackward0>)
size_loss tensor(-7.9555, grad_fn=<MulBackward0>)
size_num_loss 0.036036936936936936
loss: tensor([-6.5826], grad_fn=<AddBackward0>)
5
epoch:  5 ; loss:  -6.582553863525391 ; pred:  tensor([0.4876, 0.1989, 0.1208, 0.1926], grad_fn=<SoftmaxBackward0>)
masked_adj tensor([0.5065, 0.3219, 0.3108, 0.2948, 0.2776, 0.2948, 0.2969, 0.2946, 0.2953,
        0.3546, 0.2960, 0.2947, 0.2954, 0.2957, 0.2953, 0.2890, 0.3516, 0.2965,
        0.2958, 0.2526, 0.2953, 0.3438, 0.2947, 0.3770, 0.3858, 0.3896, 0.2694,
        0.3979, 0.2964, 0.2390, 0.2963, 0.3040, 0.2947, 0.2952, 0.2964, 0.4302,
        0.2436, 0.2959, 0.2428, 0.2953, 0.2946, 0.3278, 0.2951, 0.2956, 0.2948,
        0.7773, 0.2948, 0.2958, 0.2951, 0.2955, 0.2311, 0.2605, 0.2958, 0.3671,
        0.2952, 0.2954, 0.2947, 0.2295, 0.2968, 0.2773, 0.2967, 0.4853, 0.2964,
        0.4199, 0.3236, 0.3285, 0.7903, 0.2650, 0.2516, 0.3013, 0.3003, 0.4083,
        0.2914, 0.2709, 0.2510, 0.2646, 0.2802, 0.3134, 0.3126, 0.3132, 0.2360,
        0.3141, 0.3179, 0.3186, 0.2562, 0.2965, 0.4161, 0.2401, 0.3258, 0.3431,
        0.3381, 0.2912, 0.2911, 0.3028, 0.4333, 0.2606, 0.3170, 0.3187, 0.2890,
        0.2733, 0.3030, 0.2435, 0.2882, 0.3781, 0.2423, 0.2329, 0.2497, 0.3701,
        0.3521, 0.2627, 0.2806], grad_fn=<MulBackward0>)
res: tensor([0.4693, 0.2051, 0.1283, 0.1973], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.5065, 0.7773, 0.7903], grad_fn=<IndexBackward0>)
num_high 3 len(mask) 111
pred_loss tensor([0.7565], grad_fn=<MulBackward0>)
size_loss tensor(-25.6739, grad_fn=<MulBackward0>)
size_num_loss 0.02702792792792793
loss: tensor([0.7565], grad_fn=<MulBackward0>)
6
epoch:  6 ; loss:  0.7565315961837769 ; pred:  tensor([0.4693, 0.2051, 0.1283, 0.1973], grad_fn=<SoftmaxBackward0>)
masked_adj tensor([0.4449, 0.2853, 0.3102, 0.3153, 0.2786, 0.3093, 0.3032, 0.3117, 0.3066,
        0.3241, 0.3047, 0.3104, 0.3064, 0.3056, 0.3066, 0.2897, 0.3205, 0.3037,
        0.3052, 0.2529, 0.3066, 0.3404, 0.2951, 0.3507, 0.3784, 0.3819, 0.2705,
        0.3895, 0.3040, 0.2389, 0.3041, 0.3039, 0.3103, 0.3072, 0.3040, 0.4113,
        0.2429, 0.3049, 0.2419, 0.3067, 0.3114, 0.3259, 0.3073, 0.3058, 0.3096,
        0.8221, 0.3093, 0.3052, 0.3074, 0.3061, 0.2277, 0.2612, 0.3052, 0.3614,
        0.3069, 0.3065, 0.3104, 0.2257, 0.3033, 0.2783, 0.3034, 0.4220, 0.3111,
        0.4143, 0.2885, 0.3358, 0.8361, 0.2672, 0.2532, 0.3101, 0.3152, 0.4072,
        0.2960, 0.2755, 0.2555, 0.2691, 0.2856, 0.3290, 0.3388, 0.3295, 0.2346,
        0.3276, 0.3396, 0.3350, 0.2554, 0.3092, 0.4280, 0.2401, 0.3366, 0.3653,
        0.3630, 0.2980, 0.3069, 0.3156, 0.4325, 0.2632, 0.3398, 0.3321, 0.2973,
        0.2794, 0.3096, 0.2448, 0.2948, 0.3687, 0.2438, 0.2291, 0.2533, 0.3952,
        0.3348, 0.2683, 0.2815], grad_fn=<MulBackward0>)
res: tensor([0.4734, 0.2041, 0.1261, 0.1965], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.8221, 0.8361], grad_fn=<IndexBackward0>)
num_high 2 len(mask) 111
pred_loss tensor([0.7478], grad_fn=<MulBackward0>)
size_loss tensor(-0.0987, grad_fn=<MulBackward0>)
size_num_loss 0.018018918918918922
loss: tensor([0.7478], grad_fn=<MulBackward0>)
7
epoch:  7 ; loss:  0.7478394508361816 ; pred:  tensor([0.4734, 0.2041, 0.1261, 0.1965], grad_fn=<SoftmaxBackward0>)
masked_adj tensor([0.3920, 0.2584, 0.3239, 0.3480, 0.2945, 0.3367, 0.3233, 0.3414, 0.3310,
        0.3051, 0.3270, 0.3388, 0.3307, 0.3288, 0.3312, 0.3052, 0.3008, 0.3245,
        0.3282, 0.2679, 0.3311, 0.3499, 0.3102, 0.3355, 0.3816, 0.3845, 0.2864,
        0.3908, 0.3253, 0.2528, 0.3255, 0.3182, 0.3386, 0.3324, 0.3252, 0.4006,
        0.2566, 0.3274, 0.2554, 0.3313, 0.3408, 0.3376, 0.3325, 0.3293, 0.3372,
        0.8549, 0.3367, 0.3280, 0.3329, 0.3299, 0.2382, 0.2768, 0.3280, 0.3675,
        0.3317, 0.3308, 0.3388, 0.2355, 0.3235, 0.2942, 0.3239, 0.3683, 0.3387,
        0.4175, 0.2631, 0.3577, 0.8691, 0.2844, 0.2693, 0.3325, 0.3429, 0.4159,
        0.3142, 0.2950, 0.2744, 0.2882, 0.3059, 0.3575, 0.3768, 0.3586, 0.2469,
        0.3543, 0.3737, 0.3646, 0.2695, 0.3373, 0.4510, 0.2542, 0.3618, 0.4002,
        0.4006, 0.3196, 0.3349, 0.3415, 0.4400, 0.2802, 0.3753, 0.3594, 0.3188,
        0.3003, 0.3311, 0.2603, 0.3150, 0.3701, 0.2593, 0.2395, 0.2712, 0.4330,
        0.3297, 0.2887, 0.2972], grad_fn=<MulBackward0>)
res: tensor([0.4959, 0.1970, 0.1162, 0.1909], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.8549, 0.8691], grad_fn=<IndexBackward0>)
num_high 2 len(mask) 111
pred_loss tensor([0.7013], grad_fn=<MulBackward0>)
size_loss tensor(-0.1012, grad_fn=<MulBackward0>)
size_num_loss 0.018018918918918922
loss: tensor([0.7013], grad_fn=<MulBackward0>)
8
epoch:  8 ; loss:  0.7013198137283325 ; pred:  tensor([0.4959, 0.1970, 0.1162, 0.1909], grad_fn=<SoftmaxBackward0>)
masked_adj tensor([0.3472, 0.2393, 0.3490, 0.3896, 0.3224, 0.3740, 0.3543, 0.3805, 0.3658,
        0.2958, 0.3599, 0.3769, 0.3654, 0.3626, 0.3660, 0.3325, 0.2908, 0.3562,
        0.3616, 0.2949, 0.3659, 0.3699, 0.3371, 0.3301, 0.3936, 0.3957, 0.3144,
        0.4004, 0.3573, 0.2785, 0.3576, 0.3441, 0.3766, 0.3678, 0.3572, 0.3973,
        0.2823, 0.3604, 0.2809, 0.3661, 0.3796, 0.3602, 0.3679, 0.3633, 0.3746,
        0.8792, 0.3740, 0.3614, 0.3685, 0.3642, 0.2602, 0.3045, 0.3614, 0.3832,
        0.3668, 0.3655, 0.3769, 0.2568, 0.3546, 0.3221, 0.3552, 0.3236, 0.3762,
        0.4283, 0.2455, 0.3909, 0.8931, 0.3135, 0.2974, 0.3655, 0.3805, 0.4322,
        0.3433, 0.3265, 0.3051, 0.3194, 0.3380, 0.3957, 0.4227, 0.3974, 0.2707,
        0.3910, 0.4171, 0.4041, 0.2955, 0.3780, 0.4823, 0.2802, 0.3983, 0.4443,
        0.4474, 0.3532, 0.3722, 0.3776, 0.4546, 0.3090, 0.4203, 0.3975, 0.3509,
        0.3335, 0.3645, 0.2878, 0.3459, 0.3805, 0.2865, 0.2617, 0.3012, 0.4786,
        0.3347, 0.3212, 0.3250], grad_fn=<MulBackward0>)
res: tensor([0.5358, 0.1835, 0.1004, 0.1802], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.8792, 0.8931], grad_fn=<IndexBackward0>)
num_high 2 len(mask) 111
pred_loss tensor([0.6239], grad_fn=<MulBackward0>)
size_loss tensor(-0.0971, grad_fn=<MulBackward0>)
size_num_loss 0.018018918918918922
loss: tensor([0.6239], grad_fn=<MulBackward0>)
9
epoch:  9 ; loss:  0.623909056186676 ; pred:  tensor([0.5358, 0.1835, 0.1004, 0.1802], grad_fn=<SoftmaxBackward0>)
Finished Training
sel masked ver tensor(indices=tensor([[5990, 5493],
                       [5350, 5845]]),
       values=tensor([0.8792, 0.8931]),
       size=(5991, 5846), nnz=2, layout=torch.sparse_coo,
       grad_fn=<SparseCooTensorWithDimsAndTensorsBackward0>)
Counter({'isWorkedOnBy': 1, 'isAbout': 1})
dict index: {}
node_idx 5845
 node original label [0]
 node predicted label explain 0
 node prediction probability explain tensor([0.5358, 0.1835, 0.1004, 0.1802], grad_fn=<SoftmaxBackward0>)
 node predicted label full 0 most important relations  {'isWorkedOnBy': 1, 'isAbout': 1, 'label': 0, 'node_idx': '5845'}
 final masks and lenght tensor(indices=tensor([[ 23947,  23948,  23949,  23954,  24148,  24181,  24183,
                         24184,  24202,  24258,  24523,  39063,  39081,  39130,
                         46927,  46927,  46927,  63345,  63405,  63405,  63405,
                         63442,  63443,  63480,  63481,  63481,  63488,  63488,
                         63489,  88695, 115082, 115083, 115084, 115089, 115283,
                        115316, 115337, 115393, 115393, 115658, 146768, 146768,
                        146768, 146786, 146786, 146835, 146835, 146835, 146835,
                        146835, 146835, 146835, 148222, 148223, 148229, 148423,
                        148456, 148477, 148533, 148798, 154480, 154540, 154577,
                        154578, 154615, 154616, 154623, 154624, 179487, 179908,
                        179926, 179975, 196400, 229540, 237825, 246188, 246188,
                        246206, 246255, 246255, 246255, 246255, 246255, 246255,
                        246255, 254395, 254395, 254395, 254395, 254395, 254395,
                        254395, 254395, 254395, 254395, 254395, 262337, 262337,
                        262337, 262337, 262337, 262337, 262337, 262337, 262337,
                        262337, 262337, 304105, 304105, 328960, 328960],
                       [  5845,   5845,   5845,   5845,   5845,   5845,   5845,
                          5845,   5845,   5845,   5845,   5502,   5502,   5502,
                          5923,   5941,   5990,   5990,   5923,   5941,   5990,
                          5990,   5990,   5990,   5923,   5990,   5923,   5990,
                          5941,     98,   5990,   5990,   5941,   5990,   5990,
                          5990,   5923,   5923,   5990,   5990,   5410,   5486,
                          5493,   5410,   5494,   5350,   5410,   5447,   5448,
                          5485,   5486,   5493,   5410,   5410,   5410,   5410,
                          5410,   5410,   5410,   5410,   5845,   5845,   5845,
                          5845,   5845,   5845,   5845,   5845,   5845,   5845,
                          5845,   5845,   1728,     86,   5627,   7632,   7688,
                          7379,   7377,   7378,   7384,   7578,   7611,   7688,
                          7953,   7377,   7378,   7379,   7384,   7578,   7611,
                          7613,   7614,   7632,   7688,   7953,   7377,   7378,
                          7379,   7384,   7578,   7611,   7613,   7614,   7632,
                          7688,   7953,   5941,   5990,   5230,   5231]]),
       values=tensor([0.3472, 0.2393, 0.3490, 0.3896, 0.3224, 0.3740, 0.3543,
                      0.3805, 0.3658, 0.2958, 0.3599, 0.3769, 0.3654, 0.3626,
                      0.3660, 0.3325, 0.2908, 0.3562, 0.3616, 0.2949, 0.3659,
                      0.3699, 0.3371, 0.3301, 0.3936, 0.3957, 0.3144, 0.4004,
                      0.3573, 0.2785, 0.3576, 0.3441, 0.3766, 0.3678, 0.3572,
                      0.3973, 0.2823, 0.3604, 0.2809, 0.3661, 0.3796, 0.3602,
                      0.3679, 0.3633, 0.3746, 0.8792, 0.3740, 0.3614, 0.3685,
                      0.3642, 0.2602, 0.3045, 0.3614, 0.3832, 0.3668, 0.3655,
                      0.3769, 0.2568, 0.3546, 0.3221, 0.3552, 0.3236, 0.3762,
                      0.4283, 0.2455, 0.3909, 0.8931, 0.3135, 0.2974, 0.3655,
                      0.3805, 0.4322, 0.3433, 0.3265, 0.3051, 0.3194, 0.3380,
                      0.3957, 0.4227, 0.3974, 0.2707, 0.3910, 0.4171, 0.4041,
                      0.2955, 0.3780, 0.4823, 0.2802, 0.3983, 0.4443, 0.4474,
                      0.3532, 0.3722, 0.3776, 0.4546, 0.3090, 0.4203, 0.3975,
                      0.3509, 0.3335, 0.3645, 0.2878, 0.3459, 0.3805, 0.2865,
                      0.2617, 0.3012, 0.4786, 0.3347, 0.3212, 0.3250]),
       size=(753935, 8285), nnz=111, layout=torch.sparse_coo,
       grad_fn=<SparseCooTensorWithDimsAndTensorsBackward0>) 2 ---------------------------------------------------------------
wandb: WARNING Calling wandb.login() after wandb.init() has no effect.
node label: 0
13
masked_adj tensor([0.8275, 0.8082, 0.7800, 0.5937, 0.7685, 0.6538, 0.7286, 0.8169, 0.7512,
        0.6894, 0.7015, 0.7448, 0.6620, 0.7363, 0.5785, 0.7779, 0.7127, 0.7075,
        0.7750, 0.6935, 0.6954, 0.7274, 0.6804], grad_fn=<MulBackward0>)
res: tensor([0.4958, 0.1698, 0.1845, 0.1499], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.8275, 0.8082, 0.7800, 0.5937, 0.7685, 0.6538, 0.7286, 0.8169, 0.7512,
        0.6894, 0.7015, 0.7448, 0.6620, 0.7363, 0.5785, 0.7779, 0.7127, 0.7075,
        0.7750, 0.6935, 0.6954, 0.7274, 0.6804], grad_fn=<IndexBackward0>)
num_high 23 len(mask) 23
pred_loss tensor([0.7016], grad_fn=<MulBackward0>)
size_loss tensor(8.2641, grad_fn=<AddBackward0>)
size_num_loss 1.000004347826087
loss: tensor([10.5468], grad_fn=<AddBackward0>)
0
epoch:  0 ; loss:  10.5467529296875 ; pred:  tensor([0.4958, 0.1698, 0.1845, 0.1499], grad_fn=<SoftmaxBackward0>)
masked_adj tensor([0.7443, 0.7188, 0.6826, 0.4698, 0.6682, 0.5339, 0.6195, 0.7302, 0.6468,
        0.5738, 0.5877, 0.6390, 0.5430, 0.6288, 0.4543, 0.6800, 0.6008, 0.5947,
        0.6763, 0.5785, 0.5806, 0.6181, 0.5636], grad_fn=<MulBackward0>)
res: tensor([0.4284, 0.1914, 0.2120, 0.1682], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.7443, 0.7188, 0.6826, 0.6682, 0.5339, 0.6195, 0.7302, 0.6468, 0.5738,
        0.5877, 0.6390, 0.5430, 0.6288, 0.6800, 0.6008, 0.5947, 0.6763, 0.5785,
        0.5806, 0.6181, 0.5636], grad_fn=<IndexBackward0>)
num_high 21 len(mask) 23
pred_loss tensor([0.8476], grad_fn=<MulBackward0>)
size_loss tensor(-3.6891, grad_fn=<MulBackward0>)
size_num_loss 0.9130478260869566
loss: tensor([-1.2733], grad_fn=<AddBackward0>)
1
epoch:  1 ; loss:  -1.2732744216918945 ; pred:  tensor([0.4284, 0.1914, 0.2120, 0.1682], grad_fn=<SoftmaxBackward0>)
masked_adj tensor([0.8031, 0.7807, 0.7452, 0.3933, 0.7280, 0.4354, 0.4977, 0.7909, 0.6798,
        0.4714, 0.4829, 0.6526, 0.4440, 0.5294, 0.3916, 0.7433, 0.4910, 0.4872,
        0.7405, 0.4762, 0.4781, 0.4976, 0.4637], grad_fn=<MulBackward0>)
res: tensor([0.3879, 0.2071, 0.2302, 0.1748], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.8031, 0.7807, 0.7452, 0.7280, 0.7909, 0.6798, 0.6526, 0.5294, 0.7433,
        0.7405], grad_fn=<IndexBackward0>)
num_high 10 len(mask) 23
pred_loss tensor([0.9469], grad_fn=<MulBackward0>)
size_loss tensor(-6.6407, grad_fn=<MulBackward0>)
size_num_loss 0.4347869565217391
loss: tensor([-4.6182], grad_fn=<AddBackward0>)
2
epoch:  2 ; loss:  -4.618245601654053 ; pred:  tensor([0.3879, 0.2071, 0.2302, 0.1748], grad_fn=<SoftmaxBackward0>)
masked_adj tensor([0.8603, 0.8428, 0.8121, 0.3536, 0.7853, 0.3633, 0.4168, 0.8508, 0.6134,
        0.3945, 0.4043, 0.5782, 0.3710, 0.4484, 0.3678, 0.8105, 0.4065, 0.4048,
        0.8080, 0.4002, 0.4023, 0.4125, 0.3912], grad_fn=<MulBackward0>)
res: tensor([0.3626, 0.2156, 0.2389, 0.1828], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.8603, 0.8428, 0.8121, 0.7853, 0.8508, 0.6134, 0.5782, 0.8105, 0.8080],
       grad_fn=<IndexBackward0>)
num_high 9 len(mask) 23
pred_loss tensor([1.0143], grad_fn=<MulBackward0>)
size_loss tensor(-10.7857, grad_fn=<MulBackward0>)
size_num_loss 0.3913086956521739
loss: tensor([-8.7728], grad_fn=<AddBackward0>)
3
epoch:  3 ; loss:  -8.772809028625488 ; pred:  tensor([0.3626, 0.2156, 0.2389, 0.1828], grad_fn=<SoftmaxBackward0>)
masked_adj tensor([0.9056, 0.8930, 0.8694, 0.3417, 0.8385, 0.3099, 0.3730, 0.8988, 0.5319,
        0.3375, 0.3467, 0.4913, 0.3167, 0.3840, 0.3693, 0.8682, 0.3440, 0.3435,
        0.8663, 0.3443, 0.3468, 0.3604, 0.3390], grad_fn=<MulBackward0>)
res: tensor([0.3463, 0.2198, 0.2439, 0.1900], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.9056, 0.8930, 0.8694, 0.8385, 0.8988, 0.5319, 0.8682, 0.8663],
       grad_fn=<IndexBackward0>)
num_high 8 len(mask) 23
pred_loss tensor([1.0603], grad_fn=<MulBackward0>)
size_loss tensor(-15.3668, grad_fn=<MulBackward0>)
size_num_loss 0.34783043478260867
loss: tensor([-13.3933], grad_fn=<AddBackward0>)
4
epoch:  4 ; loss:  -13.393291473388672 ; pred:  tensor([0.3463, 0.2198, 0.2439, 0.1900], grad_fn=<SoftmaxBackward0>)
masked_adj tensor([0.9374, 0.9289, 0.9124, 0.3500, 0.8754, 0.2703, 0.3578, 0.9328, 0.4410,
        0.2957, 0.3055, 0.4182, 0.2762, 0.3328, 0.3876, 0.9116, 0.2997, 0.2993,
        0.9102, 0.3037, 0.3068, 0.3348, 0.3020], grad_fn=<MulBackward0>)
res: tensor([0.3367, 0.2207, 0.2460, 0.1966], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.9374, 0.9289, 0.9124, 0.8754, 0.9328, 0.9116, 0.9102],
       grad_fn=<IndexBackward0>)
num_high 7 len(mask) 23
pred_loss tensor([1.0886], grad_fn=<MulBackward0>)
size_loss tensor(-0.4364, grad_fn=<MulBackward0>)
size_num_loss 0.3043521739130435
loss: tensor([1.4817], grad_fn=<AddBackward0>)
5
epoch:  5 ; loss:  1.4816737174987793 ; pred:  tensor([0.3367, 0.2207, 0.2460, 0.1966], grad_fn=<SoftmaxBackward0>)
masked_adj tensor([0.9568, 0.9507, 0.9364, 0.3730, 0.8647, 0.2409, 0.3636, 0.9536, 0.3655,
        0.2655, 0.2771, 0.3582, 0.2459, 0.2919, 0.4167, 0.9355, 0.2697, 0.2685,
        0.9341, 0.2749, 0.2786, 0.3296, 0.2765], grad_fn=<MulBackward0>)
res: tensor([0.3311, 0.2198, 0.2472, 0.2018], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.9568, 0.9507, 0.9364, 0.8647, 0.9536, 0.9355, 0.9341],
       grad_fn=<IndexBackward0>)
num_high 7 len(mask) 23
pred_loss tensor([1.1052], grad_fn=<MulBackward0>)
size_loss tensor(-0.9986, grad_fn=<MulBackward0>)
size_num_loss 0.3043521739130435
loss: tensor([0.9080], grad_fn=<AddBackward0>)
6
epoch:  6 ; loss:  0.9079596996307373 ; pred:  tensor([0.3311, 0.2198, 0.2472, 0.2018], grad_fn=<SoftmaxBackward0>)
masked_adj tensor([0.9690, 0.9644, 0.9511, 0.4063, 0.8318, 0.2192, 0.3856, 0.9666, 0.3045,
        0.2443, 0.2586, 0.3095, 0.2234, 0.2590, 0.4520, 0.9500, 0.2511, 0.2483,
        0.9485, 0.2550, 0.2595, 0.3404, 0.2597], grad_fn=<MulBackward0>)
res: tensor([0.3291, 0.2176, 0.2478, 0.2055], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.9690, 0.9644, 0.9511, 0.8318, 0.9666, 0.9500, 0.9485],
       grad_fn=<IndexBackward0>)
num_high 7 len(mask) 23
pred_loss tensor([1.1113], grad_fn=<MulBackward0>)
size_loss tensor(-2.3595, grad_fn=<MulBackward0>)
size_num_loss 0.3043521739130435
loss: tensor([-0.4661], grad_fn=<AddBackward0>)
7
epoch:  7 ; loss:  -0.4661121070384979 ; pred:  tensor([0.3291, 0.2176, 0.2478, 0.2055], grad_fn=<SoftmaxBackward0>)
masked_adj tensor([0.9768, 0.9733, 0.9609, 0.4454, 0.7802, 0.2034, 0.4201, 0.9750, 0.2559,
        0.2303, 0.2480, 0.2702, 0.2070, 0.2325, 0.4890, 0.9596, 0.2415, 0.2365,
        0.9579, 0.2424, 0.2477, 0.3643, 0.2498], grad_fn=<MulBackward0>)
res: tensor([0.3301, 0.2142, 0.2477, 0.2080], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.9768, 0.9733, 0.9609, 0.7802, 0.9750, 0.9596, 0.9579],
       grad_fn=<IndexBackward0>)
num_high 7 len(mask) 23
pred_loss tensor([1.1085], grad_fn=<MulBackward0>)
size_loss tensor(-5.0620, grad_fn=<MulBackward0>)
size_num_loss 0.3043521739130435
loss: tensor([-3.1846], grad_fn=<AddBackward0>)
8
epoch:  8 ; loss:  -3.184563636779785 ; pred:  tensor([0.3301, 0.2142, 0.2477, 0.2080], grad_fn=<SoftmaxBackward0>)
masked_adj tensor([0.9821, 0.9792, 0.9677, 0.4861, 0.7109, 0.1922, 0.4638, 0.9806, 0.2175,
        0.2220, 0.2439, 0.2385, 0.1954, 0.2110, 0.5239, 0.9664, 0.2394, 0.2315,
        0.9646, 0.2355, 0.2417, 0.3989, 0.2456], grad_fn=<MulBackward0>)
res: tensor([0.3335, 0.2099, 0.2470, 0.2096], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.9821, 0.9792, 0.9677, 0.7109, 0.9806, 0.5239, 0.9664, 0.9646],
       grad_fn=<IndexBackward0>)
num_high 8 len(mask) 23
pred_loss tensor([1.0982], grad_fn=<MulBackward0>)
size_loss tensor(-29.7074, grad_fn=<MulBackward0>)
size_num_loss 0.34783043478260867
loss: tensor([-27.8056], grad_fn=<AddBackward0>)
9
epoch:  9 ; loss:  -27.805591583251953 ; pred:  tensor([0.3335, 0.2099, 0.2470, 0.2096], grad_fn=<SoftmaxBackward0>)
Finished Training
sel masked ver tensor(indices=tensor([[6843, 7452, 7477, 7479, 5502, 5778, 5778, 5502],
                       [5778, 5778, 5778, 5778, 5778, 7478, 7479, 7477]]),
       values=tensor([0.9821, 0.9792, 0.9677, 0.7109, 0.9806, 0.5239, 0.9664,
                      0.9646]),
       size=(7480, 7480), nnz=8, layout=torch.sparse_coo,
       grad_fn=<SparseCooTensorWithDimsAndTensorsBackward0>)
Counter({'author': 4, 'publication': 2, 'member': 1, 'publishes': 1})
dict index: {}
node_idx 5778
 node original label [0]
 node predicted label explain 0
 node prediction probability explain tensor([0.3335, 0.2099, 0.2470, 0.2096], grad_fn=<SoftmaxBackward0>)
 node predicted label full 0 most important relations  {'member': 1, 'publishes': 1, 'publication': 2, 'author': 4, 'label': 0, 'node_idx': '5778'}
 final masks and lenght tensor(indices=tensor([[ 23413,  24022,  24047,  24048,  24049,  88628, 130053,
                        179487, 196333, 229473, 237758, 254328, 254328, 254328,
                        254328, 254328, 262337, 262337, 262337, 262337, 262337,
                        328893, 328893],
                       [  5778,   5778,   5778,   5778,   5778,      0,   5181,
                          5778,   2965,     62,   5579,   6843,   7452,   7477,
                          7478,   7479,   6843,   7452,   7477,   7478,   7479,
                          5230,   5231]]),
       values=tensor([0.9821, 0.9792, 0.9677, 0.4861, 0.7109, 0.1922, 0.4638,
                      0.9806, 0.2175, 0.2220, 0.2439, 0.2385, 0.1954, 0.2110,
                      0.5239, 0.9664, 0.2394, 0.2315, 0.9646, 0.2355, 0.2417,
                      0.3989, 0.2456]),
       size=(753935, 8285), nnz=23, layout=torch.sparse_coo,
       grad_fn=<SparseCooTensorWithDimsAndTensorsBackward0>) 8 ---------------------------------------------------------------
wandb: WARNING Calling wandb.login() after wandb.init() has no effect.
node label: 1
30
masked_adj tensor([0.7940, 0.7806, 0.7619, 0.6499, 0.7545, 0.6849, 0.7295, 0.6703, 0.7035,
        0.7856, 0.7169, 0.6783, 0.7044, 0.7107, 0.7028, 0.7573, 0.7854, 0.7253,
        0.7130, 0.7464, 0.7032, 0.7677, 0.7586, 0.7866, 0.7741, 0.7746, 0.7522,
        0.7759, 0.7227, 0.7325, 0.7220, 0.7605, 0.6790, 0.6989, 0.7230, 0.7877,
        0.7423, 0.7157, 0.7418, 0.7026, 0.6722, 0.7650, 0.6986, 0.7091, 0.6834,
        0.7373, 0.7774, 0.7149, 0.7793, 0.7604, 0.8024, 0.7493, 0.7432, 0.7506,
        0.7146, 0.7027, 0.7249, 0.7493, 0.7331, 0.7460, 0.7510],
       grad_fn=<MulBackward0>)
res: tensor([0.0061, 0.9767, 0.0078, 0.0094], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.7940, 0.7806, 0.7619, 0.6499, 0.7545, 0.6849, 0.7295, 0.6703, 0.7035,
        0.7856, 0.7169, 0.6783, 0.7044, 0.7107, 0.7028, 0.7573, 0.7854, 0.7253,
        0.7130, 0.7464, 0.7032, 0.7677, 0.7586, 0.7866, 0.7741, 0.7746, 0.7522,
        0.7759, 0.7227, 0.7325, 0.7220, 0.7605, 0.6790, 0.6989, 0.7230, 0.7877,
        0.7423, 0.7157, 0.7418, 0.7026, 0.6722, 0.7650, 0.6986, 0.7091, 0.6834,
        0.7373, 0.7774, 0.7149, 0.7793, 0.7604, 0.8024, 0.7493, 0.7432, 0.7506,
        0.7146, 0.7027, 0.7249, 0.7493, 0.7331, 0.7460, 0.7510],
       grad_fn=<IndexBackward0>)
num_high 61 len(mask) 61
pred_loss tensor([0.0236], grad_fn=<MulBackward0>)
size_loss tensor(22.4082, grad_fn=<AddBackward0>)
size_num_loss 1.0000016393442623
loss: tensor([24.0067], grad_fn=<AddBackward0>)
0
epoch:  0 ; loss:  24.006702423095703 ; pred:  tensor([0.0061, 0.9767, 0.0078, 0.0094], grad_fn=<SoftmaxBackward0>)
masked_adj tensor([0.7003, 0.6834, 0.6600, 0.5297, 0.6509, 0.5687, 0.6206, 0.5522, 0.5900,
        0.6897, 0.6056, 0.5612, 0.5910, 0.5984, 0.5892, 0.6543, 0.6894, 0.6156,
        0.6011, 0.6410, 0.5897, 0.6671, 0.6559, 0.6909, 0.6752, 0.6758, 0.6481,
        0.6774, 0.6126, 0.6242, 0.6117, 0.6583, 0.5620, 0.5847, 0.6129, 0.6923,
        0.6359, 0.6042, 0.6354, 0.5890, 0.5543, 0.6638, 0.5844, 0.5966, 0.5669,
        0.6299, 0.6794, 0.6033, 0.6817, 0.6582, 0.7113, 0.6444, 0.6371, 0.6460,
        0.6030, 0.5891, 0.6151, 0.6445, 0.6249, 0.6404, 0.6466],
       grad_fn=<MulBackward0>)
res: tensor([0.0210, 0.9227, 0.0270, 0.0293], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.7003, 0.6834, 0.6600, 0.5297, 0.6509, 0.5687, 0.6206, 0.5522, 0.5900,
        0.6897, 0.6056, 0.5612, 0.5910, 0.5984, 0.5892, 0.6543, 0.6894, 0.6156,
        0.6011, 0.6410, 0.5897, 0.6671, 0.6559, 0.6909, 0.6752, 0.6758, 0.6481,
        0.6774, 0.6126, 0.6242, 0.6117, 0.6583, 0.5620, 0.5847, 0.6129, 0.6923,
        0.6359, 0.6042, 0.6354, 0.5890, 0.5543, 0.6638, 0.5844, 0.5966, 0.5669,
        0.6299, 0.6794, 0.6033, 0.6817, 0.6582, 0.7113, 0.6444, 0.6371, 0.6460,
        0.6030, 0.5891, 0.6151, 0.6445, 0.6249, 0.6404, 0.6466],
       grad_fn=<IndexBackward0>)
num_high 61 len(mask) 61
pred_loss tensor([0.0804], grad_fn=<MulBackward0>)
size_loss tensor(19.1400, grad_fn=<AddBackward0>)
size_num_loss 1.0000016393442623
loss: tensor([20.8765], grad_fn=<AddBackward0>)
1
epoch:  1 ; loss:  20.876487731933594 ; pred:  tensor([0.0210, 0.9227, 0.0270, 0.0293], grad_fn=<SoftmaxBackward0>)
masked_adj tensor([0.5871, 0.5677, 0.5415, 0.4072, 0.5315, 0.4455, 0.4990, 0.4291, 0.4671,
        0.5749, 0.4832, 0.4380, 0.4682, 0.4757, 0.4660, 0.5353, 0.5746, 0.4937,
        0.4785, 0.5208, 0.4668, 0.5495, 0.5370, 0.5763, 0.5584, 0.5592, 0.5285,
        0.5609, 0.4905, 0.5028, 0.4896, 0.5397, 0.4391, 0.4619, 0.4910, 0.5780,
        0.5154, 0.4820, 0.5148, 0.4663, 0.4313, 0.5459, 0.4615, 0.4742, 0.4441,
        0.5088, 0.5632, 0.4813, 0.5662, 0.5399, 0.6001, 0.5248, 0.5173, 0.5268,
        0.4810, 0.4666, 0.4937, 0.5249, 0.5039, 0.5208, 0.5270],
       grad_fn=<MulBackward0>)
res: tensor([0.0614, 0.7845, 0.0772, 0.0769], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.5871, 0.5677, 0.5415, 0.5315, 0.5749, 0.5353, 0.5746, 0.5208, 0.5495,
        0.5370, 0.5763, 0.5584, 0.5592, 0.5285, 0.5609, 0.5028, 0.5397, 0.5780,
        0.5154, 0.5148, 0.5459, 0.5088, 0.5632, 0.5662, 0.5399, 0.6001, 0.5248,
        0.5173, 0.5268, 0.5249, 0.5039, 0.5208, 0.5270],
       grad_fn=<IndexBackward0>)
num_high 33 len(mask) 61
pred_loss tensor([0.2427], grad_fn=<MulBackward0>)
size_loss tensor(-0.6603, grad_fn=<MulBackward0>)
size_num_loss 0.5409852459016394
loss: tensor([0.8124], grad_fn=<AddBackward0>)
2
epoch:  2 ; loss:  0.8124008178710938 ; pred:  tensor([0.0614, 0.7845, 0.0772, 0.0769], grad_fn=<SoftmaxBackward0>)
masked_adj tensor([0.6202, 0.5718, 0.4349, 0.3311, 0.4080, 0.3606, 0.4044, 0.3478, 0.3779,
        0.5932, 0.3912, 0.3547, 0.3792, 0.3850, 0.3703, 0.4144, 0.5924, 0.4000,
        0.3873, 0.4010, 0.3777, 0.4801, 0.4187, 0.5968, 0.5328, 0.5367, 0.4050,
        0.5448, 0.3973, 0.3943, 0.3965, 0.4280, 0.3595, 0.3776, 0.4010, 0.6026,
        0.3992, 0.3941, 0.3990, 0.3811, 0.3517, 0.4625, 0.3772, 0.3900, 0.3658,
        0.3969, 0.5552, 0.3988, 0.5726, 0.4341, 0.6433, 0.4025, 0.3994, 0.4037,
        0.3996, 0.3858, 0.4106, 0.4025, 0.3948, 0.4005, 0.4038],
       grad_fn=<MulBackward0>)
res: tensor([0.1039, 0.6424, 0.1311, 0.1226], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.6202, 0.5718, 0.5932, 0.5924, 0.5968, 0.5328, 0.5367, 0.5448, 0.6026,
        0.5552, 0.5726, 0.6433], grad_fn=<IndexBackward0>)
num_high 12 len(mask) 61
pred_loss tensor([0.4425], grad_fn=<MulBackward0>)
size_loss tensor(-1.1659, grad_fn=<MulBackward0>)
size_num_loss 0.19672295081967212
loss: tensor([0.1444], grad_fn=<AddBackward0>)
3
epoch:  3 ; loss:  0.14440935850143433 ; pred:  tensor([0.1039, 0.6424, 0.1311, 0.1226], grad_fn=<SoftmaxBackward0>)
masked_adj tensor([0.6904, 0.5259, 0.3580, 0.2932, 0.3216, 0.3120, 0.3417, 0.3037, 0.3235,
        0.6517, 0.3325, 0.3082, 0.3252, 0.3283, 0.3028, 0.3296, 0.6497, 0.3386,
        0.3298, 0.3162, 0.3233, 0.4258, 0.3353, 0.6599, 0.4579, 0.4620, 0.3184,
        0.4711, 0.3367, 0.3161, 0.3362, 0.3492, 0.3187, 0.3308, 0.3460, 0.6699,
        0.3173, 0.3435, 0.3178, 0.3333, 0.3105, 0.4054, 0.3303, 0.3448, 0.3287,
        0.3158, 0.4846, 0.3576, 0.5336, 0.3689, 0.7137, 0.3212, 0.3260, 0.3287,
        0.3588, 0.3457, 0.3682, 0.3219, 0.3202, 0.3252, 0.3190],
       grad_fn=<MulBackward0>)
res: tensor([0.1294, 0.5574, 0.1644, 0.1488], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.6904, 0.5259, 0.6517, 0.6497, 0.6599, 0.6699, 0.5336, 0.7137],
       grad_fn=<IndexBackward0>)
num_high 8 len(mask) 61
pred_loss tensor([0.5844], grad_fn=<MulBackward0>)
size_loss tensor(-4.8219, grad_fn=<MulBackward0>)
size_num_loss 0.13114918032786885
loss: tensor([-3.4663], grad_fn=<AddBackward0>)
4
epoch:  4 ; loss:  -3.466279983520508 ; pred:  tensor([0.1294, 0.5574, 0.1644, 0.1488], grad_fn=<SoftmaxBackward0>)
masked_adj tensor([0.7630, 0.4551, 0.3111, 0.2828, 0.2696, 0.2917, 0.3075, 0.2876, 0.2976,
        0.7254, 0.3025, 0.2898, 0.2999, 0.3002, 0.2608, 0.2790, 0.7227, 0.3058,
        0.3010, 0.2622, 0.2976, 0.3879, 0.2855, 0.7340, 0.3964, 0.4006, 0.2654,
        0.4102, 0.3048, 0.2631, 0.3045, 0.3026, 0.3065, 0.3130, 0.3202, 0.7437,
        0.2648, 0.3233, 0.2659, 0.3147, 0.2977, 0.3753, 0.3123, 0.3298, 0.3220,
        0.2607, 0.4258, 0.3488, 0.4632, 0.3388, 0.7840, 0.2744, 0.2873, 0.2923,
        0.3488, 0.3362, 0.3572, 0.2762, 0.2732, 0.2852, 0.2679],
       grad_fn=<MulBackward0>)
res: tensor([0.1396, 0.5219, 0.1791, 0.1594], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.7630, 0.7254, 0.7227, 0.7340, 0.7437, 0.7840],
       grad_fn=<IndexBackward0>)
num_high 6 len(mask) 61
pred_loss tensor([0.6502], grad_fn=<MulBackward0>)
size_loss tensor(-0.5696, grad_fn=<MulBackward0>)
size_num_loss 0.09836229508196721
loss: tensor([0.7914], grad_fn=<AddBackward0>)
5
epoch:  5 ; loss:  0.7913675308227539 ; pred:  tensor([0.1396, 0.5219, 0.1791, 0.1594], grad_fn=<SoftmaxBackward0>)
masked_adj tensor([0.8248, 0.3955, 0.2890, 0.2920, 0.2437, 0.2923, 0.2960, 0.2920, 0.2934,
        0.7164, 0.2945, 0.2921, 0.2962, 0.2940, 0.2397, 0.2550, 0.7052, 0.2955,
        0.2942, 0.2316, 0.2934, 0.3660, 0.2623, 0.7581, 0.3469, 0.3511, 0.2381,
        0.3610, 0.2952, 0.2293, 0.2951, 0.2817, 0.3147, 0.3163, 0.3164, 0.7935,
        0.2344, 0.3251, 0.2360, 0.3176, 0.3052, 0.3682, 0.3153, 0.3368, 0.3369,
        0.2258, 0.3782, 0.3636, 0.4040, 0.3345, 0.8448, 0.2527, 0.2727, 0.2827,
        0.3608, 0.3489, 0.3689, 0.2554, 0.2463, 0.2698, 0.2420],
       grad_fn=<MulBackward0>)
res: tensor([0.1388, 0.5201, 0.1813, 0.1598], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.8248, 0.7164, 0.7052, 0.7581, 0.7935, 0.8448],
       grad_fn=<IndexBackward0>)
num_high 6 len(mask) 61
pred_loss tensor([0.6537], grad_fn=<MulBackward0>)
size_loss tensor(-3.2559, grad_fn=<MulBackward0>)
size_num_loss 0.09836229508196721
loss: tensor([-1.9053], grad_fn=<AddBackward0>)
6
epoch:  6 ; loss:  -1.9052684307098389 ; pred:  tensor([0.1388, 0.5201, 0.1813, 0.1598], grad_fn=<SoftmaxBackward0>)
masked_adj tensor([0.8772, 0.3461, 0.2860, 0.3160, 0.2370, 0.3087, 0.3017, 0.3116, 0.3055,
        0.6634, 0.3034, 0.3100, 0.3089, 0.3044, 0.2353, 0.2506, 0.6477, 0.3023,
        0.3040, 0.2181, 0.3055, 0.3582, 0.2586, 0.7496, 0.3077, 0.3119, 0.2293,
        0.3221, 0.3026, 0.2096, 0.3027, 0.2801, 0.3381, 0.3354, 0.3292, 0.8428,
        0.2202, 0.3432, 0.2220, 0.3367, 0.3281, 0.3795, 0.3341, 0.3601, 0.3679,
        0.2059, 0.3409, 0.3958, 0.3550, 0.3495, 0.8937, 0.2491, 0.2754, 0.2925,
        0.3891, 0.3781, 0.3975, 0.2525, 0.2342, 0.2721, 0.2343],
       grad_fn=<MulBackward0>)
res: tensor([0.1303, 0.5428, 0.1740, 0.1529], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.8772, 0.6634, 0.6477, 0.7496, 0.8428, 0.8937],
       grad_fn=<IndexBackward0>)
num_high 6 len(mask) 61
pred_loss tensor([0.6110], grad_fn=<MulBackward0>)
size_loss tensor(-11.6646, grad_fn=<MulBackward0>)
size_num_loss 0.09836229508196721
loss: tensor([-10.3587], grad_fn=<AddBackward0>)
7
epoch:  7 ; loss:  -10.358667373657227 ; pred:  tensor([0.1303, 0.5428, 0.1740, 0.1529], grad_fn=<SoftmaxBackward0>)
masked_adj tensor([0.9174, 0.3056, 0.2980, 0.3514, 0.2447, 0.3375, 0.3210, 0.3432, 0.3304,
        0.5889, 0.3255, 0.3400, 0.3344, 0.3278, 0.2443, 0.2610, 0.5702, 0.3225,
        0.3269, 0.2176, 0.3305, 0.3624, 0.2700, 0.7094, 0.2771, 0.2814, 0.2345,
        0.2920, 0.3234, 0.2007, 0.3237, 0.2935, 0.3730, 0.3667, 0.3550, 0.8866,
        0.2183, 0.3733, 0.2201, 0.3682, 0.3629, 0.4050, 0.3651, 0.3957, 0.4107,
        0.1973, 0.3124, 0.4410, 0.3148, 0.3792, 0.9299, 0.2593, 0.2916, 0.3175,
        0.4296, 0.4200, 0.4387, 0.2633, 0.2333, 0.2883, 0.2405],
       grad_fn=<MulBackward0>)
res: tensor([0.1161, 0.5849, 0.1591, 0.1399], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.9174, 0.5889, 0.5702, 0.7094, 0.8866, 0.9299],
       grad_fn=<IndexBackward0>)
num_high 6 len(mask) 61
pred_loss tensor([0.5363], grad_fn=<MulBackward0>)
size_loss tensor(-27.4485, grad_fn=<MulBackward0>)
size_num_loss 0.09836229508196721
loss: tensor([-26.2115], grad_fn=<AddBackward0>)
8
epoch:  8 ; loss:  -26.211477279663086 ; pred:  tensor([0.1161, 0.5849, 0.1591, 0.1399], grad_fn=<SoftmaxBackward0>)
masked_adj tensor([0.9463, 0.2724, 0.3221, 0.3951, 0.2643, 0.3756, 0.3511, 0.3837, 0.3654,
        0.4996, 0.3580, 0.3792, 0.3698, 0.3614, 0.2644, 0.2836, 0.4790, 0.3534,
        0.3601, 0.2275, 0.3655, 0.3764, 0.2936, 0.6430, 0.2535, 0.2580, 0.2513,
        0.2692, 0.3548, 0.2003, 0.3552, 0.3190, 0.4159, 0.4071, 0.3907, 0.9214,
        0.2263, 0.4120, 0.2279, 0.4091, 0.4062, 0.4410, 0.4049, 0.4398, 0.4612,
        0.1978, 0.2916, 0.4941, 0.2820, 0.4202, 0.9550, 0.2810, 0.3192, 0.3551,
        0.4780, 0.4704, 0.4880, 0.2855, 0.2416, 0.3160, 0.2581],
       grad_fn=<MulBackward0>)
res: tensor([0.0980, 0.6421, 0.1379, 0.1220], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.9463, 0.6430, 0.9214, 0.9550], grad_fn=<IndexBackward0>)
num_high 4 len(mask) 61
pred_loss tensor([0.4430], grad_fn=<MulBackward0>)
size_loss tensor(-22.3909, grad_fn=<MulBackward0>)
size_num_loss 0.06557540983606557
loss: tensor([-21.2708], grad_fn=<AddBackward0>)
9
epoch:  9 ; loss:  -21.270776748657227 ; pred:  tensor([0.0980, 0.6421, 0.1379, 0.1220], grad_fn=<SoftmaxBackward0>)
Finished Training
sel masked ver tensor(indices=tensor([[6842, 7466, 5429, 5731],
                       [5731, 5457, 5731, 7361]]),
       values=tensor([0.9463, 0.6430, 0.9214, 0.9550]),
       size=(7467, 7362), nnz=4, layout=torch.sparse_coo,
       grad_fn=<SparseCooTensorWithDimsAndTensorsBackward0>)
Counter({'isWorkedOnBy': 1, 'publication': 1, 'author': 1, 'isAbout': 1})
dict index: {}
node_idx 5731
 node original label [1]
 node predicted label explain 1
 node prediction probability explain tensor([0.0980, 0.6421, 0.1379, 0.1220], grad_fn=<SoftmaxBackward0>)
 node predicted label full 1 most important relations  {'isWorkedOnBy': 1, 'publication': 1, 'author': 1, 'isAbout': 1, 'label': 1, 'node_idx': '5731'}
 final masks and lenght tensor(indices=tensor([[ 23412,  23422,  23466,  23931,  24007,  24036,  24126,
                         24444,  24449,  24451,  24472,  63450,  88581, 115261,
                        130006, 146769, 147687, 147697, 147741, 148311, 148311,
                        148311, 148311, 148311, 148401, 148719, 148724, 148726,
                        148726, 148747, 154542, 154550, 154551, 154552, 154553,
                        154559, 154567, 154581, 154585, 154587, 154594, 179909,
                        179976, 196286, 229426, 237711, 246256, 254281, 254281,
                        254281, 254281, 254281, 254281, 254281, 254281, 254281,
                        254281, 254281, 303991, 303991, 328846],
                       [  5731,   5731,   5731,   5731,   5731,   5731,   5731,
                          5731,   5731,   5731,   5731,   5924,      0,   5991,
                          5184,   5455,   5451,   5451,   5451,   5412,   5423,
                          5437,   5451,   5457,   5451,   5451,   5451,   5423,
                          5451,   5451,   5731,   5731,   5731,   5731,   5731,
                          5731,   5731,   5731,   5731,   5731,   5731,   5731,
                          5731,   3023,    108,   5543,   7556,   6842,   6852,
                          6896,   7361,   7437,   7466,   7556,   7874,   7879,
                          7881,   7902,   5924,   5991,   5230]]),
       values=tensor([0.9463, 0.2724, 0.3221, 0.3951, 0.2643, 0.3756, 0.3511,
                      0.3837, 0.3654, 0.4996, 0.3580, 0.3792, 0.3698, 0.3614,
                      0.2644, 0.2836, 0.4790, 0.3534, 0.3601, 0.2275, 0.3655,
                      0.3764, 0.2936, 0.6430, 0.2535, 0.2580, 0.2513, 0.2692,
                      0.3548, 0.2003, 0.3552, 0.3190, 0.4159, 0.4071, 0.3907,
                      0.9214, 0.2263, 0.4120, 0.2279, 0.4091, 0.4062, 0.4410,
                      0.4049, 0.4398, 0.4612, 0.1978, 0.2916, 0.4941, 0.2820,
                      0.4202, 0.9550, 0.2810, 0.3192, 0.3551, 0.4780, 0.4704,
                      0.4880, 0.2855, 0.2416, 0.3160, 0.2581]),
       size=(753935, 8285), nnz=61, layout=torch.sparse_coo,
       grad_fn=<SparseCooTensorWithDimsAndTensorsBackward0>) 4 ---------------------------------------------------------------
wandb: WARNING Calling wandb.login() after wandb.init() has no effect.
node label: 1
6
masked_adj tensor([0.7649, 0.7444, 0.7550, 0.7546, 0.5986, 0.7110, 0.8985],
       grad_fn=<MulBackward0>)
res: tensor([0.2758, 0.3275, 0.2286, 0.1681], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.7649, 0.7444, 0.7550, 0.7546, 0.5986, 0.7110, 0.8985],
       grad_fn=<IndexBackward0>)
num_high 7 len(mask) 7
pred_loss tensor([1.1161], grad_fn=<MulBackward0>)
size_loss tensor(2.5357, grad_fn=<AddBackward0>)
size_num_loss 1.0000142857142857
loss: tensor([5.1991], grad_fn=<AddBackward0>)
0
epoch:  0 ; loss:  5.1991167068481445 ; pred:  tensor([0.2758, 0.3275, 0.2286, 0.1681], grad_fn=<SoftmaxBackward0>)
masked_adj tensor([0.6637, 0.6385, 0.6514, 0.6509, 0.4750, 0.5988, 0.8429],
       grad_fn=<MulBackward0>)
res: tensor([0.2789, 0.3114, 0.2323, 0.1773], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.6637, 0.6385, 0.6514, 0.6509, 0.5988, 0.8429],
       grad_fn=<IndexBackward0>)
num_high 6 len(mask) 7
pred_loss tensor([1.1666], grad_fn=<MulBackward0>)
size_loss tensor(-7.3215, grad_fn=<MulBackward0>)
size_num_loss 0.8571571428571428
loss: tensor([-4.6712], grad_fn=<AddBackward0>)
1
epoch:  1 ; loss:  -4.671199798583984 ; pred:  tensor([0.2789, 0.3114, 0.2323, 0.1773], grad_fn=<SoftmaxBackward0>)
masked_adj tensor([0.5641, 0.5448, 0.5572, 0.5572, 0.3882, 0.5055, 0.8855],
       grad_fn=<MulBackward0>)
res: tensor([0.2813, 0.3013, 0.2309, 0.1865], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.5641, 0.5448, 0.5572, 0.5572, 0.5055, 0.8855],
       grad_fn=<IndexBackward0>)
num_high 6 len(mask) 7
pred_loss tensor([1.1996], grad_fn=<MulBackward0>)
size_loss tensor(-19.6834, grad_fn=<MulBackward0>)
size_num_loss 0.8571571428571428
loss: tensor([-16.9889], grad_fn=<AddBackward0>)
2
epoch:  2 ; loss:  -16.98893165588379 ; pred:  tensor([0.2813, 0.3013, 0.2309, 0.1865], grad_fn=<SoftmaxBackward0>)
masked_adj tensor([0.4663, 0.4384, 0.4527, 0.4526, 0.3277, 0.3991, 0.9221],
       grad_fn=<MulBackward0>)
res: tensor([0.2849, 0.2880, 0.2298, 0.1973], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.9221], grad_fn=<IndexBackward0>)
num_high 1 len(mask) 7
pred_loss tensor([1.2447], grad_fn=<MulBackward0>)
size_loss tensor(nan, grad_fn=<MulBackward0>)
size_num_loss 0.14287142857142857
loss: tensor([1.2447], grad_fn=<MulBackward0>)
3
epoch:  3 ; loss:  1.2446669340133667 ; pred:  tensor([0.2849, 0.2880, 0.2298, 0.1973], grad_fn=<SoftmaxBackward0>)
masked_adj tensor([0.3880, 0.3550, 0.3703, 0.3704, 0.2933, 0.3187, 0.9435],
       grad_fn=<MulBackward0>)
res: tensor([0.2878, 0.2771, 0.2289, 0.2061], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.9435], grad_fn=<IndexBackward0>)
num_high 1 len(mask) 7
pred_loss tensor([1.2833], grad_fn=<MulBackward0>)
size_loss tensor(nan, grad_fn=<MulBackward0>)
size_num_loss 0.14287142857142857
loss: tensor([1.2833], grad_fn=<MulBackward0>)
4
epoch:  4 ; loss:  1.2832655906677246 ; pred:  tensor([0.2878, 0.2771, 0.2289, 0.2061], grad_fn=<SoftmaxBackward0>)
masked_adj tensor([0.3266, 0.2912, 0.3068, 0.3072, 0.2792, 0.2587, 0.9569],
       grad_fn=<MulBackward0>)
res: tensor([0.2902, 0.2685, 0.2283, 0.2130], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.9569], grad_fn=<IndexBackward0>)
num_high 1 len(mask) 7
pred_loss tensor([1.3149], grad_fn=<MulBackward0>)
size_loss tensor(nan, grad_fn=<MulBackward0>)
size_num_loss 0.14287142857142857
loss: tensor([1.3149], grad_fn=<MulBackward0>)
5
epoch:  5 ; loss:  1.3149335384368896 ; pred:  tensor([0.2902, 0.2685, 0.2283, 0.2130], grad_fn=<SoftmaxBackward0>)
masked_adj tensor([0.2789, 0.2426, 0.2583, 0.2591, 0.2806, 0.2138, 0.9658],
       grad_fn=<MulBackward0>)
res: tensor([0.2923, 0.2617, 0.2277, 0.2184], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.9658], grad_fn=<IndexBackward0>)
num_high 1 len(mask) 7
pred_loss tensor([1.3406], grad_fn=<MulBackward0>)
size_loss tensor(nan, grad_fn=<MulBackward0>)
size_num_loss 0.14287142857142857
loss: tensor([1.3406], grad_fn=<MulBackward0>)
6
epoch:  6 ; loss:  1.3406391143798828 ; pred:  tensor([0.2923, 0.2617, 0.2277, 0.2184], grad_fn=<SoftmaxBackward0>)
masked_adj tensor([0.2421, 0.2057, 0.2215, 0.2225, 0.2940, 0.1801, 0.9719],
       grad_fn=<MulBackward0>)
res: tensor([0.2941, 0.2562, 0.2272, 0.2225], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.9719], grad_fn=<IndexBackward0>)
num_high 1 len(mask) 7
pred_loss tensor([1.3616], grad_fn=<MulBackward0>)
size_loss tensor(nan, grad_fn=<MulBackward0>)
size_num_loss 0.14287142857142857
loss: tensor([1.3616], grad_fn=<MulBackward0>)
7
epoch:  7 ; loss:  1.3616410493850708 ; pred:  tensor([0.2941, 0.2562, 0.2272, 0.2225], grad_fn=<SoftmaxBackward0>)
masked_adj tensor([0.2138, 0.1775, 0.1934, 0.1947, 0.3171, 0.1544, 0.9763],
       grad_fn=<MulBackward0>)
res: tensor([0.2958, 0.2518, 0.2267, 0.2258], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.9763], grad_fn=<IndexBackward0>)
num_high 1 len(mask) 7
pred_loss tensor([1.3791], grad_fn=<MulBackward0>)
size_loss tensor(nan, grad_fn=<MulBackward0>)
size_num_loss 0.14287142857142857
loss: tensor([1.3791], grad_fn=<MulBackward0>)
8
epoch:  8 ; loss:  1.3790605068206787 ; pred:  tensor([0.2958, 0.2518, 0.2267, 0.2258], grad_fn=<SoftmaxBackward0>)
masked_adj tensor([0.1920, 0.1559, 0.1719, 0.1735, 0.3475, 0.1347, 0.9795],
       grad_fn=<MulBackward0>)
res: tensor([0.2974, 0.2481, 0.2262, 0.2283], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.9795], grad_fn=<IndexBackward0>)
num_high 1 len(mask) 7
pred_loss tensor([1.3937], grad_fn=<MulBackward0>)
size_loss tensor(nan, grad_fn=<MulBackward0>)
size_num_loss 0.14287142857142857
loss: tensor([1.3937], grad_fn=<MulBackward0>)
9
epoch:  9 ; loss:  1.3937463760375977 ; pred:  tensor([0.2974, 0.2481, 0.2262, 0.2283], grad_fn=<SoftmaxBackward0>)
Finished Training
sel masked ver tensor(indices=tensor([[5905],
                       [5230]]),
       values=tensor([0.9795]),
       size=(5906, 5231), nnz=1, layout=torch.sparse_coo,
       grad_fn=<SparseCooTensorWithDimsAndTensorsBackward0>)
Counter({'type': 1})
dict index: {}
node_idx 5905
 node original label [1]
 node predicted label explain 0
 node prediction probability explain tensor([0.2974, 0.2481, 0.2262, 0.2283], grad_fn=<SoftmaxBackward0>)
 node predicted label full 1 most important relations  {'type': 1, 'label': 1, 'node_idx': '5905'}
 final masks and lenght tensor(indices=tensor([[ 24263,  88755, 196460, 229600, 237885, 254455, 329020],
                       [  5905,     27,   2259,     28,   5670,   7693,   5230]]),
       values=tensor([0.1920, 0.1559, 0.1719, 0.1735, 0.3475, 0.1347, 0.9795]),
       size=(753935, 8285), nnz=7, layout=torch.sparse_coo,
       grad_fn=<SparseCooTensorWithDimsAndTensorsBackward0>) 1 ---------------------------------------------------------------
wandb: WARNING Calling wandb.login() after wandb.init() has no effect.
node label: 1
13
masked_adj tensor([0.8160, 0.7986, 0.7736, 0.6142, 0.7636, 0.6652, 0.7289, 0.6439, 0.6919,
        0.8051, 0.7110, 0.6555, 0.6932, 0.7022, 0.6910, 0.7201, 0.6566, 0.6854,
        0.7198, 0.8079, 0.7467, 0.7093, 0.7460, 0.7473, 0.6467, 0.7778, 0.6849,
        0.7000, 0.6629, 0.8233, 0.6652], grad_fn=<MulBackward0>)
res: tensor([0.2795, 0.4711, 0.1444, 0.1050], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.8160, 0.7986, 0.7736, 0.6142, 0.7636, 0.6652, 0.7289, 0.6439, 0.6919,
        0.8051, 0.7110, 0.6555, 0.6932, 0.7022, 0.6910, 0.7201, 0.6566, 0.6854,
        0.7198, 0.8079, 0.7467, 0.7093, 0.7460, 0.7473, 0.6467, 0.7778, 0.6849,
        0.7000, 0.6629, 0.8233, 0.6652], grad_fn=<IndexBackward0>)
num_high 31 len(mask) 31
pred_loss tensor([0.7527], grad_fn=<MulBackward0>)
size_loss tensor(11.0947, grad_fn=<AddBackward0>)
size_num_loss 1.0000032258064515
loss: tensor([13.4345], grad_fn=<AddBackward0>)
0
epoch:  0 ; loss:  13.43453598022461 ; pred:  tensor([0.2795, 0.4711, 0.1444, 0.1050], grad_fn=<SoftmaxBackward0>)
masked_adj tensor([0.7290, 0.7064, 0.6745, 0.4913, 0.6620, 0.5465, 0.6199, 0.5231, 0.5766,
        0.7148, 0.5988, 0.5358, 0.5781, 0.5885, 0.5756, 0.6094, 0.5370, 0.5692,
        0.6090, 0.7183, 0.6413, 0.5968, 0.6405, 0.6420, 0.5261, 0.6798, 0.5687,
        0.5860, 0.5440, 0.7387, 0.5465], grad_fn=<MulBackward0>)
res: tensor([0.2955, 0.3979, 0.1757, 0.1310], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.7290, 0.7064, 0.6745, 0.6620, 0.5465, 0.6199, 0.5231, 0.5766, 0.7148,
        0.5988, 0.5358, 0.5781, 0.5885, 0.5756, 0.6094, 0.5370, 0.5692, 0.6090,
        0.7183, 0.6413, 0.5968, 0.6405, 0.6420, 0.5261, 0.6798, 0.5687, 0.5860,
        0.5440, 0.7387, 0.5465], grad_fn=<IndexBackward0>)
num_high 30 len(mask) 31
pred_loss tensor([0.9216], grad_fn=<MulBackward0>)
size_loss tensor(-4.2466, grad_fn=<MulBackward0>)
size_num_loss 0.9677451612903226
loss: tensor([-1.6975], grad_fn=<AddBackward0>)
1
epoch:  1 ; loss:  -1.6974821090698242 ; pred:  tensor([0.2955, 0.3979, 0.1757, 0.1310], grad_fn=<SoftmaxBackward0>)
masked_adj tensor([0.7878, 0.7671, 0.7345, 0.4102, 0.7192, 0.4419, 0.5746, 0.4217, 0.4646,
        0.7726, 0.4759, 0.4328, 0.4655, 0.4711, 0.4639, 0.4945, 0.4338, 0.4604,
        0.5031, 0.7777, 0.6871, 0.4748, 0.6890, 0.7040, 0.4258, 0.7427, 0.4611,
        0.4690, 0.4461, 0.7972, 0.4410], grad_fn=<MulBackward0>)
res: tensor([0.2994, 0.3702, 0.1877, 0.1427], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.7878, 0.7671, 0.7345, 0.7192, 0.5746, 0.7726, 0.5031, 0.7777, 0.6871,
        0.6890, 0.7040, 0.7427, 0.7972], grad_fn=<IndexBackward0>)
num_high 13 len(mask) 31
pred_loss tensor([0.9938], grad_fn=<MulBackward0>)
size_loss tensor(-7.4365, grad_fn=<MulBackward0>)
size_num_loss 0.419358064516129
loss: tensor([-5.3789], grad_fn=<AddBackward0>)
2
epoch:  2 ; loss:  -5.378940105438232 ; pred:  tensor([0.2994, 0.3702, 0.1877, 0.1427], grad_fn=<SoftmaxBackward0>)
masked_adj tensor([0.8477, 0.8310, 0.7997, 0.3651, 0.7701, 0.3653, 0.4941, 0.3485, 0.3813,
        0.8346, 0.3842, 0.3579, 0.3817, 0.3835, 0.3809, 0.4061, 0.3588, 0.3819,
        0.4225, 0.8395, 0.6378, 0.3850, 0.6434, 0.7125, 0.3553, 0.8102, 0.3855,
        0.3783, 0.3839, 0.8554, 0.3629], grad_fn=<MulBackward0>)
res: tensor([0.3077, 0.3408, 0.1974, 0.1541], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.8477, 0.8310, 0.7997, 0.7701, 0.8346, 0.8395, 0.6378, 0.6434, 0.7125,
        0.8102, 0.8554], grad_fn=<IndexBackward0>)
num_high 11 len(mask) 31
pred_loss tensor([1.0766], grad_fn=<MulBackward0>)
size_loss tensor(-6.4222, grad_fn=<MulBackward0>)
size_num_loss 0.35484193548387094
loss: tensor([-4.3815], grad_fn=<AddBackward0>)
3
epoch:  3 ; loss:  -4.381465911865234 ; pred:  tensor([0.3077, 0.3408, 0.1974, 0.1541], grad_fn=<SoftmaxBackward0>)
masked_adj tensor([0.8959, 0.8835, 0.8555, 0.3489, 0.7753, 0.3099, 0.4281, 0.2956, 0.3221,
        0.8858, 0.3234, 0.3037, 0.3223, 0.3228, 0.3219, 0.3493, 0.3044, 0.3274,
        0.3599, 0.8898, 0.5623, 0.3258, 0.5689, 0.6525, 0.3060, 0.8672, 0.3347,
        0.3120, 0.3458, 0.9016, 0.3058], grad_fn=<MulBackward0>)
res: tensor([0.3206, 0.3091, 0.2063, 0.1639], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.8959, 0.8835, 0.8555, 0.7753, 0.8858, 0.8898, 0.5623, 0.5689, 0.6525,
        0.8672, 0.9016], grad_fn=<IndexBackward0>)
num_high 11 len(mask) 31
pred_loss tensor([1.1740], grad_fn=<MulBackward0>)
size_loss tensor(-18.1434, grad_fn=<MulBackward0>)
size_num_loss 0.35484193548387094
loss: tensor([-16.0437], grad_fn=<AddBackward0>)
4
epoch:  4 ; loss:  -16.043743133544922 ; pred:  tensor([0.3206, 0.3091, 0.2063, 0.1639], grad_fn=<SoftmaxBackward0>)
masked_adj tensor([0.9314, 0.9229, 0.9006, 0.3541, 0.7457, 0.2705, 0.3745, 0.2575, 0.2820,
        0.9242, 0.2884, 0.2648, 0.2823, 0.2841, 0.2817, 0.3212, 0.2655, 0.2912,
        0.3111, 0.9272, 0.4701, 0.2916, 0.4773, 0.5710, 0.2721, 0.9108, 0.3025,
        0.2665, 0.3244, 0.9352, 0.2646], grad_fn=<MulBackward0>)
res: tensor([0.3338, 0.2811, 0.2136, 0.1715], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.9314, 0.9229, 0.9006, 0.7457, 0.9242, 0.9272, 0.5710, 0.9108, 0.9352],
       grad_fn=<IndexBackward0>)
num_high 9 len(mask) 31
pred_loss tensor([1.2689], grad_fn=<MulBackward0>)
size_loss tensor(-15.5121, grad_fn=<MulBackward0>)
size_num_loss 0.2903258064516129
loss: tensor([-13.4177], grad_fn=<AddBackward0>)
5
epoch:  5 ; loss:  -13.417658805847168 ; pred:  tensor([0.3338, 0.2811, 0.2136, 0.1715], grad_fn=<SoftmaxBackward0>)
masked_adj tensor([0.9548, 0.9494, 0.9331, 0.3751, 0.6882, 0.2430, 0.3311, 0.2303, 0.2566,
        0.9501, 0.2733, 0.2374, 0.2573, 0.2623, 0.2562, 0.3156, 0.2380, 0.2690,
        0.2728, 0.9521, 0.3921, 0.2766, 0.3996, 0.4836, 0.2497, 0.9413, 0.2839,
        0.2376, 0.3152, 0.9573, 0.2357], grad_fn=<MulBackward0>)
res: tensor([0.3451, 0.2593, 0.2189, 0.1767], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.9548, 0.9494, 0.9331, 0.6882, 0.9501, 0.9521, 0.9413, 0.9573],
       grad_fn=<IndexBackward0>)
num_high 8 len(mask) 31
pred_loss tensor([1.3499], grad_fn=<MulBackward0>)
size_loss tensor(-8.5197, grad_fn=<MulBackward0>)
size_num_loss 0.2580677419354839
loss: tensor([-6.4056], grad_fn=<AddBackward0>)
6
epoch:  6 ; loss:  -6.405567169189453 ; pred:  tensor([0.3451, 0.2593, 0.2189, 0.1767], grad_fn=<SoftmaxBackward0>)
masked_adj tensor([0.9693, 0.9657, 0.9534, 0.4070, 0.6137, 0.2247, 0.2960, 0.2114, 0.2428,
        0.9661, 0.2736, 0.2186, 0.2439, 0.2537, 0.2420, 0.3272, 0.2193, 0.2577,
        0.2427, 0.9675, 0.3284, 0.2766, 0.3358, 0.4083, 0.2360, 0.9601, 0.2756,
        0.2219, 0.3155, 0.9709, 0.2160], grad_fn=<MulBackward0>)
res: tensor([0.3538, 0.2439, 0.2223, 0.1801], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.9693, 0.9657, 0.9534, 0.6137, 0.9661, 0.9675, 0.9601, 0.9709],
       grad_fn=<IndexBackward0>)
num_high 8 len(mask) 31
pred_loss tensor([1.4112], grad_fn=<MulBackward0>)
size_loss tensor(-15.4325, grad_fn=<MulBackward0>)
size_num_loss 0.2580677419354839
loss: tensor([-13.2782], grad_fn=<AddBackward0>)
7
epoch:  7 ; loss:  -13.278231620788574 ; pred:  tensor([0.3538, 0.2439, 0.2223, 0.1801], grad_fn=<SoftmaxBackward0>)
masked_adj tensor([0.9783, 0.9760, 0.9668, 0.4454, 0.5230, 0.2135, 0.2676, 0.1988, 0.2380,
        0.9761, 0.2862, 0.2067, 0.2397, 0.2557, 0.2369, 0.3522, 0.2074, 0.2550,
        0.2189, 0.9771, 0.2772, 0.2886, 0.2845, 0.3457, 0.2293, 0.9722, 0.2756,
        0.2167, 0.3234, 0.9794, 0.2035], grad_fn=<MulBackward0>)
res: tensor([0.3604, 0.2334, 0.2241, 0.1821], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.9783, 0.9760, 0.9668, 0.5230, 0.9761, 0.9771, 0.9722, 0.9794],
       grad_fn=<IndexBackward0>)
num_high 8 len(mask) 31
pred_loss tensor([1.4550], grad_fn=<MulBackward0>)
size_loss tensor(-25.5705, grad_fn=<MulBackward0>)
size_num_loss 0.2580677419354839
loss: tensor([-23.3870], grad_fn=<AddBackward0>)
8
epoch:  8 ; loss:  -23.386951446533203 ; pred:  tensor([0.3604, 0.2334, 0.2241, 0.1821], grad_fn=<SoftmaxBackward0>)
masked_adj tensor([0.9841, 0.9826, 0.9758, 0.4857, 0.4230, 0.2082, 0.2447, 0.1913, 0.2409,
        0.9825, 0.3089, 0.2001, 0.2434, 0.2666, 0.2392, 0.3871, 0.2010, 0.2598,
        0.1999, 0.9833, 0.2364, 0.3106, 0.2435, 0.2945, 0.2284, 0.9799, 0.2824,
        0.2202, 0.3379, 0.9848, 0.1968], grad_fn=<MulBackward0>)
res: tensor([0.3655, 0.2267, 0.2247, 0.1831], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.9841, 0.9826, 0.9758, 0.9825, 0.9833, 0.9799, 0.9848],
       grad_fn=<IndexBackward0>)
num_high 7 len(mask) 31
pred_loss tensor([1.4841], grad_fn=<MulBackward0>)
size_loss tensor(-0.0094, grad_fn=<MulBackward0>)
size_num_loss 0.22580967741935484
loss: tensor([2.1613], grad_fn=<AddBackward0>)
9
epoch:  9 ; loss:  2.1612696647644043 ; pred:  tensor([0.3655, 0.2267, 0.2247, 0.1831], grad_fn=<SoftmaxBackward0>)
Finished Training
sel masked ver tensor(indices=tensor([[6927, 7069, 7081, 5808, 5808, 5503, 5808],
                       [5808, 5808, 5808, 8159, 8158, 6927, 5230]]),
       values=tensor([0.9841, 0.9826, 0.9758, 0.9825, 0.9833, 0.9799, 0.9848]),
       size=(7082, 8160), nnz=7, layout=torch.sparse_coo,
       grad_fn=<SparseCooTensorWithDimsAndTensorsBackward0>)
Counter({'author': 3, 'publishes': 1, 'type': 1, 'photo': 1, 'homepage': 1})
dict index: {}
node_idx 5808
 node original label [1]
 node predicted label explain 0
 node prediction probability explain tensor([0.3655, 0.2267, 0.2247, 0.1831], grad_fn=<SoftmaxBackward0>)
 node predicted label full 1 most important relations  {'publishes': 1, 'type': 1, 'photo': 1, 'homepage': 1, 'author': 3, 'label': 1, 'node_idx': '5808'}
 final masks and lenght tensor(indices=tensor([[ 23497,  23639,  23651,  39110,  46928,  63380,  88658,
                        114774, 114786, 130083, 146815, 147772, 147914, 147926,
                        154515, 179488, 179955, 196363, 229503, 237788, 246235,
                        246235, 254358, 254358, 254358, 262338, 262338, 262338,
                        304068, 328923, 328923],
                       [  5808,   5808,   5808,   5503,   5970,   5970,      0,
                          5970,   5970,   8159,   5385,   5385,   5385,   5385,
                          5808,   5808,   5808,   2855,     44,   8158,   7069,
                          7081,   6927,   7069,   7081,   6927,   7069,   7081,
                          5970,   5230,   5231]]),
       values=tensor([0.9841, 0.9826, 0.9758, 0.4857, 0.4230, 0.2082, 0.2447,
                      0.1913, 0.2409, 0.9825, 0.3089, 0.2001, 0.2434, 0.2666,
                      0.2392, 0.3871, 0.2010, 0.2598, 0.1999, 0.9833, 0.2364,
                      0.3106, 0.2435, 0.2945, 0.2284, 0.9799, 0.2824, 0.2202,
                      0.3379, 0.9848, 0.1968]),
       size=(753935, 8285), nnz=31, layout=torch.sparse_coo,
       grad_fn=<SparseCooTensorWithDimsAndTensorsBackward0>) 7 ---------------------------------------------------------------
node label: 1
20
masked_adj tensor([0.7970, 0.7831, 0.7635, 0.6453, 0.7557, 0.6823, 0.7294, 0.6669, 0.7019,
        0.7882, 0.7161, 0.6753, 0.7029, 0.7096, 0.7013, 0.7587, 0.7880, 0.7250,
        0.7120, 0.7472, 0.7017, 0.7695, 0.7600, 0.7893, 0.7762, 0.7768, 0.7533,
        0.7781, 0.7223, 0.7326, 0.7215, 0.7621, 0.6761, 0.6972, 0.7226, 0.7904,
        0.7428, 0.7148, 0.7424, 0.7026, 0.6985, 0.6951, 0.7934, 0.7370, 0.7446,
        0.7376, 0.7797, 0.7797, 0.6837, 0.7568, 0.6889, 0.7507, 0.7728, 0.7516,
        0.7137], grad_fn=<MulBackward0>)
wandb: WARNING Calling wandb.login() after wandb.init() has no effect.
res: tensor([0.0116, 0.9471, 0.0180, 0.0232], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.7970, 0.7831, 0.7635, 0.6453, 0.7557, 0.6823, 0.7294, 0.6669, 0.7019,
        0.7882, 0.7161, 0.6753, 0.7029, 0.7096, 0.7013, 0.7587, 0.7880, 0.7250,
        0.7120, 0.7472, 0.7017, 0.7695, 0.7600, 0.7893, 0.7762, 0.7768, 0.7533,
        0.7781, 0.7223, 0.7326, 0.7215, 0.7621, 0.6761, 0.6972, 0.7226, 0.7904,
        0.7428, 0.7148, 0.7424, 0.7026, 0.6985, 0.6951, 0.7934, 0.7370, 0.7446,
        0.7376, 0.7797, 0.7797, 0.6837, 0.7568, 0.6889, 0.7507, 0.7728, 0.7516,
        0.7137], grad_fn=<IndexBackward0>)
num_high 55 len(mask) 55
pred_loss tensor([0.0543], grad_fn=<MulBackward0>)
size_loss tensor(20.2184, grad_fn=<AddBackward0>)
size_num_loss 1.0000018181818182
loss: tensor([21.8465], grad_fn=<AddBackward0>)
0
epoch:  0 ; loss:  21.84650993347168 ; pred:  tensor([0.0116, 0.9471, 0.0180, 0.0232], grad_fn=<SoftmaxBackward0>)
masked_adj tensor([0.7042, 0.6865, 0.6619, 0.5246, 0.6523, 0.5658, 0.6205, 0.5483, 0.5882,
        0.6930, 0.6047, 0.5578, 0.5893, 0.5971, 0.5874, 0.6560, 0.6928, 0.6153,
        0.5999, 0.6419, 0.5879, 0.6694, 0.6576, 0.6943, 0.6779, 0.6786, 0.6494,
        0.6802, 0.6120, 0.6243, 0.6111, 0.6602, 0.5587, 0.5827, 0.6124, 0.6958,
        0.6366, 0.6033, 0.6361, 0.5890, 0.5843, 0.5803, 0.6996, 0.6296, 0.6388,
        0.6303, 0.6823, 0.6822, 0.5673, 0.6536, 0.5732, 0.6461, 0.6735, 0.6473,
        0.6020], grad_fn=<MulBackward0>)
res: tensor([0.0328, 0.8673, 0.0456, 0.0543], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.7042, 0.6865, 0.6619, 0.5246, 0.6523, 0.5658, 0.6205, 0.5483, 0.5882,
        0.6930, 0.6047, 0.5578, 0.5893, 0.5971, 0.5874, 0.6560, 0.6928, 0.6153,
        0.5999, 0.6419, 0.5879, 0.6694, 0.6576, 0.6943, 0.6779, 0.6786, 0.6494,
        0.6802, 0.6120, 0.6243, 0.6111, 0.6602, 0.5587, 0.5827, 0.6124, 0.6958,
        0.6366, 0.6033, 0.6361, 0.5890, 0.5843, 0.5803, 0.6996, 0.6296, 0.6388,
        0.6303, 0.6823, 0.6822, 0.5673, 0.6536, 0.5732, 0.6461, 0.6735, 0.6473,
        0.6020], grad_fn=<IndexBackward0>)
num_high 55 len(mask) 55
pred_loss tensor([0.1423], grad_fn=<MulBackward0>)
size_loss tensor(17.2770, grad_fn=<AddBackward0>)
size_num_loss 1.0000018181818182
loss: tensor([19.0745], grad_fn=<AddBackward0>)
1
epoch:  1 ; loss:  19.074462890625 ; pred:  tensor([0.0328, 0.8673, 0.0456, 0.0543], grad_fn=<SoftmaxBackward0>)
masked_adj tensor([0.5916, 0.5712, 0.5437, 0.4024, 0.5331, 0.4426, 0.4989, 0.4253, 0.4653,
        0.5787, 0.4823, 0.4347, 0.4664, 0.4744, 0.4645, 0.5371, 0.5784, 0.4933,
        0.4773, 0.5218, 0.4650, 0.5520, 0.5389, 0.5802, 0.5615, 0.5623, 0.5299,
        0.5641, 0.4899, 0.5029, 0.4890, 0.5420, 0.4360, 0.4601, 0.4903, 0.5821,
        0.5163, 0.4810, 0.5163, 0.4671, 0.4619, 0.4582, 0.5867, 0.5089, 0.5193,
        0.5095, 0.5666, 0.5665, 0.4444, 0.5348, 0.4503, 0.5266, 0.5595, 0.5276,
        0.4794], grad_fn=<MulBackward0>)
res: tensor([0.0793, 0.7139, 0.0985, 0.1082], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.5916, 0.5712, 0.5437, 0.5331, 0.5787, 0.5371, 0.5784, 0.5218, 0.5520,
        0.5389, 0.5802, 0.5615, 0.5623, 0.5299, 0.5641, 0.5029, 0.5420, 0.5821,
        0.5163, 0.5163, 0.5867, 0.5089, 0.5193, 0.5095, 0.5666, 0.5665, 0.5348,
        0.5266, 0.5595, 0.5276], grad_fn=<IndexBackward0>)
num_high 30 len(mask) 55
pred_loss tensor([0.3370], grad_fn=<MulBackward0>)
size_loss tensor(-0.6855, grad_fn=<MulBackward0>)
size_num_loss 0.5454563636363636
loss: tensor([0.8853], grad_fn=<AddBackward0>)
2
epoch:  2 ; loss:  0.8852625489234924 ; pred:  tensor([0.0793, 0.7139, 0.0985, 0.1082], grad_fn=<SoftmaxBackward0>)
masked_adj tensor([0.6290, 0.5802, 0.4304, 0.3273, 0.4096, 0.3583, 0.4043, 0.3448, 0.3764,
        0.6021, 0.3904, 0.3520, 0.3781, 0.3839, 0.3758, 0.4137, 0.6013, 0.3996,
        0.3863, 0.4054, 0.3762, 0.4781, 0.4168, 0.6057, 0.5383, 0.5426, 0.4079,
        0.5515, 0.3968, 0.3981, 0.3960, 0.4267, 0.3609, 0.3810, 0.3961, 0.6124,
        0.4039, 0.3943, 0.4041, 0.3953, 0.3846, 0.3867, 0.6230, 0.4011, 0.4049,
        0.4014, 0.5662, 0.5649, 0.3652, 0.4111, 0.3691, 0.4067, 0.5671, 0.4070,
        0.3883], grad_fn=<MulBackward0>)
res: tensor([0.1106, 0.6212, 0.1305, 0.1378], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.6290, 0.5802, 0.6021, 0.6013, 0.6057, 0.5383, 0.5426, 0.5515, 0.6124,
        0.6230, 0.5662, 0.5649, 0.5671], grad_fn=<IndexBackward0>)
num_high 13 len(mask) 55
pred_loss tensor([0.4761], grad_fn=<MulBackward0>)
size_loss tensor(-0.9410, grad_fn=<MulBackward0>)
size_num_loss 0.23636545454545455
loss: tensor([0.4422], grad_fn=<AddBackward0>)
3
epoch:  3 ; loss:  0.4422367811203003 ; pred:  tensor([0.1106, 0.6212, 0.1305, 0.1378], grad_fn=<SoftmaxBackward0>)
masked_adj tensor([0.6996, 0.5639, 0.3490, 0.2905, 0.3221, 0.3103, 0.3416, 0.3015, 0.3224,
        0.6659, 0.3319, 0.3062, 0.3248, 0.3274, 0.3219, 0.3265, 0.6645, 0.3383,
        0.3291, 0.3216, 0.3222, 0.4203, 0.3303, 0.6719, 0.4635, 0.4683, 0.3211,
        0.4789, 0.3363, 0.3215, 0.3357, 0.3506, 0.3269, 0.3425, 0.3340, 0.6817,
        0.3247, 0.3446, 0.3290, 0.3673, 0.3472, 0.3593, 0.6939, 0.3245, 0.3294,
        0.3239, 0.5035, 0.5006, 0.3258, 0.3266, 0.3258, 0.3232, 0.5100, 0.3211,
        0.3308], grad_fn=<MulBackward0>)
res: tensor([0.1339, 0.5524, 0.1558, 0.1579], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.6996, 0.5639, 0.6659, 0.6645, 0.6719, 0.6817, 0.6939, 0.5035, 0.5006,
        0.5100], grad_fn=<IndexBackward0>)
num_high 10 len(mask) 55
pred_loss tensor([0.5935], grad_fn=<MulBackward0>)
size_loss tensor(-7.2521, grad_fn=<MulBackward0>)
size_num_loss 0.18182
loss: tensor([-5.8362], grad_fn=<AddBackward0>)
4
epoch:  4 ; loss:  -5.836198806762695 ; pred:  tensor([0.1339, 0.5524, 0.1558, 0.1579], grad_fn=<SoftmaxBackward0>)
masked_adj tensor([0.7700, 0.4947, 0.2995, 0.2811, 0.2679, 0.2906, 0.3074, 0.2862, 0.2969,
        0.7363, 0.3020, 0.2885, 0.3001, 0.2996, 0.2966, 0.2735, 0.7349, 0.3056,
        0.3005, 0.2664, 0.2968, 0.3804, 0.2781, 0.7424, 0.4019, 0.4069, 0.2662,
        0.4186, 0.3044, 0.2682, 0.3041, 0.3100, 0.3223, 0.3345, 0.2999, 0.7524,
        0.2733, 0.3246, 0.2837, 0.3710, 0.3389, 0.3627, 0.7646, 0.2732, 0.2849,
        0.2717, 0.4294, 0.4261, 0.3167, 0.2770, 0.3109, 0.2711, 0.4369, 0.2661,
        0.3020], grad_fn=<MulBackward0>)
res: tensor([0.1469, 0.5133, 0.1716, 0.1682], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.7700, 0.7363, 0.7349, 0.7424, 0.7524, 0.7646],
       grad_fn=<IndexBackward0>)
num_high 6 len(mask) 55
pred_loss tensor([0.6670], grad_fn=<MulBackward0>)
size_loss tensor(-0.2190, grad_fn=<MulBackward0>)
size_num_loss 0.10909272727272727
loss: tensor([1.1696], grad_fn=<AddBackward0>)
5
epoch:  5 ; loss:  1.1695952415466309 ; pred:  tensor([0.1469, 0.5133, 0.1716, 0.1682], grad_fn=<SoftmaxBackward0>)
masked_adj tensor([0.8297, 0.4351, 0.2760, 0.2910, 0.2391, 0.2917, 0.2957, 0.2911, 0.2929,
        0.7656, 0.2942, 0.2914, 0.2970, 0.2935, 0.2928, 0.2469, 0.7611, 0.2952,
        0.2937, 0.2332, 0.2928, 0.3575, 0.2525, 0.7832, 0.3523, 0.3574, 0.2358,
        0.3698, 0.2949, 0.2329, 0.2948, 0.2957, 0.3384, 0.3483, 0.2878, 0.8052,
        0.2426, 0.3263, 0.2599, 0.3976, 0.3512, 0.3872, 0.8235, 0.2411, 0.2625,
        0.2388, 0.3682, 0.3648, 0.3298, 0.2534, 0.3166, 0.2426, 0.3776, 0.2349,
        0.2951], grad_fn=<MulBackward0>)
res: tensor([0.1509, 0.4998, 0.1784, 0.1709], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.8297, 0.7656, 0.7611, 0.7832, 0.8052, 0.8235],
       grad_fn=<IndexBackward0>)
num_high 6 len(mask) 55
pred_loss tensor([0.6936], grad_fn=<MulBackward0>)
size_loss tensor(-0.8543, grad_fn=<MulBackward0>)
size_num_loss 0.10909272727272727
loss: tensor([0.5434], grad_fn=<AddBackward0>)
6
epoch:  6 ; loss:  0.5434420108795166 ; pred:  tensor([0.1509, 0.4998, 0.1784, 0.1709], grad_fn=<SoftmaxBackward0>)
masked_adj tensor([0.8790, 0.3851, 0.2721, 0.3157, 0.2289, 0.3084, 0.3014, 0.3113, 0.3052,
        0.7570, 0.3031, 0.3097, 0.3102, 0.3040, 0.3053, 0.2397, 0.7463, 0.3019,
        0.3037, 0.2162, 0.3052, 0.3497, 0.2466, 0.8020, 0.3131, 0.3183, 0.2234,
        0.3313, 0.3023, 0.2109, 0.3024, 0.3006, 0.3698, 0.3785, 0.2925, 0.8486,
        0.2270, 0.3441, 0.2520, 0.4412, 0.3788, 0.4268, 0.8727, 0.2231, 0.2562,
        0.2202, 0.3186, 0.3151, 0.3599, 0.2485, 0.3377, 0.2314, 0.3304, 0.2211,
        0.3050], grad_fn=<MulBackward0>)
res: tensor([0.1485, 0.5041, 0.1788, 0.1686], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.8790, 0.7570, 0.7463, 0.8020, 0.8486, 0.8727],
       grad_fn=<IndexBackward0>)
num_high 6 len(mask) 55
pred_loss tensor([0.6850], grad_fn=<MulBackward0>)
size_loss tensor(-3.3499, grad_fn=<MulBackward0>)
size_num_loss 0.10909272727272727
loss: tensor([-1.9668], grad_fn=<AddBackward0>)
7
epoch:  7 ; loss:  -1.9668248891830444 ; pred:  tensor([0.1485, 0.5041, 0.1788, 0.1686], grad_fn=<SoftmaxBackward0>)
masked_adj tensor([0.9174, 0.3437, 0.2834, 0.3517, 0.2328, 0.3374, 0.3206, 0.3433, 0.3303,
        0.7142, 0.3252, 0.3401, 0.3361, 0.3275, 0.3305, 0.2471, 0.6975, 0.3221,
        0.3267, 0.2116, 0.3303, 0.3544, 0.2555, 0.8036, 0.2826, 0.2879, 0.2245,
        0.3016, 0.3231, 0.1991, 0.3234, 0.3200, 0.4123, 0.4205, 0.3102, 0.8876,
        0.2230, 0.3741, 0.2563, 0.4966, 0.4174, 0.4765, 0.9121, 0.2158, 0.2622,
        0.2125, 0.2786, 0.2752, 0.4032, 0.2580, 0.3708, 0.2332, 0.2931, 0.2206,
        0.3280], grad_fn=<MulBackward0>)
res: tensor([0.1416, 0.5213, 0.1744, 0.1627], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.9174, 0.7142, 0.6975, 0.8036, 0.8876, 0.9121],
       grad_fn=<IndexBackward0>)
num_high 6 len(mask) 55
pred_loss tensor([0.6514], grad_fn=<MulBackward0>)
size_loss tensor(-9.8001, grad_fn=<MulBackward0>)
size_num_loss 0.10909272727272727
loss: tensor([-8.4483], grad_fn=<AddBackward0>)
8
epoch:  8 ; loss:  -8.448323249816895 ; pred:  tensor([0.1416, 0.5213, 0.1744, 0.1627], grad_fn=<SoftmaxBackward0>)
masked_adj tensor([0.9456, 0.3100, 0.3070, 0.3958, 0.2482, 0.3758, 0.3507, 0.3841, 0.3653,
        0.6466, 0.3578, 0.3795, 0.3718, 0.3612, 0.3656, 0.2665, 0.6255, 0.3530,
        0.3599, 0.2167, 0.3654, 0.3695, 0.2765, 0.7905, 0.2593, 0.2649, 0.2365,
        0.2795, 0.3545, 0.1953, 0.3549, 0.3509, 0.4618, 0.4704, 0.3383, 0.9209,
        0.2281, 0.4126, 0.2710, 0.5583, 0.4632, 0.5309, 0.9416, 0.2172, 0.2787,
        0.2136, 0.2467, 0.2432, 0.4558, 0.2792, 0.4124, 0.2457, 0.2638, 0.2307,
        0.3613], grad_fn=<MulBackward0>)
res: tensor([0.1317, 0.5476, 0.1665, 0.1541], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.9456, 0.6466, 0.6255, 0.7905, 0.9209, 0.5583, 0.5309, 0.9416],
       grad_fn=<IndexBackward0>)
num_high 8 len(mask) 55
pred_loss tensor([0.6022], grad_fn=<MulBackward0>)
size_loss tensor(-30.9342, grad_fn=<MulBackward0>)
size_num_loss 0.14545636363636363
loss: tensor([-29.5886], grad_fn=<AddBackward0>)
9
epoch:  9 ; loss:  -29.58863067626953 ; pred:  tensor([0.1317, 0.5476, 0.1665, 0.1541], grad_fn=<SoftmaxBackward0>)
Finished Training
sel masked ver tensor(indices=tensor([[6898, 5420, 5965, 7163, 5965, 5785, 5785, 5785],
                       [5785, 5965, 5420, 5446, 6898, 6918, 6972, 6973]]),
       values=tensor([0.9456, 0.6466, 0.6255, 0.7905, 0.9209, 0.5583, 0.5309,
                      0.9416]),
       size=(7164, 6974), nnz=8, layout=torch.sparse_coo,
       grad_fn=<SparseCooTensorWithDimsAndTensorsBackward0>)
Counter({'publication': 3, 'isAbout': 2, 'dealtWithIn': 1, 'projectInfo': 1, 'author': 1})
dict index: {}
node_idx 5785
 node original label [1]
 node predicted label explain 1
 node prediction probability explain tensor([0.1317, 0.5476, 0.1665, 0.1541], grad_fn=<SoftmaxBackward0>)
 node predicted label full 1 most important relations  {'dealtWithIn': 1, 'publication': 3, 'isAbout': 2, 'projectInfo': 1, 'author': 1, 'label': 1, 'node_idx': '5785'}
 final masks and lenght tensor(indices=tensor([[ 23468,  23488,  23499,  23542,  23543,  23690,  23733,
                         39105,  46928,  63415,  63441,  63472,  88635, 114603,
                        114623, 114868, 146810, 146810, 146810, 147763, 147763,
                        147817, 148008, 148008, 148008, 154533, 154534, 154550,
                        154576, 154607, 179488, 179950, 196340, 229480, 237765,
                        246230, 246230, 246230, 254335, 254335, 254335, 254335,
                        254335, 254335, 254335, 262338, 262338, 262338, 262338,
                        262338, 262338, 262338, 304045, 328900, 328900],
                       [  5785,   5785,   5785,   5785,   5785,   5785,   5785,
                          5503,   5965,   5965,   5965,   5965,     81,   5965,
                          5965,   5965,   5420,   5446,   5477,   5446,   5477,
                          5420,   5403,   5446,   5477,   5785,   5785,   5785,
                          5785,   5785,   5785,   5785,   2489,     41,   5585,
                          6898,   6918,   7163,   6898,   6918,   6929,   6972,
                          6973,   7120,   7163,   6898,   6918,   6929,   6972,
                          6973,   7120,   7163,   5965,   5230,   5231]]),
       values=tensor([0.9456, 0.3100, 0.3070, 0.3958, 0.2482, 0.3758, 0.3507,
                      0.3841, 0.3653, 0.6466, 0.3578, 0.3795, 0.3718, 0.3612,
                      0.3656, 0.2665, 0.6255, 0.3530, 0.3599, 0.2167, 0.3654,
                      0.3695, 0.2765, 0.7905, 0.2593, 0.2649, 0.2365, 0.2795,
                      0.3545, 0.1953, 0.3549, 0.3509, 0.4618, 0.4704, 0.3383,
                      0.9209, 0.2281, 0.4126, 0.2710, 0.5583, 0.4632, 0.5309,
                      0.9416, 0.2172, 0.2787, 0.2136, 0.2467, 0.2432, 0.4558,
                      0.2792, 0.4124, 0.2457, 0.2638, 0.2307, 0.3613]),
       size=(753935, 8285), nnz=55, layout=torch.sparse_coo,
       grad_fn=<SparseCooTensorWithDimsAndTensorsBackward0>) 8 ---------------------------------------------------------------
node label: 2
14
masked_adj tensor([0.8258, 0.8068, 0.7790, 0.5968, 0.7678, 0.6556, 0.7286, 0.6311, 0.7508,
        0.6904, 0.7022, 0.7445, 0.6636, 0.7362, 0.5820, 0.7186, 0.7131, 0.7080,
        0.7742, 0.6944, 0.6962, 0.7275, 0.6815, 0.7494],
       grad_fn=<MulBackward0>)
res: tensor([0.0677, 0.1524, 0.6624, 0.1175], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.8258, 0.8068, 0.7790, 0.5968, 0.7678, 0.6556, 0.7286, 0.6311, 0.7508,
        0.6904, 0.7022, 0.7445, 0.6636, 0.7362, 0.5820, 0.7186, 0.7131, 0.7080,
        0.7742, 0.6944, 0.6962, 0.7275, 0.6815, 0.7494],
       grad_fn=<IndexBackward0>)
num_high 24 len(mask) 24
pred_loss tensor([0.4119], grad_fn=<MulBackward0>)
size_loss tensor(8.5260, grad_fn=<AddBackward0>)
size_num_loss 1.0000041666666666
loss: tensor([10.5284], grad_fn=<AddBackward0>)
wandb: WARNING Calling wandb.login() after wandb.init() has no effect.
0
epoch:  0 ; loss:  10.528438568115234 ; pred:  tensor([0.0677, 0.1524, 0.6624, 0.1175], grad_fn=<SoftmaxBackward0>)
masked_adj tensor([0.7420, 0.7169, 0.6814, 0.4731, 0.6673, 0.5358, 0.6195, 0.5092, 0.6463,
        0.5749, 0.5885, 0.6387, 0.5447, 0.6286, 0.4579, 0.6076, 0.6012, 0.5953,
        0.6752, 0.5795, 0.5815, 0.6182, 0.5648, 0.6446],
       grad_fn=<MulBackward0>)
res: tensor([0.1014, 0.1969, 0.5504, 0.1513], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.7420, 0.7169, 0.6814, 0.6673, 0.5358, 0.6195, 0.5092, 0.6463, 0.5749,
        0.5885, 0.6387, 0.5447, 0.6286, 0.6076, 0.6012, 0.5953, 0.6752, 0.5795,
        0.5815, 0.6182, 0.5648, 0.6446], grad_fn=<IndexBackward0>)
num_high 22 len(mask) 24
pred_loss tensor([0.5971], grad_fn=<MulBackward0>)
size_loss tensor(-3.3388, grad_fn=<MulBackward0>)
size_num_loss 0.9166708333333333
loss: tensor([-1.1635], grad_fn=<AddBackward0>)
1
epoch:  1 ; loss:  -1.1634624004364014 ; pred:  tensor([0.1014, 0.1969, 0.5504, 0.1513], grad_fn=<SoftmaxBackward0>)
masked_adj tensor([0.8015, 0.7797, 0.7460, 0.3959, 0.7309, 0.4357, 0.5512, 0.4114, 0.7018,
        0.4688, 0.4781, 0.6909, 0.4429, 0.6527, 0.4185, 0.4845, 0.4835, 0.4824,
        0.7417, 0.4739, 0.4752, 0.5591, 0.4626, 0.6993],
       grad_fn=<MulBackward0>)
res: tensor([0.1220, 0.2197, 0.4919, 0.1664], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.8015, 0.7797, 0.7460, 0.7309, 0.5512, 0.7018, 0.6909, 0.6527, 0.7417,
        0.5591, 0.6993], grad_fn=<IndexBackward0>)
num_high 11 len(mask) 24
pred_loss tensor([0.7094], grad_fn=<MulBackward0>)
size_loss tensor(-6.5580, grad_fn=<MulBackward0>)
size_num_loss 0.4583375
loss: tensor([-4.7428], grad_fn=<AddBackward0>)
2
epoch:  2 ; loss:  -4.742793083190918 ; pred:  tensor([0.1220, 0.2197, 0.4919, 0.1664], grad_fn=<SoftmaxBackward0>)
masked_adj tensor([0.8590, 0.8420, 0.8146, 0.3556, 0.8014, 0.3623, 0.4702, 0.3409, 0.7555,
        0.3890, 0.3966, 0.6943, 0.3664, 0.5811, 0.4263, 0.4042, 0.4033, 0.4051,
        0.8115, 0.3990, 0.3992, 0.4784, 0.3895, 0.7447],
       grad_fn=<MulBackward0>)
res: tensor([0.1385, 0.2328, 0.4510, 0.1776], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.8590, 0.8420, 0.8146, 0.8014, 0.7555, 0.6943, 0.5811, 0.8115, 0.7447],
       grad_fn=<IndexBackward0>)
num_high 9 len(mask) 24
pred_loss tensor([0.7963], grad_fn=<MulBackward0>)
size_loss tensor(-7.5003, grad_fn=<MulBackward0>)
size_num_loss 0.37500416666666664
loss: tensor([-5.7135], grad_fn=<AddBackward0>)
3
epoch:  3 ; loss:  -5.713469982147217 ; pred:  tensor([0.1385, 0.2328, 0.4510, 0.1776], grad_fn=<SoftmaxBackward0>)
masked_adj tensor([0.9046, 0.8923, 0.8720, 0.3434, 0.8616, 0.3081, 0.4049, 0.2892, 0.7468,
        0.3303, 0.3390, 0.6306, 0.3087, 0.4994, 0.4599, 0.3619, 0.3544, 0.3574,
        0.8699, 0.3479, 0.3466, 0.4135, 0.3382, 0.7102],
       grad_fn=<MulBackward0>)
res: tensor([0.1503, 0.2391, 0.4235, 0.1872], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.9046, 0.8923, 0.8720, 0.8616, 0.7468, 0.6306, 0.8699, 0.7102],
       grad_fn=<IndexBackward0>)
num_high 8 len(mask) 24
pred_loss tensor([0.8593], grad_fn=<MulBackward0>)
size_loss tensor(-10.2784, grad_fn=<MulBackward0>)
size_num_loss 0.3333375
loss: tensor([-8.5011], grad_fn=<AddBackward0>)
4
epoch:  4 ; loss:  -8.501081466674805 ; pred:  tensor([0.1503, 0.2391, 0.4235, 0.1872], grad_fn=<SoftmaxBackward0>)
masked_adj tensor([0.9371, 0.9289, 0.9151, 0.3517, 0.9077, 0.2681, 0.3524, 0.2510, 0.6916,
        0.2880, 0.3005, 0.5499, 0.2653, 0.4303, 0.5072, 0.3485, 0.3295, 0.3322,
        0.9138, 0.3150, 0.3116, 0.3613, 0.3029, 0.6453],
       grad_fn=<MulBackward0>)
res: tensor([0.1574, 0.2411, 0.4070, 0.1945], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.9371, 0.9289, 0.9151, 0.9077, 0.6916, 0.5499, 0.5072, 0.9138, 0.6453],
       grad_fn=<IndexBackward0>)
num_high 9 len(mask) 24
pred_loss tensor([0.8988], grad_fn=<MulBackward0>)
size_loss tensor(-31.5705, grad_fn=<MulBackward0>)
size_num_loss 0.37500416666666664
loss: tensor([-29.7394], grad_fn=<AddBackward0>)
5
epoch:  5 ; loss:  -29.739368438720703 ; pred:  tensor([0.1574, 0.2411, 0.4070, 0.1945], grad_fn=<SoftmaxBackward0>)
masked_adj tensor([0.9593, 0.9542, 0.9453, 0.3748, 0.9399, 0.2386, 0.3102, 0.2227, 0.6122,
        0.2584, 0.2766, 0.4535, 0.2328, 0.3731, 0.4427, 0.3565, 0.3226, 0.3242,
        0.9445, 0.2959, 0.2899, 0.3195, 0.2796, 0.5583],
       grad_fn=<MulBackward0>)
res: tensor([0.1650, 0.2426, 0.3908, 0.2016], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.9593, 0.9542, 0.9453, 0.9399, 0.6122, 0.9445, 0.5583],
       grad_fn=<IndexBackward0>)
num_high 7 len(mask) 24
pred_loss tensor([0.9396], grad_fn=<MulBackward0>)
size_loss tensor(-31.7344, grad_fn=<MulBackward0>)
size_num_loss 0.2916708333333333
loss: tensor([-29.9697], grad_fn=<AddBackward0>)
6
epoch:  6 ; loss:  -29.9697265625 ; pred:  tensor([0.1650, 0.2426, 0.3908, 0.2016], grad_fn=<SoftmaxBackward0>)
masked_adj tensor([0.9735, 0.9706, 0.9653, 0.4082, 0.9617, 0.2172, 0.2763, 0.2018, 0.5304,
        0.2388, 0.2643, 0.3720, 0.2087, 0.3261, 0.3880, 0.3805, 0.3301, 0.3300,
        0.9648, 0.2876, 0.2784, 0.2859, 0.2652, 0.4696],
       grad_fn=<MulBackward0>)
res: tensor([0.1691, 0.2421, 0.3826, 0.2062], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.9735, 0.9706, 0.9653, 0.9617, 0.5304, 0.9648],
       grad_fn=<IndexBackward0>)
num_high 6 len(mask) 24
pred_loss tensor([0.9608], grad_fn=<MulBackward0>)
size_loss tensor(-31.8156, grad_fn=<MulBackward0>)
size_num_loss 0.25000416666666664
loss: tensor([-30.0924], grad_fn=<AddBackward0>)
7
epoch:  7 ; loss:  -30.092416763305664 ; pred:  tensor([0.1691, 0.2421, 0.3826, 0.2062], grad_fn=<SoftmaxBackward0>)
masked_adj tensor([0.9823, 0.9806, 0.9775, 0.4474, 0.9753, 0.2020, 0.2490, 0.1865, 0.4413,
        0.2269, 0.2611, 0.3058, 0.1912, 0.2878, 0.3422, 0.4164, 0.3493, 0.3473,
        0.9773, 0.2881, 0.2753, 0.2589, 0.2578, 0.3928],
       grad_fn=<MulBackward0>)
res: tensor([0.1704, 0.2402, 0.3805, 0.2089], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.9823, 0.9806, 0.9775, 0.9753, 0.9773], grad_fn=<IndexBackward0>)
num_high 5 len(mask) 24
pred_loss tensor([0.9663], grad_fn=<MulBackward0>)
size_loss tensor(-0.0079, grad_fn=<MulBackward0>)
size_num_loss 0.20833749999999998
loss: tensor([1.6611], grad_fn=<AddBackward0>)
8
epoch:  8 ; loss:  1.6611065864562988 ; pred:  tensor([0.1704, 0.2402, 0.3805, 0.2089], grad_fn=<SoftmaxBackward0>)
masked_adj tensor([0.9874, 0.9864, 0.9845, 0.4881, 0.9829, 0.1917, 0.2269, 0.1756, 0.3651,
        0.2214, 0.2657, 0.2533, 0.1788, 0.2565, 0.3042, 0.4606, 0.3786, 0.3744,
        0.9843, 0.2962, 0.2790, 0.2372, 0.2560, 0.3288],
       grad_fn=<MulBackward0>)
res: tensor([0.1693, 0.2373, 0.3834, 0.2100], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.9874, 0.9864, 0.9845, 0.9829, 0.9843], grad_fn=<IndexBackward0>)
num_high 5 len(mask) 24
pred_loss tensor([0.9587], grad_fn=<MulBackward0>)
size_loss tensor(-0.0033, grad_fn=<MulBackward0>)
size_num_loss 0.20833749999999998
loss: tensor([1.6435], grad_fn=<AddBackward0>)
9
epoch:  9 ; loss:  1.643518328666687 ; pred:  tensor([0.1693, 0.2373, 0.3834, 0.2100], grad_fn=<SoftmaxBackward0>)
Finished Training
sel masked ver tensor(indices=tensor([[6860, 6874, 6920, 7731, 5757],
                       [5757, 5757, 5757, 5757, 7837]]),
       values=tensor([0.9874, 0.9864, 0.9845, 0.9829, 0.9843]),
       size=(7732, 7838), nnz=5, layout=torch.sparse_coo,
       grad_fn=<SparseCooTensorWithDimsAndTensorsBackward0>)
Counter({'author': 4, 'publication': 1})
dict index: {}
node_idx 5757
 node original label [2]
 node predicted label explain 2
 node prediction probability explain tensor([0.1693, 0.2373, 0.3834, 0.2100], grad_fn=<SoftmaxBackward0>)
 node predicted label full 2 most important relations  {'publication': 1, 'author': 4, 'label': 2, 'node_idx': '5757'}
 final masks and lenght tensor(indices=tensor([[ 23430,  23444,  23490,  23546,  24301,  24407,  24427,
                         24475,  24503,  24543,  88607, 196312, 229452, 254307,
                        254307, 254307, 254307, 254307, 254307, 254307, 254307,
                        254307, 254307, 328872],
                       [  5757,   5757,   5757,   5757,   5757,   5757,   5757,
                          5757,   5757,   5757,    908,   2227,   1002,   6860,
                          6874,   6920,   6976,   7731,   7837,   7857,   7905,
                          7933,   7973,   5230]]),
       values=tensor([0.9874, 0.9864, 0.9845, 0.4881, 0.9829, 0.1917, 0.2269,
                      0.1756, 0.3651, 0.2214, 0.2657, 0.2533, 0.1788, 0.2565,
                      0.3042, 0.4606, 0.3786, 0.3744, 0.9843, 0.2962, 0.2790,
                      0.2372, 0.2560, 0.3288]),
       size=(753935, 8285), nnz=24, layout=torch.sparse_coo,
       grad_fn=<SparseCooTensorWithDimsAndTensorsBackward0>) 5 ---------------------------------------------------------------
wandb: WARNING Calling wandb.login() after wandb.init() has no effect.
node label: 2
41
masked_adj tensor([0.7647, 0.7573, 0.7471, 0.6910, 0.7432, 0.7080, 0.7303, 0.7008, 0.7171,
        0.7600, 0.7238, 0.7047, 0.7176, 0.7207, 0.7168, 0.7447, 0.7599, 0.7281,
        0.7219, 0.7390, 0.7170, 0.7502, 0.7454, 0.7606, 0.7537, 0.7540, 0.7420,
        0.7547, 0.7268, 0.7318, 0.7265, 0.7464, 0.7051, 0.7149, 0.7270, 0.7612,
        0.7368, 0.7233, 0.7366, 0.7167, 0.7018, 0.7488, 0.7147, 0.7200, 0.7072,
        0.7679, 0.7080, 0.7221, 0.7141, 0.7189, 0.7325, 0.7405, 0.7221, 0.7522,
        0.7160, 0.7174, 0.7047, 0.7317, 0.7299, 0.7432, 0.7293, 0.7633, 0.7089,
        0.7555, 0.7566, 0.7464, 0.7695, 0.7405, 0.7373, 0.7274, 0.7114, 0.7537,
        0.7279, 0.7405, 0.7321, 0.7388, 0.7414, 0.7192, 0.6890, 0.7172, 0.7313,
        0.7248, 0.7059, 0.7203, 0.7407, 0.7405, 0.7513, 0.7320, 0.7444, 0.7222,
        0.7115, 0.7419, 0.6986, 0.7157, 0.7547, 0.7398, 0.6830, 0.7399, 0.7451,
        0.7316, 0.7426, 0.7415, 0.7500, 0.7228, 0.7277, 0.7445, 0.7384, 0.7343,
        0.7359, 0.7536, 0.7310, 0.7255, 0.7037, 0.7292, 0.7200, 0.7397, 0.7441,
        0.7327, 0.7239, 0.7406, 0.7308, 0.7354, 0.7335, 0.7448, 0.7505, 0.7372,
        0.7440, 0.7385, 0.7648, 0.7491, 0.7041, 0.7100, 0.7286, 0.7598, 0.7428,
        0.7414, 0.7513, 0.7314, 0.6969, 0.7476, 0.7242, 0.7495, 0.7184, 0.7425,
        0.7130, 0.7482, 0.7595, 0.7566, 0.7359, 0.7272, 0.7175, 0.7330, 0.7374,
        0.7483, 0.7225, 0.7593, 0.6836, 0.7234, 0.7087, 0.7456, 0.6951, 0.7352,
        0.7315, 0.7247, 0.7363, 0.7175, 0.7342, 0.7107, 0.7009, 0.7550, 0.7539,
        0.7320, 0.7020, 0.7446, 0.7449, 0.7663, 0.7317, 0.7332, 0.7161, 0.7273,
        0.7137, 0.7011, 0.7099, 0.7214, 0.7215, 0.7029, 0.6946, 0.7334, 0.7493,
        0.7208, 0.7437, 0.7438, 0.7621, 0.7139, 0.7482, 0.7249, 0.7091, 0.7375,
        0.7397, 0.7550, 0.7405, 0.7678, 0.7215, 0.7137, 0.7344, 0.7501, 0.7542,
        0.7394, 0.7160, 0.7121, 0.7219, 0.7201, 0.7339, 0.7390, 0.7283, 0.6868,
        0.7237, 0.7299, 0.7054, 0.7370, 0.7128, 0.7365, 0.7343, 0.7287, 0.7138,
        0.7700, 0.7489, 0.7552, 0.7424, 0.7384, 0.7372, 0.7270, 0.7533],
       grad_fn=<MulBackward0>)
res: tensor([3.5507e-11, 1.9416e-07, 1.0000e+00, 3.6828e-08],
       grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.7647, 0.7573, 0.7471, 0.6910, 0.7432, 0.7080, 0.7303, 0.7008, 0.7171,
        0.7600, 0.7238, 0.7047, 0.7176, 0.7207, 0.7168, 0.7447, 0.7599, 0.7281,
        0.7219, 0.7390, 0.7170, 0.7502, 0.7454, 0.7606, 0.7537, 0.7540, 0.7420,
        0.7547, 0.7268, 0.7318, 0.7265, 0.7464, 0.7051, 0.7149, 0.7270, 0.7612,
        0.7368, 0.7233, 0.7366, 0.7167, 0.7018, 0.7488, 0.7147, 0.7200, 0.7072,
        0.7679, 0.7080, 0.7221, 0.7141, 0.7189, 0.7325, 0.7405, 0.7221, 0.7522,
        0.7160, 0.7174, 0.7047, 0.7317, 0.7299, 0.7432, 0.7293, 0.7633, 0.7089,
        0.7555, 0.7566, 0.7464, 0.7695, 0.7405, 0.7373, 0.7274, 0.7114, 0.7537,
        0.7279, 0.7405, 0.7321, 0.7388, 0.7414, 0.7192, 0.6890, 0.7172, 0.7313,
        0.7248, 0.7059, 0.7203, 0.7407, 0.7405, 0.7513, 0.7320, 0.7444, 0.7222,
        0.7115, 0.7419, 0.6986, 0.7157, 0.7547, 0.7398, 0.6830, 0.7399, 0.7451,
        0.7316, 0.7426, 0.7415, 0.7500, 0.7228, 0.7277, 0.7445, 0.7384, 0.7343,
        0.7359, 0.7536, 0.7310, 0.7255, 0.7037, 0.7292, 0.7200, 0.7397, 0.7441,
        0.7327, 0.7239, 0.7406, 0.7308, 0.7354, 0.7335, 0.7448, 0.7505, 0.7372,
        0.7440, 0.7385, 0.7648, 0.7491, 0.7041, 0.7100, 0.7286, 0.7598, 0.7428,
        0.7414, 0.7513, 0.7314, 0.6969, 0.7476, 0.7242, 0.7495, 0.7184, 0.7425,
        0.7130, 0.7482, 0.7595, 0.7566, 0.7359, 0.7272, 0.7175, 0.7330, 0.7374,
        0.7483, 0.7225, 0.7593, 0.6836, 0.7234, 0.7087, 0.7456, 0.6951, 0.7352,
        0.7315, 0.7247, 0.7363, 0.7175, 0.7342, 0.7107, 0.7009, 0.7550, 0.7539,
        0.7320, 0.7020, 0.7446, 0.7449, 0.7663, 0.7317, 0.7332, 0.7161, 0.7273,
        0.7137, 0.7011, 0.7099, 0.7214, 0.7215, 0.7029, 0.6946, 0.7334, 0.7493,
        0.7208, 0.7437, 0.7438, 0.7621, 0.7139, 0.7482, 0.7249, 0.7091, 0.7375,
        0.7397, 0.7550, 0.7405, 0.7678, 0.7215, 0.7137, 0.7344, 0.7501, 0.7542,
        0.7394, 0.7160, 0.7121, 0.7219, 0.7201, 0.7339, 0.7390, 0.7283, 0.6868,
        0.7237, 0.7299, 0.7054, 0.7370, 0.7128, 0.7365, 0.7343, 0.7287, 0.7138,
        0.7700, 0.7489, 0.7552, 0.7424, 0.7384, 0.7372, 0.7270, 0.7533],
       grad_fn=<IndexBackward0>)
num_high 233 len(mask) 233
pred_loss tensor([2.3842e-07], grad_fn=<MulBackward0>)
size_loss tensor(85.2541, grad_fn=<AddBackward0>)
size_num_loss 1.0000004291845495
loss: tensor([86.8347], grad_fn=<AddBackward0>)
0
epoch:  0 ; loss:  86.83466339111328 ; pred:  tensor([3.5507e-11, 1.9416e-07, 1.0000e+00, 3.6828e-08],
       grad_fn=<SoftmaxBackward0>)
masked_adj tensor([0.6634, 0.6543, 0.6419, 0.5756, 0.6371, 0.5952, 0.6215, 0.5869, 0.6059,
        0.6576, 0.6139, 0.5914, 0.6065, 0.6102, 0.6056, 0.6389, 0.6575, 0.6190,
        0.6116, 0.6320, 0.6058, 0.6456, 0.6397, 0.6583, 0.6499, 0.6502, 0.6357,
        0.6511, 0.6174, 0.6234, 0.6170, 0.6410, 0.5919, 0.6033, 0.6176, 0.6591,
        0.6294, 0.6132, 0.6291, 0.6055, 0.5880, 0.6439, 0.6031, 0.6093, 0.5943,
        0.6674, 0.5952, 0.6118, 0.6024, 0.6080, 0.6242, 0.6338, 0.6118, 0.6480,
        0.6046, 0.6063, 0.5915, 0.6232, 0.6211, 0.6371, 0.6203, 0.6617, 0.5963,
        0.6521, 0.6534, 0.6409, 0.6694, 0.6338, 0.6300, 0.6182, 0.5992, 0.6499,
        0.6187, 0.6338, 0.6237, 0.6317, 0.6349, 0.6084, 0.5734, 0.6060, 0.6227,
        0.6151, 0.5929, 0.6096, 0.6341, 0.6338, 0.6470, 0.6236, 0.6385, 0.6119,
        0.5994, 0.6355, 0.5843, 0.6043, 0.6511, 0.6329, 0.5665, 0.6330, 0.6394,
        0.6231, 0.6363, 0.6351, 0.6454, 0.6126, 0.6184, 0.6387, 0.6312, 0.6263,
        0.6282, 0.6498, 0.6224, 0.6158, 0.5903, 0.6202, 0.6093, 0.6328, 0.6381,
        0.6244, 0.6140, 0.6339, 0.6222, 0.6277, 0.6253, 0.6389, 0.6460, 0.6298,
        0.6380, 0.6314, 0.6635, 0.6442, 0.5907, 0.5976, 0.6195, 0.6574, 0.6366,
        0.6349, 0.6470, 0.6229, 0.5824, 0.6424, 0.6143, 0.6447, 0.6074, 0.6362,
        0.6011, 0.6431, 0.6570, 0.6535, 0.6283, 0.6179, 0.6064, 0.6247, 0.6300,
        0.6433, 0.6123, 0.6567, 0.5671, 0.6133, 0.5961, 0.6400, 0.5803, 0.6274,
        0.6230, 0.6149, 0.6287, 0.6064, 0.6263, 0.5984, 0.5870, 0.6514, 0.6501,
        0.6236, 0.5882, 0.6388, 0.6392, 0.6655, 0.6232, 0.6251, 0.6048, 0.6179,
        0.6020, 0.5872, 0.5974, 0.6110, 0.6111, 0.5893, 0.5797, 0.6252, 0.6445,
        0.6103, 0.6377, 0.6378, 0.6602, 0.6022, 0.6432, 0.6151, 0.5965, 0.6302,
        0.6328, 0.6514, 0.6338, 0.6672, 0.6111, 0.6020, 0.6265, 0.6454, 0.6505,
        0.6324, 0.6046, 0.6000, 0.6116, 0.6095, 0.6258, 0.6320, 0.6192, 0.5708,
        0.6138, 0.6211, 0.5923, 0.6296, 0.6008, 0.6290, 0.6263, 0.6196, 0.6020,
        0.6701, 0.6440, 0.6517, 0.6361, 0.6312, 0.6299, 0.6176, 0.6494],
       grad_fn=<MulBackward0>)
res: tensor([1.6645e-08, 1.1604e-05, 9.9999e-01, 3.3007e-06],
       grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.6634, 0.6543, 0.6419, 0.5756, 0.6371, 0.5952, 0.6215, 0.5869, 0.6059,
        0.6576, 0.6139, 0.5914, 0.6065, 0.6102, 0.6056, 0.6389, 0.6575, 0.6190,
        0.6116, 0.6320, 0.6058, 0.6456, 0.6397, 0.6583, 0.6499, 0.6502, 0.6357,
        0.6511, 0.6174, 0.6234, 0.6170, 0.6410, 0.5919, 0.6033, 0.6176, 0.6591,
        0.6294, 0.6132, 0.6291, 0.6055, 0.5880, 0.6439, 0.6031, 0.6093, 0.5943,
        0.6674, 0.5952, 0.6118, 0.6024, 0.6080, 0.6242, 0.6338, 0.6118, 0.6480,
        0.6046, 0.6063, 0.5915, 0.6232, 0.6211, 0.6371, 0.6203, 0.6617, 0.5963,
        0.6521, 0.6534, 0.6409, 0.6694, 0.6338, 0.6300, 0.6182, 0.5992, 0.6499,
        0.6187, 0.6338, 0.6237, 0.6317, 0.6349, 0.6084, 0.5734, 0.6060, 0.6227,
        0.6151, 0.5929, 0.6096, 0.6341, 0.6338, 0.6470, 0.6236, 0.6385, 0.6119,
        0.5994, 0.6355, 0.5843, 0.6043, 0.6511, 0.6329, 0.5665, 0.6330, 0.6394,
        0.6231, 0.6363, 0.6351, 0.6454, 0.6126, 0.6184, 0.6387, 0.6312, 0.6263,
        0.6282, 0.6498, 0.6224, 0.6158, 0.5903, 0.6202, 0.6093, 0.6328, 0.6381,
        0.6244, 0.6140, 0.6339, 0.6222, 0.6277, 0.6253, 0.6389, 0.6460, 0.6298,
        0.6380, 0.6314, 0.6635, 0.6442, 0.5907, 0.5976, 0.6195, 0.6574, 0.6366,
        0.6349, 0.6470, 0.6229, 0.5824, 0.6424, 0.6143, 0.6447, 0.6074, 0.6362,
        0.6011, 0.6431, 0.6570, 0.6535, 0.6283, 0.6179, 0.6064, 0.6247, 0.6300,
        0.6433, 0.6123, 0.6567, 0.5671, 0.6133, 0.5961, 0.6400, 0.5803, 0.6274,
        0.6230, 0.6149, 0.6287, 0.6064, 0.6263, 0.5984, 0.5870, 0.6514, 0.6501,
        0.6236, 0.5882, 0.6388, 0.6392, 0.6655, 0.6232, 0.6251, 0.6048, 0.6179,
        0.6020, 0.5872, 0.5974, 0.6110, 0.6111, 0.5893, 0.5797, 0.6252, 0.6445,
        0.6103, 0.6377, 0.6378, 0.6602, 0.6022, 0.6432, 0.6151, 0.5965, 0.6302,
        0.6328, 0.6514, 0.6338, 0.6672, 0.6111, 0.6020, 0.6265, 0.6454, 0.6505,
        0.6324, 0.6046, 0.6000, 0.6116, 0.6095, 0.6258, 0.6320, 0.6192, 0.5708,
        0.6138, 0.6211, 0.5923, 0.6296, 0.6008, 0.6290, 0.6263, 0.6196, 0.6020,
        0.6701, 0.6440, 0.6517, 0.6361, 0.6312, 0.6299, 0.6176, 0.6494],
       grad_fn=<IndexBackward0>)
num_high 233 len(mask) 233
pred_loss tensor([1.4901e-05], grad_fn=<MulBackward0>)
size_loss tensor(72.6434, grad_fn=<AddBackward0>)
size_num_loss 1.0000004291845495
loss: tensor([74.3047], grad_fn=<AddBackward0>)
1
epoch:  1 ; loss:  74.30469512939453 ; pred:  tensor([1.6645e-08, 1.1604e-05, 9.9999e-01, 3.3007e-06],
       grad_fn=<SoftmaxBackward0>)
masked_adj tensor([0.5453, 0.5352, 0.5217, 0.4526, 0.5166, 0.4726, 0.5000, 0.4641, 0.4836,
        0.5389, 0.4919, 0.4687, 0.4842, 0.4881, 0.4833, 0.5186, 0.5388, 0.4973,
        0.4895, 0.5111, 0.4835, 0.5258, 0.5194, 0.5397, 0.5305, 0.5308, 0.5151,
        0.5317, 0.4956, 0.5019, 0.4952, 0.5208, 0.4691, 0.4809, 0.4958, 0.5405,
        0.5083, 0.4912, 0.5080, 0.4831, 0.4652, 0.5239, 0.4807, 0.4871, 0.4717,
        0.5498, 0.4726, 0.4897, 0.4799, 0.4858, 0.5028, 0.5131, 0.4897, 0.5284,
        0.4822, 0.4840, 0.4687, 0.5018, 0.4995, 0.5166, 0.4987, 0.5434, 0.4737,
        0.5329, 0.5343, 0.5207, 0.5520, 0.5130, 0.5090, 0.4964, 0.4767, 0.5304,
        0.4970, 0.5131, 0.5023, 0.5108, 0.5142, 0.4862, 0.4503, 0.4837, 0.5012,
        0.4932, 0.4701, 0.4875, 0.5133, 0.5131, 0.5273, 0.5022, 0.5181, 0.4899,
        0.4768, 0.5149, 0.4614, 0.4819, 0.5317, 0.5121, 0.4435, 0.5122, 0.5191,
        0.5016, 0.5158, 0.5144, 0.5256, 0.4906, 0.4967, 0.5183, 0.5103, 0.5051,
        0.5071, 0.5303, 0.5009, 0.4940, 0.4675, 0.4986, 0.4872, 0.5120, 0.5177,
        0.5031, 0.4920, 0.5131, 0.5007, 0.5065, 0.5040, 0.5186, 0.5262, 0.5088,
        0.5176, 0.5105, 0.5454, 0.5243, 0.4679, 0.4750, 0.4978, 0.5386, 0.5161,
        0.5143, 0.5273, 0.5014, 0.4595, 0.5223, 0.4923, 0.5248, 0.4852, 0.5157,
        0.4786, 0.5231, 0.5383, 0.5344, 0.5072, 0.4961, 0.4841, 0.5034, 0.5090,
        0.5233, 0.4902, 0.5379, 0.4441, 0.4914, 0.4735, 0.5197, 0.4573, 0.5062,
        0.5015, 0.4930, 0.5076, 0.4841, 0.5050, 0.4758, 0.4641, 0.5321, 0.5307,
        0.5022, 0.4654, 0.5184, 0.5189, 0.5476, 0.5018, 0.5038, 0.4824, 0.4962,
        0.4795, 0.4644, 0.4748, 0.4889, 0.4890, 0.4664, 0.4567, 0.5039, 0.5246,
        0.4882, 0.5172, 0.5173, 0.5418, 0.4798, 0.5232, 0.4932, 0.4739, 0.5092,
        0.5120, 0.5322, 0.5131, 0.5496, 0.4890, 0.4795, 0.5052, 0.5256, 0.5311,
        0.5116, 0.4822, 0.4775, 0.4896, 0.4873, 0.5045, 0.5111, 0.4976, 0.4478,
        0.4918, 0.4995, 0.4695, 0.5086, 0.4783, 0.5079, 0.5051, 0.4980, 0.4796,
        0.5527, 0.5241, 0.5324, 0.5156, 0.5103, 0.5089, 0.4959, 0.5299],
       grad_fn=<MulBackward0>)
res: tensor([5.9988e-06, 5.7622e-04, 9.9918e-01, 2.3591e-04],
       grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([0.5453, 0.5352, 0.5217, 0.5166, 0.5389, 0.5186, 0.5388, 0.5111, 0.5258,
        0.5194, 0.5397, 0.5305, 0.5308, 0.5151, 0.5317, 0.5019, 0.5208, 0.5405,
        0.5083, 0.5080, 0.5239, 0.5498, 0.5028, 0.5131, 0.5284, 0.5018, 0.5166,
        0.5434, 0.5329, 0.5343, 0.5207, 0.5520, 0.5130, 0.5090, 0.5304, 0.5131,
        0.5023, 0.5108, 0.5142, 0.5012, 0.5133, 0.5131, 0.5273, 0.5022, 0.5181,
        0.5149, 0.5317, 0.5121, 0.5122, 0.5191, 0.5016, 0.5158, 0.5144, 0.5256,
        0.5183, 0.5103, 0.5051, 0.5071, 0.5303, 0.5009, 0.5120, 0.5177, 0.5031,
        0.5131, 0.5007, 0.5065, 0.5040, 0.5186, 0.5262, 0.5088, 0.5176, 0.5105,
        0.5454, 0.5243, 0.5386, 0.5161, 0.5143, 0.5273, 0.5014, 0.5223, 0.5248,
        0.5157, 0.5231, 0.5383, 0.5344, 0.5072, 0.5034, 0.5090, 0.5233, 0.5379,
        0.5197, 0.5062, 0.5015, 0.5076, 0.5050, 0.5321, 0.5307, 0.5022, 0.5184,
        0.5189, 0.5476, 0.5018, 0.5038, 0.5039, 0.5246, 0.5172, 0.5173, 0.5418,
        0.5232, 0.5092, 0.5120, 0.5322, 0.5131, 0.5496, 0.5052, 0.5256, 0.5311,
        0.5116, 0.5045, 0.5111, 0.5086, 0.5079, 0.5051, 0.5527, 0.5241, 0.5324,
        0.5156, 0.5103, 0.5089, 0.5299], grad_fn=<IndexBackward0>)
num_high 130 len(mask) 233
pred_loss tensor([0.0008], grad_fn=<MulBackward0>)
size_loss tensor(-0.1735, grad_fn=<MulBackward0>)
size_num_loss 0.5579403433476395
loss: tensor([1.0774], grad_fn=<AddBackward0>)
2
epoch:  2 ; loss:  1.0773602724075317 ; pred:  tensor([5.9988e-06, 5.7622e-04, 9.9918e-01, 2.3591e-04],
       grad_fn=<SoftmaxBackward0>)
masked_adj tensor([0.4769, 0.4551, 0.4269, 0.3666, 0.4168, 0.3826, 0.4053, 0.3757, 0.3917,
        0.4631, 0.3985, 0.3794, 0.3921, 0.3953, 0.3913, 0.4206, 0.4628, 0.4030,
        0.3965, 0.4066, 0.3915, 0.4352, 0.4223, 0.4647, 0.4449, 0.4457, 0.4138,
        0.4476, 0.4016, 0.3909, 0.4012, 0.4250, 0.3798, 0.3894, 0.4018, 0.4665,
        0.4016, 0.3979, 0.4011, 0.3912, 0.3766, 0.4313, 0.3892, 0.3945, 0.3818,
        0.4866, 0.3826, 0.3967, 0.3886, 0.3934, 0.3922, 0.4102, 0.3967, 0.4406,
        0.3905, 0.3920, 0.3795, 0.3907, 0.4049, 0.4167, 0.4042, 0.4728, 0.3835,
        0.4500, 0.4530, 0.4249, 0.4913, 0.4101, 0.4027, 0.4023, 0.3860, 0.4448,
        0.4028, 0.4101, 0.3914, 0.4060, 0.4123, 0.3938, 0.3648, 0.3917, 0.3897,
        0.3996, 0.3806, 0.3948, 0.4106, 0.4101, 0.4382, 0.3912, 0.4198, 0.3968,
        0.3861, 0.4136, 0.3736, 0.3902, 0.4476, 0.4084, 0.3594, 0.4086, 0.4216,
        0.3904, 0.4152, 0.4127, 0.4347, 0.3974, 0.4025, 0.4201, 0.4051, 0.3961,
        0.3994, 0.4446, 0.3893, 0.4002, 0.3785, 0.4041, 0.3946, 0.4081, 0.4190,
        0.3927, 0.3986, 0.4103, 0.3889, 0.3985, 0.3943, 0.4207, 0.4360, 0.4025,
        0.4187, 0.4054, 0.4771, 0.4321, 0.3788, 0.3846, 0.4035, 0.4625, 0.4158,
        0.4124, 0.4382, 0.3900, 0.3721, 0.4281, 0.3989, 0.4331, 0.3929, 0.4150,
        0.3875, 0.4296, 0.4617, 0.4533, 0.3996, 0.4020, 0.3920, 0.3932, 0.4028,
        0.4300, 0.3971, 0.4609, 0.3599, 0.3981, 0.3833, 0.4228, 0.3703, 0.3980,
        0.3903, 0.3994, 0.4003, 0.3921, 0.3959, 0.3852, 0.3758, 0.4485, 0.4454,
        0.3913, 0.3768, 0.4203, 0.4212, 0.4819, 0.3907, 0.3938, 0.3906, 0.4021,
        0.3883, 0.3760, 0.3844, 0.3961, 0.3962, 0.3777, 0.3699, 0.3941, 0.4326,
        0.3954, 0.4180, 0.4183, 0.4694, 0.3885, 0.4298, 0.3996, 0.3837, 0.4032,
        0.4082, 0.4486, 0.4102, 0.4861, 0.3961, 0.3883, 0.3963, 0.4348, 0.4462,
        0.4074, 0.3905, 0.3866, 0.3966, 0.3947, 0.3951, 0.4066, 0.4032, 0.3628,
        0.3984, 0.4049, 0.3801, 0.4020, 0.3873, 0.4009, 0.3961, 0.4036, 0.3883,
        0.4930, 0.4316, 0.4492, 0.4149, 0.4051, 0.4025, 0.4018, 0.4437],
       grad_fn=<MulBackward0>)
res: tensor([2.5572e-04, 6.7078e-03, 9.8953e-01, 3.5061e-03],
       grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([], grad_fn=<IndexBackward0>)
num_high 0 len(mask) 233
pred_loss tensor([0.0105], grad_fn=<MulBackward0>)
size_loss tensor(nan, grad_fn=<MulBackward0>)
size_num_loss 4.291845493562232e-07
loss: tensor([0.0105], grad_fn=<MulBackward0>)
3
epoch:  3 ; loss:  0.010524868033826351 ; pred:  tensor([2.5572e-04, 6.7078e-03, 9.8953e-01, 3.5061e-03],
       grad_fn=<SoftmaxBackward0>)
masked_adj tensor([0.4237, 0.3957, 0.3605, 0.3170, 0.3484, 0.3273, 0.3427, 0.3228, 0.3334,
        0.4059, 0.3380, 0.3252, 0.3337, 0.3358, 0.3332, 0.3529, 0.4055, 0.3411,
        0.3367, 0.3363, 0.3333, 0.3707, 0.3550, 0.4079, 0.3828, 0.3838, 0.3449,
        0.3862, 0.3401, 0.3185, 0.3399, 0.3582, 0.3255, 0.3319, 0.3402, 0.4103,
        0.3306, 0.3376, 0.3300, 0.3331, 0.3234, 0.3659, 0.3317, 0.3353, 0.3268,
        0.4364, 0.3273, 0.3369, 0.3313, 0.3346, 0.3200, 0.3406, 0.3368, 0.3775,
        0.3326, 0.3336, 0.3253, 0.3182, 0.3424, 0.3483, 0.3419, 0.4184, 0.3279,
        0.3892, 0.3930, 0.3581, 0.4425, 0.3404, 0.3319, 0.3406, 0.3295, 0.3827,
        0.3409, 0.3405, 0.3191, 0.3357, 0.3430, 0.3348, 0.3158, 0.3334, 0.3172,
        0.3387, 0.3261, 0.3356, 0.3412, 0.3406, 0.3745, 0.3189, 0.3520, 0.3369,
        0.3297, 0.3446, 0.3215, 0.3325, 0.3863, 0.3385, 0.3124, 0.3387, 0.3541,
        0.3180, 0.3465, 0.3436, 0.3701, 0.3373, 0.3408, 0.3524, 0.3347, 0.3243,
        0.3282, 0.3825, 0.3168, 0.3393, 0.3247, 0.3419, 0.3354, 0.3382, 0.3510,
        0.3206, 0.3381, 0.3407, 0.3163, 0.3270, 0.3224, 0.3531, 0.3718, 0.3316,
        0.3507, 0.3350, 0.4240, 0.3669, 0.3249, 0.3287, 0.3415, 0.4051, 0.3473,
        0.3431, 0.3745, 0.3175, 0.3205, 0.3620, 0.3384, 0.3682, 0.3343, 0.3463,
        0.3306, 0.3638, 0.4041, 0.3935, 0.3286, 0.3402, 0.3337, 0.3212, 0.3321,
        0.3644, 0.3371, 0.4031, 0.3127, 0.3378, 0.3279, 0.3556, 0.3194, 0.3266,
        0.3179, 0.3387, 0.3291, 0.3338, 0.3242, 0.3291, 0.3230, 0.3873, 0.3835,
        0.3190, 0.3236, 0.3527, 0.3536, 0.4303, 0.3183, 0.3219, 0.3327, 0.3405,
        0.3311, 0.3230, 0.3287, 0.3368, 0.3369, 0.3245, 0.3194, 0.3224, 0.3678,
        0.3364, 0.3504, 0.3505, 0.4143, 0.3315, 0.3645, 0.3392, 0.3286, 0.3327,
        0.3386, 0.3877, 0.3411, 0.4361, 0.3367, 0.3315, 0.3246, 0.3703, 0.3845,
        0.3374, 0.3326, 0.3301, 0.3368, 0.3355, 0.3233, 0.3365, 0.3413, 0.3146,
        0.3381, 0.3425, 0.3257, 0.3311, 0.3306, 0.3299, 0.3244, 0.3416, 0.3311,
        0.4455, 0.3667, 0.3891, 0.3467, 0.3350, 0.3319, 0.3402, 0.3812],
       grad_fn=<MulBackward0>)
res: tensor([0.0021, 0.0263, 0.9559, 0.0156], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([], grad_fn=<IndexBackward0>)
num_high 0 len(mask) 233
pred_loss tensor([0.0451], grad_fn=<MulBackward0>)
size_loss tensor(nan, grad_fn=<MulBackward0>)
size_num_loss 4.291845493562232e-07
loss: tensor([0.0451], grad_fn=<MulBackward0>)
4
epoch:  4 ; loss:  0.04506991058588028 ; pred:  tensor([0.0021, 0.0263, 0.9559, 0.0156], grad_fn=<SoftmaxBackward0>)
masked_adj tensor([0.3873, 0.3577, 0.3217, 0.2961, 0.3095, 0.3010, 0.3089, 0.2989, 0.3041,
        0.3684, 0.3065, 0.3000, 0.3042, 0.3054, 0.3040, 0.3140, 0.3680, 0.3081,
        0.3058, 0.2975, 0.3040, 0.3321, 0.3161, 0.3705, 0.3444, 0.3455, 0.3060,
        0.3479, 0.3076, 0.2799, 0.3075, 0.3194, 0.3001, 0.3033, 0.3077, 0.3730,
        0.2918, 0.3063, 0.2912, 0.3039, 0.2991, 0.3272, 0.3033, 0.3051, 0.3008,
        0.4008, 0.3010, 0.3064, 0.3030, 0.3047, 0.2814, 0.3017, 0.3058, 0.3389,
        0.3037, 0.3042, 0.3000, 0.2797, 0.3088, 0.3094, 0.3085, 0.3816, 0.3013,
        0.3510, 0.3550, 0.3192, 0.4073, 0.3016, 0.2931, 0.3078, 0.3021, 0.3443,
        0.3080, 0.3016, 0.2805, 0.2968, 0.3042, 0.3048, 0.2956, 0.3041, 0.2786,
        0.3069, 0.3007, 0.3053, 0.3026, 0.3020, 0.3362, 0.2806, 0.3134, 0.3061,
        0.3025, 0.3060, 0.2985, 0.3039, 0.3483, 0.3000, 0.2943, 0.2999, 0.3153,
        0.2798, 0.3079, 0.3050, 0.3317, 0.3063, 0.3082, 0.3138, 0.2961, 0.2858,
        0.2896, 0.3443, 0.2784, 0.3074, 0.2999, 0.3088, 0.3053, 0.2995, 0.3123,
        0.2821, 0.3068, 0.3021, 0.2779, 0.2883, 0.2840, 0.3145, 0.3334, 0.2931,
        0.3121, 0.2965, 0.3876, 0.3281, 0.2999, 0.3021, 0.3087, 0.3679, 0.3086,
        0.3044, 0.3359, 0.2789, 0.2978, 0.3236, 0.3071, 0.3296, 0.3049, 0.3078,
        0.3027, 0.3252, 0.3669, 0.3557, 0.2908, 0.3067, 0.3044, 0.2829, 0.2935,
        0.3260, 0.3063, 0.3655, 0.2943, 0.3068, 0.3015, 0.3169, 0.2977, 0.2881,
        0.2797, 0.3072, 0.2905, 0.3047, 0.2858, 0.3023, 0.2993, 0.3492, 0.3455,
        0.2807, 0.2996, 0.3142, 0.3149, 0.3946, 0.2799, 0.2835, 0.3038, 0.3077,
        0.3030, 0.2991, 0.3022, 0.3077, 0.3077, 0.3013, 0.2984, 0.2845, 0.3299,
        0.3074, 0.3134, 0.3131, 0.3786, 0.3041, 0.3272, 0.3087, 0.3036, 0.2949,
        0.3010, 0.3508, 0.3041, 0.4019, 0.3071, 0.3045, 0.2860, 0.3320, 0.3464,
        0.2988, 0.3038, 0.3029, 0.3061, 0.3056, 0.2847, 0.2980, 0.3084, 0.2954,
        0.3069, 0.3091, 0.3003, 0.2924, 0.3032, 0.2915, 0.2861, 0.3088, 0.3030,
        0.4137, 0.3291, 0.3545, 0.3097, 0.2972, 0.2938, 0.3072, 0.3425],
       grad_fn=<MulBackward0>)
res: tensor([0.0058, 0.0492, 0.9138, 0.0312], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([], grad_fn=<IndexBackward0>)
num_high 0 len(mask) 233
pred_loss tensor([0.0901], grad_fn=<MulBackward0>)
size_loss tensor(nan, grad_fn=<MulBackward0>)
size_num_loss 4.291845493562232e-07
loss: tensor([0.0901], grad_fn=<MulBackward0>)
5
epoch:  5 ; loss:  0.09011413156986237 ; pred:  tensor([0.0058, 0.0492, 0.9138, 0.0312], grad_fn=<SoftmaxBackward0>)
masked_adj tensor([0.3667, 0.3390, 0.3056, 0.2966, 0.2943, 0.2967, 0.2980, 0.2966, 0.2971,
        0.3490, 0.2975, 0.2966, 0.2971, 0.2973, 0.2971, 0.2985, 0.3486, 0.2978,
        0.2973, 0.2831, 0.2971, 0.3152, 0.3005, 0.3510, 0.3266, 0.3276, 0.2910,
        0.3299, 0.2977, 0.2665, 0.2977, 0.3035, 0.2967, 0.2970, 0.2977, 0.3533,
        0.2778, 0.2974, 0.2772, 0.2971, 0.2966, 0.3107, 0.2970, 0.2972, 0.2967,
        0.3795, 0.2967, 0.2987, 0.2969, 0.2972, 0.2679, 0.2871, 0.2974, 0.3216,
        0.2970, 0.2971, 0.2966, 0.2663, 0.2979, 0.2942, 0.2979, 0.3614, 0.2967,
        0.3328, 0.3365, 0.3033, 0.3858, 0.2869, 0.2790, 0.2977, 0.2968, 0.3266,
        0.2978, 0.2870, 0.2671, 0.2825, 0.2894, 0.2972, 0.2967, 0.2971, 0.2653,
        0.2975, 0.2975, 0.2974, 0.2885, 0.2878, 0.3195, 0.2677, 0.2985, 0.2980,
        0.2976, 0.2916, 0.2974, 0.2976, 0.3309, 0.2859, 0.2973, 0.2854, 0.2998,
        0.2669, 0.2934, 0.2906, 0.3152, 0.2979, 0.2984, 0.2986, 0.2821, 0.2725,
        0.2760, 0.3269, 0.2654, 0.2982, 0.2971, 0.2985, 0.2978, 0.2853, 0.2973,
        0.2690, 0.2981, 0.2879, 0.2648, 0.2746, 0.2709, 0.2994, 0.3170, 0.2795,
        0.2972, 0.2826, 0.3671, 0.3115, 0.2968, 0.2978, 0.2987, 0.3491, 0.2940,
        0.2899, 0.3189, 0.2656, 0.2967, 0.3080, 0.2986, 0.3134, 0.2980, 0.2934,
        0.2971, 0.3090, 0.3484, 0.3378, 0.2787, 0.2952, 0.2977, 0.2699, 0.2799,
        0.3102, 0.2982, 0.3462, 0.2970, 0.2985, 0.2973, 0.3014, 0.2976, 0.2747,
        0.2669, 0.2985, 0.2767, 0.2982, 0.2727, 0.2977, 0.2976, 0.3313, 0.3285,
        0.2679, 0.2975, 0.2994, 0.2995, 0.3745, 0.2668, 0.2704, 0.2972, 0.2977,
        0.2971, 0.2969, 0.2981, 0.3022, 0.3023, 0.3009, 0.2996, 0.2723, 0.3147,
        0.3019, 0.3014, 0.3003, 0.3614, 0.2995, 0.3136, 0.3017, 0.3018, 0.2824,
        0.2886, 0.3351, 0.2926, 0.3836, 0.3008, 0.3005, 0.2725, 0.3159, 0.3289,
        0.2849, 0.2973, 0.2981, 0.2980, 0.2982, 0.2713, 0.2842, 0.2984, 0.2975,
        0.2985, 0.2988, 0.2968, 0.2786, 0.2983, 0.2782, 0.2730, 0.2989, 0.2972,
        0.3983, 0.3147, 0.3431, 0.2978, 0.2846, 0.2808, 0.2965, 0.3241],
       grad_fn=<MulBackward0>)
res: tensor([0.0078, 0.0586, 0.8956, 0.0379], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([], grad_fn=<IndexBackward0>)
num_high 0 len(mask) 233
pred_loss tensor([0.1102], grad_fn=<MulBackward0>)
size_loss tensor(nan, grad_fn=<MulBackward0>)
size_num_loss 4.291845493562232e-07
loss: tensor([0.1102], grad_fn=<MulBackward0>)
6
epoch:  6 ; loss:  0.11023927479982376 ; pred:  tensor([0.0078, 0.0586, 0.8956, 0.0379], grad_fn=<SoftmaxBackward0>)
masked_adj tensor([0.3602, 0.3364, 0.3073, 0.3132, 0.2973, 0.3090, 0.3044, 0.3107, 0.3070,
        0.3449, 0.3057, 0.3098, 0.3069, 0.3063, 0.3071, 0.3010, 0.3446, 0.3048,
        0.3060, 0.2871, 0.3070, 0.3158, 0.3028, 0.3466, 0.3257, 0.3265, 0.2943,
        0.3285, 0.3051, 0.2717, 0.3052, 0.3055, 0.3097, 0.3075, 0.3051, 0.3486,
        0.2822, 0.3058, 0.2817, 0.3071, 0.3105, 0.3118, 0.3075, 0.3064, 0.3092,
        0.3713, 0.3090, 0.3085, 0.3077, 0.3067, 0.2730, 0.2907, 0.3060, 0.3213,
        0.3073, 0.3070, 0.3098, 0.2715, 0.3045, 0.2972, 0.3046, 0.3556, 0.3088,
        0.3310, 0.3342, 0.3053, 0.3767, 0.2906, 0.2834, 0.3050, 0.3083, 0.3256,
        0.3049, 0.2906, 0.2722, 0.2866, 0.2928, 0.3066, 0.3137, 0.3070, 0.2706,
        0.3055, 0.3110, 0.3067, 0.2929, 0.2922, 0.3201, 0.2734, 0.3018, 0.3071,
        0.3097, 0.2957, 0.3128, 0.3085, 0.3303, 0.2904, 0.3162, 0.2893, 0.3023,
        0.2728, 0.2972, 0.2947, 0.3163, 0.3067, 0.3061, 0.3016, 0.2867, 0.2777,
        0.2809, 0.3265, 0.2710, 0.3064, 0.3109, 0.3057, 0.3073, 0.2896, 0.3005,
        0.2744, 0.3067, 0.2921, 0.2703, 0.2794, 0.2765, 0.3026, 0.3181, 0.2844,
        0.3006, 0.2873, 0.3607, 0.3125, 0.3102, 0.3103, 0.3063, 0.3459, 0.2977,
        0.2937, 0.3191, 0.2707, 0.3119, 0.3103, 0.3076, 0.3147, 0.3083, 0.2974,
        0.3082, 0.3106, 0.3456, 0.3362, 0.2856, 0.3004, 0.3079, 0.2756, 0.2848,
        0.3123, 0.3074, 0.3425, 0.3155, 0.3076, 0.3098, 0.3038, 0.3139, 0.2800,
        0.2729, 0.3071, 0.2815, 0.3089, 0.2783, 0.3099, 0.3126, 0.3300, 0.3284,
        0.2737, 0.3120, 0.3027, 0.3022, 0.3682, 0.2724, 0.2760, 0.3076, 0.3049,
        0.3081, 0.3112, 0.3110, 0.3147, 0.3148, 0.3177, 0.3175, 0.2790, 0.3175,
        0.3144, 0.3084, 0.3063, 0.3599, 0.3123, 0.3184, 0.3128, 0.3176, 0.2888,
        0.2952, 0.3368, 0.3004, 0.3795, 0.3123, 0.3141, 0.2775, 0.3174, 0.3282,
        0.2894, 0.3078, 0.3103, 0.3072, 0.3081, 0.2764, 0.2890, 0.3058, 0.3157,
        0.3074, 0.3060, 0.3099, 0.2832, 0.3104, 0.2836, 0.2787, 0.3066, 0.3082,
        0.3979, 0.3185, 0.3505, 0.3050, 0.2910, 0.2866, 0.3028, 0.3223],
       grad_fn=<MulBackward0>)
res: tensor([0.0064, 0.0520, 0.9083, 0.0332], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([], grad_fn=<IndexBackward0>)
num_high 0 len(mask) 233
pred_loss tensor([0.0961], grad_fn=<MulBackward0>)
size_loss tensor(nan, grad_fn=<MulBackward0>)
size_num_loss 4.291845493562232e-07
loss: tensor([0.0961], grad_fn=<MulBackward0>)
7
epoch:  7 ; loss:  0.09612610936164856 ; pred:  tensor([0.0064, 0.0520, 0.9083, 0.0332], grad_fn=<SoftmaxBackward0>)
masked_adj tensor([0.3655, 0.3468, 0.3231, 0.3422, 0.3145, 0.3343, 0.3246, 0.3376, 0.3302,
        0.3535, 0.3273, 0.3358, 0.3300, 0.3286, 0.3304, 0.3177, 0.3533, 0.3255,
        0.3281, 0.3055, 0.3303, 0.3301, 0.3192, 0.3549, 0.3382, 0.3389, 0.3119,
        0.3405, 0.3260, 0.2912, 0.3262, 0.3215, 0.3356, 0.3312, 0.3260, 0.3565,
        0.3011, 0.3276, 0.3006, 0.3304, 0.3372, 0.3268, 0.3313, 0.3290, 0.3347,
        0.3742, 0.3343, 0.3319, 0.3316, 0.3294, 0.2925, 0.3087, 0.3281, 0.3346,
        0.3307, 0.3301, 0.3358, 0.2910, 0.3247, 0.3144, 0.3250, 0.3619, 0.3339,
        0.3425, 0.3450, 0.3213, 0.3785, 0.3086, 0.3021, 0.3258, 0.3328, 0.3381,
        0.3256, 0.3086, 0.2917, 0.3050, 0.3106, 0.3293, 0.3432, 0.3302, 0.2901,
        0.3269, 0.3374, 0.3293, 0.3117, 0.3109, 0.3345, 0.2936, 0.3194, 0.3297,
        0.3349, 0.3140, 0.3410, 0.3325, 0.3431, 0.3093, 0.3474, 0.3076, 0.3190,
        0.2931, 0.3154, 0.3132, 0.3312, 0.3290, 0.3274, 0.3189, 0.3057, 0.2974,
        0.3003, 0.3396, 0.2910, 0.3282, 0.3376, 0.3266, 0.3303, 0.3082, 0.3179,
        0.2942, 0.3288, 0.3107, 0.2903, 0.2986, 0.2966, 0.3200, 0.3331, 0.3038,
        0.3182, 0.3064, 0.3661, 0.3273, 0.3365, 0.3360, 0.3276, 0.3555, 0.3156,
        0.3118, 0.3330, 0.2903, 0.3398, 0.3267, 0.3301, 0.3299, 0.3319, 0.3156,
        0.3326, 0.3262, 0.3557, 0.3478, 0.3072, 0.3189, 0.3315, 0.2958, 0.3042,
        0.3284, 0.3301, 0.3516, 0.3464, 0.3303, 0.3355, 0.3205, 0.3430, 0.2996,
        0.2932, 0.3293, 0.3007, 0.3329, 0.2983, 0.3353, 0.3403, 0.3421, 0.3419,
        0.2939, 0.3393, 0.3204, 0.3191, 0.3735, 0.2923, 0.2959, 0.3312, 0.3257,
        0.3323, 0.3383, 0.3369, 0.3410, 0.3411, 0.3477, 0.3484, 0.3004, 0.3344,
        0.3407, 0.3300, 0.3269, 0.3712, 0.3384, 0.3375, 0.3376, 0.3468, 0.3098,
        0.3164, 0.3523, 0.3229, 0.3871, 0.3374, 0.3411, 0.2970, 0.3328, 0.3410,
        0.3083, 0.3316, 0.3357, 0.3298, 0.3315, 0.2960, 0.3081, 0.3268, 0.3464,
        0.3299, 0.3269, 0.3360, 0.3022, 0.3358, 0.3035, 0.2988, 0.3281, 0.3324,
        0.4091, 0.3365, 0.3721, 0.3269, 0.3119, 0.3069, 0.3226, 0.3339],
       grad_fn=<MulBackward0>)
res: tensor([0.0035, 0.0358, 0.9387, 0.0220], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([], grad_fn=<IndexBackward0>)
num_high 0 len(mask) 233
pred_loss tensor([0.0632], grad_fn=<MulBackward0>)
size_loss tensor(nan, grad_fn=<MulBackward0>)
size_num_loss 4.291845493562232e-07
loss: tensor([0.0632], grad_fn=<MulBackward0>)
8
epoch:  8 ; loss:  0.06324668228626251 ; pred:  tensor([0.0035, 0.0358, 0.9387, 0.0220], grad_fn=<SoftmaxBackward0>)
masked_adj tensor([0.3806, 0.3676, 0.3500, 0.3807, 0.3430, 0.3696, 0.3555, 0.3743, 0.3638,
        0.3724, 0.3595, 0.3717, 0.3635, 0.3615, 0.3640, 0.3457, 0.3722, 0.3568,
        0.3607, 0.3354, 0.3638, 0.3554, 0.3469, 0.3733, 0.3615, 0.3620, 0.3409,
        0.3631, 0.3577, 0.3224, 0.3579, 0.3487, 0.3715, 0.3652, 0.3576, 0.3744,
        0.3314, 0.3599, 0.3310, 0.3640, 0.3737, 0.3529, 0.3653, 0.3620, 0.3701,
        0.3864, 0.3696, 0.3656, 0.3657, 0.3626, 0.3236, 0.3381, 0.3606, 0.3588,
        0.3645, 0.3636, 0.3717, 0.3222, 0.3557, 0.3429, 0.3561, 0.3781, 0.3690,
        0.3646, 0.3664, 0.3486, 0.3894, 0.3381, 0.3323, 0.3573, 0.3674, 0.3614,
        0.3570, 0.3381, 0.3228, 0.3349, 0.3397, 0.3625, 0.3820, 0.3638, 0.3213,
        0.3589, 0.3739, 0.3623, 0.3419, 0.3410, 0.3598, 0.3254, 0.3482, 0.3628,
        0.3702, 0.3437, 0.3788, 0.3669, 0.3666, 0.3396, 0.3877, 0.3373, 0.3469,
        0.3250, 0.3449, 0.3430, 0.3571, 0.3618, 0.3594, 0.3473, 0.3361, 0.3286,
        0.3312, 0.3634, 0.3226, 0.3605, 0.3741, 0.3582, 0.3637, 0.3383, 0.3466,
        0.3257, 0.3615, 0.3408, 0.3218, 0.3293, 0.3283, 0.3486, 0.3590, 0.3347,
        0.3471, 0.3370, 0.3812, 0.3532, 0.3727, 0.3717, 0.3596, 0.3753, 0.3449,
        0.3413, 0.3578, 0.3214, 0.3773, 0.3542, 0.3631, 0.3562, 0.3660, 0.3453,
        0.3671, 0.3529, 0.3759, 0.3699, 0.3403, 0.3483, 0.3654, 0.3277, 0.3351,
        0.3556, 0.3633, 0.3709, 0.3864, 0.3636, 0.3711, 0.3483, 0.3815, 0.3309,
        0.3252, 0.3621, 0.3314, 0.3673, 0.3299, 0.3709, 0.3779, 0.3649, 0.3660,
        0.3258, 0.3765, 0.3492, 0.3472, 0.3882, 0.3239, 0.3275, 0.3651, 0.3572,
        0.3667, 0.3752, 0.3730, 0.3776, 0.3776, 0.3875, 0.3888, 0.3333, 0.3622,
        0.3773, 0.3628, 0.3585, 0.3922, 0.3748, 0.3675, 0.3730, 0.3860, 0.3423,
        0.3489, 0.3781, 0.3567, 0.4037, 0.3729, 0.3781, 0.3280, 0.3591, 0.3644,
        0.3387, 0.3656, 0.3713, 0.3630, 0.3652, 0.3271, 0.3388, 0.3586, 0.3863,
        0.3629, 0.3585, 0.3719, 0.3328, 0.3713, 0.3348, 0.3305, 0.3602, 0.3668,
        0.4286, 0.3653, 0.4036, 0.3599, 0.3443, 0.3386, 0.3531, 0.3563],
       grad_fn=<MulBackward0>)
res: tensor([0.0013, 0.0191, 0.9686, 0.0110], grad_fn=<SoftmaxBackward0>)
mask_without_small tensor([], grad_fn=<IndexBackward0>)
num_high 0 len(mask) 233
pred_loss tensor([0.0319], grad_fn=<MulBackward0>)
size_loss tensor(nan, grad_fn=<MulBackward0>)
size_num_loss 4.291845493562232e-07
loss: tensor([0.0319], grad_fn=<MulBackward0>)
Traceback (most recent call last):
  File "r_exp.py", line 499, in <module>
    main('aifb',node_idx= 5731, prune= True, explain_all = True)
  File "r_exp.py", line 417, in main
    h = visualize(node_idx, n_hops, data, masked_ver,threshold=0.5, name = name, result_weights=False, low_threshold=False)
  File "/home/tliberatore/RGCN-Explainer/RGCN_stuff/src/rgcn_explainer_utils.py", line 315, in visualize
    triples_matched = match_to_triples(np.array(new_index), data.triples)
  File "/home/tliberatore/RGCN-Explainer/RGCN_stuff/src/rgcn_explainer_utils.py", line 191, in match_to_triples
    result = torch.stack(matching)
RuntimeError: stack expects a non-empty TensorList
9
epoch:  9 ; loss:  0.031914714723825455 ; pred:  tensor([0.0013, 0.0191, 0.9686, 0.0110], grad_fn=<SoftmaxBackward0>)
Finished Training
sel masked ver tensor(indices=tensor([], size=(2, 0)),
       values=tensor([], size=(0,)),
       size=(0, 0), nnz=0, layout=torch.sparse_coo,
       grad_fn=<SparseCooTensorWithDimsAndTensorsBackward0>)