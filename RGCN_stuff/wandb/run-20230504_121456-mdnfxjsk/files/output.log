
wandb: WARNING Calling wandb.login() after wandb.init() has no effect.
num_high 119 len(mask) 119
mask_without_small tensor([0.7773, 0.7672, 0.7534, 0.6742, 0.7480, 0.6985, 0.7300, 0.6883, 0.7115,
        0.7710, 0.7209, 0.6938, 0.7121, 0.7166, 0.7110, 0.7500, 0.7708, 0.7270,
        0.7182, 0.7421, 0.7113, 0.7576, 0.7510, 0.7717, 0.7624, 0.7628, 0.7463,
        0.7637, 0.7251, 0.7321, 0.7246, 0.7524, 0.6943, 0.7083, 0.7253, 0.7725,
        0.7391, 0.7201, 0.7388, 0.7109, 0.6896, 0.7557, 0.7081, 0.7155, 0.6974,
        0.7816, 0.6985, 0.7184, 0.7071, 0.7140, 0.7330, 0.7442, 0.7184, 0.7603,
        0.7098, 0.7119, 0.6938, 0.7320, 0.7294, 0.7479, 0.7286, 0.7754, 0.6998,
        0.7648, 0.7663, 0.7523, 0.7837, 0.7442, 0.7398, 0.7260, 0.7033, 0.7624,
        0.7266, 0.7442, 0.7325, 0.7418, 0.7455, 0.7144, 0.6713, 0.7115, 0.7313,
        0.7223, 0.6955, 0.7159, 0.7445, 0.7442, 0.7591, 0.7324, 0.7496, 0.7186,
        0.7035, 0.7462, 0.6850, 0.7094, 0.7637, 0.7432, 0.6625, 0.7433, 0.7506,
        0.7318, 0.7471, 0.7457, 0.7574, 0.7443, 0.7565, 0.7127, 0.7561, 0.7225,
        0.7440, 0.7408, 0.7160, 0.7414, 0.7162, 0.7438, 0.7188, 0.7183, 0.7379,
        0.7165, 0.7480], grad_fn=<IndexBackward0>)
pred_loss tensor([0.0145], grad_fn=<MulBackward0>)
size_loss tensor(-2.5075, grad_fn=<MulBackward0>)
size_num_loss 11.9
loss: tensor([18.6958], grad_fn=<AddBackward0>)
0
epoch:  0 ; loss:  18.695796966552734 ; pred:  tensor([9.9855e-01, 6.6735e-04, 9.8767e-05, 6.8133e-04],
       grad_fn=<SoftmaxBackward0>)
num_high 119 len(mask) 119
mask_without_small tensor([0.7941, 0.7846, 0.7715, 0.6518, 0.7287, 0.6770, 0.7098, 0.6664, 0.6905,
        0.7881, 0.7004, 0.6722, 0.6912, 0.6958, 0.6900, 0.7308, 0.7880, 0.7067,
        0.6975, 0.7225, 0.6903, 0.7755, 0.7318, 0.7888, 0.7800, 0.7804, 0.7269,
        0.7813, 0.7047, 0.7121, 0.7042, 0.7706, 0.6727, 0.6872, 0.7050, 0.7896,
        0.7194, 0.6995, 0.7190, 0.6899, 0.6678, 0.7737, 0.6870, 0.6947, 0.6759,
        0.7982, 0.6770, 0.6978, 0.6860, 0.6931, 0.7130, 0.7248, 0.6978, 0.7781,
        0.6888, 0.6910, 0.6722, 0.7119, 0.7093, 0.7286, 0.7083, 0.7923, 0.6784,
        0.7823, 0.7837, 0.7705, 0.8002, 0.7247, 0.7201, 0.7057, 0.6821, 0.7800,
        0.7063, 0.7247, 0.7125, 0.7222, 0.7260, 0.6936, 0.6488, 0.6905, 0.7112,
        0.7018, 0.6740, 0.6951, 0.7250, 0.7247, 0.7769, 0.7123, 0.7304, 0.6980,
        0.6822, 0.7268, 0.6630, 0.6884, 0.7813, 0.7237, 0.6398, 0.7238, 0.7314,
        0.7117, 0.7277, 0.7262, 0.7753, 0.7248, 0.7745, 0.6918, 0.7741, 0.7021,
        0.7245, 0.7212, 0.6953, 0.7218, 0.6954, 0.7243, 0.6981, 0.6977, 0.7181,
        0.6957, 0.7287], grad_fn=<IndexBackward0>)
pred_loss tensor([0.0196], grad_fn=<MulBackward0>)
size_loss tensor(-3.8062, grad_fn=<MulBackward0>)
size_num_loss 11.9
loss: tensor([17.2674], grad_fn=<AddBackward0>)
1
epoch:  1 ; loss:  17.267440795898438 ; pred:  tensor([9.9805e-01, 9.0168e-04, 1.4484e-04, 9.0767e-04],
       grad_fn=<SoftmaxBackward0>)
num_high 118 len(mask) 119
mask_without_small tensor([0.8100, 0.8010, 0.7854, 0.6290, 0.7104, 0.6549, 0.6888, 0.6440, 0.6688,
        0.8044, 0.6790, 0.6499, 0.6695, 0.6743, 0.6683, 0.7139, 0.8042, 0.6855,
        0.6760, 0.7025, 0.6686, 0.7914, 0.7157, 0.8050, 0.7964, 0.7968, 0.7080,
        0.7977, 0.6835, 0.6911, 0.6829, 0.7837, 0.6505, 0.6654, 0.6837, 0.8058,
        0.6990, 0.6781, 0.6986, 0.6681, 0.6454, 0.7889, 0.6651, 0.6731, 0.6537,
        0.8138, 0.6549, 0.6763, 0.6641, 0.6715, 0.6921, 0.7052, 0.6763, 0.7944,
        0.6670, 0.6692, 0.6499, 0.6910, 0.6882, 0.7103, 0.6872, 0.8083, 0.6563,
        0.7988, 0.8001, 0.7836, 0.8156, 0.7051, 0.6998, 0.6844, 0.6601, 0.7964,
        0.6852, 0.7051, 0.6915, 0.7021, 0.7068, 0.6719, 0.6260, 0.6688, 0.6903,
        0.6805, 0.6518, 0.6735, 0.7055, 0.7052, 0.7931, 0.6914, 0.7132, 0.6765,
        0.6602, 0.7078, 0.6405, 0.6666, 0.7977, 0.7039, 0.6168, 0.7040, 0.7152,
        0.6908, 0.7091, 0.7071, 0.7911, 0.7053, 0.7900, 0.6701, 0.7895, 0.6807,
        0.7049, 0.7010, 0.6737, 0.7017, 0.6739, 0.7046, 0.6766, 0.6762, 0.6976,
        0.6742, 0.7105], grad_fn=<IndexBackward0>)
pred_loss tensor([0.0265], grad_fn=<MulBackward0>)
size_loss tensor(-5.2235, grad_fn=<MulBackward0>)
size_num_loss 11.8
loss: tensor([15.6127], grad_fn=<AddBackward0>)
2
epoch:  2 ; loss:  15.61270809173584 ; pred:  tensor([9.9735e-01, 1.2189e-03, 2.1302e-04, 1.2133e-03],
       grad_fn=<SoftmaxBackward0>)
num_high 114 len(mask) 119
mask_without_small tensor([0.8247, 0.8165, 0.8001, 0.6059, 0.6908, 0.6322, 0.6669, 0.6211, 0.6464,
        0.8196, 0.6568, 0.6271, 0.6471, 0.6519, 0.6459, 0.6950, 0.8195, 0.6635,
        0.6537, 0.6816, 0.6462, 0.8068, 0.6973, 0.8202, 0.8121, 0.8125, 0.6878,
        0.8133, 0.6614, 0.6693, 0.6608, 0.7980, 0.6277, 0.6429, 0.6616, 0.8209,
        0.6777, 0.6558, 0.6772, 0.6457, 0.6225, 0.8041, 0.6426, 0.6507, 0.6310,
        0.8282, 0.6322, 0.6540, 0.6416, 0.6491, 0.6704, 0.6846, 0.6540, 0.8100,
        0.6445, 0.6468, 0.6272, 0.6692, 0.6663, 0.6906, 0.6653, 0.8232, 0.6337,
        0.8144, 0.8156, 0.7979, 0.8298, 0.6845, 0.6785, 0.6624, 0.6375, 0.8121,
        0.6631, 0.6845, 0.6698, 0.6811, 0.6864, 0.6496, 0.6028, 0.6464, 0.6684,
        0.6583, 0.6290, 0.6512, 0.6850, 0.6846, 0.8087, 0.6696, 0.6942, 0.6542,
        0.6377, 0.6876, 0.6176, 0.6442, 0.8133, 0.6831, 0.5935, 0.6833, 0.6968,
        0.6689, 0.6891, 0.6868, 0.8065, 0.6847, 0.8054, 0.6477, 0.8048, 0.6586,
        0.6843, 0.6799, 0.6514, 0.6806, 0.6515, 0.6839, 0.6544, 0.6539, 0.6761,
        0.6519, 0.6909], grad_fn=<IndexBackward0>)
pred_loss tensor([0.0363], grad_fn=<MulBackward0>)
size_loss tensor(-6.6899, grad_fn=<MulBackward0>)
size_num_loss 11.4
loss: tensor([13.6010], grad_fn=<AddBackward0>)
3
epoch:  3 ; loss:  13.601011276245117 ; pred:  tensor([9.9637e-01, 1.6729e-03, 3.1672e-04, 1.6354e-03],
       grad_fn=<SoftmaxBackward0>)
num_high 98 len(mask) 119
mask_without_small tensor([0.8381, 0.8308, 0.8146, 0.5827, 0.6698, 0.6091, 0.6442, 0.5979, 0.6234,
        0.8336, 0.6339, 0.6040, 0.6241, 0.6290, 0.6229, 0.6747, 0.8335, 0.6407,
        0.6308, 0.6596, 0.6232, 0.8216, 0.6773, 0.8341, 0.8267, 0.8271, 0.6665,
        0.8279, 0.6386, 0.6467, 0.6380, 0.8124, 0.6046, 0.6198, 0.6388, 0.8347,
        0.6554, 0.6329, 0.6550, 0.6227, 0.5994, 0.8188, 0.6196, 0.6278, 0.6079,
        0.8412, 0.6091, 0.6311, 0.6186, 0.6261, 0.6478, 0.6629, 0.6311, 0.8247,
        0.6215, 0.6238, 0.6040, 0.6465, 0.6436, 0.6697, 0.6425, 0.8368, 0.6106,
        0.8289, 0.8300, 0.8122, 0.8427, 0.6628, 0.6564, 0.6396, 0.6144, 0.8267,
        0.6403, 0.6629, 0.6472, 0.6591, 0.6650, 0.6266, 0.5796, 0.6234, 0.6458,
        0.6355, 0.6059, 0.6283, 0.6634, 0.6629, 0.8235, 0.6470, 0.6738, 0.6313,
        0.6146, 0.6663, 0.5944, 0.6211, 0.8279, 0.6613, 0.5702, 0.6615, 0.6767,
        0.6463, 0.6680, 0.6654, 0.8213, 0.6630, 0.8201, 0.6248, 0.8195, 0.6357,
        0.6626, 0.6578, 0.6284, 0.6586, 0.6286, 0.6622, 0.6315, 0.6310, 0.6538,
        0.6289, 0.6700], grad_fn=<IndexBackward0>)
pred_loss tensor([0.0503], grad_fn=<MulBackward0>)
size_loss tensor(-8.1680, grad_fn=<MulBackward0>)
size_num_loss 9.8
loss: tensor([10.3711], grad_fn=<AddBackward0>)
4
epoch:  4 ; loss:  10.371091842651367 ; pred:  tensor([9.9498e-01, 2.3188e-03, 4.7625e-04, 2.2219e-03],
       grad_fn=<SoftmaxBackward0>)
num_high 62 len(mask) 119
mask_without_small tensor([0.8501, 0.8438, 0.8284, 0.5593, 0.6478, 0.5856, 0.6208, 0.5746, 0.5999,
        0.8462, 0.6104, 0.5806, 0.6006, 0.6055, 0.5994, 0.6532, 0.8461, 0.6173,
        0.6073, 0.6368, 0.5997, 0.8353, 0.6560, 0.8467, 0.8402, 0.8405, 0.6442,
        0.8412, 0.6151, 0.6234, 0.6146, 0.8261, 0.5812, 0.5964, 0.6154, 0.8472,
        0.6324, 0.6095, 0.6319, 0.5992, 0.5760, 0.8326, 0.5961, 0.6043, 0.5845,
        0.8527, 0.5856, 0.6076, 0.5951, 0.6026, 0.6245, 0.6403, 0.6076, 0.8383,
        0.5981, 0.6004, 0.5806, 0.6232, 0.6202, 0.6477, 0.6191, 0.8490, 0.5872,
        0.8421, 0.8431, 0.8260, 0.8540, 0.6402, 0.6334, 0.6162, 0.5910, 0.8402,
        0.6169, 0.6403, 0.6238, 0.6363, 0.6426, 0.6031, 0.5563, 0.5999, 0.6224,
        0.6120, 0.5826, 0.6048, 0.6408, 0.6403, 0.8372, 0.6237, 0.6522, 0.6079,
        0.5912, 0.6440, 0.5711, 0.5977, 0.8413, 0.6386, 0.5470, 0.6388, 0.6554,
        0.6230, 0.6458, 0.6430, 0.8350, 0.6404, 0.8339, 0.6013, 0.8333, 0.6122,
        0.6400, 0.6349, 0.6050, 0.6357, 0.6051, 0.6395, 0.6080, 0.6075, 0.6308,
        0.6055, 0.6480], grad_fn=<IndexBackward0>)
pred_loss tensor([0.0701], grad_fn=<MulBackward0>)
size_loss tensor(-9.6339, grad_fn=<MulBackward0>)
size_num_loss 6.2
loss: tensor([5.1489], grad_fn=<AddBackward0>)
5
epoch:  5 ; loss:  5.148879051208496 ; pred:  tensor([9.9302e-01, 3.2268e-03, 7.2255e-04, 3.0347e-03],
       grad_fn=<SoftmaxBackward0>)
num_high 34 len(mask) 119
mask_without_small tensor([0.8605, 0.8552, 0.8411, 0.5361, 0.6249, 0.5620, 0.5968, 0.5511, 0.5761,
        0.8573, 0.5865, 0.5571, 0.5768, 0.5817, 0.5756, 0.6306, 0.8572, 0.5933,
        0.5834, 0.6132, 0.5759, 0.8477, 0.6337, 0.8576, 0.8521, 0.8524, 0.6211,
        0.8531, 0.5912, 0.5994, 0.5906, 0.8389, 0.5576, 0.5726, 0.5914, 0.8581,
        0.6086, 0.5856, 0.6082, 0.5754, 0.5525, 0.8451, 0.5724, 0.5804, 0.5609,
        0.8627, 0.5620, 0.5837, 0.5714, 0.5788, 0.6005, 0.6169, 0.5837, 0.8505,
        0.5743, 0.5765, 0.5571, 0.5992, 0.5962, 0.6247, 0.5952, 0.8595, 0.5636,
        0.8538, 0.8547, 0.8388, 0.8637, 0.6168, 0.6096, 0.5922, 0.5674, 0.8522,
        0.5930, 0.6168, 0.5999, 0.6127, 0.6193, 0.5793, 0.5331, 0.5761, 0.5985,
        0.5881, 0.5591, 0.5810, 0.6174, 0.6169, 0.8497, 0.5998, 0.6296, 0.5841,
        0.5676, 0.6208, 0.5479, 0.5740, 0.8534, 0.6151, 0.5241, 0.6153, 0.6331,
        0.5990, 0.6228, 0.6197, 0.8475, 0.6170, 0.8465, 0.5775, 0.8459, 0.5883,
        0.6166, 0.6112, 0.5811, 0.6121, 0.5813, 0.6161, 0.5841, 0.5836, 0.6071,
        0.5816, 0.6250], grad_fn=<IndexBackward0>)
pred_loss tensor([0.0978], grad_fn=<MulBackward0>)
size_loss tensor(-11.0682, grad_fn=<MulBackward0>)
size_num_loss 3.4000000000000004
loss: tensor([0.7564], grad_fn=<AddBackward0>)
6
epoch:  6 ; loss:  0.7563872337341309 ; pred:  tensor([0.9903, 0.0045, 0.0011, 0.0041], grad_fn=<SoftmaxBackward0>)
num_high 26 len(mask) 119
mask_without_small tensor([0.8691, 0.8649, 0.8524, 0.5131, 0.6012, 0.5384, 0.5724, 0.5277, 0.5521,
        0.8666, 0.5622, 0.5335, 0.5528, 0.5575, 0.5516, 0.6072, 0.8665, 0.5689,
        0.5593, 0.5890, 0.5519, 0.8585, 0.6104, 0.8669, 0.8623, 0.8626, 0.5971,
        0.8632, 0.5669, 0.5750, 0.5663, 0.8504, 0.5341, 0.5487, 0.5671, 0.8672,
        0.5843, 0.5613, 0.5839, 0.5515, 0.5291, 0.8562, 0.5485, 0.5563, 0.5373,
        0.8709, 0.5384, 0.5595, 0.5475, 0.5547, 0.5761, 0.5928, 0.5595, 0.8609,
        0.5503, 0.5525, 0.5336, 0.5748, 0.5718, 0.6010, 0.5708, 0.8684, 0.5399,
        0.8638, 0.8645, 0.8504, 0.8718, 0.5927, 0.5853, 0.5679, 0.5436, 0.8626,
        0.5686, 0.5927, 0.5755, 0.5885, 0.5953, 0.5553, 0.5102, 0.5521, 0.5740,
        0.5639, 0.5356, 0.5570, 0.5933, 0.5929, 0.8606, 0.5755, 0.6062, 0.5601,
        0.5440, 0.5969, 0.5248, 0.5501, 0.8639, 0.5910, 0.5015, 0.5912, 0.6099,
        0.5747, 0.5989, 0.5957, 0.8584, 0.5930, 0.8576, 0.5536, 0.8570, 0.5641,
        0.5925, 0.5869, 0.5571, 0.5878, 0.5572, 0.5920, 0.5600, 0.5595, 0.5833,
        0.5575, 0.6013], grad_fn=<IndexBackward0>)
pred_loss tensor([0.1360], grad_fn=<MulBackward0>)
size_loss tensor(-12.4537, grad_fn=<MulBackward0>)
size_num_loss 2.6
loss: tensor([-1.5854], grad_fn=<AddBackward0>)
7
epoch:  7 ; loss:  -1.585409164428711 ; pred:  tensor([0.9865, 0.0062, 0.0017, 0.0056], grad_fn=<SoftmaxBackward0>)
num_high 26 len(mask) 119
mask_without_small tensor([0.8760, 0.8727, 0.8620, 0.5768, 0.5149, 0.5477, 0.5046, 0.5280, 0.8740,
        0.5378, 0.5102, 0.5287, 0.5333, 0.5276, 0.5830, 0.8739, 0.5443, 0.5350,
        0.5642, 0.5279, 0.8674, 0.5864, 0.8742, 0.8706, 0.8708, 0.5726, 0.8715,
        0.5423, 0.5502, 0.5417, 0.8602, 0.5107, 0.5248, 0.5425, 0.8745, 0.5595,
        0.5370, 0.5592, 0.5274, 0.5059, 0.8653, 0.5246, 0.5321, 0.5138, 0.8774,
        0.5149, 0.5352, 0.5236, 0.5306, 0.5514, 0.5682, 0.5352, 0.8694, 0.5264,
        0.5285, 0.5102, 0.5501, 0.5471, 0.5766, 0.5461, 0.8754, 0.5164, 0.8718,
        0.8724, 0.8602, 0.8781, 0.5681, 0.5606, 0.5433, 0.5200, 0.8711, 0.5441,
        0.5681, 0.5508, 0.5638, 0.5707, 0.5313, 0.5281, 0.5493, 0.5395, 0.5124,
        0.5330, 0.5687, 0.5684, 0.8699, 0.5510, 0.5821, 0.5360, 0.5205, 0.5724,
        0.5022, 0.5263, 0.8728, 0.5665, 0.5666, 0.5861, 0.5500, 0.5745, 0.5712,
        0.8674, 0.5684, 0.8669, 0.5296, 0.8663, 0.5397, 0.5679, 0.5622, 0.5329,
        0.5631, 0.5330, 0.5674, 0.5357, 0.5353, 0.5598, 0.5334, 0.5770],
       grad_fn=<IndexBackward0>)
pred_loss tensor([0.1881], grad_fn=<MulBackward0>)
size_loss tensor(-1379.4301, grad_fn=<MulBackward0>)
size_num_loss 2.6
loss: tensor([-1375.3036], grad_fn=<AddBackward0>)
8
epoch:  8 ; loss:  -1375.3035888671875 ; pred:  tensor([0.9814, 0.0085, 0.0025, 0.0076], grad_fn=<SoftmaxBackward0>)
num_high 26 len(mask) 119
mask_without_small tensor([0.8813, 0.8781, 0.8678, 0.5638, 0.5017, 0.5347, 0.5149, 0.8793, 0.5248,
        0.5156, 0.5202, 0.5145, 0.5700, 0.8793, 0.5313, 0.5219, 0.5513, 0.5148,
        0.8730, 0.5734, 0.8796, 0.8761, 0.8763, 0.5597, 0.8769, 0.5293, 0.5372,
        0.5287, 0.8660, 0.5117, 0.5295, 0.8798, 0.5465, 0.5239, 0.5463, 0.5143,
        0.8710, 0.5114, 0.5190, 0.5006, 0.8827, 0.5017, 0.5221, 0.5105, 0.5175,
        0.5384, 0.5552, 0.5221, 0.8749, 0.5132, 0.5154, 0.5371, 0.5341, 0.5636,
        0.5331, 0.8807, 0.5033, 0.8772, 0.8778, 0.8661, 0.8833, 0.5552, 0.5476,
        0.5303, 0.5068, 0.8766, 0.5311, 0.5552, 0.5378, 0.5508, 0.5578, 0.5182,
        0.5150, 0.5363, 0.5264, 0.5199, 0.5558, 0.5556, 0.8754, 0.5380, 0.5693,
        0.5230, 0.5074, 0.5595, 0.5132, 0.8782, 0.5537, 0.5537, 0.5732, 0.5370,
        0.5616, 0.5583, 0.8730, 0.5555, 0.8725, 0.5165, 0.8719, 0.5267, 0.5549,
        0.5493, 0.5198, 0.5502, 0.5200, 0.5545, 0.5227, 0.5222, 0.5471, 0.5203,
        0.5641], grad_fn=<IndexBackward0>)
pred_loss tensor([0.2238], grad_fn=<MulBackward0>)
size_loss tensor(-1472.7786, grad_fn=<MulBackward0>)
size_num_loss 2.6
loss: tensor([-1468.6622], grad_fn=<AddBackward0>)
9
epoch:  9 ; loss:  -1468.6622314453125 ; pred:  tensor([0.9779, 0.0101, 0.0031, 0.0090], grad_fn=<SoftmaxBackward0>)
num_high 26 len(mask) 119
mask_without_small tensor([0.8880, 0.8849, 0.8751, 0.5472, 0.5179, 0.8861, 0.5080, 0.5034, 0.5535,
        0.8861, 0.5145, 0.5051, 0.5346, 0.8800, 0.5569, 0.8863, 0.8830, 0.8832,
        0.5430, 0.8838, 0.5125, 0.5205, 0.5119, 0.8734, 0.5127, 0.8866, 0.5298,
        0.5071, 0.5296, 0.8781, 0.5022, 0.8893, 0.5053, 0.5007, 0.5216, 0.5386,
        0.5053, 0.8819, 0.5203, 0.5173, 0.5471, 0.5163, 0.8874, 0.8841, 0.8847,
        0.8735, 0.8899, 0.5385, 0.5310, 0.5135, 0.8835, 0.5143, 0.5386, 0.5211,
        0.5342, 0.5412, 0.5014, 0.5195, 0.5096, 0.5031, 0.5392, 0.5390, 0.8823,
        0.5213, 0.5528, 0.5062, 0.5429, 0.8850, 0.5370, 0.5371, 0.5567, 0.5203,
        0.5450, 0.5417, 0.8801, 0.5388, 0.8796, 0.8791, 0.5099, 0.5383, 0.5326,
        0.5030, 0.5335, 0.5031, 0.5378, 0.5059, 0.5054, 0.5305, 0.5035, 0.5475],
       grad_fn=<IndexBackward0>)
pred_loss tensor([0.2754], grad_fn=<MulBackward0>)
size_loss tensor(-1638.9182, grad_fn=<MulBackward0>)
size_num_loss 2.6
loss: tensor([-1634.8580], grad_fn=<AddBackward0>)
10
epoch:  10 ; loss:  -1634.8580322265625 ; pred:  tensor([0.9728, 0.0123, 0.0040, 0.0109], grad_fn=<SoftmaxBackward0>)
num_high 26 len(mask) 119
mask_without_small tensor([0.8953, 0.8924, 0.8831, 0.5287, 0.8935, 0.5352, 0.8935, 0.5158, 0.8878,
        0.5387, 0.8937, 0.8906, 0.8908, 0.5244, 0.8914, 0.5015, 0.8815, 0.8940,
        0.5109, 0.5107, 0.8860, 0.8965, 0.5027, 0.5198, 0.8896, 0.5013, 0.5285,
        0.8948, 0.8916, 0.8922, 0.8816, 0.8971, 0.5198, 0.5121, 0.8911, 0.5199,
        0.5021, 0.5154, 0.5225, 0.5005, 0.5204, 0.5203, 0.8900, 0.5023, 0.5344,
        0.5243, 0.8925, 0.5183, 0.5184, 0.5385, 0.5013, 0.5264, 0.5230, 0.8879,
        0.5201, 0.8874, 0.8869, 0.5196, 0.5138, 0.5147, 0.5191, 0.5117, 0.5290],
       grad_fn=<IndexBackward0>)
pred_loss tensor([0.3424], grad_fn=<MulBackward0>)
size_loss tensor(-1850.5563, grad_fn=<MulBackward0>)
size_num_loss 2.6
loss: tensor([-1846.5743], grad_fn=<AddBackward0>)
11
epoch:  11 ; loss:  -1846.5743408203125 ; pred:  tensor([0.9663, 0.0152, 0.0052, 0.0133], grad_fn=<SoftmaxBackward0>)
num_high 26 len(mask) 119
mask_without_small tensor([0.9028, 0.9002, 0.8915, 0.5101, 0.9012, 0.5168, 0.9012, 0.8958, 0.5206,
        0.9014, 0.8985, 0.8986, 0.5056, 0.8992, 0.8899, 0.9016, 0.8942, 0.9040,
        0.5009, 0.8975, 0.5099, 0.9024, 0.8994, 0.8999, 0.8900, 0.9046, 0.5008,
        0.8989, 0.5009, 0.5037, 0.5015, 0.5013, 0.8979, 0.5161, 0.5055, 0.9002,
        0.5204, 0.5077, 0.5042, 0.8959, 0.5012, 0.8954, 0.8950, 0.5006, 0.5001,
        0.5104], grad_fn=<IndexBackward0>)
pred_loss tensor([0.4194], grad_fn=<MulBackward0>)
size_loss tensor(-1962.5957, grad_fn=<MulBackward0>)
size_num_loss 2.6
loss: tensor([-1958.6292], grad_fn=<AddBackward0>)
12
epoch:  12 ; loss:  -1958.629150390625 ; pred:  tensor([0.9589, 0.0184, 0.0067, 0.0160], grad_fn=<SoftmaxBackward0>)
num_high 26 len(mask) 119
mask_without_small tensor([0.9103, 0.9078, 0.8997, 0.9088, 0.9088, 0.9038, 0.5021, 0.9090, 0.9063,
        0.9064, 0.9069, 0.8983, 0.9092, 0.9022, 0.9114, 0.9054, 0.9099, 0.9071,
        0.9076, 0.8984, 0.9119, 0.9067, 0.9057, 0.9079, 0.5019, 0.9039, 0.9034,
        0.9030], grad_fn=<IndexBackward0>)
pred_loss tensor([0.5024], grad_fn=<MulBackward0>)
size_loss tensor(-1060.4863, grad_fn=<MulBackward0>)
size_num_loss 2.6
loss: tensor([-1056.5311], grad_fn=<AddBackward0>)
13
epoch:  13 ; loss:  -1056.5311279296875 ; pred:  tensor([0.9510, 0.0218, 0.0083, 0.0189], grad_fn=<SoftmaxBackward0>)
num_high 26 len(mask) 119
mask_without_small tensor([0.9173, 0.9150, 0.9073, 0.9159, 0.9159, 0.9112, 0.9161, 0.9135, 0.9136,
        0.9141, 0.9060, 0.9163, 0.9097, 0.9184, 0.9126, 0.9169, 0.9143, 0.9148,
        0.9060, 0.9188, 0.9139, 0.9130, 0.9150, 0.9112, 0.9108, 0.9104],
       grad_fn=<IndexBackward0>)
pred_loss tensor([0.5871], grad_fn=<MulBackward0>)
size_loss tensor(-34.9542, grad_fn=<MulBackward0>)
size_num_loss 2.6
loss: tensor([-30.9278], grad_fn=<AddBackward0>)
14
epoch:  14 ; loss:  -30.927818298339844 ; pred:  tensor([0.9430, 0.0252, 0.0101, 0.0218], grad_fn=<SoftmaxBackward0>)
num_high 26 len(mask) 119
mask_without_small tensor([0.9241, 0.9218, 0.9077, 0.9228, 0.9228, 0.9150, 0.9230, 0.9197, 0.9199,
        0.9207, 0.9056, 0.9232, 0.9120, 0.9249, 0.9181, 0.9238, 0.9210, 0.9216,
        0.9057, 0.9253, 0.9203, 0.9187, 0.9219, 0.9151, 0.9143, 0.9134],
       grad_fn=<IndexBackward0>)
pred_loss tensor([0.6739], grad_fn=<MulBackward0>)
size_loss tensor(-57.5271, grad_fn=<MulBackward0>)
size_num_loss 2.6
loss: tensor([-53.4174], grad_fn=<AddBackward0>)
15
epoch:  15 ; loss:  -53.41737747192383 ; pred:  tensor([0.9348, 0.0286, 0.0119, 0.0246], grad_fn=<SoftmaxBackward0>)
num_high 26 len(mask) 119
mask_without_small tensor([0.9306, 0.9284, 0.9053, 0.9295, 0.9295, 0.9167, 0.9297, 0.9253, 0.9257,
        0.9269, 0.9026, 0.9298, 0.9115, 0.9313, 0.9225, 0.9304, 0.9273, 0.9281,
        0.9027, 0.9315, 0.9263, 0.9236, 0.9285, 0.9169, 0.9154, 0.9139],
       grad_fn=<IndexBackward0>)
pred_loss tensor([0.7607], grad_fn=<MulBackward0>)
size_loss tensor(-90.8296, grad_fn=<MulBackward0>)
size_num_loss 2.6
loss: tensor([-86.6365], grad_fn=<AddBackward0>)
16
epoch:  16 ; loss:  -86.6364974975586 ; pred:  tensor([0.9267, 0.0319, 0.0138, 0.0275], grad_fn=<SoftmaxBackward0>)
num_high 26 len(mask) 119
mask_without_small tensor([0.9368, 0.9346, 0.9010, 0.9358, 0.9358, 0.9167, 0.9360, 0.9306, 0.9312,
        0.9328, 0.8977, 0.9361, 0.9090, 0.9373, 0.9262, 0.9366, 0.9333, 0.9343,
        0.8978, 0.9375, 0.9320, 0.9281, 0.9347, 0.9170, 0.9147, 0.9124],
       grad_fn=<IndexBackward0>)
pred_loss tensor([0.8462], grad_fn=<MulBackward0>)
size_loss tensor(-130.8972, grad_fn=<MulBackward0>)
size_num_loss 2.6
loss: tensor([-126.6220], grad_fn=<AddBackward0>)
17
epoch:  17 ; loss:  -126.6220474243164 ; pred:  tensor([0.9189, 0.0352, 0.0157, 0.0302], grad_fn=<SoftmaxBackward0>)
num_high 26 len(mask) 119
mask_without_small tensor([0.9426, 0.9405, 0.8951, 0.9417, 0.9416, 0.9152, 0.9418, 0.9356, 0.9364,
        0.9384, 0.8913, 0.9420, 0.9049, 0.9429, 0.9295, 0.9424, 0.9390, 0.9401,
        0.8914, 0.9430, 0.9375, 0.9322, 0.9406, 0.9157, 0.9124, 0.9093],
       grad_fn=<IndexBackward0>)
pred_loss tensor([0.9294], grad_fn=<MulBackward0>)
size_loss tensor(-176.0923, grad_fn=<MulBackward0>)
size_num_loss 2.6
loss: tensor([-171.7373], grad_fn=<AddBackward0>)
18
epoch:  18 ; loss:  -171.73727416992188 ; pred:  tensor([0.9112, 0.0383, 0.0176, 0.0328], grad_fn=<SoftmaxBackward0>)
num_high 26 len(mask) 119
mask_without_small tensor([0.9478, 0.9459, 0.8879, 0.9471, 0.9470, 0.9124, 0.9472, 0.9404, 0.9413,
        0.9436, 0.8835, 0.9473, 0.8994, 0.9480, 0.9324, 0.9477, 0.9443, 0.9455,
        0.8837, 0.9481, 0.9426, 0.9361, 0.9460, 0.9130, 0.9088, 0.9048],
       grad_fn=<IndexBackward0>)
pred_loss tensor([1.0095], grad_fn=<MulBackward0>)
size_loss tensor(-225.6796, grad_fn=<MulBackward0>)
size_num_loss 2.6
loss: tensor([-221.2477], grad_fn=<AddBackward0>)
19
epoch:  19 ; loss:  -221.2477264404297 ; pred:  tensor([0.9040, 0.0413, 0.0194, 0.0353], grad_fn=<SoftmaxBackward0>)
Finished Training
sel masked ver tensor(indices=tensor([[6881, 6888, 6981, 7911, 5937, 5408, 5431, 5450, 5494,
                        5678, 6997, 8013, 5939, 6881, 7393, 5408, 5431, 5450,
                        5494, 5502, 5678, 5678, 5678, 5502, 5502, 5502],
                       [5678, 5678, 5678, 5678, 5502, 5939, 5937, 5939, 5937,
                          22, 5939, 5939, 5357, 5357, 5450, 5678, 5678, 5678,
                        5678, 5678, 5535, 6997, 8010, 6981, 7045, 7100]]),
       values=tensor([0.9478, 0.9459, 0.8879, 0.9471, 0.9470, 0.9124, 0.9472,
                      0.9404, 0.9413, 0.9436, 0.8835, 0.9473, 0.8994, 0.9480,
                      0.9324, 0.9477, 0.9443, 0.9455, 0.8837, 0.9481, 0.9426,
                      0.9361, 0.9460, 0.9130, 0.9088, 0.9048]),
       size=(8014, 8011), nnz=26, layout=torch.sparse_coo,
       grad_fn=<SparseCooTensorWithDimsAndTensorsBackward0>)
Counter({'isWorkedOnBy': 4, 'dealtWithIn': 4, 'author': 4, 'publishes': 3, 'isAbout': 3, 'publication': 2, 'hasProject': 2, 'member': 1, 'fax': 1, 'photo': 1, 'carriedOutBy': 1})
dict index: {}
node_idx 5678
 node original label [0]
 node predicted label explain 0
 node prediction probability explain tensor([0.9040, 0.0413, 0.0194, 0.0353], grad_fn=<SoftmaxBackward0>)
 node predicted label full 0 most important relations  {'isWorkedOnBy': 4, 'dealtWithIn': 4, 'member': 1, 'publishes': 3, 'fax': 1, 'photo': 1, 'publication': 2, 'carriedOutBy': 1, 'isAbout': 3, 'author': 4, 'hasProject': 2, 'label': 0, 'node_idx': '5678'}
 final masks and lenght tensor(indices=tensor([[ 23451,  23458,  23551,  23567,  23615,  23638,  23670,
                         23963,  24431,  24481,  24538,  24580,  24582,  24583,
                         24584,  24585,  39077,  39079,  46927,  46927,  63352,
                         63403,  63408,  63426,  63445,  63489,  81452,  88528,
                        114586, 114592, 114593, 114702, 115616, 115715, 115717,
                        115718, 115719, 115720, 129953, 146782, 146782, 146784,
                        146784, 146784, 146784, 147726, 147732, 147826, 147842,
                        147890, 147913, 147945, 148238, 148238, 148756, 148855,
                        148857, 148858, 148859, 148860, 154487, 154538, 154543,
                        154561, 154580, 154624, 179487, 179922, 179924, 196233,
                        229373, 237658, 246202, 246204, 246204, 246204, 246204,
                        246204, 246204, 246204, 246204, 246204, 254228, 254228,
                        254228, 254228, 254228, 254228, 254228, 254228, 254228,
                        254228, 254228, 254228, 254228, 254228, 254228, 254228,
                        254228, 262337, 262337, 262337, 262337, 262337, 262337,
                        262337, 262337, 262337, 262337, 262337, 262337, 262337,
                        262337, 262337, 262337, 262337, 303938, 328793, 328793],
                       [  5678,   5678,   5678,   5678,   5678,   5678,   5678,
                          5678,   5678,   5678,   5678,   5678,   5678,   5678,
                          5678,   5678,   5502,   5502,   5937,   5939,   5939,
                          5939,   5939,   5937,   5939,   5937,   5678,     22,
                          5939,   5939,   5937,   5939,   5939,   5939,   5939,
                          5939,   5939,   5939,    117,   5431,   5494,   5357,
                          5408,   5413,   5450,   5357,   5450,   5450,   5450,
                          5450,   5450,   5450,   5357,   5450,   5450,   5450,
                          5450,   5450,   5450,   5450,   5678,   5678,   5678,
                          5678,   5678,   5678,   5678,   5678,   5678,   2215,
                            35,   5535,   6888,   6881,   6887,   6997,   7911,
                          8010,   8012,   8013,   8014,   8015,   6881,   6887,
                          6888,   6981,   6997,   7045,   7068,   7100,   7393,
                          7861,   7911,   7968,   8010,   8012,   8013,   8014,
                          8015,   6881,   6887,   6888,   6981,   6997,   7045,
                          7068,   7100,   7393,   7861,   7911,   7968,   8010,
                          8012,   8013,   8014,   8015,   5939,   5230,   5231]]),
       values=tensor([0.9478, 0.9459, 0.8879, 0.3712, 0.4113, 0.3933, 0.4017,
                      0.4156, 0.4061, 0.9471, 0.3918, 0.4211, 0.4067, 0.3872,
                      0.4056, 0.4191, 0.9470, 0.3983, 0.3889, 0.4068, 0.4059,
                      0.9124, 0.4303, 0.9472, 0.9404, 0.9413, 0.4063, 0.9436,
                      0.3962, 0.3910, 0.3957, 0.8835, 0.4216, 0.4029, 0.3964,
                      0.9473, 0.4013, 0.3909, 0.4015, 0.4055, 0.4169, 0.8994,
                      0.4027, 0.3861, 0.3922, 0.9480, 0.3933, 0.3892, 0.4018,
                      0.3846, 0.3923, 0.4011, 0.3892, 0.9324, 0.4044, 0.4065,
                      0.4211, 0.3908, 0.4011, 0.4111, 0.4001, 0.9477, 0.3951,
                      0.9443, 0.9455, 0.8837, 0.9481, 0.4010, 0.4027, 0.3974,
                      0.3985, 0.9426, 0.3983, 0.4011, 0.3918, 0.4064, 0.4042,
                      0.3856, 0.3777, 0.4061, 0.3900, 0.3937, 0.4245, 0.3876,
                      0.4017, 0.4018, 0.9361, 0.3924, 0.4184, 0.3911, 0.3996,
                      0.4063, 0.4156, 0.4049, 0.9460, 0.4102, 0.3909, 0.4101,
                      0.4302, 0.3910, 0.4088, 0.4047, 0.9130, 0.4015, 0.9088,
                      0.4083, 0.9048, 0.3940, 0.4008, 0.4046, 0.3872, 0.4057,
                      0.3872, 0.4003, 0.3900, 0.3895, 0.4042, 0.3877, 0.4118]),
       size=(753935, 8285), nnz=119, layout=torch.sparse_coo,
       grad_fn=<SparseCooTensorWithDimsAndTensorsBackward0>) 26
 ---------------------------------------------------------------
r_exp.py:102: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  hor_graph, ver_graph = torch.tensor(sub_hor_graph, dtype=torch.float), torch.tensor(sub_ver_graph, dtype=torch.float)
wandb: WARNING Calling wandb.login() after wandb.init() has no effect.
node label: 0
5
num_high 6 len(mask) 7
mask_without_small tensor([0.7649, 0.7444, 0.7550, 0.7546, 0.5986, 0.7110, 0.8985],
       grad_fn=<IndexBackward0>)
pred_loss tensor([9.7398], grad_fn=<MulBackward0>)
size_loss tensor(-8.8219, grad_fn=<MulBackward0>)
size_num_loss 0.6000000000000001
loss: tensor([2.5878], grad_fn=<AddBackward0>)
0
epoch:  0 ; loss:  2.587822198867798 ; pred:  tensor([0.3776, 0.2412, 0.2576, 0.1236], grad_fn=<SoftmaxBackward0>)
num_high 6 len(mask) 7
mask_without_small tensor([0.7824, 0.7249, 0.7730, 0.7356, 0.5744, 0.6901, 0.9072],
       grad_fn=<IndexBackward0>)
pred_loss tensor([9.7227], grad_fn=<MulBackward0>)
size_loss tensor(-10.0843, grad_fn=<MulBackward0>)
size_num_loss 0.6000000000000001
loss: tensor([1.3050], grad_fn=<AddBackward0>)
1
epoch:  1 ; loss:  1.3049927949905396 ; pred:  tensor([0.3782, 0.2374, 0.2592, 0.1252], grad_fn=<SoftmaxBackward0>)
num_high 6 len(mask) 7
mask_without_small tensor([0.7984, 0.7084, 0.7893, 0.7207, 0.5498, 0.6683, 0.9153],
       grad_fn=<IndexBackward0>)
pred_loss tensor([9.7138], grad_fn=<MulBackward0>)
size_loss tensor(-11.4840, grad_fn=<MulBackward0>)
size_num_loss 0.6000000000000001
loss: tensor([-0.1085], grad_fn=<AddBackward0>)
2
epoch:  2 ; loss:  -0.10845422744750977 ; pred:  tensor([0.3786, 0.2338, 0.2611, 0.1265], grad_fn=<SoftmaxBackward0>)
num_high 6 len(mask) 7
mask_without_small tensor([0.8136, 0.6899, 0.8049, 0.7032, 0.5249, 0.6458, 0.9226],
       grad_fn=<IndexBackward0>)
pred_loss tensor([9.7082], grad_fn=<MulBackward0>)
size_loss tensor(-12.9839, grad_fn=<MulBackward0>)
size_num_loss 0.6000000000000001
loss: tensor([-1.6200], grad_fn=<AddBackward0>)
3
epoch:  3 ; loss:  -1.6200060844421387 ; pred:  tensor([0.3788, 0.2302, 0.2630, 0.1281], grad_fn=<SoftmaxBackward0>)
num_high 6 len(mask) 7
mask_without_small tensor([0.8280, 0.6700, 0.8198, 0.6842, 0.5000, 0.6225, 0.9292],
       grad_fn=<IndexBackward0>)
pred_loss tensor([9.7061], grad_fn=<MulBackward0>)
size_loss tensor(-14.5520, grad_fn=<MulBackward0>)
size_num_loss 0.6000000000000001
loss: tensor([-3.1976], grad_fn=<AddBackward0>)
4
epoch:  4 ; loss:  -3.1976232528686523 ; pred:  tensor([0.3789, 0.2266, 0.2647, 0.1298], grad_fn=<SoftmaxBackward0>)
num_high 5 len(mask) 7
mask_without_small tensor([0.8417, 0.6490, 0.8340, 0.6639, 0.5986, 0.9352],
       grad_fn=<IndexBackward0>)
pred_loss tensor([9.7075], grad_fn=<MulBackward0>)
size_loss tensor(-1343.4264, grad_fn=<MulBackward0>)
size_num_loss 0.5
loss: tensor([-1332.6339], grad_fn=<AddBackward0>)
5
epoch:  5 ; loss:  -1332.6339111328125 ; pred:  tensor([0.3788, 0.2232, 0.2663, 0.1317], grad_fn=<SoftmaxBackward0>)
num_high 5 len(mask) 7
mask_without_small tensor([0.8487, 0.6369, 0.8413, 0.6520, 0.5858, 0.9384],
       grad_fn=<IndexBackward0>)
pred_loss tensor([9.7167], grad_fn=<MulBackward0>)
size_loss tensor(-1434.8967, grad_fn=<MulBackward0>)
size_num_loss 0.5
loss: tensor([-1424.0975], grad_fn=<AddBackward0>)
6
epoch:  6 ; loss:  -1424.0975341796875 ; pred:  tensor([0.3785, 0.2208, 0.2679, 0.1329], grad_fn=<SoftmaxBackward0>)
num_high 4 len(mask) 7
mask_without_small tensor([0.8573, 0.6210, 0.8503, 0.6363, 0.5691, 0.9423],
       grad_fn=<IndexBackward0>)
pred_loss tensor([9.7211], grad_fn=<MulBackward0>)
size_loss tensor(-1554.2592, grad_fn=<MulBackward0>)
size_num_loss 0.4
loss: tensor([-1543.5590], grad_fn=<AddBackward0>)
7
epoch:  7 ; loss:  -1543.5589599609375 ; pred:  tensor([0.3783, 0.2184, 0.2689, 0.1343], grad_fn=<SoftmaxBackward0>)
num_high 3 len(mask) 7
mask_without_small tensor([0.8666, 0.6024, 0.8600, 0.6181, 0.5499, 0.9464],
       grad_fn=<IndexBackward0>)
pred_loss tensor([9.7230], grad_fn=<MulBackward0>)
size_loss tensor(-1691.0375, grad_fn=<MulBackward0>)
size_num_loss 0.30000000000000004
loss: tensor([-1680.4397], grad_fn=<AddBackward0>)
8
epoch:  8 ; loss:  -1680.439697265625 ; pred:  tensor([0.3782, 0.2162, 0.2696, 0.1361], grad_fn=<SoftmaxBackward0>)
num_high 3 len(mask) 7
mask_without_small tensor([0.8761, 0.5820, 0.8699, 0.5979, 0.5289, 0.9505],
       grad_fn=<IndexBackward0>)
pred_loss tensor([9.7239], grad_fn=<MulBackward0>)
size_loss tensor(-1839.6173, grad_fn=<MulBackward0>)
size_num_loss 0.30000000000000004
loss: tensor([-1829.0238], grad_fn=<AddBackward0>)
9
epoch:  9 ; loss:  -1829.0238037109375 ; pred:  tensor([0.3782, 0.2140, 0.2698, 0.1380], grad_fn=<SoftmaxBackward0>)
num_high 3 len(mask) 7
mask_without_small tensor([0.8854, 0.5601, 0.8797, 0.5762, 0.5067, 0.9544],
       grad_fn=<IndexBackward0>)
pred_loss tensor([9.7243], grad_fn=<MulBackward0>)
size_loss tensor(-1996.2838, grad_fn=<MulBackward0>)
size_num_loss 0.30000000000000004
loss: tensor([-1985.6958], grad_fn=<AddBackward0>)
10
epoch:  10 ; loss:  -1985.69580078125 ; pred:  tensor([0.3782, 0.2120, 0.2697, 0.1401], grad_fn=<SoftmaxBackward0>)
num_high 3 len(mask) 7
mask_without_small tensor([0.8945, 0.5371, 0.8891, 0.5533, 0.9581], grad_fn=<IndexBackward0>)
pred_loss tensor([9.7251], grad_fn=<MulBackward0>)
size_loss tensor(-2038.6385, grad_fn=<MulBackward0>)
size_num_loss 0.30000000000000004
loss: tensor([-2028.0613], grad_fn=<AddBackward0>)
11
epoch:  11 ; loss:  -2028.061279296875 ; pred:  tensor([0.3781, 0.2102, 0.2694, 0.1423], grad_fn=<SoftmaxBackward0>)
num_high 3 len(mask) 7
mask_without_small tensor([0.9030, 0.5132, 0.8981, 0.5295, 0.9616], grad_fn=<IndexBackward0>)
pred_loss tensor([9.7245], grad_fn=<MulBackward0>)
size_loss tensor(-2203.6101, grad_fn=<MulBackward0>)
size_num_loss 0.30000000000000004
loss: tensor([-2193.0408], grad_fn=<AddBackward0>)
12
epoch:  12 ; loss:  -2193.040771484375 ; pred:  tensor([0.3782, 0.2084, 0.2688, 0.1446], grad_fn=<SoftmaxBackward0>)
num_high 3 len(mask) 7
mask_without_small tensor([0.9111, 0.9066, 0.5049, 0.9648], grad_fn=<IndexBackward0>)
pred_loss tensor([9.7230], grad_fn=<MulBackward0>)
size_loss tensor(-2129.6658, grad_fn=<MulBackward0>)
size_num_loss 0.30000000000000004
loss: tensor([-2119.1108], grad_fn=<AddBackward0>)
13
epoch:  13 ; loss:  -2119.11083984375 ; pred:  tensor([0.3782, 0.2068, 0.2679, 0.1471], grad_fn=<SoftmaxBackward0>)
num_high 3 len(mask) 7
mask_without_small tensor([0.9185, 0.9144, 0.9678], grad_fn=<IndexBackward0>)
pred_loss tensor([9.7197], grad_fn=<MulBackward0>)
size_loss tensor(-296.9228, grad_fn=<MulBackward0>)
size_num_loss 0.30000000000000004
loss: tensor([9.7197], grad_fn=<MulBackward0>)
14
epoch:  14 ; loss:  9.719718933105469 ; pred:  tensor([0.3783, 0.2054, 0.2669, 0.1494], grad_fn=<SoftmaxBackward0>)
num_high 3 len(mask) 7
mask_without_small tensor([0.9247, 0.9209, 0.9702], grad_fn=<IndexBackward0>)
pred_loss tensor([9.7180], grad_fn=<MulBackward0>)
size_loss tensor(-274.1487, grad_fn=<MulBackward0>)
size_num_loss 0.30000000000000004
loss: tensor([9.7180], grad_fn=<MulBackward0>)
15
epoch:  15 ; loss:  9.718015670776367 ; pred:  tensor([0.3784, 0.2041, 0.2660, 0.1515], grad_fn=<SoftmaxBackward0>)
num_high 3 len(mask) 7
mask_without_small tensor([0.9300, 0.9265, 0.9723], grad_fn=<IndexBackward0>)
pred_loss tensor([9.7174], grad_fn=<MulBackward0>)
size_loss tensor(-254.8566, grad_fn=<MulBackward0>)
size_num_loss 0.30000000000000004
loss: tensor([9.7174], grad_fn=<MulBackward0>)
16
epoch:  16 ; loss:  9.71738052368164 ; pred:  tensor([0.3784, 0.2030, 0.2651, 0.1535], grad_fn=<SoftmaxBackward0>)
num_high 3 len(mask) 7
mask_without_small tensor([0.9345, 0.9312, 0.9740], grad_fn=<IndexBackward0>)
pred_loss tensor([9.7175], grad_fn=<MulBackward0>)
size_loss tensor(-238.4115, grad_fn=<MulBackward0>)
size_num_loss 0.30000000000000004
loss: tensor([9.7175], grad_fn=<MulBackward0>)
17
epoch:  17 ; loss:  9.717455863952637 ; pred:  tensor([0.3784, 0.2020, 0.2643, 0.1552], grad_fn=<SoftmaxBackward0>)
num_high 3 len(mask) 7
mask_without_small tensor([0.9383, 0.9352, 0.9755], grad_fn=<IndexBackward0>)
pred_loss tensor([9.7180], grad_fn=<MulBackward0>)
size_loss tensor(-224.3121, grad_fn=<MulBackward0>)
size_num_loss 0.30000000000000004
loss: tensor([9.7180], grad_fn=<MulBackward0>)
18
epoch:  18 ; loss:  9.717979431152344 ; pred:  tensor([0.3784, 0.2012, 0.2636, 0.1568], grad_fn=<SoftmaxBackward0>)
num_high 3 len(mask) 7
mask_without_small tensor([0.9416, 0.9387, 0.9768], grad_fn=<IndexBackward0>)
pred_loss tensor([9.7188], grad_fn=<MulBackward0>)
size_loss tensor(-212.1597, grad_fn=<MulBackward0>)
size_num_loss 0.30000000000000004
loss: tensor([9.7188], grad_fn=<MulBackward0>)
19
epoch:  19 ; loss:  9.718759536743164 ; pred:  tensor([0.3784, 0.2005, 0.2629, 0.1583], grad_fn=<SoftmaxBackward0>)
Finished Training
sel masked ver tensor(indices=tensor([[7327, 5724, 5724, 5724, 5724],
                       [5724,    0, 3162,    0, 5230]]),
       values=tensor([0.9416, 1.5369, 0.9387, 1.3630, 0.9768]),
       size=(7328, 5725), nnz=5, layout=torch.sparse_coo,
       grad_fn=<SparseCooTensorWithDimsAndTensorsBackward0>)
Counter({'fax': 1, 'phone': 1, 'homepage': 1, 'type': 1, 'author': 1})
dict index: {}
node_idx 5724
 node original label [0]
 node predicted label explain 0
 node prediction probability explain tensor([0.4025, 0.2226, 0.2418, 0.1331], grad_fn=<SoftmaxBackward0>)
 node predicted label full 0 most important relations  {'fax': 1, 'phone': 1, 'homepage': 1, 'type': 1, 'author': 1, 'label': 0, 'node_idx': '5724'}
 final masks and lenght tensor(indices=tensor([[ 23897,  88574, 129999, 196279, 229419, 254274, 328839],
                       [  5724,      0,   3162,   4552,      0,   7327,   5230]]),
       values=tensor([0.9416, 1.5369, 0.9387, 0.3910, 1.3630, 0.3668, 0.9768]),
       size=(753935, 8285), nnz=7, layout=torch.sparse_coo,
       grad_fn=<SparseCooTensorWithDimsAndTensorsBackward0>) 5
 ---------------------------------------------------------------
r_exp.py:102: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  hor_graph, ver_graph = torch.tensor(sub_hor_graph, dtype=torch.float), torch.tensor(sub_ver_graph, dtype=torch.float)
wandb: WARNING Calling wandb.login() after wandb.init() has no effect.
node label: 0
4
num_high 4 len(mask) 5
mask_without_small tensor([0.7708, 0.7468, 0.7592, 0.7587, 0.5720], grad_fn=<IndexBackward0>)
pred_loss tensor([10.1398], grad_fn=<MulBackward0>)
size_loss tensor(-8.4024, grad_fn=<MulBackward0>)
size_num_loss 0.4
loss: tensor([3.0764], grad_fn=<AddBackward0>)
0
epoch:  0 ; loss:  3.0763533115386963 ; pred:  tensor([0.3628, 0.2503, 0.2652, 0.1217], grad_fn=<SoftmaxBackward0>)
num_high 4 len(mask) 5
mask_without_small tensor([0.7880, 0.7652, 0.7770, 0.7766, 0.5473], grad_fn=<IndexBackward0>)
pred_loss tensor([10.1078], grad_fn=<MulBackward0>)
size_loss tensor(-10.2895, grad_fn=<MulBackward0>)
size_num_loss 0.4

loss: tensor([1.1461], grad_fn=<AddBackward0>)
1
epoch:  1 ; loss:  1.1460683345794678 ; pred:  tensor([0.3639, 0.2503, 0.2663, 0.1195], grad_fn=<SoftmaxBackward0>)
num_high 4 len(mask) 5
mask_without_small tensor([0.8042, 0.7827, 0.7938, 0.7934, 0.5224], grad_fn=<IndexBackward0>)
pred_loss tensor([10.0787], grad_fn=<MulBackward0>)
size_loss tensor(-12.1468, grad_fn=<MulBackward0>)
size_num_loss 0.4
loss: tensor([-0.7528], grad_fn=<AddBackward0>)
2
epoch:  2 ; loss:  -0.7527912855148315 ; pred:  tensor([0.3650, 0.2502, 0.2675, 0.1173], grad_fn=<SoftmaxBackward0>)
num_high 4 len(mask) 5
mask_without_small tensor([0.8193, 0.7992, 0.8096, 0.8092], grad_fn=<IndexBackward0>)
pred_loss tensor([10.0524], grad_fn=<MulBackward0>)
size_loss tensor(-81.6772, grad_fn=<MulBackward0>)
size_num_loss 0.4
loss: tensor([-70.6643], grad_fn=<AddBackward0>)
3
epoch:  3 ; loss:  -70.66426086425781 ; pred:  tensor([0.3660, 0.2502, 0.2685, 0.1153], grad_fn=<SoftmaxBackward0>)
num_high 4 len(mask) 5
mask_without_small tensor([0.8278, 0.7898, 0.8203, 0.8067], grad_fn=<IndexBackward0>)
pred_loss tensor([10.0873], grad_fn=<MulBackward0>)
size_loss tensor(-166.7864, grad_fn=<MulBackward0>)
size_num_loss 0.4
loss: tensor([-155.7412], grad_fn=<AddBackward0>)
4
epoch:  4 ; loss:  -155.74119567871094 ; pred:  tensor([0.3647, 0.2509, 0.2699, 0.1145], grad_fn=<SoftmaxBackward0>)
num_high 4 len(mask) 5
mask_without_small tensor([0.8379, 0.7774, 0.8287, 0.7979], grad_fn=<IndexBackward0>)
pred_loss tensor([10.1287], grad_fn=<MulBackward0>)
size_loss tensor(-279.0013, grad_fn=<MulBackward0>)
size_num_loss 0.4
loss: tensor([-267.9153], grad_fn=<AddBackward0>)
5
epoch:  5 ; loss:  -267.91534423828125 ; pred:  tensor([0.3632, 0.2513, 0.2716, 0.1139], grad_fn=<SoftmaxBackward0>)
num_high 4 len(mask) 5
mask_without_small tensor([0.8485, 0.7630, 0.8387, 0.7866], grad_fn=<IndexBackward0>)
pred_loss tensor([10.1767], grad_fn=<MulBackward0>)
size_loss tensor(-411.0756, grad_fn=<MulBackward0>)
size_num_loss 0.4
loss: tensor([-399.9425], grad_fn=<AddBackward0>)
6
epoch:  6 ; loss:  -399.9425048828125 ; pred:  tensor([0.3614, 0.2517, 0.2736, 0.1133], grad_fn=<SoftmaxBackward0>)
num_high 4 len(mask) 5
mask_without_small tensor([0.8591, 0.7469, 0.8494, 0.7732], grad_fn=<IndexBackward0>)
pred_loss tensor([10.2296], grad_fn=<MulBackward0>)
size_loss tensor(-556.0496, grad_fn=<MulBackward0>)
size_num_loss 0.4
loss: tensor([-544.8646], grad_fn=<AddBackward0>)
7
epoch:  7 ; loss:  -544.8645629882812 ; pred:  tensor([0.3595, 0.2520, 0.2758, 0.1127], grad_fn=<SoftmaxBackward0>)
num_high 4 len(mask) 5
mask_without_small tensor([0.8694, 0.7293, 0.8602, 0.7580], grad_fn=<IndexBackward0>)
pred_loss tensor([10.2867], grad_fn=<MulBackward0>)
size_loss tensor(-710.0989, grad_fn=<MulBackward0>)
size_num_loss 0.4
loss: tensor([-698.8580], grad_fn=<AddBackward0>)
8
epoch:  8 ; loss:  -698.8579711914062 ; pred:  tensor([0.3575, 0.2524, 0.2780, 0.1122], grad_fn=<SoftmaxBackward0>)
num_high 4 len(mask) 5
mask_without_small tensor([0.8793, 0.7105, 0.8708, 0.7413], grad_fn=<IndexBackward0>)
pred_loss tensor([10.3474], grad_fn=<MulBackward0>)
size_loss tensor(-870.9395, grad_fn=<MulBackward0>)
size_num_loss 0.4
loss: tensor([-859.6390], grad_fn=<AddBackward0>)
9
epoch:  9 ; loss:  -859.6390380859375 ; pred:  tensor([0.3553, 0.2527, 0.2802, 0.1117], grad_fn=<SoftmaxBackward0>)
num_high 4 len(mask) 5
mask_without_small tensor([0.8885, 0.6904, 0.8809, 0.7230], grad_fn=<IndexBackward0>)
pred_loss tensor([10.4116], grad_fn=<MulBackward0>)
size_loss tensor(-1037.1072, grad_fn=<MulBackward0>)
size_num_loss 0.4
loss: tensor([-1025.7440], grad_fn=<AddBackward0>)
10
epoch:  10 ; loss:  -1025.7440185546875 ; pred:  tensor([0.3530, 0.2530, 0.2826, 0.1114], grad_fn=<SoftmaxBackward0>)
num_high 4 len(mask) 5
mask_without_small tensor([0.8971, 0.6691, 0.8906, 0.7032], grad_fn=<IndexBackward0>)
pred_loss tensor([10.4790], grad_fn=<MulBackward0>)
size_loss tensor(-1207.5941, grad_fn=<MulBackward0>)
size_num_loss 0.4
loss: tensor([-1196.1653], grad_fn=<AddBackward0>)
11
epoch:  11 ; loss:  -1196.165283203125 ; pred:  tensor([0.3507, 0.2533, 0.2849, 0.1111], grad_fn=<SoftmaxBackward0>)
num_high 4 len(mask) 5
mask_without_small tensor([0.9051, 0.6468, 0.8997, 0.6821], grad_fn=<IndexBackward0>)
pred_loss tensor([10.5493], grad_fn=<MulBackward0>)
size_loss tensor(-1381.6484, grad_fn=<MulBackward0>)
size_num_loss 0.4
loss: tensor([-1370.1511], grad_fn=<AddBackward0>)
12
epoch:  12 ; loss:  -1370.151123046875 ; pred:  tensor([0.3482, 0.2536, 0.2873, 0.1109], grad_fn=<SoftmaxBackward0>)
num_high 4 len(mask) 5
mask_without_small tensor([0.9125, 0.6235, 0.9082, 0.6597], grad_fn=<IndexBackward0>)
pred_loss tensor([10.6224], grad_fn=<MulBackward0>)
size_loss tensor(-1558.6498, grad_fn=<MulBackward0>)
size_num_loss 0.4
loss: tensor([-1547.0817], grad_fn=<AddBackward0>)
13
epoch:  13 ; loss:  -1547.0816650390625 ; pred:  tensor([0.3457, 0.2538, 0.2898, 0.1108], grad_fn=<SoftmaxBackward0>)
num_high 3 len(mask) 5
mask_without_small tensor([0.9193, 0.5994, 0.9160, 0.6360], grad_fn=<IndexBackward0>)
pred_loss tensor([10.6980], grad_fn=<MulBackward0>)
size_loss tensor(-1738.0269, grad_fn=<MulBackward0>)
size_num_loss 0.30000000000000004
loss: tensor([-1726.4857], grad_fn=<AddBackward0>)
14
epoch:  14 ; loss:  -1726.4857177734375 ; pred:  tensor([0.3431, 0.2539, 0.2923, 0.1107], grad_fn=<SoftmaxBackward0>)
num_high 2 len(mask) 5
mask_without_small tensor([0.9255, 0.5746, 0.9232, 0.6113], grad_fn=<IndexBackward0>)
pred_loss tensor([10.7758], grad_fn=<MulBackward0>)
size_loss tensor(-1919.2047, grad_fn=<MulBackward0>)
size_num_loss 0.2
loss: tensor([-1907.6891], grad_fn=<AddBackward0>)
15
epoch:  15 ; loss:  -1907.6890869140625 ; pred:  tensor([0.3404, 0.2540, 0.2948, 0.1108], grad_fn=<SoftmaxBackward0>)
num_high 2 len(mask) 5
mask_without_small tensor([0.9312, 0.5492, 0.9297, 0.5856], grad_fn=<IndexBackward0>)
pred_loss tensor([10.8554], grad_fn=<MulBackward0>)
size_loss tensor(-2101.5671, grad_fn=<MulBackward0>)
size_num_loss 0.2
loss: tensor([-2089.9756], grad_fn=<AddBackward0>)
16
epoch:  16 ; loss:  -2089.9755859375 ; pred:  tensor([0.3377, 0.2541, 0.2973, 0.1109], grad_fn=<SoftmaxBackward0>)
num_high 2 len(mask) 5
mask_without_small tensor([0.9364, 0.5233, 0.9357, 0.5590], grad_fn=<IndexBackward0>)
pred_loss tensor([10.9365], grad_fn=<MulBackward0>)
size_loss tensor(-2284.4333, grad_fn=<MulBackward0>)
size_num_loss 0.2
loss: tensor([-2272.7651], grad_fn=<AddBackward0>)
17
epoch:  17 ; loss:  -2272.76513671875 ; pred:  tensor([0.3350, 0.2541, 0.2998, 0.1110], grad_fn=<SoftmaxBackward0>)
num_high 2 len(mask) 5
mask_without_small tensor([0.9411, 0.9411, 0.5318], grad_fn=<IndexBackward0>)
pred_loss tensor([11.0187], grad_fn=<MulBackward0>)
size_loss tensor(-2363.1477, grad_fn=<MulBackward0>)
size_num_loss 0.2
loss: tensor([11.0187], grad_fn=<MulBackward0>)
18
epoch:  18 ; loss:  11.018733978271484 ; pred:  tensor([0.3322, 0.2541, 0.3024, 0.1112], grad_fn=<SoftmaxBackward0>)
num_high 2 len(mask) 5
mask_without_small tensor([0.9451, 0.9457, 0.5069], grad_fn=<IndexBackward0>)
pred_loss tensor([11.0937], grad_fn=<MulBackward0>)
size_loss tensor(-2531.6707, grad_fn=<MulBackward0>)
size_num_loss 0.2
loss: tensor([11.0937], grad_fn=<MulBackward0>)
19
epoch:  19 ; loss:  11.093650817871094 ; pred:  tensor([0.3298, 0.2541, 0.3047, 0.1114], grad_fn=<SoftmaxBackward0>)
Finished Training
sel masked ver tensor(indices=tensor([[5699, 5699, 5699],
                       [   0, 3153,    0]]),
       values=tensor([3.7802, 0.9457, 2.0274]),
       size=(5700, 3154), nnz=3, layout=torch.sparse_coo,
       grad_fn=<SparseCooTensorWithDimsAndTensorsBackward0>)
Counter({'fax': 1, 'phone': 1, 'name': 1})
dict index: {}
node_idx 5699
 node original label [0]
 node predicted label explain 0
 node prediction probability explain tensor([0.3631, 0.2871, 0.2776, 0.0723], grad_fn=<SoftmaxBackward0>)
 node predicted label full 0 most important relations  {'fax': 1, 'phone': 1, 'name': 1, 'label': 0, 'node_idx': '5699'}
 final masks and lenght tensor(indices=tensor([[ 88549, 129974, 196254, 229394, 328814],
                       [     0,   3162,   3153,      0,   5230]]),
       values=tensor([3.7802, 0.4735, 0.9457, 2.0274, 0.3617]),
       size=(753935, 8285), nnz=5, layout=torch.sparse_coo,
       grad_fn=<SparseCooTensorWithDimsAndTensorsBackward0>) 3
 ---------------------------------------------------------------
node label: 0
3
r_exp.py:102: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  hor_graph, ver_graph = torch.tensor(sub_hor_graph, dtype=torch.float), torch.tensor(sub_ver_graph, dtype=torch.float)
wandb: WARNING Calling wandb.login() after wandb.init() has no effect.
num_high 4 len(mask) 4
mask_without_small tensor([0.7752, 0.7486, 0.7624, 0.7619], grad_fn=<IndexBackward0>)
pred_loss tensor([11.5813], grad_fn=<MulBackward0>)
size_loss tensor(-1.0880, grad_fn=<MulBackward0>)
size_num_loss 0.4
loss: tensor([11.7466], grad_fn=<AddBackward0>)
0
epoch:  0 ; loss:  11.746602058410645 ; pred:  tensor([0.3141, 0.3214, 0.2250, 0.1395], grad_fn=<SoftmaxBackward0>)
num_high 4 len(mask) 4
mask_without_small tensor([0.7922, 0.7293, 0.7800, 0.7432], grad_fn=<IndexBackward0>)
pred_loss tensor([11.5459], grad_fn=<MulBackward0>)
size_loss tensor(-2.9742, grad_fn=<MulBackward0>)
size_num_loss 0.4
loss: tensor([9.8241], grad_fn=<AddBackward0>)
1
epoch:  1 ; loss:  9.824122428894043 ; pred:  tensor([0.3152, 0.3225, 0.2235, 0.1389], grad_fn=<SoftmaxBackward0>)
num_high 4 len(mask) 4
mask_without_small tensor([0.8080, 0.7092, 0.7932, 0.7285], grad_fn=<IndexBackward0>)
pred_loss tensor([11.5118], grad_fn=<MulBackward0>)
size_loss tensor(-4.8238, grad_fn=<MulBackward0>)
size_num_loss 0.4
loss: tensor([7.9385], grad_fn=<AddBackward0>)
2
epoch:  2 ; loss:  7.938486576080322 ; pred:  tensor([0.3163, 0.3231, 0.2222, 0.1384], grad_fn=<SoftmaxBackward0>)
num_high 4 len(mask) 4
mask_without_small tensor([0.8227, 0.6882, 0.8073, 0.7110], grad_fn=<IndexBackward0>)
pred_loss tensor([11.4784], grad_fn=<MulBackward0>)
size_loss tensor(-6.7578, grad_fn=<MulBackward0>)
size_num_loss 0.4
loss: tensor([5.9683], grad_fn=<AddBackward0>)
3
epoch:  3 ; loss:  5.968287467956543 ; pred:  tensor([0.3173, 0.3238, 0.2208, 0.1380], grad_fn=<SoftmaxBackward0>)
num_high 4 len(mask) 4
mask_without_small tensor([0.8365, 0.6664, 0.8212, 0.6917], grad_fn=<IndexBackward0>)
pred_loss tensor([11.4460], grad_fn=<MulBackward0>)
size_loss tensor(-8.7316, grad_fn=<MulBackward0>)
size_num_loss 0.4
loss: tensor([3.9584], grad_fn=<AddBackward0>)
4
epoch:  4 ; loss:  3.9583818912506104 ; pred:  tensor([0.3183, 0.3245, 0.2194, 0.1377], grad_fn=<SoftmaxBackward0>)
num_high 4 len(mask) 4
mask_without_small tensor([0.8492, 0.6438, 0.8348, 0.6711], grad_fn=<IndexBackward0>)
pred_loss tensor([11.4147], grad_fn=<MulBackward0>)
size_loss tensor(-10.7248, grad_fn=<MulBackward0>)
size_num_loss 0.4
loss: tensor([1.9290], grad_fn=<AddBackward0>)
5
epoch:  5 ; loss:  1.9290106296539307 ; pred:  tensor([0.3193, 0.3251, 0.2181, 0.1375], grad_fn=<SoftmaxBackward0>)
num_high 3 len(mask) 4
mask_without_small tensor([0.8609, 0.6206, 0.8476, 0.6494], grad_fn=<IndexBackward0>)
pred_loss tensor([11.3845], grad_fn=<MulBackward0>)
size_loss tensor(-12.7257, grad_fn=<MulBackward0>)
size_num_loss 0.30000000000000004
loss: tensor([-0.2080], grad_fn=<AddBackward0>)
6
epoch:  6 ; loss:  -0.2079809606075287 ; pred:  tensor([0.3203, 0.3256, 0.2167, 0.1373], grad_fn=<SoftmaxBackward0>)
num_high 3 len(mask) 4
mask_without_small tensor([0.8717, 0.5967, 0.8598, 0.6266], grad_fn=<IndexBackward0>)
pred_loss tensor([11.3553], grad_fn=<MulBackward0>)
size_loss tensor(-14.7260, grad_fn=<MulBackward0>)
size_num_loss 0.30000000000000004
loss: tensor([-2.2443], grad_fn=<AddBackward0>)
7
epoch:  7 ; loss:  -2.2442703247070312 ; pred:  tensor([0.3213, 0.3261, 0.2154, 0.1372], grad_fn=<SoftmaxBackward0>)
num_high 2 len(mask) 4
mask_without_small tensor([0.8816, 0.5724, 0.8711, 0.6030], grad_fn=<IndexBackward0>)
pred_loss tensor([11.3273], grad_fn=<MulBackward0>)
size_loss tensor(-16.7188, grad_fn=<MulBackward0>)
size_num_loss 0.2
loss: tensor([-4.3731], grad_fn=<AddBackward0>)
8
epoch:  8 ; loss:  -4.373109340667725 ; pred:  tensor([0.3222, 0.3264, 0.2142, 0.1372], grad_fn=<SoftmaxBackward0>)
num_high 2 len(mask) 4
mask_without_small tensor([0.8907, 0.5476, 0.8816, 0.5786], grad_fn=<IndexBackward0>)
pred_loss tensor([11.3004], grad_fn=<MulBackward0>)
size_loss tensor(-18.6982, grad_fn=<MulBackward0>)
size_num_loss 0.2
loss: tensor([-6.3884], grad_fn=<AddBackward0>)
9
epoch:  9 ; loss:  -6.388400554656982 ; pred:  tensor([0.3230, 0.3267, 0.2130, 0.1372], grad_fn=<SoftmaxBackward0>)
num_high 2 len(mask) 4
mask_without_small tensor([0.8990, 0.5226, 0.8913, 0.5535], grad_fn=<IndexBackward0>)
pred_loss tensor([11.2746], grad_fn=<MulBackward0>)
size_loss tensor(-20.6582, grad_fn=<MulBackward0>)
size_num_loss 0.2
loss: tensor([-8.3843], grad_fn=<AddBackward0>)
10
epoch:  10 ; loss:  -8.38425064086914 ; pred:  tensor([0.3239, 0.3269, 0.2119, 0.1373], grad_fn=<SoftmaxBackward0>)
num_high 2 len(mask) 4
mask_without_small tensor([0.9066, 0.9003, 0.5280], grad_fn=<IndexBackward0>)
pred_loss tensor([11.2500], grad_fn=<MulBackward0>)
size_loss tensor(-2167.6040, grad_fn=<MulBackward0>)
size_num_loss 0.2
loss: tensor([11.2500], grad_fn=<MulBackward0>)
11
epoch:  11 ; loss:  11.249991416931152 ; pred:  tensor([0.3247, 0.3270, 0.2109, 0.1375], grad_fn=<SoftmaxBackward0>)
num_high 2 len(mask) 4
mask_without_small tensor([0.9129, 0.9077, 0.5050], grad_fn=<IndexBackward0>)
pred_loss tensor([11.2285], grad_fn=<MulBackward0>)
size_loss tensor(-2340.0369, grad_fn=<MulBackward0>)
size_num_loss 0.2
loss: tensor([11.2285], grad_fn=<MulBackward0>)
12
epoch:  12 ; loss:  11.228537559509277 ; pred:  tensor([0.3254, 0.3271, 0.2100, 0.1376], grad_fn=<SoftmaxBackward0>)
num_high 2 len(mask) 4
mask_without_small tensor([0.9182, 0.9138], grad_fn=<IndexBackward0>)
pred_loss tensor([11.2098], grad_fn=<MulBackward0>)
size_loss tensor(-30.9219, grad_fn=<MulBackward0>)
size_num_loss 0.2
loss: tensor([11.2098], grad_fn=<MulBackward0>)
13
epoch:  13 ; loss:  11.209758758544922 ; pred:  tensor([0.3260, 0.3271, 0.2092, 0.1378], grad_fn=<SoftmaxBackward0>)
num_high 2 len(mask) 4
mask_without_small tensor([0.9227, 0.9190], grad_fn=<IndexBackward0>)
pred_loss tensor([11.1933], grad_fn=<MulBackward0>)
size_loss tensor(-26.1816, grad_fn=<MulBackward0>)
size_num_loss 0.2
loss: tensor([11.1933], grad_fn=<MulBackward0>)
14
epoch:  14 ; loss:  11.193252563476562 ; pred:  tensor([0.3265, 0.3270, 0.2085, 0.1380], grad_fn=<SoftmaxBackward0>)
num_high 2 len(mask) 4
mask_without_small tensor([0.9266, 0.9234], grad_fn=<IndexBackward0>)
pred_loss tensor([11.1787], grad_fn=<MulBackward0>)
size_loss tensor(-22.4145, grad_fn=<MulBackward0>)
size_num_loss 0.2
loss: tensor([11.1787], grad_fn=<MulBackward0>)
15
epoch:  15 ; loss:  11.178685188293457 ; pred:  tensor([0.3270, 0.3270, 0.2079, 0.1381], grad_fn=<SoftmaxBackward0>)
num_high 2 len(mask) 4
mask_without_small tensor([0.9299, 0.9271], grad_fn=<IndexBackward0>)
pred_loss tensor([11.1658], grad_fn=<MulBackward0>)
size_loss tensor(-19.4263, grad_fn=<MulBackward0>)
size_num_loss 0.2
loss: tensor([11.1658], grad_fn=<MulBackward0>)
16
epoch:  16 ; loss:  11.16578483581543 ; pred:  tensor([0.3274, 0.3269, 0.2074, 0.1383], grad_fn=<SoftmaxBackward0>)
num_high 2 len(mask) 4
mask_without_small tensor([0.9327, 0.9303], grad_fn=<IndexBackward0>)
pred_loss tensor([11.1543], grad_fn=<MulBackward0>)
size_loss tensor(-17.0686, grad_fn=<MulBackward0>)
size_num_loss 0.2
loss: tensor([11.1543], grad_fn=<MulBackward0>)
17
epoch:  17 ; loss:  11.15432357788086 ; pred:  tensor([0.3278, 0.3268, 0.2070, 0.1385], grad_fn=<SoftmaxBackward0>)
num_high 2 len(mask) 4
mask_without_small tensor([0.9351, 0.9330], grad_fn=<IndexBackward0>)
pred_loss tensor([11.1441], grad_fn=<MulBackward0>)
size_loss tensor(-15.2260, grad_fn=<MulBackward0>)
size_num_loss 0.2
loss: tensor([11.1441], grad_fn=<MulBackward0>)
18
epoch:  18 ; loss:  11.144112586975098 ; pred:  tensor([0.3281, 0.3267, 0.2066, 0.1386], grad_fn=<SoftmaxBackward0>)
num_high 2 len(mask) 4
mask_without_small tensor([0.9372, 0.9353], grad_fn=<IndexBackward0>)
pred_loss tensor([11.1350], grad_fn=<MulBackward0>)
size_loss tensor(-13.8082, grad_fn=<MulBackward0>)
size_num_loss 0.2
loss: tensor([11.1350], grad_fn=<MulBackward0>)
19
epoch:  19 ; loss:  11.134992599487305 ; pred:  tensor([0.3284, 0.3266, 0.2062, 0.1388], grad_fn=<SoftmaxBackward0>)
Finished Training
sel masked ver tensor(indices=tensor([[5688, 5688],
                       [   0,    0]]),
       values=tensor([3.7488, 3.7410]),
       size=(5689, 1), nnz=2, layout=torch.sparse_coo,
       grad_fn=<SparseCooTensorWithDimsAndTensorsBackward0>)
Counter({'fax': 1, 'phone': 1})
dict index: {}
node_idx 5688
 node original label [0]
 node predicted label explain 1
 node prediction probability explain tensor([0.3590, 0.4067, 0.1501, 0.0843], grad_fn=<SoftmaxBackward0>)
 node predicted label full 0 most important relations  {'fax': 1, 'phone': 1, 'label': 0, 'node_idx': '5688'}
 final masks and lenght tensor(indices=tensor([[ 88538, 196243, 229383, 328803],
                       [     0,   1579,      0,   5230]]),
       values=tensor([3.7488, 0.3670, 3.7410, 0.3982]),
       size=(753935, 8285), nnz=4, layout=torch.sparse_coo,
       grad_fn=<SparseCooTensorWithDimsAndTensorsBackward0>) 2
 ---------------------------------------------------------------
node label: 0
4
num_high 5 len(mask) 6
mask_without_small tensor([0.7675, 0.7454, 0.7568, 0.7564, 0.5870, 0.7094],
       grad_fn=<IndexBackward0>)
pred_loss tensor([12.1878], grad_fn=<MulBackward0>)
size_loss tensor(-6.8377, grad_fn=<MulBackward0>)
size_num_loss 0.5
loss: tensor([6.8657], grad_fn=<AddBackward0>)
r_exp.py:102: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  hor_graph, ver_graph = torch.tensor(sub_hor_graph, dtype=torch.float), torch.tensor(sub_ver_graph, dtype=torch.float)
wandb: WARNING Calling wandb.login() after wandb.init() has no effect.
0
epoch:  0 ; loss:  6.865707874298096 ; pred:  tensor([0.2956, 0.3296, 0.2312, 0.1436], grad_fn=<SoftmaxBackward0>)
num_high 5 len(mask) 6
mask_without_small tensor([0.7849, 0.7639, 0.7748, 0.7743, 0.5626, 0.6884],
       grad_fn=<IndexBackward0>)
pred_loss tensor([12.1648], grad_fn=<MulBackward0>)
size_loss tensor(-8.6867, grad_fn=<MulBackward0>)
size_num_loss 0.5
loss: tensor([4.9864], grad_fn=<AddBackward0>)
1
epoch:  1 ; loss:  4.9863996505737305 ; pred:  tensor([0.2963, 0.3313, 0.2312, 0.1412], grad_fn=<SoftmaxBackward0>)
num_high 5 len(mask) 6
mask_without_small tensor([0.8013, 0.7815, 0.7917, 0.7913, 0.5379, 0.6680],
       grad_fn=<IndexBackward0>)
pred_loss tensor([12.1423], grad_fn=<MulBackward0>)
size_loss tensor(-10.5872, grad_fn=<MulBackward0>)
size_num_loss 0.5
loss: tensor([3.0545], grad_fn=<AddBackward0>)
2
epoch:  2 ; loss:  3.054490566253662 ; pred:  tensor([0.2969, 0.3330, 0.2312, 0.1389], grad_fn=<SoftmaxBackward0>)
num_high 5 len(mask) 6
mask_without_small tensor([0.8166, 0.7982, 0.8077, 0.8074, 0.5130, 0.6468],
       grad_fn=<IndexBackward0>)
pred_loss tensor([12.1208], grad_fn=<MulBackward0>)
size_loss tensor(-12.5040, grad_fn=<MulBackward0>)
size_num_loss 0.5
loss: tensor([1.1060], grad_fn=<AddBackward0>)
3
epoch:  3 ; loss:  1.105954647064209 ; pred:  tensor([0.2976, 0.3345, 0.2312, 0.1368], grad_fn=<SoftmaxBackward0>)
num_high 5 len(mask) 6
mask_without_small tensor([0.8310, 0.8139, 0.8228, 0.8224, 0.6246], grad_fn=<IndexBackward0>)
pred_loss tensor([12.1005], grad_fn=<MulBackward0>)
size_loss tensor(-887.1658, grad_fn=<MulBackward0>)
size_num_loss 0.5
loss: tensor([-873.9888], grad_fn=<AddBackward0>)
4
epoch:  4 ; loss:  -873.98876953125 ; pred:  tensor([0.2982, 0.3359, 0.2312, 0.1347], grad_fn=<SoftmaxBackward0>)
num_high 4 len(mask) 6
mask_without_small tensor([0.8387, 0.8223, 0.8308, 0.8305, 0.6116], grad_fn=<IndexBackward0>)
pred_loss tensor([12.0829], grad_fn=<MulBackward0>)
size_loss tensor(-980.9611, grad_fn=<MulBackward0>)
size_num_loss 0.4
loss: tensor([-967.9091], grad_fn=<AddBackward0>)
5
epoch:  5 ; loss:  -967.9091186523438 ; pred:  tensor([0.2987, 0.3365, 0.2312, 0.1336], grad_fn=<SoftmaxBackward0>)
num_high 4 len(mask) 6
mask_without_small tensor([0.8481, 0.8325, 0.8406, 0.8403, 0.5948], grad_fn=<IndexBackward0>)
pred_loss tensor([12.0691], grad_fn=<MulBackward0>)
size_loss tensor(-1099.7625, grad_fn=<MulBackward0>)
size_num_loss 0.4
loss: tensor([-1086.7340], grad_fn=<AddBackward0>)
6
epoch:  6 ; loss:  -1086.7340087890625 ; pred:  tensor([0.2991, 0.3374, 0.2313, 0.1322], grad_fn=<SoftmaxBackward0>)
num_high 4 len(mask) 6
mask_without_small tensor([0.8581, 0.8434, 0.8510, 0.8507, 0.5755], grad_fn=<IndexBackward0>)
pred_loss tensor([12.0584], grad_fn=<MulBackward0>)
size_loss tensor(-1232.3645, grad_fn=<MulBackward0>)
size_num_loss 0.4
loss: tensor([-1219.3579], grad_fn=<AddBackward0>)
7
epoch:  7 ; loss:  -1219.35791015625 ; pred:  tensor([0.2994, 0.3384, 0.2314, 0.1308], grad_fn=<SoftmaxBackward0>)
num_high 4 len(mask) 6
mask_without_small tensor([0.8681, 0.8544, 0.8615, 0.8613, 0.5545], grad_fn=<IndexBackward0>)
pred_loss tensor([12.0503], grad_fn=<MulBackward0>)
size_loss tensor(-1373.1823, grad_fn=<MulBackward0>)
size_num_loss 0.4
loss: tensor([-1360.1958], grad_fn=<AddBackward0>)
8
epoch:  8 ; loss:  -1360.19580078125 ; pred:  tensor([0.2997, 0.3394, 0.2316, 0.1294], grad_fn=<SoftmaxBackward0>)
num_high 4 len(mask) 6
mask_without_small tensor([0.8779, 0.8652, 0.8718, 0.8716, 0.5321], grad_fn=<IndexBackward0>)
pred_loss tensor([12.0445], grad_fn=<MulBackward0>)
size_loss tensor(-1518.7949, grad_fn=<MulBackward0>)
size_num_loss 0.4
loss: tensor([-1505.8270], grad_fn=<AddBackward0>)
9
epoch:  9 ; loss:  -1505.8270263671875 ; pred:  tensor([0.2999, 0.3403, 0.2318, 0.1280], grad_fn=<SoftmaxBackward0>)
num_high 4 len(mask) 6
mask_without_small tensor([0.8873, 0.8756, 0.8816, 0.8815, 0.5089], grad_fn=<IndexBackward0>)
pred_loss tensor([12.0406], grad_fn=<MulBackward0>)
size_loss tensor(-1666.8424, grad_fn=<MulBackward0>)
size_num_loss 0.4
loss: tensor([-1653.8915], grad_fn=<AddBackward0>)
10
epoch:  10 ; loss:  -1653.8914794921875 ; pred:  tensor([0.3000, 0.3413, 0.2321, 0.1266], grad_fn=<SoftmaxBackward0>)
num_high 4 len(mask) 6
mask_without_small tensor([0.8961, 0.8855, 0.8910, 0.8908], grad_fn=<IndexBackward0>)
pred_loss tensor([12.0384], grad_fn=<MulBackward0>)
size_loss tensor(-43.1920, grad_fn=<MulBackward0>)
size_num_loss 0.4
loss: tensor([-30.2617], grad_fn=<AddBackward0>)
11
epoch:  11 ; loss:  -30.261653900146484 ; pred:  tensor([0.3000, 0.3422, 0.2324, 0.1253], grad_fn=<SoftmaxBackward0>)
num_high 4 len(mask) 6
mask_without_small tensor([0.9045, 0.8871, 0.8989, 0.8986], grad_fn=<IndexBackward0>)
pred_loss tensor([12.0422], grad_fn=<MulBackward0>)
size_loss tensor(-73.1583, grad_fn=<MulBackward0>)
size_num_loss 0.4
loss: tensor([-60.2343], grad_fn=<AddBackward0>)
12
epoch:  12 ; loss:  -60.234283447265625 ; pred:  tensor([0.2999, 0.3430, 0.2325, 0.1246], grad_fn=<SoftmaxBackward0>)
num_high 4 len(mask) 6
mask_without_small tensor([0.9125, 0.8849, 0.9062, 0.9058], grad_fn=<IndexBackward0>)
pred_loss tensor([12.0485], grad_fn=<MulBackward0>)
size_loss tensor(-120.3842, grad_fn=<MulBackward0>)
size_num_loss 0.4
loss: tensor([-107.4627], grad_fn=<AddBackward0>)
13
epoch:  13 ; loss:  -107.46270751953125 ; pred:  tensor([0.2997, 0.3438, 0.2324, 0.1241], grad_fn=<SoftmaxBackward0>)
num_high 4 len(mask) 6
mask_without_small tensor([0.9201, 0.8804, 0.9131, 0.9126], grad_fn=<IndexBackward0>)
pred_loss tensor([12.0563], grad_fn=<MulBackward0>)
size_loss tensor(-177.6809, grad_fn=<MulBackward0>)
size_num_loss 0.4
loss: tensor([-164.7597], grad_fn=<AddBackward0>)
14
epoch:  14 ; loss:  -164.7596893310547 ; pred:  tensor([0.2995, 0.3445, 0.2322, 0.1237], grad_fn=<SoftmaxBackward0>)
num_high 4 len(mask) 6
mask_without_small tensor([0.9271, 0.8741, 0.9197, 0.9190], grad_fn=<IndexBackward0>)
pred_loss tensor([12.0652], grad_fn=<MulBackward0>)
size_loss tensor(-242.1076, grad_fn=<MulBackward0>)
size_num_loss 0.4
loss: tensor([-229.1848], grad_fn=<AddBackward0>)
15
epoch:  15 ; loss:  -229.18475341796875 ; pred:  tensor([0.2992, 0.3453, 0.2320, 0.1235], grad_fn=<SoftmaxBackward0>)
num_high 4 len(mask) 6
mask_without_small tensor([0.9335, 0.8661, 0.9259, 0.9251], grad_fn=<IndexBackward0>)
pred_loss tensor([12.0751], grad_fn=<MulBackward0>)
size_loss tensor(-312.3958, grad_fn=<MulBackward0>)
size_num_loss 0.4
loss: tensor([-299.4699], grad_fn=<AddBackward0>)
16
epoch:  16 ; loss:  -299.4698791503906 ; pred:  tensor([0.2989, 0.3460, 0.2317, 0.1234], grad_fn=<SoftmaxBackward0>)
num_high 4 len(mask) 6
mask_without_small tensor([0.9392, 0.8567, 0.9317, 0.9308], grad_fn=<IndexBackward0>)
pred_loss tensor([12.0859], grad_fn=<MulBackward0>)
size_loss tensor(-388.0402, grad_fn=<MulBackward0>)
size_num_loss 0.4
loss: tensor([-375.1095], grad_fn=<AddBackward0>)
17
epoch:  17 ; loss:  -375.1095275878906 ; pred:  tensor([0.2986, 0.3467, 0.2313, 0.1234], grad_fn=<SoftmaxBackward0>)
num_high 4 len(mask) 6
mask_without_small tensor([0.9444, 0.8458, 0.9372, 0.9362], grad_fn=<IndexBackward0>)
pred_loss tensor([12.0975], grad_fn=<MulBackward0>)
size_loss tensor(-468.9306, grad_fn=<MulBackward0>)
size_num_loss 0.4
loss: tensor([-455.9938], grad_fn=<AddBackward0>)
18
epoch:  18 ; loss:  -455.9938049316406 ; pred:  tensor([0.2983, 0.3474, 0.2308, 0.1236], grad_fn=<SoftmaxBackward0>)
num_high 4 len(mask) 6
mask_without_small tensor([0.9491, 0.8334, 0.9422, 0.9412], grad_fn=<IndexBackward0>)
pred_loss tensor([12.1100], grad_fn=<MulBackward0>)
size_loss tensor(-555.1893, grad_fn=<MulBackward0>)
size_num_loss 0.4
loss: tensor([-542.2449], grad_fn=<AddBackward0>)
19
epoch:  19 ; loss:  -542.244873046875 ; pred:  tensor([0.2979, 0.3480, 0.2303, 0.1238], grad_fn=<SoftmaxBackward0>)
Finished Training
sel masked ver tensor(indices=tensor([[7996, 5702, 5702, 5702],
                       [5702,    0, 3466,    0]]),
       values=tensor([0.9491, 3.3334, 0.9422, 3.7647]),
       size=(7997, 5703), nnz=4, layout=torch.sparse_coo,
       grad_fn=<SparseCooTensorWithDimsAndTensorsBackward0>)
Counter({'fax': 1, 'phone': 1, 'name': 1, 'author': 1})
dict index: {}
node_idx 5702
 node original label [0]
 node predicted label explain 1
 node prediction probability explain tensor([0.3227, 0.4347, 0.1644, 0.0783], grad_fn=<SoftmaxBackward0>)
 node predicted label full 0 most important relations  {'fax': 1, 'phone': 1, 'name': 1, 'author': 1, 'label': 0, 'node_idx': '5702'}
 final masks and lenght tensor(indices=tensor([[ 24566,  88552, 196257, 229397, 254252, 328817],
                       [  5702,      0,   3466,      0,   7996,   5230]]),
       values=tensor([0.9491, 3.3334, 0.9422, 3.7647, 0.3308, 0.3642]),
       size=(753935, 8285), nnz=6, layout=torch.sparse_coo,
       grad_fn=<SparseCooTensorWithDimsAndTensorsBackward0>) 4
 ---------------------------------------------------------------
r_exp.py:102: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  hor_graph, ver_graph = torch.tensor(sub_hor_graph, dtype=torch.float), torch.tensor(sub_ver_graph, dtype=torch.float)
wandb: WARNING Calling wandb.login() after wandb.init() has no effect.
node label: 0
3
num_high 4 len(mask) 4
mask_without_small tensor([0.7752, 0.7486, 0.7624, 0.7619], grad_fn=<IndexBackward0>)
pred_loss tensor([11.5813], grad_fn=<MulBackward0>)
size_loss tensor(-1.0880, grad_fn=<MulBackward0>)
size_num_loss 0.4
loss: tensor([11.7466], grad_fn=<AddBackward0>)
0
epoch:  0 ; loss:  11.746602058410645 ; pred:  tensor([0.3141, 0.3214, 0.2250, 0.1395], grad_fn=<SoftmaxBackward0>)
num_high 4 len(mask) 4
mask_without_small tensor([0.7922, 0.7293, 0.7800, 0.7432], grad_fn=<IndexBackward0>)
pred_loss tensor([11.5459], grad_fn=<MulBackward0>)
size_loss tensor(-2.9742, grad_fn=<MulBackward0>)
size_num_loss 0.4
loss: tensor([9.8241], grad_fn=<AddBackward0>)
1
epoch:  1 ; loss:  9.824122428894043 ; pred:  tensor([0.3152, 0.3225, 0.2235, 0.1389], grad_fn=<SoftmaxBackward0>)
num_high 4 len(mask) 4
mask_without_small tensor([0.8080, 0.7092, 0.7932, 0.7285], grad_fn=<IndexBackward0>)
pred_loss tensor([11.5118], grad_fn=<MulBackward0>)
size_loss tensor(-4.8238, grad_fn=<MulBackward0>)
size_num_loss 0.4
loss: tensor([7.9385], grad_fn=<AddBackward0>)
2
epoch:  2 ; loss:  7.938486576080322 ; pred:  tensor([0.3163, 0.3231, 0.2222, 0.1384], grad_fn=<SoftmaxBackward0>)
num_high 4 len(mask) 4
mask_without_small tensor([0.8227, 0.6882, 0.8073, 0.7110], grad_fn=<IndexBackward0>)
pred_loss tensor([11.4784], grad_fn=<MulBackward0>)
size_loss tensor(-6.7578, grad_fn=<MulBackward0>)
size_num_loss 0.4
loss: tensor([5.9683], grad_fn=<AddBackward0>)
3
epoch:  3 ; loss:  5.968287467956543 ; pred:  tensor([0.3173, 0.3238, 0.2208, 0.1380], grad_fn=<SoftmaxBackward0>)
num_high 4 len(mask) 4
mask_without_small tensor([0.8365, 0.6664, 0.8212, 0.6917], grad_fn=<IndexBackward0>)
pred_loss tensor([11.4460], grad_fn=<MulBackward0>)
size_loss tensor(-8.7316, grad_fn=<MulBackward0>)
size_num_loss 0.4
loss: tensor([3.9584], grad_fn=<AddBackward0>)
4
epoch:  4 ; loss:  3.9583818912506104 ; pred:  tensor([0.3183, 0.3245, 0.2194, 0.1377], grad_fn=<SoftmaxBackward0>)
num_high 4 len(mask) 4
mask_without_small tensor([0.8492, 0.6438, 0.8348, 0.6711], grad_fn=<IndexBackward0>)
pred_loss tensor([11.4147], grad_fn=<MulBackward0>)
size_loss tensor(-10.7248, grad_fn=<MulBackward0>)
size_num_loss 0.4
loss: tensor([1.9290], grad_fn=<AddBackward0>)
5
epoch:  5 ; loss:  1.9290106296539307 ; pred:  tensor([0.3193, 0.3251, 0.2181, 0.1375], grad_fn=<SoftmaxBackward0>)
num_high 3 len(mask) 4
mask_without_small tensor([0.8609, 0.6206, 0.8476, 0.6494], grad_fn=<IndexBackward0>)
pred_loss tensor([11.3845], grad_fn=<MulBackward0>)
size_loss tensor(-12.7257, grad_fn=<MulBackward0>)
size_num_loss 0.30000000000000004
loss: tensor([-0.2080], grad_fn=<AddBackward0>)
6
epoch:  6 ; loss:  -0.2079809606075287 ; pred:  tensor([0.3203, 0.3256, 0.2167, 0.1373], grad_fn=<SoftmaxBackward0>)
num_high 3 len(mask) 4
mask_without_small tensor([0.8717, 0.5967, 0.8598, 0.6266], grad_fn=<IndexBackward0>)
pred_loss tensor([11.3553], grad_fn=<MulBackward0>)
size_loss tensor(-14.7260, grad_fn=<MulBackward0>)
size_num_loss 0.30000000000000004
loss: tensor([-2.2443], grad_fn=<AddBackward0>)
7
epoch:  7 ; loss:  -2.2442703247070312 ; pred:  tensor([0.3213, 0.3261, 0.2154, 0.1372], grad_fn=<SoftmaxBackward0>)
num_high 2 len(mask) 4
mask_without_small tensor([0.8816, 0.5724, 0.8711, 0.6030], grad_fn=<IndexBackward0>)
pred_loss tensor([11.3273], grad_fn=<MulBackward0>)
size_loss tensor(-16.7188, grad_fn=<MulBackward0>)
size_num_loss 0.2
loss: tensor([-4.3731], grad_fn=<AddBackward0>)
8
epoch:  8 ; loss:  -4.373109340667725 ; pred:  tensor([0.3222, 0.3264, 0.2142, 0.1372], grad_fn=<SoftmaxBackward0>)
num_high 2 len(mask) 4
mask_without_small tensor([0.8907, 0.5476, 0.8816, 0.5786], grad_fn=<IndexBackward0>)
pred_loss tensor([11.3004], grad_fn=<MulBackward0>)
size_loss tensor(-18.6982, grad_fn=<MulBackward0>)
size_num_loss 0.2
loss: tensor([-6.3884], grad_fn=<AddBackward0>)
9
epoch:  9 ; loss:  -6.388400554656982 ; pred:  tensor([0.3230, 0.3267, 0.2130, 0.1372], grad_fn=<SoftmaxBackward0>)
num_high 2 len(mask) 4
mask_without_small tensor([0.8990, 0.5226, 0.8913, 0.5535], grad_fn=<IndexBackward0>)
pred_loss tensor([11.2746], grad_fn=<MulBackward0>)
size_loss tensor(-20.6582, grad_fn=<MulBackward0>)
size_num_loss 0.2
loss: tensor([-8.3843], grad_fn=<AddBackward0>)
10
epoch:  10 ; loss:  -8.38425064086914 ; pred:  tensor([0.3239, 0.3269, 0.2119, 0.1373], grad_fn=<SoftmaxBackward0>)
num_high 2 len(mask) 4
mask_without_small tensor([0.9066, 0.9003, 0.5280], grad_fn=<IndexBackward0>)
pred_loss tensor([11.2500], grad_fn=<MulBackward0>)
size_loss tensor(-2167.6040, grad_fn=<MulBackward0>)
size_num_loss 0.2
loss: tensor([11.2500], grad_fn=<MulBackward0>)
11
epoch:  11 ; loss:  11.249991416931152 ; pred:  tensor([0.3247, 0.3270, 0.2109, 0.1375], grad_fn=<SoftmaxBackward0>)
num_high 2 len(mask) 4
mask_without_small tensor([0.9129, 0.9077, 0.5050], grad_fn=<IndexBackward0>)
pred_loss tensor([11.2285], grad_fn=<MulBackward0>)
size_loss tensor(-2340.0369, grad_fn=<MulBackward0>)
size_num_loss 0.2
loss: tensor([11.2285], grad_fn=<MulBackward0>)
12
epoch:  12 ; loss:  11.228537559509277 ; pred:  tensor([0.3254, 0.3271, 0.2100, 0.1376], grad_fn=<SoftmaxBackward0>)
num_high 2 len(mask) 4
mask_without_small tensor([0.9182, 0.9138], grad_fn=<IndexBackward0>)
pred_loss tensor([11.2098], grad_fn=<MulBackward0>)
size_loss tensor(-30.9219, grad_fn=<MulBackward0>)
size_num_loss 0.2
loss: tensor([11.2098], grad_fn=<MulBackward0>)
13
epoch:  13 ; loss:  11.209758758544922 ; pred:  tensor([0.3260, 0.3271, 0.2092, 0.1378], grad_fn=<SoftmaxBackward0>)
num_high 2 len(mask) 4
mask_without_small tensor([0.9227, 0.9190], grad_fn=<IndexBackward0>)
pred_loss tensor([11.1933], grad_fn=<MulBackward0>)
size_loss tensor(-26.1816, grad_fn=<MulBackward0>)
size_num_loss 0.2
loss: tensor([11.1933], grad_fn=<MulBackward0>)
14
epoch:  14 ; loss:  11.193252563476562 ; pred:  tensor([0.3265, 0.3270, 0.2085, 0.1380], grad_fn=<SoftmaxBackward0>)
num_high 2 len(mask) 4
mask_without_small tensor([0.9266, 0.9234], grad_fn=<IndexBackward0>)
pred_loss tensor([11.1787], grad_fn=<MulBackward0>)
size_loss tensor(-22.4145, grad_fn=<MulBackward0>)
size_num_loss 0.2
loss: tensor([11.1787], grad_fn=<MulBackward0>)
15
epoch:  15 ; loss:  11.178685188293457 ; pred:  tensor([0.3270, 0.3270, 0.2079, 0.1381], grad_fn=<SoftmaxBackward0>)
num_high 2 len(mask) 4
mask_without_small tensor([0.9299, 0.9271], grad_fn=<IndexBackward0>)
pred_loss tensor([11.1658], grad_fn=<MulBackward0>)
size_loss tensor(-19.4263, grad_fn=<MulBackward0>)
size_num_loss 0.2
loss: tensor([11.1658], grad_fn=<MulBackward0>)
16
epoch:  16 ; loss:  11.16578483581543 ; pred:  tensor([0.3274, 0.3269, 0.2074, 0.1383], grad_fn=<SoftmaxBackward0>)
num_high 2 len(mask) 4
mask_without_small tensor([0.9327, 0.9303], grad_fn=<IndexBackward0>)
pred_loss tensor([11.1543], grad_fn=<MulBackward0>)
size_loss tensor(-17.0686, grad_fn=<MulBackward0>)
size_num_loss 0.2
loss: tensor([11.1543], grad_fn=<MulBackward0>)
17
epoch:  17 ; loss:  11.15432357788086 ; pred:  tensor([0.3278, 0.3268, 0.2070, 0.1385], grad_fn=<SoftmaxBackward0>)
num_high 2 len(mask) 4
mask_without_small tensor([0.9351, 0.9330], grad_fn=<IndexBackward0>)
pred_loss tensor([11.1441], grad_fn=<MulBackward0>)
size_loss tensor(-15.2260, grad_fn=<MulBackward0>)
size_num_loss 0.2
loss: tensor([11.1441], grad_fn=<MulBackward0>)
18
epoch:  18 ; loss:  11.144112586975098 ; pred:  tensor([0.3281, 0.3267, 0.2066, 0.1386], grad_fn=<SoftmaxBackward0>)
num_high 2 len(mask) 4
mask_without_small tensor([0.9372, 0.9353], grad_fn=<IndexBackward0>)
pred_loss tensor([11.1350], grad_fn=<MulBackward0>)
size_loss tensor(-13.8082, grad_fn=<MulBackward0>)
size_num_loss 0.2
loss: tensor([11.1350], grad_fn=<MulBackward0>)
19
epoch:  19 ; loss:  11.134992599487305 ; pred:  tensor([0.3284, 0.3266, 0.2062, 0.1388], grad_fn=<SoftmaxBackward0>)
Finished Training
sel masked ver tensor(indices=tensor([[5714, 5714],
                       [   0,    0]]),
       values=tensor([3.7488, 3.7410]),
       size=(5715, 1), nnz=2, layout=torch.sparse_coo,
       grad_fn=<SparseCooTensorWithDimsAndTensorsBackward0>)
Counter({'fax': 1, 'phone': 1})
dict index: {}
node_idx 5714
 node original label [0]
 node predicted label explain 1
 node prediction probability explain tensor([0.3590, 0.4067, 0.1501, 0.0843], grad_fn=<SoftmaxBackward0>)
 node predicted label full 0 most important relations  {'fax': 1, 'phone': 1, 'label': 0, 'node_idx': '5714'}
 final masks and lenght tensor(indices=tensor([[ 88564, 196269, 229409, 328829],
                       [     0,   4560,      0,   5230]]),
       values=tensor([3.7488, 0.3670, 3.7410, 0.3982]),
       size=(753935, 8285), nnz=4, layout=torch.sparse_coo,
       grad_fn=<SparseCooTensorWithDimsAndTensorsBackward0>) 2
 ---------------------------------------------------------------
r_exp.py:102: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  hor_graph, ver_graph = torch.tensor(sub_hor_graph, dtype=torch.float), torch.tensor(sub_ver_graph, dtype=torch.float)
wandb: WARNING Calling wandb.login() after wandb.init() has no effect.
node label: 0
4
num_high 4 len(mask) 5
mask_without_small tensor([0.7708, 0.7468, 0.7592, 0.7587, 0.5720], grad_fn=<IndexBackward0>)
pred_loss tensor([10.1398], grad_fn=<MulBackward0>)
size_loss tensor(-8.4024, grad_fn=<MulBackward0>)
size_num_loss 0.4
loss: tensor([3.0764], grad_fn=<AddBackward0>)
0
epoch:  0 ; loss:  3.0763533115386963 ; pred:  tensor([0.3628, 0.2503, 0.2652, 0.1217], grad_fn=<SoftmaxBackward0>)
num_high 4 len(mask) 5
mask_without_small tensor([0.7880, 0.7652, 0.7770, 0.7766, 0.5473], grad_fn=<IndexBackward0>)
pred_loss tensor([10.1078], grad_fn=<MulBackward0>)
size_loss tensor(-10.2895, grad_fn=<MulBackward0>)
size_num_loss 0.4
loss: tensor([1.1461], grad_fn=<AddBackward0>)
1
epoch:  1 ; loss:  1.1460683345794678 ; pred:  tensor([0.3639, 0.2503, 0.2663, 0.1195], grad_fn=<SoftmaxBackward0>)
num_high 4 len(mask) 5
mask_without_small tensor([0.8042, 0.7827, 0.7938, 0.7934, 0.5224], grad_fn=<IndexBackward0>)
pred_loss tensor([10.0787], grad_fn=<MulBackward0>)
size_loss tensor(-12.1468, grad_fn=<MulBackward0>)
size_num_loss 0.4
loss: tensor([-0.7528], grad_fn=<AddBackward0>)
2
epoch:  2 ; loss:  -0.7527912855148315 ; pred:  tensor([0.3650, 0.2502, 0.2675, 0.1173], grad_fn=<SoftmaxBackward0>)
num_high 4 len(mask) 5
mask_without_small tensor([0.8193, 0.7992, 0.8096, 0.8092], grad_fn=<IndexBackward0>)
pred_loss tensor([10.0524], grad_fn=<MulBackward0>)
size_loss tensor(-81.6772, grad_fn=<MulBackward0>)
size_num_loss 0.4
loss: tensor([-70.6643], grad_fn=<AddBackward0>)
3
epoch:  3 ; loss:  -70.66426086425781 ; pred:  tensor([0.3660, 0.2502, 0.2685, 0.1153], grad_fn=<SoftmaxBackward0>)
num_high 4 len(mask) 5
mask_without_small tensor([0.8278, 0.7898, 0.8203, 0.8067], grad_fn=<IndexBackward0>)
pred_loss tensor([10.0873], grad_fn=<MulBackward0>)
size_loss tensor(-166.7864, grad_fn=<MulBackward0>)
size_num_loss 0.4
loss: tensor([-155.7412], grad_fn=<AddBackward0>)
4
epoch:  4 ; loss:  -155.74119567871094 ; pred:  tensor([0.3647, 0.2509, 0.2699, 0.1145], grad_fn=<SoftmaxBackward0>)
num_high 4 len(mask) 5
mask_without_small tensor([0.8379, 0.7774, 0.8287, 0.7979], grad_fn=<IndexBackward0>)
pred_loss tensor([10.1287], grad_fn=<MulBackward0>)
size_loss tensor(-279.0013, grad_fn=<MulBackward0>)
size_num_loss 0.4
loss: tensor([-267.9153], grad_fn=<AddBackward0>)
5
epoch:  5 ; loss:  -267.91534423828125 ; pred:  tensor([0.3632, 0.2513, 0.2716, 0.1139], grad_fn=<SoftmaxBackward0>)
num_high 4 len(mask) 5
mask_without_small tensor([0.8485, 0.7630, 0.8387, 0.7866], grad_fn=<IndexBackward0>)
pred_loss tensor([10.1767], grad_fn=<MulBackward0>)
size_loss tensor(-411.0756, grad_fn=<MulBackward0>)
size_num_loss 0.4
loss: tensor([-399.9425], grad_fn=<AddBackward0>)
6
epoch:  6 ; loss:  -399.9425048828125 ; pred:  tensor([0.3614, 0.2517, 0.2736, 0.1133], grad_fn=<SoftmaxBackward0>)
num_high 4 len(mask) 5
mask_without_small tensor([0.8591, 0.7469, 0.8494, 0.7732], grad_fn=<IndexBackward0>)
pred_loss tensor([10.2296], grad_fn=<MulBackward0>)
size_loss tensor(-556.0496, grad_fn=<MulBackward0>)
size_num_loss 0.4
loss: tensor([-544.8646], grad_fn=<AddBackward0>)
7
epoch:  7 ; loss:  -544.8645629882812 ; pred:  tensor([0.3595, 0.2520, 0.2758, 0.1127], grad_fn=<SoftmaxBackward0>)
num_high 4 len(mask) 5
mask_without_small tensor([0.8694, 0.7293, 0.8602, 0.7580], grad_fn=<IndexBackward0>)
pred_loss tensor([10.2867], grad_fn=<MulBackward0>)
size_loss tensor(-710.0989, grad_fn=<MulBackward0>)
size_num_loss 0.4
loss: tensor([-698.8580], grad_fn=<AddBackward0>)
8
epoch:  8 ; loss:  -698.8579711914062 ; pred:  tensor([0.3575, 0.2524, 0.2780, 0.1122], grad_fn=<SoftmaxBackward0>)
num_high 4 len(mask) 5
mask_without_small tensor([0.8793, 0.7105, 0.8708, 0.7413], grad_fn=<IndexBackward0>)
pred_loss tensor([10.3474], grad_fn=<MulBackward0>)
size_loss tensor(-870.9395, grad_fn=<MulBackward0>)
size_num_loss 0.4
loss: tensor([-859.6390], grad_fn=<AddBackward0>)
9
epoch:  9 ; loss:  -859.6390380859375 ; pred:  tensor([0.3553, 0.2527, 0.2802, 0.1117], grad_fn=<SoftmaxBackward0>)
num_high 4 len(mask) 5
mask_without_small tensor([0.8885, 0.6904, 0.8809, 0.7230], grad_fn=<IndexBackward0>)
pred_loss tensor([10.4116], grad_fn=<MulBackward0>)
size_loss tensor(-1037.1072, grad_fn=<MulBackward0>)
size_num_loss 0.4
loss: tensor([-1025.7440], grad_fn=<AddBackward0>)
10
epoch:  10 ; loss:  -1025.7440185546875 ; pred:  tensor([0.3530, 0.2530, 0.2826, 0.1114], grad_fn=<SoftmaxBackward0>)
num_high 4 len(mask) 5
mask_without_small tensor([0.8971, 0.6691, 0.8906, 0.7032], grad_fn=<IndexBackward0>)
pred_loss tensor([10.4790], grad_fn=<MulBackward0>)
size_loss tensor(-1207.5941, grad_fn=<MulBackward0>)
size_num_loss 0.4
loss: tensor([-1196.1653], grad_fn=<AddBackward0>)
11
epoch:  11 ; loss:  -1196.165283203125 ; pred:  tensor([0.3507, 0.2533, 0.2849, 0.1111], grad_fn=<SoftmaxBackward0>)
num_high 4 len(mask) 5
mask_without_small tensor([0.9051, 0.6468, 0.8997, 0.6821], grad_fn=<IndexBackward0>)
pred_loss tensor([10.5493], grad_fn=<MulBackward0>)
size_loss tensor(-1381.6484, grad_fn=<MulBackward0>)
size_num_loss 0.4
loss: tensor([-1370.1511], grad_fn=<AddBackward0>)
12
epoch:  12 ; loss:  -1370.151123046875 ; pred:  tensor([0.3482, 0.2536, 0.2873, 0.1109], grad_fn=<SoftmaxBackward0>)
num_high 4 len(mask) 5
mask_without_small tensor([0.9125, 0.6235, 0.9082, 0.6597], grad_fn=<IndexBackward0>)
pred_loss tensor([10.6224], grad_fn=<MulBackward0>)
size_loss tensor(-1558.6498, grad_fn=<MulBackward0>)
size_num_loss 0.4
loss: tensor([-1547.0817], grad_fn=<AddBackward0>)
13
epoch:  13 ; loss:  -1547.0816650390625 ; pred:  tensor([0.3457, 0.2538, 0.2898, 0.1108], grad_fn=<SoftmaxBackward0>)
num_high 3 len(mask) 5
mask_without_small tensor([0.9193, 0.5994, 0.9160, 0.6360], grad_fn=<IndexBackward0>)
pred_loss tensor([10.6980], grad_fn=<MulBackward0>)
size_loss tensor(-1738.0269, grad_fn=<MulBackward0>)
size_num_loss 0.30000000000000004
loss: tensor([-1726.4857], grad_fn=<AddBackward0>)
14
epoch:  14 ; loss:  -1726.4857177734375 ; pred:  tensor([0.3431, 0.2539, 0.2923, 0.1107], grad_fn=<SoftmaxBackward0>)
num_high 2 len(mask) 5
mask_without_small tensor([0.9255, 0.5746, 0.9232, 0.6113], grad_fn=<IndexBackward0>)
pred_loss tensor([10.7758], grad_fn=<MulBackward0>)
size_loss tensor(-1919.2047, grad_fn=<MulBackward0>)
size_num_loss 0.2
loss: tensor([-1907.6891], grad_fn=<AddBackward0>)
15
epoch:  15 ; loss:  -1907.6890869140625 ; pred:  tensor([0.3404, 0.2540, 0.2948, 0.1108], grad_fn=<SoftmaxBackward0>)
num_high 2 len(mask) 5
mask_without_small tensor([0.9312, 0.5492, 0.9297, 0.5856], grad_fn=<IndexBackward0>)
pred_loss tensor([10.8554], grad_fn=<MulBackward0>)
size_loss tensor(-2101.5671, grad_fn=<MulBackward0>)
size_num_loss 0.2
loss: tensor([-2089.9756], grad_fn=<AddBackward0>)
16
epoch:  16 ; loss:  -2089.9755859375 ; pred:  tensor([0.3377, 0.2541, 0.2973, 0.1109], grad_fn=<SoftmaxBackward0>)
num_high 2 len(mask) 5
mask_without_small tensor([0.9364, 0.5233, 0.9357, 0.5590], grad_fn=<IndexBackward0>)
pred_loss tensor([10.9365], grad_fn=<MulBackward0>)
size_loss tensor(-2284.4333, grad_fn=<MulBackward0>)
size_num_loss 0.2
loss: tensor([-2272.7651], grad_fn=<AddBackward0>)
17
epoch:  17 ; loss:  -2272.76513671875 ; pred:  tensor([0.3350, 0.2541, 0.2998, 0.1110], grad_fn=<SoftmaxBackward0>)
num_high 2 len(mask) 5
mask_without_small tensor([0.9411, 0.9411, 0.5318], grad_fn=<IndexBackward0>)
pred_loss tensor([11.0187], grad_fn=<MulBackward0>)
size_loss tensor(-2363.1477, grad_fn=<MulBackward0>)
size_num_loss 0.2
loss: tensor([11.0187], grad_fn=<MulBackward0>)
18
epoch:  18 ; loss:  11.018733978271484 ; pred:  tensor([0.3322, 0.2541, 0.3024, 0.1112], grad_fn=<SoftmaxBackward0>)
num_high 2 len(mask) 5
mask_without_small tensor([0.9451, 0.9457, 0.5069], grad_fn=<IndexBackward0>)
pred_loss tensor([11.0937], grad_fn=<MulBackward0>)
size_loss tensor(-2531.6707, grad_fn=<MulBackward0>)
size_num_loss 0.2
loss: tensor([11.0937], grad_fn=<MulBackward0>)
19
epoch:  19 ; loss:  11.093650817871094 ; pred:  tensor([0.3298, 0.2541, 0.3047, 0.1114], grad_fn=<SoftmaxBackward0>)
Finished Training
sel masked ver tensor(indices=tensor([[5708, 5708, 5708],
                       [   0, 4855,    0]]),
       values=tensor([3.7802, 0.9457, 2.0274]),
       size=(5709, 4856), nnz=3, layout=torch.sparse_coo,
       grad_fn=<SparseCooTensorWithDimsAndTensorsBackward0>)
Counter({'fax': 1, 'phone': 1, 'name': 1})
dict index: {}
node_idx 5708
 node original label [0]
 node predicted label explain 0
 node prediction probability explain tensor([0.3631, 0.2871, 0.2776, 0.0723], grad_fn=<SoftmaxBackward0>)
 node predicted label full 0 most important relations  {'fax': 1, 'phone': 1, 'name': 1, 'label': 0, 'node_idx': '5708'}
 final masks and lenght tensor(indices=tensor([[ 88558, 129983, 196263, 229403, 328823],
                       [     0,   3162,   4855,      0,   5230]]),
       values=tensor([3.7802, 0.4735, 0.9457, 2.0274, 0.3617]),
       size=(753935, 8285), nnz=5, layout=torch.sparse_coo,
       grad_fn=<SparseCooTensorWithDimsAndTensorsBackward0>) 3
 ---------------------------------------------------------------
r_exp.py:102: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  hor_graph, ver_graph = torch.tensor(sub_hor_graph, dtype=torch.float), torch.tensor(sub_ver_graph, dtype=torch.float)
wandb: WARNING Calling wandb.login() after wandb.init() has no effect.
node label: 0
18
num_high 48 len(mask) 48
mask_without_small tensor([0.8011, 0.7864, 0.7656, 0.6388, 0.7574, 0.6787, 0.7293, 0.6621, 0.6998,
        0.7919, 0.7150, 0.6712, 0.7009, 0.7080, 0.6991, 0.7605, 0.7917, 0.7246,
        0.7106, 0.7483, 0.6996, 0.7721, 0.7620, 0.7930, 0.7792, 0.7798, 0.7548,
        0.7812, 0.7217, 0.7327, 0.7208, 0.7641, 0.6720, 0.6947, 0.7220, 0.7942,
        0.7437, 0.7137, 0.7431, 0.6989, 0.6642, 0.7691, 0.6943, 0.7063, 0.6770,
        0.8074, 0.6787, 0.7110], grad_fn=<IndexBackward0>)
pred_loss tensor([3.9520], grad_fn=<MulBackward0>)
size_loss tensor(-4.3514, grad_fn=<MulBackward0>)
size_num_loss 4.800000000000001
loss: tensor([8.4873], grad_fn=<AddBackward0>)
0
epoch:  0 ; loss:  8.487285614013672 ; pred:  tensor([0.6735, 0.1458, 0.0849, 0.0958], grad_fn=<SoftmaxBackward0>)
num_high 47 len(mask) 48
mask_without_small tensor([0.8166, 0.8028, 0.7831, 0.6154, 0.7753, 0.6566, 0.7091, 0.6393, 0.6784,
        0.8079, 0.6942, 0.6487, 0.6795, 0.6869, 0.6777, 0.7783, 0.8077, 0.7441,
        0.6896, 0.7667, 0.6781, 0.7892, 0.7796, 0.8089, 0.7959, 0.7965, 0.7729,
        0.7978, 0.7011, 0.7127, 0.7003, 0.7817, 0.6496, 0.6731, 0.7015, 0.8101,
        0.7623, 0.6928, 0.7618, 0.6774, 0.6415, 0.7864, 0.6727, 0.6851, 0.6547,
        0.8225, 0.7001, 0.6901], grad_fn=<IndexBackward0>)
pred_loss tensor([3.9043], grad_fn=<MulBackward0>)
size_loss tensor(-6.0301, grad_fn=<MulBackward0>)
size_num_loss 4.7
loss: tensor([6.6456], grad_fn=<AddBackward0>)
1
epoch:  1 ; loss:  6.645623683929443 ; pred:  tensor([0.6768, 0.1463, 0.0836, 0.0933], grad_fn=<SoftmaxBackward0>)
num_high 45 len(mask) 48
mask_without_small tensor([0.8309, 0.8180, 0.7996, 0.5916, 0.7922, 0.6337, 0.6887, 0.6160, 0.6562,
        0.8228, 0.6726, 0.6256, 0.6573, 0.6650, 0.6554, 0.7950, 0.8226, 0.7607,
        0.6679, 0.7826, 0.6559, 0.8053, 0.7963, 0.8238, 0.8116, 0.8122, 0.7897,
        0.8134, 0.6800, 0.6927, 0.6790, 0.7983, 0.6266, 0.6507, 0.6805, 0.8248,
        0.7797, 0.6712, 0.7792, 0.6552, 0.6183, 0.8027, 0.6503, 0.6632, 0.6318,
        0.8365, 0.7194, 0.6683], grad_fn=<IndexBackward0>)
pred_loss tensor([3.8757], grad_fn=<MulBackward0>)
size_loss tensor(-7.8179, grad_fn=<MulBackward0>)
size_num_loss 4.5
loss: tensor([4.6084], grad_fn=<AddBackward0>)
2
epoch:  2 ; loss:  4.608380317687988 ; pred:  tensor([0.6787, 0.1472, 0.0828, 0.0913], grad_fn=<SoftmaxBackward0>)
num_high 41 len(mask) 48
mask_without_small tensor([0.8440, 0.8321, 0.8152, 0.5675, 0.8081, 0.6102, 0.6673, 0.5922, 0.6332,
        0.8365, 0.6502, 0.6020, 0.6344, 0.6423, 0.6324, 0.8109, 0.8364, 0.7772,
        0.6453, 0.7983, 0.6329, 0.8205, 0.8121, 0.8374, 0.8263, 0.8267, 0.8057,
        0.8278, 0.6579, 0.6718, 0.6570, 0.8139, 0.6030, 0.6276, 0.6586, 0.8385,
        0.7963, 0.6489, 0.7958, 0.6322, 0.5945, 0.8180, 0.6271, 0.6404, 0.6083,
        0.8493, 0.7383, 0.6457], grad_fn=<IndexBackward0>)
pred_loss tensor([3.8555], grad_fn=<MulBackward0>)
size_loss tensor(-9.6454, grad_fn=<MulBackward0>)
size_num_loss 4.1000000000000005
loss: tensor([2.3350], grad_fn=<AddBackward0>)
3
epoch:  3 ; loss:  2.3349928855895996 ; pred:  tensor([0.6801, 0.1483, 0.0821, 0.0896], grad_fn=<SoftmaxBackward0>)
num_high 30 len(mask) 48
mask_without_small tensor([0.8559, 0.8450, 0.8297, 0.5431, 0.8232, 0.5862, 0.6450, 0.5680, 0.6096,
        0.8491, 0.6270, 0.5779, 0.6107, 0.6189, 0.6087, 0.8258, 0.8489, 0.7934,
        0.6219, 0.8134, 0.6093, 0.8345, 0.8269, 0.8499, 0.8397, 0.8402, 0.8209,
        0.8412, 0.6351, 0.6499, 0.6341, 0.8286, 0.5789, 0.6038, 0.6359, 0.8509,
        0.8120, 0.6257, 0.8115, 0.6085, 0.5704, 0.8322, 0.6033, 0.6170, 0.5844,
        0.8610, 0.7567, 0.6224], grad_fn=<IndexBackward0>)
pred_loss tensor([3.8456], grad_fn=<MulBackward0>)
size_loss tensor(-11.4820, grad_fn=<MulBackward0>)
size_num_loss 3.0
loss: tensor([-0.6421], grad_fn=<AddBackward0>)
4
epoch:  4 ; loss:  -0.6420624256134033 ; pred:  tensor([0.6807, 0.1495, 0.0817, 0.0880], grad_fn=<SoftmaxBackward0>)
num_high 24 len(mask) 48
mask_without_small tensor([0.8665, 0.8568, 0.8431, 0.5188, 0.8372, 0.5617, 0.6219, 0.5436, 0.5853,
        0.8604, 0.6032, 0.5535, 0.5865, 0.5948, 0.5845, 0.8396, 0.8602, 0.8089,
        0.5979, 0.8279, 0.5850, 0.8474, 0.8406, 0.8611, 0.8520, 0.8524, 0.8351,
        0.8533, 0.6115, 0.6271, 0.6104, 0.8421, 0.5546, 0.5795, 0.6124, 0.8622,
        0.8269, 0.6018, 0.8264, 0.5842, 0.5460, 0.8454, 0.5790, 0.5929, 0.5602,
        0.8716, 0.7744, 0.5984], grad_fn=<IndexBackward0>)
pred_loss tensor([3.8459], grad_fn=<MulBackward0>)
size_loss tensor(-13.3094, grad_fn=<MulBackward0>)
size_num_loss 2.4000000000000004
loss: tensor([-3.1046], grad_fn=<AddBackward0>)
5
epoch:  5 ; loss:  -3.104562997817993 ; pred:  tensor([0.6807, 0.1510, 0.0815, 0.0868], grad_fn=<SoftmaxBackward0>)
num_high 23 len(mask) 48
mask_without_small tensor([0.8760, 0.8673, 0.8554, 0.8502, 0.5371, 0.5980, 0.5190, 0.5606, 0.8705,
        0.5787, 0.5288, 0.5618, 0.5702, 0.5598, 0.8523, 0.8704, 0.8237, 0.5733,
        0.8414, 0.5604, 0.8591, 0.8531, 0.8711, 0.8631, 0.8635, 0.8482, 0.8642,
        0.5872, 0.6034, 0.5861, 0.8545, 0.5302, 0.5548, 0.5882, 0.8723, 0.8407,
        0.5773, 0.8403, 0.5594, 0.5215, 0.8575, 0.5541, 0.5683, 0.5358, 0.8812,
        0.7913, 0.5738], grad_fn=<IndexBackward0>)
pred_loss tensor([3.8546], grad_fn=<MulBackward0>)
size_loss tensor(-1496.4801, grad_fn=<MulBackward0>)
size_num_loss 2.3000000000000003
loss: tensor([-1489.4397], grad_fn=<AddBackward0>)
6
epoch:  6 ; loss:  -1489.439697265625 ; pred:  tensor([0.6801, 0.1526, 0.0815, 0.0858], grad_fn=<SoftmaxBackward0>)
num_high 23 len(mask) 48
mask_without_small tensor([0.8815, 0.8732, 0.8617, 0.8566, 0.5239, 0.5853, 0.5058, 0.5476, 0.8763,
        0.5658, 0.5156, 0.5489, 0.5573, 0.5468, 0.8587, 0.8762, 0.8312, 0.5604,
        0.8482, 0.5473, 0.8652, 0.8595, 0.8769, 0.8692, 0.8695, 0.8548, 0.8702,
        0.5744, 0.5908, 0.5733, 0.8609, 0.5171, 0.5418, 0.5755, 0.8780, 0.8476,
        0.5645, 0.8472, 0.5465, 0.5084, 0.8639, 0.5412, 0.5556, 0.5228, 0.8867,
        0.8005, 0.5609], grad_fn=<IndexBackward0>)
pred_loss tensor([3.8582], grad_fn=<MulBackward0>)
size_loss tensor(-1592.6536, grad_fn=<MulBackward0>)
size_num_loss 2.3000000000000003
loss: tensor([-1585.6152], grad_fn=<AddBackward0>)
7
epoch:  7 ; loss:  -1585.615234375 ; pred:  tensor([0.6799, 0.1534, 0.0814, 0.0853], grad_fn=<SoftmaxBackward0>)
num_high 23 len(mask) 48
mask_without_small tensor([0.8884, 0.8805, 0.8695, 0.8647, 0.5069, 0.5688, 0.5308, 0.8834, 0.5491,
        0.5321, 0.5405, 0.5299, 0.8666, 0.8833, 0.8405, 0.5436, 0.8566, 0.5305,
        0.8729, 0.8674, 0.8840, 0.8766, 0.8770, 0.8629, 0.8777, 0.5577, 0.5743,
        0.5566, 0.8688, 0.5002, 0.5249, 0.5589, 0.8851, 0.8562, 0.5479, 0.8558,
        0.5297, 0.8717, 0.5244, 0.5389, 0.5059, 0.8933, 0.8115, 0.5442],
       grad_fn=<IndexBackward0>)
pred_loss tensor([3.8681], grad_fn=<MulBackward0>)
size_loss tensor(-1684.7445, grad_fn=<MulBackward0>)
size_num_loss 2.3000000000000003
loss: tensor([-1677.7191], grad_fn=<AddBackward0>)
8
epoch:  8 ; loss:  -1677.7191162109375 ; pred:  tensor([0.6792, 0.1545, 0.0815, 0.0847], grad_fn=<SoftmaxBackward0>)
num_high 23 len(mask) 48
mask_without_small tensor([0.8958, 0.8883, 0.8780, 0.8734, 0.5497, 0.5114, 0.8911, 0.5298, 0.5128,
        0.5211, 0.5106, 0.8753, 0.8910, 0.8506, 0.5243, 0.8658, 0.5111, 0.8812,
        0.8760, 0.8916, 0.8847, 0.8850, 0.8718, 0.8857, 0.5386, 0.5553, 0.5374,
        0.8773, 0.5055, 0.5398, 0.8927, 0.8654, 0.5286, 0.8650, 0.5104, 0.8801,
        0.5051, 0.5196, 0.9005, 0.8233, 0.5249], grad_fn=<IndexBackward0>)
pred_loss tensor([3.8819], grad_fn=<MulBackward0>)
size_loss tensor(-1783.5406, grad_fn=<MulBackward0>)
size_num_loss 2.3000000000000003
loss: tensor([-1776.5250], grad_fn=<AddBackward0>)
9
epoch:  9 ; loss:  -1776.5250244140625 ; pred:  tensor([0.6783, 0.1558, 0.0817, 0.0842], grad_fn=<SoftmaxBackward0>)
num_high 23 len(mask) 48
mask_without_small tensor([0.9033, 0.8963, 0.8866, 0.8823, 0.5288, 0.8989, 0.5088, 0.5001, 0.8841,
        0.8988, 0.8609, 0.5033, 0.8752, 0.8896, 0.8848, 0.8994, 0.8929, 0.8932,
        0.8808, 0.8938, 0.5176, 0.5345, 0.5165, 0.8860, 0.5188, 0.9004, 0.8748,
        0.5076, 0.8745, 0.8887, 0.9077, 0.8354, 0.5038],
       grad_fn=<IndexBackward0>)
pred_loss tensor([3.8945], grad_fn=<MulBackward0>)
size_loss tensor(-1744.2532, grad_fn=<MulBackward0>)
size_num_loss 2.3000000000000003
loss: tensor([-1737.2740], grad_fn=<AddBackward0>)
10
epoch:  10 ; loss:  -1737.2740478515625 ; pred:  tensor([0.6774, 0.1570, 0.0818, 0.0837], grad_fn=<SoftmaxBackward0>)
num_high 23 len(mask) 48
mask_without_small tensor([0.9107, 0.9041, 0.8951, 0.8911, 0.5074, 0.9066, 0.8927, 0.9065, 0.8711,
        0.8844, 0.8979, 0.8934, 0.9070, 0.9010, 0.9012, 0.8896, 0.9018, 0.5132,
        0.8945, 0.9080, 0.8841, 0.8838, 0.8970, 0.9148, 0.8472],
       grad_fn=<IndexBackward0>)
pred_loss tensor([3.8996], grad_fn=<MulBackward0>)
size_loss tensor(-1074.2052, grad_fn=<MulBackward0>)
size_num_loss 2.3000000000000003
loss: tensor([-1067.2692], grad_fn=<AddBackward0>)
11
epoch:  11 ; loss:  -1067.2691650390625 ; pred:  tensor([0.6771, 0.1579, 0.0818, 0.0832], grad_fn=<SoftmaxBackward0>)
num_high 23 len(mask) 48
mask_without_small tensor([0.9176, 0.9115, 0.9029, 0.8991, 0.9138, 0.9007, 0.9137, 0.8799, 0.8927,
        0.9056, 0.9013, 0.9142, 0.9085, 0.9087, 0.8977, 0.9093, 0.9024, 0.9151,
        0.8925, 0.8922, 0.9048, 0.9215, 0.8560], grad_fn=<IndexBackward0>)
pred_loss tensor([3.9095], grad_fn=<MulBackward0>)
size_loss tensor(-140.8750, grad_fn=<MulBackward0>)
size_num_loss 2.3000000000000003
loss: tensor([-133.9468], grad_fn=<AddBackward0>)
12
epoch:  12 ; loss:  -133.94679260253906 ; pred:  tensor([0.6764, 0.1588, 0.0820, 0.0828], grad_fn=<SoftmaxBackward0>)
num_high 23 len(mask) 48
mask_without_small tensor([0.9244, 0.9186, 0.9095, 0.9050, 0.9208, 0.9069, 0.9207, 0.8800, 0.8967,
        0.9125, 0.9077, 0.9212, 0.9156, 0.9159, 0.9033, 0.9164, 0.9090, 0.9220,
        0.8964, 0.8960, 0.9117, 0.9279, 0.8520], grad_fn=<IndexBackward0>)
pred_loss tensor([3.9796], grad_fn=<MulBackward0>)
size_loss tensor(-165.8932, grad_fn=<MulBackward0>)
size_num_loss 2.3000000000000003
loss: tensor([-158.9014], grad_fn=<AddBackward0>)
13
epoch:  13 ; loss:  -158.9014129638672 ; pred:  tensor([0.6717, 0.1614, 0.0835, 0.0834], grad_fn=<SoftmaxBackward0>)
num_high 23 len(mask) 48
mask_without_small tensor([0.9309, 0.9253, 0.9154, 0.9095, 0.9275, 0.9120, 0.9274, 0.8766, 0.8979,
        0.9189, 0.9130, 0.9279, 0.9223, 0.9226, 0.9071, 0.9231, 0.9146, 0.9286,
        0.8976, 0.8970, 0.9180, 0.9341, 0.8450], grad_fn=<IndexBackward0>)
pred_loss tensor([4.0641], grad_fn=<MulBackward0>)
size_loss tensor(-200.6777, grad_fn=<MulBackward0>)
size_num_loss 2.3000000000000003
loss: tensor([-193.6074], grad_fn=<AddBackward0>)
14
epoch:  14 ; loss:  -193.6073760986328 ; pred:  tensor([0.6660, 0.1644, 0.0853, 0.0843], grad_fn=<SoftmaxBackward0>)
num_high 23 len(mask) 48
mask_without_small tensor([0.9369, 0.9317, 0.9206, 0.9129, 0.9337, 0.9163, 0.9337, 0.8709, 0.8971,
        0.9249, 0.9176, 0.9341, 0.9286, 0.9288, 0.9096, 0.9294, 0.9197, 0.9349,
        0.8967, 0.8960, 0.9238, 0.9400, 0.8359], grad_fn=<IndexBackward0>)
pred_loss tensor([4.1584], grad_fn=<MulBackward0>)
size_loss tensor(-242.0459, grad_fn=<MulBackward0>)
size_num_loss 2.3000000000000003
loss: tensor([-234.8869], grad_fn=<AddBackward0>)
15
epoch:  15 ; loss:  -234.88693237304688 ; pred:  tensor([0.6598, 0.1674, 0.0874, 0.0853], grad_fn=<SoftmaxBackward0>)
num_high 23 len(mask) 48
mask_without_small tensor([0.9426, 0.9375, 0.9254, 0.9155, 0.9396, 0.9200, 0.9395, 0.8633, 0.8946,
        0.9304, 0.9217, 0.9399, 0.9344, 0.9347, 0.9111, 0.9353, 0.9243, 0.9406,
        0.8941, 0.8931, 0.9292, 0.9454, 0.8248], grad_fn=<IndexBackward0>)
pred_loss tensor([4.2613], grad_fn=<MulBackward0>)
size_loss tensor(-288.7260, grad_fn=<MulBackward0>)
size_num_loss 2.3000000000000003
loss: tensor([-281.4694], grad_fn=<AddBackward0>)
16
epoch:  16 ; loss:  -281.4693908691406 ; pred:  tensor([0.6530, 0.1706, 0.0898, 0.0866], grad_fn=<SoftmaxBackward0>)
num_high 23 len(mask) 48
mask_without_small tensor([0.9477, 0.9430, 0.9299, 0.9176, 0.9449, 0.9232, 0.9448, 0.8541, 0.8905,
        0.9356, 0.9254, 0.9453, 0.9398, 0.9401, 0.9117, 0.9408, 0.9286, 0.9459,
        0.8899, 0.8888, 0.9343, 0.9503, 0.8120], grad_fn=<IndexBackward0>)
pred_loss tensor([4.3728], grad_fn=<MulBackward0>)
size_loss tensor(-340.1596, grad_fn=<MulBackward0>)
size_num_loss 2.3000000000000003
loss: tensor([-332.7961], grad_fn=<AddBackward0>)
17
epoch:  17 ; loss:  -332.796142578125 ; pred:  tensor([0.6458, 0.1740, 0.0923, 0.0880], grad_fn=<SoftmaxBackward0>)
num_high 23 len(mask) 48
mask_without_small tensor([0.9524, 0.9479, 0.9342, 0.9191, 0.9498, 0.9262, 0.9497, 0.8433, 0.8850,
        0.9404, 0.9288, 0.9501, 0.9448, 0.9451, 0.9116, 0.9458, 0.9326, 0.9508,
        0.8843, 0.8829, 0.9391, 0.9547, 0.7976], grad_fn=<IndexBackward0>)
pred_loss tensor([4.4935], grad_fn=<MulBackward0>)
size_loss tensor(-396.1477, grad_fn=<MulBackward0>)
size_num_loss 2.3000000000000003
loss: tensor([-388.6680], grad_fn=<AddBackward0>)
18
epoch:  18 ; loss:  -388.66796875 ; pred:  tensor([0.6380, 0.1775, 0.0950, 0.0895], grad_fn=<SoftmaxBackward0>)
num_high 23 len(mask) 48
mask_without_small tensor([0.9566, 0.9525, 0.9382, 0.9203, 0.9542, 0.9290, 0.9542, 0.8308, 0.8780,
        0.9450, 0.9321, 0.9545, 0.9494, 0.9497, 0.9109, 0.9504, 0.9365, 0.9551,
        0.8773, 0.8756, 0.9435, 0.9587, 0.7814], grad_fn=<IndexBackward0>)
pred_loss tensor([4.6243], grad_fn=<MulBackward0>)
size_loss tensor(-456.7001, grad_fn=<MulBackward0>)
size_num_loss 2.3000000000000003
loss: tensor([-449.0937], grad_fn=<AddBackward0>)
19
epoch:  19 ; loss:  -449.09368896484375 ; pred:  tensor([0.6298, 0.1811, 0.0980, 0.0912], grad_fn=<SoftmaxBackward0>)
Finished Training
sel masked ver tensor(indices=tensor([[7250, 7384, 7632, 7930, 5486, 5843, 7688, 7688, 5843,
                        5923, 5990, 5990, 5990, 5990, 7930, 5350, 5438, 5486,
                        5843, 5843, 5923, 5843, 5843, 5843],
                       [5843, 5843, 5843, 5843, 5990,    0, 5923, 5990, 5342,
                        5486, 5350, 5485, 5486, 5493, 5350, 5843, 5843, 5843,
                        3118,    0, 7688, 7250, 7930, 5990]]),
       values=tensor([0.9566, 0.9525, 0.9382, 0.9203, 0.9542, 1.5114, 0.9290,
                      0.9542, 0.8308, 0.8780, 0.9450, 0.9321, 0.9545, 0.9494,
                      0.9497, 0.9109, 0.9504, 0.9365, 0.9551, 3.5091, 0.8756,
                      0.9435, 0.9587, 0.7814]),
       size=(7931, 7931), nnz=24, layout=torch.sparse_coo,
       grad_fn=<SparseCooTensorWithDimsAndTensorsBackward0>)
Counter({'isAbout': 6, 'author': 4, 'isWorkedOnBy': 3, 'publication': 2, 'hasProject': 2, 'dealtWithIn': 1, 'fax': 1, 'phone': 1, 'name': 1, 'homepage': 1, 'worksAtProject': 1, 'projectInfo': 1})
dict index: {}
node_idx 5843
 node original label [0]
 node predicted label explain 0
 node prediction probability explain tensor([0.6448, 0.2206, 0.0661, 0.0685], grad_fn=<SoftmaxBackward0>)
 node predicted label full 0 most important relations  {'isWorkedOnBy': 3, 'dealtWithIn': 1, 'fax': 1, 'phone': 1, 'name': 1, 'homepage': 1, 'worksAtProject': 1, 'publication': 2, 'isAbout': 6, 'projectInfo': 1, 'author': 4, 'hasProject': 2, 'label': 0, 'node_idx': '5843'}
 final masks and lenght tensor(indices=tensor([[ 23820,  23954,  24202,  24258,  24500,  63345,  63451,
                         63480,  63481,  63481,  63488,  63488,  88693, 115089,
                        115337, 115393, 115393, 130118, 146768, 146768, 146768,
                        146835, 146835, 146835, 146835, 148775, 154480, 154568,
                        154586, 154611, 154615, 154616, 154623, 179908, 179975,
                        196398, 229538, 246188, 246188, 246255, 246255, 254393,
                        254393, 254393, 254393, 254393, 304103, 328958],
                       [  5843,   5843,   5843,   5843,   5843,   5990,   5923,
                          5990,   5923,   5990,   5923,   5990,      0,   5990,
                          5923,   5923,   5990,   5342,   5456,   5486,   5493,
                          5350,   5485,   5486,   5493,   5350,   5843,   5843,
                          5843,   5843,   5843,   5843,   5843,   5843,   5843,
                          3118,      0,   7632,   7688,   7384,   7688,   7250,
                          7384,   7632,   7688,   7930,   5990,   5230]]),
       values=tensor([0.9566, 0.9525, 0.9382, 0.3548, 0.9203, 0.3767, 0.4018,
                      0.3874, 0.3758, 0.9542, 0.3774, 0.3968, 1.5114, 0.3688,
                      0.3750, 0.9290, 0.9542, 0.8308, 0.3719, 0.8780, 0.3756,
                      0.9450, 0.9321, 0.9545, 0.9494, 0.9497, 0.9109, 0.9504,
                      0.3862, 0.4078, 0.3851, 0.9365, 0.3714, 0.3703, 0.3877,
                      0.9551, 3.5091, 0.3767, 0.8756, 0.3756, 0.3913, 0.9435,
                      0.3706, 0.3847, 0.3771, 0.9587, 0.7814, 0.3725]),
       size=(753935, 8285), nnz=48, layout=torch.sparse_coo,
       grad_fn=<SparseCooTensorWithDimsAndTensorsBackward0>) 24
 ---------------------------------------------------------------
r_exp.py:102: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  hor_graph, ver_graph = torch.tensor(sub_hor_graph, dtype=torch.float), torch.tensor(sub_ver_graph, dtype=torch.float)
wandb: WARNING Calling wandb.login() after wandb.init() has no effect.
node label: 0
5
num_high 4 len(mask) 5
mask_without_small tensor([0.7708, 0.7468, 0.7592, 0.7587, 0.5720], grad_fn=<IndexBackward0>)
pred_loss tensor([11.5808], grad_fn=<MulBackward0>)
size_loss tensor(-8.4024, grad_fn=<MulBackward0>)
size_num_loss 0.4
loss: tensor([4.5174], grad_fn=<AddBackward0>)
0
epoch:  0 ; loss:  4.517417907714844 ; pred:  tensor([0.3141, 0.2673, 0.2520, 0.1666], grad_fn=<SoftmaxBackward0>)
num_high 4 len(mask) 5
mask_without_small tensor([0.7880, 0.7652, 0.7770, 0.7766, 0.5473], grad_fn=<IndexBackward0>)
pred_loss tensor([11.5770], grad_fn=<MulBackward0>)
size_loss tensor(-10.2895, grad_fn=<MulBackward0>)
size_num_loss 0.4
loss: tensor([2.6153], grad_fn=<AddBackward0>)
1
epoch:  1 ; loss:  2.6153342723846436 ; pred:  tensor([0.3142, 0.2680, 0.2530, 0.1647], grad_fn=<SoftmaxBackward0>)
num_high 4 len(mask) 5
mask_without_small tensor([0.8042, 0.7827, 0.7938, 0.7934, 0.5224], grad_fn=<IndexBackward0>)
pred_loss tensor([11.5745], grad_fn=<MulBackward0>)
size_loss tensor(-12.1469, grad_fn=<MulBackward0>)
size_num_loss 0.4
loss: tensor([0.7430], grad_fn=<AddBackward0>)
2
epoch:  2 ; loss:  0.742963433265686 ; pred:  tensor([0.3143, 0.2687, 0.2540, 0.1630], grad_fn=<SoftmaxBackward0>)
num_high 4 len(mask) 5
mask_without_small tensor([0.8193, 0.7993, 0.8096, 0.8092], grad_fn=<IndexBackward0>)
pred_loss tensor([11.5732], grad_fn=<MulBackward0>)
size_loss tensor(-81.5522, grad_fn=<MulBackward0>)
size_num_loss 0.4
loss: tensor([-69.0185], grad_fn=<AddBackward0>)
3
epoch:  3 ; loss:  -69.01847076416016 ; pred:  tensor([0.3143, 0.2693, 0.2550, 0.1613], grad_fn=<SoftmaxBackward0>)
num_high 4 len(mask) 5
mask_without_small tensor([0.8278, 0.7898, 0.8205, 0.8062], grad_fn=<IndexBackward0>)
pred_loss tensor([11.5673], grad_fn=<MulBackward0>)
size_loss tensor(-167.6259, grad_fn=<MulBackward0>)
size_num_loss 0.4
loss: tensor([-155.1006], grad_fn=<AddBackward0>)
4
epoch:  4 ; loss:  -155.10057067871094 ; pred:  tensor([0.3145, 0.2696, 0.2547, 0.1611], grad_fn=<SoftmaxBackward0>)
num_high 4 len(mask) 5
mask_without_small tensor([0.8379, 0.7774, 0.8290, 0.7974], grad_fn=<IndexBackward0>)
pred_loss tensor([11.5628], grad_fn=<MulBackward0>)
size_loss tensor(-280.2694, grad_fn=<MulBackward0>)
size_num_loss 0.4
loss: tensor([-267.7492], grad_fn=<AddBackward0>)
5
epoch:  5 ; loss:  -267.74920654296875 ; pred:  tensor([0.3147, 0.2699, 0.2544, 0.1611], grad_fn=<SoftmaxBackward0>)
num_high 4 len(mask) 5
mask_without_small tensor([0.8485, 0.7630, 0.8390, 0.7860], grad_fn=<IndexBackward0>)
pred_loss tensor([11.5568], grad_fn=<MulBackward0>)
size_loss tensor(-412.7152, grad_fn=<MulBackward0>)
size_num_loss 0.4
loss: tensor([-400.2019], grad_fn=<AddBackward0>)
6
epoch:  6 ; loss:  -400.201904296875 ; pred:  tensor([0.3148, 0.2702, 0.2538, 0.1611], grad_fn=<SoftmaxBackward0>)
num_high 4 len(mask) 5
mask_without_small tensor([0.8591, 0.7469, 0.8496, 0.7725], grad_fn=<IndexBackward0>)
pred_loss tensor([11.5499], grad_fn=<MulBackward0>)
size_loss tensor(-558.0015, grad_fn=<MulBackward0>)
size_num_loss 0.4
loss: tensor([-545.4961], grad_fn=<AddBackward0>)
7
epoch:  7 ; loss:  -545.49609375 ; pred:  tensor([0.3151, 0.2707, 0.2531, 0.1612], grad_fn=<SoftmaxBackward0>)
num_high 4 len(mask) 5
mask_without_small tensor([0.8694, 0.7293, 0.8604, 0.7573], grad_fn=<IndexBackward0>)
pred_loss tensor([11.5427], grad_fn=<MulBackward0>)
size_loss tensor(-712.3170, grad_fn=<MulBackward0>)
size_num_loss 0.4
loss: tensor([-699.8200], grad_fn=<AddBackward0>)
8
epoch:  8 ; loss:  -699.8200073242188 ; pred:  tensor([0.3153, 0.2711, 0.2522, 0.1614], grad_fn=<SoftmaxBackward0>)
num_high 4 len(mask) 5
mask_without_small tensor([0.8792, 0.7104, 0.8709, 0.7404], grad_fn=<IndexBackward0>)
pred_loss tensor([11.5354], grad_fn=<MulBackward0>)
size_loss tensor(-873.3895, grad_fn=<MulBackward0>)
size_num_loss 0.4
loss: tensor([-860.9011], grad_fn=<AddBackward0>)
9
epoch:  9 ; loss:  -860.901123046875 ; pred:  tensor([0.3155, 0.2715, 0.2512, 0.1618], grad_fn=<SoftmaxBackward0>)
num_high 4 len(mask) 5
mask_without_small tensor([0.8885, 0.6903, 0.8811, 0.7220], grad_fn=<IndexBackward0>)
pred_loss tensor([11.5283], grad_fn=<MulBackward0>)
size_loss tensor(-1039.7629, grad_fn=<MulBackward0>)
size_num_loss 0.4
loss: tensor([-1027.2831], grad_fn=<AddBackward0>)
10
epoch:  10 ; loss:  -1027.2830810546875 ; pred:  tensor([0.3157, 0.2719, 0.2502, 0.1622], grad_fn=<SoftmaxBackward0>)
num_high 4 len(mask) 5
mask_without_small tensor([0.8971, 0.6691, 0.8908, 0.7022], grad_fn=<IndexBackward0>)
pred_loss tensor([11.5215], grad_fn=<MulBackward0>)
size_loss tensor(-1210.4352, grad_fn=<MulBackward0>)
size_num_loss 0.4
loss: tensor([-1197.9637], grad_fn=<AddBackward0>)
11
epoch:  11 ; loss:  -1197.9637451171875 ; pred:  tensor([0.3160, 0.2724, 0.2490, 0.1626], grad_fn=<SoftmaxBackward0>)
num_high 4 len(mask) 5
mask_without_small tensor([0.9051, 0.6468, 0.8998, 0.6811], grad_fn=<IndexBackward0>)
pred_loss tensor([11.5152], grad_fn=<MulBackward0>)
size_loss tensor(-1384.6567, grad_fn=<MulBackward0>)
size_num_loss 0.4
loss: tensor([-1372.1935], grad_fn=<AddBackward0>)
12
epoch:  12 ; loss:  -1372.1934814453125 ; pred:  tensor([0.3162, 0.2728, 0.2479, 0.1632], grad_fn=<SoftmaxBackward0>)
num_high 4 len(mask) 5
mask_without_small tensor([0.9125, 0.6235, 0.9083, 0.6586], grad_fn=<IndexBackward0>)
pred_loss tensor([11.5095], grad_fn=<MulBackward0>)
size_loss tensor(-1561.8088, grad_fn=<MulBackward0>)
size_num_loss 0.4
loss: tensor([-1549.3536], grad_fn=<AddBackward0>)
13
epoch:  13 ; loss:  -1549.3536376953125 ; pred:  tensor([0.3163, 0.2731, 0.2467, 0.1639], grad_fn=<SoftmaxBackward0>)
num_high 3 len(mask) 5
mask_without_small tensor([0.9193, 0.5994, 0.9161, 0.6349], grad_fn=<IndexBackward0>)
pred_loss tensor([11.5045], grad_fn=<MulBackward0>)
size_loss tensor(-1741.3210, grad_fn=<MulBackward0>)
size_num_loss 0.30000000000000004
loss: tensor([-1728.9735], grad_fn=<AddBackward0>)
14
epoch:  14 ; loss:  -1728.9735107421875 ; pred:  tensor([0.3165, 0.2735, 0.2454, 0.1646], grad_fn=<SoftmaxBackward0>)
num_high 2 len(mask) 5
mask_without_small tensor([0.9255, 0.5745, 0.9232, 0.6101], grad_fn=<IndexBackward0>)
pred_loss tensor([11.5001], grad_fn=<MulBackward0>)
size_loss tensor(-1922.6155, grad_fn=<MulBackward0>)
size_num_loss 0.2
loss: tensor([-1910.3756], grad_fn=<AddBackward0>)
15
epoch:  15 ; loss:  -1910.3756103515625 ; pred:  tensor([0.3166, 0.2738, 0.2442, 0.1654], grad_fn=<SoftmaxBackward0>)
num_high 2 len(mask) 5
mask_without_small tensor([0.9312, 0.5491, 0.9298, 0.5844], grad_fn=<IndexBackward0>)
pred_loss tensor([11.4965], grad_fn=<MulBackward0>)
size_loss tensor(-2105.0747, grad_fn=<MulBackward0>)
size_num_loss 0.2
loss: tensor([-2092.8420], grad_fn=<AddBackward0>)
16
epoch:  16 ; loss:  -2092.842041015625 ; pred:  tensor([0.3167, 0.2742, 0.2429, 0.1662], grad_fn=<SoftmaxBackward0>)
num_high 2 len(mask) 5
mask_without_small tensor([0.9364, 0.5232, 0.9357, 0.5578], grad_fn=<IndexBackward0>)
pred_loss tensor([11.4936], grad_fn=<MulBackward0>)
size_loss tensor(-2288.0166, grad_fn=<MulBackward0>)
size_num_loss 0.2
loss: tensor([-2275.7913], grad_fn=<AddBackward0>)
17
epoch:  17 ; loss:  -2275.791259765625 ; pred:  tensor([0.3168, 0.2745, 0.2416, 0.1671], grad_fn=<SoftmaxBackward0>)
num_high 2 len(mask) 5
mask_without_small tensor([0.9411, 0.9411, 0.5305], grad_fn=<IndexBackward0>)
pred_loss tensor([11.4914], grad_fn=<MulBackward0>)
size_loss tensor(-2370.4136, grad_fn=<MulBackward0>)
size_num_loss 0.2
loss: tensor([11.4914], grad_fn=<MulBackward0>)
18
epoch:  18 ; loss:  11.491432189941406 ; pred:  tensor([0.3169, 0.2748, 0.2403, 0.1680], grad_fn=<SoftmaxBackward0>)
num_high 2 len(mask) 5
mask_without_small tensor([0.9451, 0.9457, 0.5056], grad_fn=<IndexBackward0>)
pred_loss tensor([11.4900], grad_fn=<MulBackward0>)
size_loss tensor(-2538.9109, grad_fn=<MulBackward0>)
size_num_loss 0.2
loss: tensor([11.4900], grad_fn=<MulBackward0>)
19
epoch:  19 ; loss:  11.489959716796875 ; pred:  tensor([0.3170, 0.2750, 0.2392, 0.1688], grad_fn=<SoftmaxBackward0>)
Finished Training
sel masked ver tensor(indices=tensor([[5873, 5873, 5873],
                       [  81,   34, 5648]]),
       values=tensor([0.9451, 0.9457, 0.5056]),
       size=(5874, 5649), nnz=3, layout=torch.sparse_coo,
       grad_fn=<SparseCooTensorWithDimsAndTensorsBackward0>)
Counter({'phone': 1, 'fax': 1, 'photo': 1})
dict index: {}
node_idx 5873
 node original label [0]
 node predicted label explain 0
 node prediction probability explain tensor([0.3170, 0.2750, 0.2392, 0.1688], grad_fn=<SoftmaxBackward0>)
 node predicted label full 0 most important relations  {'phone': 1, 'fax': 1, 'photo': 1, 'label': 0, 'node_idx': '5873'}
 final masks and lenght tensor(indices=tensor([[ 88723, 196428, 229568, 237853, 328988],
                       [    81,   2258,     34,   5648,   5230]]),
       values=tensor([0.9451, 0.4734, 0.9457, 0.5056, 0.3609]),
       size=(753935, 8285), nnz=5, layout=torch.sparse_coo,
       grad_fn=<SparseCooTensorWithDimsAndTensorsBackward0>) 3
 ---------------------------------------------------------------
node label: 0
4
num_high 4 len(mask) 5
mask_without_small tensor([0.7708, 0.7468, 0.7592, 0.7587, 0.5720], grad_fn=<IndexBackward0>)
pred_loss tensor([10.1398], grad_fn=<MulBackward0>)
size_loss tensor(-8.4024, grad_fn=<MulBackward0>)
size_num_loss 0.4
loss: tensor([3.0764], grad_fn=<AddBackward0>)
r_exp.py:102: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  hor_graph, ver_graph = torch.tensor(sub_hor_graph, dtype=torch.float), torch.tensor(sub_ver_graph, dtype=torch.float)
wandb: WARNING Calling wandb.login() after wandb.init() has no effect.
0
epoch:  0 ; loss:  3.0763533115386963 ; pred:  tensor([0.3628, 0.2503, 0.2652, 0.1217], grad_fn=<SoftmaxBackward0>)
num_high 4 len(mask) 5
mask_without_small tensor([0.7880, 0.7652, 0.7770, 0.7766, 0.5473], grad_fn=<IndexBackward0>)
pred_loss tensor([10.1078], grad_fn=<MulBackward0>)
size_loss tensor(-10.2895, grad_fn=<MulBackward0>)
size_num_loss 0.4
loss: tensor([1.1461], grad_fn=<AddBackward0>)
1
epoch:  1 ; loss:  1.1460683345794678 ; pred:  tensor([0.3639, 0.2503, 0.2663, 0.1195], grad_fn=<SoftmaxBackward0>)
num_high 4 len(mask) 5
mask_without_small tensor([0.8042, 0.7827, 0.7938, 0.7934, 0.5224], grad_fn=<IndexBackward0>)
pred_loss tensor([10.0787], grad_fn=<MulBackward0>)
size_loss tensor(-12.1468, grad_fn=<MulBackward0>)
size_num_loss 0.4
loss: tensor([-0.7528], grad_fn=<AddBackward0>)
2
epoch:  2 ; loss:  -0.7527912855148315 ; pred:  tensor([0.3650, 0.2502, 0.2675, 0.1173], grad_fn=<SoftmaxBackward0>)
num_high 4 len(mask) 5
mask_without_small tensor([0.8193, 0.7992, 0.8096, 0.8092], grad_fn=<IndexBackward0>)
pred_loss tensor([10.0524], grad_fn=<MulBackward0>)
size_loss tensor(-81.6772, grad_fn=<MulBackward0>)
size_num_loss 0.4
loss: tensor([-70.6643], grad_fn=<AddBackward0>)
3
epoch:  3 ; loss:  -70.66426086425781 ; pred:  tensor([0.3660, 0.2502, 0.2685, 0.1153], grad_fn=<SoftmaxBackward0>)
num_high 4 len(mask) 5
mask_without_small tensor([0.8278, 0.7898, 0.8203, 0.8067], grad_fn=<IndexBackward0>)
pred_loss tensor([10.0873], grad_fn=<MulBackward0>)
size_loss tensor(-166.7864, grad_fn=<MulBackward0>)
size_num_loss 0.4
loss: tensor([-155.7412], grad_fn=<AddBackward0>)
4
epoch:  4 ; loss:  -155.74119567871094 ; pred:  tensor([0.3647, 0.2509, 0.2699, 0.1145], grad_fn=<SoftmaxBackward0>)
num_high 4 len(mask) 5
mask_without_small tensor([0.8379, 0.7774, 0.8287, 0.7979], grad_fn=<IndexBackward0>)
pred_loss tensor([10.1287], grad_fn=<MulBackward0>)
size_loss tensor(-279.0013, grad_fn=<MulBackward0>)
size_num_loss 0.4
loss: tensor([-267.9153], grad_fn=<AddBackward0>)
5
epoch:  5 ; loss:  -267.91534423828125 ; pred:  tensor([0.3632, 0.2513, 0.2716, 0.1139], grad_fn=<SoftmaxBackward0>)
num_high 4 len(mask) 5
mask_without_small tensor([0.8485, 0.7630, 0.8387, 0.7866], grad_fn=<IndexBackward0>)
pred_loss tensor([10.1767], grad_fn=<MulBackward0>)
size_loss tensor(-411.0756, grad_fn=<MulBackward0>)
size_num_loss 0.4
loss: tensor([-399.9425], grad_fn=<AddBackward0>)
6
epoch:  6 ; loss:  -399.9425048828125 ; pred:  tensor([0.3614, 0.2517, 0.2736, 0.1133], grad_fn=<SoftmaxBackward0>)
num_high 4 len(mask) 5
mask_without_small tensor([0.8591, 0.7469, 0.8494, 0.7732], grad_fn=<IndexBackward0>)
pred_loss tensor([10.2296], grad_fn=<MulBackward0>)
size_loss tensor(-556.0496, grad_fn=<MulBackward0>)
size_num_loss 0.4
loss: tensor([-544.8646], grad_fn=<AddBackward0>)
7
epoch:  7 ; loss:  -544.8645629882812 ; pred:  tensor([0.3595, 0.2520, 0.2758, 0.1127], grad_fn=<SoftmaxBackward0>)
num_high 4 len(mask) 5
mask_without_small tensor([0.8694, 0.7293, 0.8602, 0.7580], grad_fn=<IndexBackward0>)
pred_loss tensor([10.2867], grad_fn=<MulBackward0>)
size_loss tensor(-710.0989, grad_fn=<MulBackward0>)
size_num_loss 0.4
loss: tensor([-698.8580], grad_fn=<AddBackward0>)
8
epoch:  8 ; loss:  -698.8579711914062 ; pred:  tensor([0.3575, 0.2524, 0.2780, 0.1122], grad_fn=<SoftmaxBackward0>)
num_high 4 len(mask) 5
mask_without_small tensor([0.8793, 0.7105, 0.8708, 0.7413], grad_fn=<IndexBackward0>)
pred_loss tensor([10.3474], grad_fn=<MulBackward0>)
size_loss tensor(-870.9395, grad_fn=<MulBackward0>)
size_num_loss 0.4
loss: tensor([-859.6390], grad_fn=<AddBackward0>)
9
epoch:  9 ; loss:  -859.6390380859375 ; pred:  tensor([0.3553, 0.2527, 0.2802, 0.1117], grad_fn=<SoftmaxBackward0>)
num_high 4 len(mask) 5
mask_without_small tensor([0.8885, 0.6904, 0.8809, 0.7230], grad_fn=<IndexBackward0>)
pred_loss tensor([10.4116], grad_fn=<MulBackward0>)
size_loss tensor(-1037.1072, grad_fn=<MulBackward0>)
size_num_loss 0.4
loss: tensor([-1025.7440], grad_fn=<AddBackward0>)
10
epoch:  10 ; loss:  -1025.7440185546875 ; pred:  tensor([0.3530, 0.2530, 0.2826, 0.1114], grad_fn=<SoftmaxBackward0>)
num_high 4 len(mask) 5
mask_without_small tensor([0.8971, 0.6691, 0.8906, 0.7032], grad_fn=<IndexBackward0>)
pred_loss tensor([10.4790], grad_fn=<MulBackward0>)
size_loss tensor(-1207.5941, grad_fn=<MulBackward0>)
size_num_loss 0.4
loss: tensor([-1196.1653], grad_fn=<AddBackward0>)
11
epoch:  11 ; loss:  -1196.165283203125 ; pred:  tensor([0.3507, 0.2533, 0.2849, 0.1111], grad_fn=<SoftmaxBackward0>)
num_high 4 len(mask) 5
mask_without_small tensor([0.9051, 0.6468, 0.8997, 0.6821], grad_fn=<IndexBackward0>)
pred_loss tensor([10.5493], grad_fn=<MulBackward0>)
size_loss tensor(-1381.6484, grad_fn=<MulBackward0>)
size_num_loss 0.4
loss: tensor([-1370.1511], grad_fn=<AddBackward0>)
12
epoch:  12 ; loss:  -1370.151123046875 ; pred:  tensor([0.3482, 0.2536, 0.2873, 0.1109], grad_fn=<SoftmaxBackward0>)
num_high 4 len(mask) 5
mask_without_small tensor([0.9125, 0.6235, 0.9082, 0.6597], grad_fn=<IndexBackward0>)
pred_loss tensor([10.6224], grad_fn=<MulBackward0>)
size_loss tensor(-1558.6498, grad_fn=<MulBackward0>)
size_num_loss 0.4
loss: tensor([-1547.0817], grad_fn=<AddBackward0>)
13
epoch:  13 ; loss:  -1547.0816650390625 ; pred:  tensor([0.3457, 0.2538, 0.2898, 0.1108], grad_fn=<SoftmaxBackward0>)
num_high 3 len(mask) 5
mask_without_small tensor([0.9193, 0.5994, 0.9160, 0.6360], grad_fn=<IndexBackward0>)
pred_loss tensor([10.6980], grad_fn=<MulBackward0>)
size_loss tensor(-1738.0269, grad_fn=<MulBackward0>)
size_num_loss 0.30000000000000004
loss: tensor([-1726.4857], grad_fn=<AddBackward0>)
14
epoch:  14 ; loss:  -1726.4857177734375 ; pred:  tensor([0.3431, 0.2539, 0.2923, 0.1107], grad_fn=<SoftmaxBackward0>)
num_high 2 len(mask) 5
mask_without_small tensor([0.9255, 0.5746, 0.9232, 0.6113], grad_fn=<IndexBackward0>)
pred_loss tensor([10.7758], grad_fn=<MulBackward0>)
size_loss tensor(-1919.2047, grad_fn=<MulBackward0>)
size_num_loss 0.2
loss: tensor([-1907.6891], grad_fn=<AddBackward0>)
15
epoch:  15 ; loss:  -1907.6890869140625 ; pred:  tensor([0.3404, 0.2540, 0.2948, 0.1108], grad_fn=<SoftmaxBackward0>)
num_high 2 len(mask) 5
mask_without_small tensor([0.9312, 0.5492, 0.9297, 0.5856], grad_fn=<IndexBackward0>)
pred_loss tensor([10.8554], grad_fn=<MulBackward0>)
size_loss tensor(-2101.5671, grad_fn=<MulBackward0>)
size_num_loss 0.2
loss: tensor([-2089.9756], grad_fn=<AddBackward0>)
16
epoch:  16 ; loss:  -2089.9755859375 ; pred:  tensor([0.3377, 0.2541, 0.2973, 0.1109], grad_fn=<SoftmaxBackward0>)
num_high 2 len(mask) 5
mask_without_small tensor([0.9364, 0.5233, 0.9357, 0.5590], grad_fn=<IndexBackward0>)
pred_loss tensor([10.9365], grad_fn=<MulBackward0>)
size_loss tensor(-2284.4333, grad_fn=<MulBackward0>)
size_num_loss 0.2
loss: tensor([-2272.7651], grad_fn=<AddBackward0>)
17
epoch:  17 ; loss:  -2272.76513671875 ; pred:  tensor([0.3350, 0.2541, 0.2998, 0.1110], grad_fn=<SoftmaxBackward0>)
num_high 2 len(mask) 5
mask_without_small tensor([0.9411, 0.9411, 0.5318], grad_fn=<IndexBackward0>)
pred_loss tensor([11.0187], grad_fn=<MulBackward0>)
size_loss tensor(-2363.1477, grad_fn=<MulBackward0>)
size_num_loss 0.2
loss: tensor([11.0187], grad_fn=<MulBackward0>)
18
epoch:  18 ; loss:  11.018733978271484 ; pred:  tensor([0.3322, 0.2541, 0.3024, 0.1112], grad_fn=<SoftmaxBackward0>)
num_high 2 len(mask) 5
mask_without_small tensor([0.9451, 0.9457, 0.5069], grad_fn=<IndexBackward0>)
pred_loss tensor([11.0937], grad_fn=<MulBackward0>)
size_loss tensor(-2531.6707, grad_fn=<MulBackward0>)
size_num_loss 0.2
loss: tensor([11.0937], grad_fn=<MulBackward0>)
19
epoch:  19 ; loss:  11.093650817871094 ; pred:  tensor([0.3298, 0.2541, 0.3047, 0.1114], grad_fn=<SoftmaxBackward0>)
Finished Training
sel masked ver tensor(indices=tensor([[5697, 5697, 5697],
                       [   0, 4568,    0]]),
       values=tensor([3.7802, 0.9457, 2.0274]),
       size=(5698, 4569), nnz=3, layout=torch.sparse_coo,
       grad_fn=<SparseCooTensorWithDimsAndTensorsBackward0>)
Counter({'fax': 1, 'phone': 1, 'name': 1})
dict index: {}
node_idx 5697
 node original label [0]
 node predicted label explain 0
 node prediction probability explain tensor([0.3631, 0.2871, 0.2776, 0.0723], grad_fn=<SoftmaxBackward0>)
 node predicted label full 0 most important relations  {'fax': 1, 'phone': 1, 'name': 1, 'label': 0, 'node_idx': '5697'}
 final masks and lenght tensor(indices=tensor([[ 88547, 129972, 196252, 229392, 328812],
                       [     0,   3162,   4568,      0,   5230]]),
       values=tensor([3.7802, 0.4735, 0.9457, 2.0274, 0.3617]),
       size=(753935, 8285), nnz=5, layout=torch.sparse_coo,
       grad_fn=<SparseCooTensorWithDimsAndTensorsBackward0>) 3
 ---------------------------------------------------------------
r_exp.py:102: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  hor_graph, ver_graph = torch.tensor(sub_hor_graph, dtype=torch.float), torch.tensor(sub_ver_graph, dtype=torch.float)
wandb: WARNING Calling wandb.login() after wandb.init() has no effect.
node label: 0
12
num_high 14 len(mask) 16
mask_without_small tensor([0.8431, 0.8214, 0.7889, 0.5636, 0.7755, 0.6373, 0.7281, 0.6065, 0.6757,
        0.8296, 0.7029, 0.6233, 0.6776, 0.6904, 0.6744, 0.7807],
       grad_fn=<IndexBackward0>)
pred_loss tensor([9.6006], grad_fn=<MulBackward0>)
size_loss tensor(-8.5220, grad_fn=<MulBackward0>)
size_num_loss 1.4000000000000001
loss: tensor([4.2023], grad_fn=<AddBackward0>)
0
epoch:  0 ; loss:  4.2022929191589355 ; pred:  tensor([0.3829, 0.2449, 0.2079, 0.1644], grad_fn=<SoftmaxBackward0>)
num_high 12 len(mask) 16
mask_without_small tensor([0.8559, 0.8356, 0.8051, 0.5388, 0.7925, 0.6138, 0.7474, 0.5824, 0.6534,
        0.8433, 0.6816, 0.5996, 0.6554, 0.6687, 0.6521, 0.7973],
       grad_fn=<IndexBackward0>)
pred_loss tensor([9.7069], grad_fn=<MulBackward0>)
size_loss tensor(-10.2849, grad_fn=<MulBackward0>)
size_num_loss 1.2000000000000002
loss: tensor([2.3338], grad_fn=<AddBackward0>)
1
epoch:  1 ; loss:  2.333803653717041 ; pred:  tensor([0.3788, 0.2472, 0.2082, 0.1658], grad_fn=<SoftmaxBackward0>)
num_high 12 len(mask) 16
mask_without_small tensor([0.8677, 0.8488, 0.8203, 0.5139, 0.8084, 0.5899, 0.7642, 0.5579, 0.6304,
        0.8560, 0.6611, 0.5754, 0.6326, 0.6487, 0.6292, 0.8130],
       grad_fn=<IndexBackward0>)
pred_loss tensor([9.8094], grad_fn=<MulBackward0>)
size_loss tensor(-12.0751, grad_fn=<MulBackward0>)
size_num_loss 1.2000000000000002
loss: tensor([0.6322], grad_fn=<AddBackward0>)
2
epoch:  2 ; loss:  0.6321746110916138 ; pred:  tensor([0.3750, 0.2495, 0.2084, 0.1672], grad_fn=<SoftmaxBackward0>)
num_high 9 len(mask) 16
mask_without_small tensor([0.8786, 0.8611, 0.8346, 0.8235, 0.5654, 0.7806, 0.5332, 0.6067, 0.8677,
        0.6396, 0.5507, 0.6093, 0.6275, 0.6057, 0.8277],
       grad_fn=<IndexBackward0>)
pred_loss tensor([9.9155], grad_fn=<MulBackward0>)
size_loss tensor(-1320.8026, grad_fn=<MulBackward0>)
size_num_loss 0.9
loss: tensor([-1309.3094], grad_fn=<AddBackward0>)
3
epoch:  3 ; loss:  -1309.3094482421875 ; pred:  tensor([0.3710, 0.2517, 0.2086, 0.1686], grad_fn=<SoftmaxBackward0>)
num_high 8 len(mask) 16
mask_without_small tensor([0.8848, 0.8680, 0.8426, 0.8320, 0.5508, 0.7905, 0.5184, 0.5925, 0.8744,
        0.6260, 0.5361, 0.5953, 0.6138, 0.5916, 0.8361],
       grad_fn=<IndexBackward0>)
pred_loss tensor([9.9804], grad_fn=<MulBackward0>)
size_loss tensor(-1430.1986, grad_fn=<MulBackward0>)
size_num_loss 0.8
loss: tensor([-1418.7446], grad_fn=<AddBackward0>)
4
epoch:  4 ; loss:  -1418.74462890625 ; pred:  tensor([0.3686, 0.2531, 0.2088, 0.1695], grad_fn=<SoftmaxBackward0>)
num_high 7 len(mask) 16
mask_without_small tensor([0.8921, 0.8763, 0.8522, 0.8421, 0.5325, 0.8025, 0.5745, 0.8823, 0.6086,
        0.5176, 0.5774, 0.5962, 0.5738, 0.8460], grad_fn=<IndexBackward0>)
pred_loss tensor([10.0623], grad_fn=<MulBackward0>)
size_loss tensor(-1522.6011, grad_fn=<MulBackward0>)
size_num_loss 0.7000000000000001
loss: tensor([-1511.1755], grad_fn=<AddBackward0>)
5
epoch:  5 ; loss:  -1511.175537109375 ; pred:  tensor([0.3656, 0.2547, 0.2090, 0.1706], grad_fn=<SoftmaxBackward0>)
num_high 7 len(mask) 16
mask_without_small tensor([0.8997, 0.8849, 0.8622, 0.8527, 0.5119, 0.8152, 0.5543, 0.8906, 0.5889,
        0.5573, 0.5763, 0.5536, 0.8564], grad_fn=<IndexBackward0>)
pred_loss tensor([10.1531], grad_fn=<MulBackward0>)
size_loss tensor(-1624.5109, grad_fn=<MulBackward0>)
size_num_loss 0.7000000000000001
loss: tensor([-1613.0063], grad_fn=<AddBackward0>)
6
epoch:  6 ; loss:  -1613.00634765625 ; pred:  tensor([0.3623, 0.2566, 0.2092, 0.1719], grad_fn=<SoftmaxBackward0>)
num_high 7 len(mask) 16
mask_without_small tensor([0.9073, 0.8935, 0.8722, 0.8633, 0.8280, 0.5326, 0.8988, 0.5678, 0.5356,
        0.5550, 0.5319, 0.8668], grad_fn=<IndexBackward0>)
pred_loss tensor([10.2470], grad_fn=<MulBackward0>)
size_loss tensor(-1719.3293, grad_fn=<MulBackward0>)
size_num_loss 0.7000000000000001
loss: tensor([-1707.7432], grad_fn=<AddBackward0>)
7
epoch:  7 ; loss:  -1707.7431640625 ; pred:  tensor([0.3589, 0.2584, 0.2094, 0.1733], grad_fn=<SoftmaxBackward0>)
num_high 7 len(mask) 16
mask_without_small tensor([0.9146, 0.9017, 0.8819, 0.8736, 0.8406, 0.5098, 0.9066, 0.5456, 0.5130,
        0.5326, 0.5092, 0.8769], grad_fn=<IndexBackward0>)
pred_loss tensor([10.3421], grad_fn=<MulBackward0>)
size_loss tensor(-1881.5356, grad_fn=<MulBackward0>)
size_num_loss 0.7000000000000001
loss: tensor([-1869.8628], grad_fn=<AddBackward0>)
8
epoch:  8 ; loss:  -1869.86279296875 ; pred:  tensor([0.3555, 0.2602, 0.2096, 0.1747], grad_fn=<SoftmaxBackward0>)
num_high 7 len(mask) 16
mask_without_small tensor([0.9214, 0.9095, 0.8912, 0.8835, 0.8527, 0.9141, 0.5224, 0.5091, 0.8865],
       grad_fn=<IndexBackward0>)
pred_loss tensor([10.4379], grad_fn=<MulBackward0>)
size_loss tensor(-1681.0533, grad_fn=<MulBackward0>)
size_num_loss 0.7000000000000001
loss: tensor([-1669.3082], grad_fn=<AddBackward0>)
9
epoch:  9 ; loss:  -1669.3082275390625 ; pred:  tensor([0.3521, 0.2620, 0.2097, 0.1761], grad_fn=<SoftmaxBackward0>)
num_high 7 len(mask) 16
mask_without_small tensor([0.9278, 0.9168, 0.8998, 0.8926, 0.8640, 0.9210, 0.5003, 0.8955],
       grad_fn=<IndexBackward0>)
pred_loss tensor([10.5216], grad_fn=<MulBackward0>)
size_loss tensor(-1436.3529, grad_fn=<MulBackward0>)
size_num_loss 0.7000000000000001
loss: tensor([-1424.5377], grad_fn=<AddBackward0>)
10
epoch:  10 ; loss:  -1424.5377197265625 ; pred:  tensor([0.3492, 0.2636, 0.2098, 0.1773], grad_fn=<SoftmaxBackward0>)
num_high 7 len(mask) 16
mask_without_small tensor([0.9337, 0.9235, 0.9077, 0.9010, 0.8738, 0.9274, 0.9036],
       grad_fn=<IndexBackward0>)
pred_loss tensor([10.5941], grad_fn=<MulBackward0>)
size_loss tensor(-203.5182, grad_fn=<MulBackward0>)
size_num_loss 0.7000000000000001
loss: tensor([-191.6441], grad_fn=<AddBackward0>)
11
epoch:  11 ; loss:  -191.64410400390625 ; pred:  tensor([0.3467, 0.2651, 0.2098, 0.1784], grad_fn=<SoftmaxBackward0>)
num_high 7 len(mask) 16
mask_without_small tensor([0.9393, 0.9298, 0.9140, 0.9065, 0.8731, 0.9335, 0.9095],
       grad_fn=<IndexBackward0>)
pred_loss tensor([10.6588], grad_fn=<MulBackward0>)
size_loss tensor(-223.9523, grad_fn=<MulBackward0>)
size_num_loss 0.7000000000000001
loss: tensor([-212.0204], grad_fn=<AddBackward0>)
12
epoch:  12 ; loss:  -212.0203857421875 ; pred:  tensor([0.3444, 0.2664, 0.2098, 0.1794], grad_fn=<SoftmaxBackward0>)
num_high 7 len(mask) 16
mask_without_small tensor([0.9446, 0.9357, 0.9192, 0.9101, 0.8690, 0.9392, 0.9139],
       grad_fn=<IndexBackward0>)
pred_loss tensor([10.7160], grad_fn=<MulBackward0>)
size_loss tensor(-256.5007, grad_fn=<MulBackward0>)
size_num_loss 0.7000000000000001
loss: tensor([-244.5177], grad_fn=<AddBackward0>)
13
epoch:  13 ; loss:  -244.5176544189453 ; pred:  tensor([0.3425, 0.2675, 0.2097, 0.1803], grad_fn=<SoftmaxBackward0>)
num_high 7 len(mask) 16
mask_without_small tensor([0.9496, 0.9412, 0.9237, 0.9125, 0.8626, 0.9445, 0.9173],
       grad_fn=<IndexBackward0>)
pred_loss tensor([10.7664], grad_fn=<MulBackward0>)
size_loss tensor(-296.5399, grad_fn=<MulBackward0>)
size_num_loss 0.7000000000000001
loss: tensor([-284.5119], grad_fn=<AddBackward0>)
14
epoch:  14 ; loss:  -284.5119323730469 ; pred:  tensor([0.3407, 0.2685, 0.2096, 0.1812], grad_fn=<SoftmaxBackward0>)
num_high 7 len(mask) 16
mask_without_small tensor([0.9541, 0.9462, 0.9277, 0.9139, 0.8543, 0.9494, 0.9199],
       grad_fn=<IndexBackward0>)
pred_loss tensor([10.8109], grad_fn=<MulBackward0>)
size_loss tensor(-342.1972, grad_fn=<MulBackward0>)
size_num_loss 0.7000000000000001
loss: tensor([-330.1299], grad_fn=<AddBackward0>)
15
epoch:  15 ; loss:  -330.1298522949219 ; pred:  tensor([0.3392, 0.2693, 0.2095, 0.1819], grad_fn=<SoftmaxBackward0>)
num_high 7 len(mask) 16
mask_without_small tensor([0.9582, 0.9508, 0.9313, 0.9146, 0.8444, 0.9538, 0.9221],
       grad_fn=<IndexBackward0>)
pred_loss tensor([10.8500], grad_fn=<MulBackward0>)
size_loss tensor(-392.6908, grad_fn=<MulBackward0>)
size_num_loss 0.7000000000000001
loss: tensor([-380.5888], grad_fn=<AddBackward0>)
16
epoch:  16 ; loss:  -380.5887756347656 ; pred:  tensor([0.3379, 0.2701, 0.2094, 0.1826], grad_fn=<SoftmaxBackward0>)
num_high 7 len(mask) 16
mask_without_small tensor([0.9619, 0.9549, 0.9346, 0.9147, 0.8328, 0.9578, 0.9238],
       grad_fn=<IndexBackward0>)
pred_loss tensor([10.8846], grad_fn=<MulBackward0>)
size_loss tensor(-447.7655, grad_fn=<MulBackward0>)
size_num_loss 0.7000000000000001
loss: tensor([-435.6329], grad_fn=<AddBackward0>)
17
epoch:  17 ; loss:  -435.6329040527344 ; pred:  tensor([0.3367, 0.2708, 0.2093, 0.1832], grad_fn=<SoftmaxBackward0>)
num_high 7 len(mask) 16
mask_without_small tensor([0.9652, 0.9587, 0.9378, 0.9142, 0.8196, 0.9614, 0.9253],
       grad_fn=<IndexBackward0>)
pred_loss tensor([10.9152], grad_fn=<MulBackward0>)
size_loss tensor(-507.4441, grad_fn=<MulBackward0>)
size_num_loss 0.7000000000000001
loss: tensor([-495.2845], grad_fn=<AddBackward0>)
18
epoch:  18 ; loss:  -495.28448486328125 ; pred:  tensor([0.3357, 0.2714, 0.2091, 0.1838], grad_fn=<SoftmaxBackward0>)
num_high 7 len(mask) 16
mask_without_small tensor([0.9682, 0.9621, 0.9408, 0.9133, 0.8049, 0.9646, 0.9266],
       grad_fn=<IndexBackward0>)
pred_loss tensor([10.9424], grad_fn=<MulBackward0>)
size_loss tensor(-571.8914, grad_fn=<MulBackward0>)
size_num_loss 0.7000000000000001
loss: tensor([-559.7079], grad_fn=<AddBackward0>)
19
epoch:  19 ; loss:  -559.7078857421875 ; pred:  tensor([0.3348, 0.2719, 0.2090, 0.1843], grad_fn=<SoftmaxBackward0>)
Finished Training
sel masked ver tensor(indices=tensor([[6841, 7733, 7734, 5783, 5414, 5783, 5783],
                       [5783, 5783, 5783,   58, 5783,   62, 5230]]),
       values=tensor([0.9682, 0.9621, 0.9408, 0.9133, 0.8049, 0.9646, 0.9266]),
       size=(7735, 5784), nnz=7, layout=torch.sparse_coo,
       grad_fn=<SparseCooTensorWithDimsAndTensorsBackward0>)
Counter({'author': 3, 'isWorkedOnBy': 1, 'fax': 1, 'phone': 1, 'type': 1})
dict index: {}
node_idx 5783
 node original label [0]
 node predicted label explain 0
 node prediction probability explain tensor([0.3348, 0.2719, 0.2090, 0.1843], grad_fn=<SoftmaxBackward0>)
 node predicted label full 0 most important relations  {'isWorkedOnBy': 1, 'fax': 1, 'phone': 1, 'type': 1, 'author': 3, 'label': 0, 'node_idx': '5783'}
 final masks and lenght tensor(indices=tensor([[ 23411,  24303,  24304,  24306,  88633, 154486, 154544,
                        154562, 196338, 229478, 237763, 254333, 254333, 254333,
                        254333, 328898],
                       [  5783,   5783,   5783,   5783,     58,   5783,   5783,
                          5783,   1288,     62,   5583,   6841,   7733,   7734,
                          7736,   5230]]),
       values=tensor([0.9682, 0.9621, 0.9408, 0.3536, 0.9133, 0.3570, 0.8049,
                      0.3852, 0.3524, 0.9646, 0.3695, 0.3700, 0.3559, 0.3657,
                      0.3526, 0.9266]),
       size=(753935, 8285), nnz=16, layout=torch.sparse_coo,
       grad_fn=<SparseCooTensorWithDimsAndTensorsBackward0>) 7
 ---------------------------------------------------------------
r_exp.py:102: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  hor_graph, ver_graph = torch.tensor(sub_hor_graph, dtype=torch.float), torch.tensor(sub_ver_graph, dtype=torch.float)
wandb: WARNING Calling wandb.login() after wandb.init() has no effect.
node label: 0
4
num_high 4 len(mask) 5
mask_without_small tensor([0.7708, 0.7468, 0.7592, 0.7587, 0.5720], grad_fn=<IndexBackward0>)
pred_loss tensor([10.1398], grad_fn=<MulBackward0>)
size_loss tensor(-8.4024, grad_fn=<MulBackward0>)
size_num_loss 0.4
loss: tensor([3.0764], grad_fn=<AddBackward0>)
0
epoch:  0 ; loss:  3.0763533115386963 ; pred:  tensor([0.3628, 0.2503, 0.2652, 0.1217], grad_fn=<SoftmaxBackward0>)
num_high 4 len(mask) 5
mask_without_small tensor([0.7880, 0.7652, 0.7770, 0.7766, 0.5473], grad_fn=<IndexBackward0>)
pred_loss tensor([10.1078], grad_fn=<MulBackward0>)
size_loss tensor(-10.2895, grad_fn=<MulBackward0>)
size_num_loss 0.4
loss: tensor([1.1461], grad_fn=<AddBackward0>)
1
epoch:  1 ; loss:  1.1460683345794678 ; pred:  tensor([0.3639, 0.2503, 0.2663, 0.1195], grad_fn=<SoftmaxBackward0>)
num_high 4 len(mask) 5
mask_without_small tensor([0.8042, 0.7827, 0.7938, 0.7934, 0.5224], grad_fn=<IndexBackward0>)
pred_loss tensor([10.0787], grad_fn=<MulBackward0>)
size_loss tensor(-12.1468, grad_fn=<MulBackward0>)
size_num_loss 0.4
loss: tensor([-0.7528], grad_fn=<AddBackward0>)
2
epoch:  2 ; loss:  -0.7527912855148315 ; pred:  tensor([0.3650, 0.2502, 0.2675, 0.1173], grad_fn=<SoftmaxBackward0>)
num_high 4 len(mask) 5
mask_without_small tensor([0.8193, 0.7992, 0.8096, 0.8092], grad_fn=<IndexBackward0>)
pred_loss tensor([10.0524], grad_fn=<MulBackward0>)
size_loss tensor(-81.6772, grad_fn=<MulBackward0>)
size_num_loss 0.4
loss: tensor([-70.6643], grad_fn=<AddBackward0>)
3
epoch:  3 ; loss:  -70.66426086425781 ; pred:  tensor([0.3660, 0.2502, 0.2685, 0.1153], grad_fn=<SoftmaxBackward0>)
num_high 4 len(mask) 5
mask_without_small tensor([0.8278, 0.7898, 0.8203, 0.8067], grad_fn=<IndexBackward0>)
pred_loss tensor([10.0873], grad_fn=<MulBackward0>)
size_loss tensor(-166.7864, grad_fn=<MulBackward0>)
size_num_loss 0.4
loss: tensor([-155.7412], grad_fn=<AddBackward0>)
4
epoch:  4 ; loss:  -155.74119567871094 ; pred:  tensor([0.3647, 0.2509, 0.2699, 0.1145], grad_fn=<SoftmaxBackward0>)
num_high 4 len(mask) 5
mask_without_small tensor([0.8379, 0.7774, 0.8287, 0.7979], grad_fn=<IndexBackward0>)
pred_loss tensor([10.1287], grad_fn=<MulBackward0>)
size_loss tensor(-279.0013, grad_fn=<MulBackward0>)
size_num_loss 0.4
loss: tensor([-267.9153], grad_fn=<AddBackward0>)
5
epoch:  5 ; loss:  -267.91534423828125 ; pred:  tensor([0.3632, 0.2513, 0.2716, 0.1139], grad_fn=<SoftmaxBackward0>)
num_high 4 len(mask) 5
mask_without_small tensor([0.8485, 0.7630, 0.8387, 0.7866], grad_fn=<IndexBackward0>)
pred_loss tensor([10.1767], grad_fn=<MulBackward0>)
size_loss tensor(-411.0756, grad_fn=<MulBackward0>)
size_num_loss 0.4
loss: tensor([-399.9425], grad_fn=<AddBackward0>)
6
epoch:  6 ; loss:  -399.9425048828125 ; pred:  tensor([0.3614, 0.2517, 0.2736, 0.1133], grad_fn=<SoftmaxBackward0>)
num_high 4 len(mask) 5
mask_without_small tensor([0.8591, 0.7469, 0.8494, 0.7732], grad_fn=<IndexBackward0>)
pred_loss tensor([10.2296], grad_fn=<MulBackward0>)
size_loss tensor(-556.0496, grad_fn=<MulBackward0>)
size_num_loss 0.4
loss: tensor([-544.8646], grad_fn=<AddBackward0>)
7
epoch:  7 ; loss:  -544.8645629882812 ; pred:  tensor([0.3595, 0.2520, 0.2758, 0.1127], grad_fn=<SoftmaxBackward0>)
num_high 4 len(mask) 5
mask_without_small tensor([0.8694, 0.7293, 0.8602, 0.7580], grad_fn=<IndexBackward0>)
pred_loss tensor([10.2867], grad_fn=<MulBackward0>)
size_loss tensor(-710.0989, grad_fn=<MulBackward0>)
size_num_loss 0.4
loss: tensor([-698.8580], grad_fn=<AddBackward0>)
8
epoch:  8 ; loss:  -698.8579711914062 ; pred:  tensor([0.3575, 0.2524, 0.2780, 0.1122], grad_fn=<SoftmaxBackward0>)
num_high 4 len(mask) 5
mask_without_small tensor([0.8793, 0.7105, 0.8708, 0.7413], grad_fn=<IndexBackward0>)
pred_loss tensor([10.3474], grad_fn=<MulBackward0>)
size_loss tensor(-870.9395, grad_fn=<MulBackward0>)
size_num_loss 0.4
loss: tensor([-859.6390], grad_fn=<AddBackward0>)
9
epoch:  9 ; loss:  -859.6390380859375 ; pred:  tensor([0.3553, 0.2527, 0.2802, 0.1117], grad_fn=<SoftmaxBackward0>)
num_high 4 len(mask) 5
mask_without_small tensor([0.8885, 0.6904, 0.8809, 0.7230], grad_fn=<IndexBackward0>)
pred_loss tensor([10.4116], grad_fn=<MulBackward0>)
size_loss tensor(-1037.1072, grad_fn=<MulBackward0>)
size_num_loss 0.4
loss: tensor([-1025.7440], grad_fn=<AddBackward0>)
10
epoch:  10 ; loss:  -1025.7440185546875 ; pred:  tensor([0.3530, 0.2530, 0.2826, 0.1114], grad_fn=<SoftmaxBackward0>)
num_high 4 len(mask) 5
mask_without_small tensor([0.8971, 0.6691, 0.8906, 0.7032], grad_fn=<IndexBackward0>)
pred_loss tensor([10.4790], grad_fn=<MulBackward0>)
size_loss tensor(-1207.5941, grad_fn=<MulBackward0>)
size_num_loss 0.4
loss: tensor([-1196.1653], grad_fn=<AddBackward0>)
11
epoch:  11 ; loss:  -1196.165283203125 ; pred:  tensor([0.3507, 0.2533, 0.2849, 0.1111], grad_fn=<SoftmaxBackward0>)
num_high 4 len(mask) 5
mask_without_small tensor([0.9051, 0.6468, 0.8997, 0.6821], grad_fn=<IndexBackward0>)
pred_loss tensor([10.5493], grad_fn=<MulBackward0>)
size_loss tensor(-1381.6484, grad_fn=<MulBackward0>)
size_num_loss 0.4
loss: tensor([-1370.1511], grad_fn=<AddBackward0>)
12
epoch:  12 ; loss:  -1370.151123046875 ; pred:  tensor([0.3482, 0.2536, 0.2873, 0.1109], grad_fn=<SoftmaxBackward0>)
num_high 4 len(mask) 5
mask_without_small tensor([0.9125, 0.6235, 0.9082, 0.6597], grad_fn=<IndexBackward0>)
pred_loss tensor([10.6224], grad_fn=<MulBackward0>)
size_loss tensor(-1558.6498, grad_fn=<MulBackward0>)
size_num_loss 0.4
loss: tensor([-1547.0817], grad_fn=<AddBackward0>)
13
epoch:  13 ; loss:  -1547.0816650390625 ; pred:  tensor([0.3457, 0.2538, 0.2898, 0.1108], grad_fn=<SoftmaxBackward0>)
num_high 3 len(mask) 5
mask_without_small tensor([0.9193, 0.5994, 0.9160, 0.6360], grad_fn=<IndexBackward0>)
pred_loss tensor([10.6980], grad_fn=<MulBackward0>)
size_loss tensor(-1738.0269, grad_fn=<MulBackward0>)
size_num_loss 0.30000000000000004
loss: tensor([-1726.4857], grad_fn=<AddBackward0>)
14
epoch:  14 ; loss:  -1726.4857177734375 ; pred:  tensor([0.3431, 0.2539, 0.2923, 0.1107], grad_fn=<SoftmaxBackward0>)
num_high 2 len(mask) 5
mask_without_small tensor([0.9255, 0.5746, 0.9232, 0.6113], grad_fn=<IndexBackward0>)
pred_loss tensor([10.7758], grad_fn=<MulBackward0>)
size_loss tensor(-1919.2047, grad_fn=<MulBackward0>)
size_num_loss 0.2
loss: tensor([-1907.6891], grad_fn=<AddBackward0>)
15
epoch:  15 ; loss:  -1907.6890869140625 ; pred:  tensor([0.3404, 0.2540, 0.2948, 0.1108], grad_fn=<SoftmaxBackward0>)
num_high 2 len(mask) 5
mask_without_small tensor([0.9312, 0.5492, 0.9297, 0.5856], grad_fn=<IndexBackward0>)
pred_loss tensor([10.8554], grad_fn=<MulBackward0>)
size_loss tensor(-2101.5671, grad_fn=<MulBackward0>)
size_num_loss 0.2
loss: tensor([-2089.9756], grad_fn=<AddBackward0>)
16
epoch:  16 ; loss:  -2089.9755859375 ; pred:  tensor([0.3377, 0.2541, 0.2973, 0.1109], grad_fn=<SoftmaxBackward0>)
num_high 2 len(mask) 5
mask_without_small tensor([0.9364, 0.5233, 0.9357, 0.5590], grad_fn=<IndexBackward0>)
pred_loss tensor([10.9365], grad_fn=<MulBackward0>)
size_loss tensor(-2284.4333, grad_fn=<MulBackward0>)
size_num_loss 0.2
loss: tensor([-2272.7651], grad_fn=<AddBackward0>)
17
epoch:  17 ; loss:  -2272.76513671875 ; pred:  tensor([0.3350, 0.2541, 0.2998, 0.1110], grad_fn=<SoftmaxBackward0>)
num_high 2 len(mask) 5
mask_without_small tensor([0.9411, 0.9411, 0.5318], grad_fn=<IndexBackward0>)
pred_loss tensor([11.0187], grad_fn=<MulBackward0>)
size_loss tensor(-2363.1477, grad_fn=<MulBackward0>)
size_num_loss 0.2
loss: tensor([11.0187], grad_fn=<MulBackward0>)
18
epoch:  18 ; loss:  11.018733978271484 ; pred:  tensor([0.3322, 0.2541, 0.3024, 0.1112], grad_fn=<SoftmaxBackward0>)
num_high 2 len(mask) 5
mask_without_small tensor([0.9451, 0.9457, 0.5069], grad_fn=<IndexBackward0>)
pred_loss tensor([11.0937], grad_fn=<MulBackward0>)
size_loss tensor(-2531.6707, grad_fn=<MulBackward0>)
size_num_loss 0.2
loss: tensor([11.0937], grad_fn=<MulBackward0>)
19
epoch:  19 ; loss:  11.093650817871094 ; pred:  tensor([0.3298, 0.2541, 0.3047, 0.1114], grad_fn=<SoftmaxBackward0>)
Finished Training
sel masked ver tensor(indices=tensor([[5701, 5701, 5701],
                       [   0, 3957,    0]]),
       values=tensor([3.7802, 0.9457, 2.0274]),
       size=(5702, 3958), nnz=3, layout=torch.sparse_coo,
       grad_fn=<SparseCooTensorWithDimsAndTensorsBackward0>)
Counter({'fax': 1, 'phone': 1, 'name': 1})
dict index: {}
node_idx 5701
 node original label [0]
 node predicted label explain 0
 node prediction probability explain tensor([0.3631, 0.2871, 0.2776, 0.0723], grad_fn=<SoftmaxBackward0>)
 node predicted label full 0 most important relations  {'fax': 1, 'phone': 1, 'name': 1, 'label': 0, 'node_idx': '5701'}
 final masks and lenght tensor(indices=tensor([[ 88551, 129976, 196256, 229396, 328816],
                       [     0,   3162,   3957,      0,   5230]]),
       values=tensor([3.7802, 0.4735, 0.9457, 2.0274, 0.3617]),
       size=(753935, 8285), nnz=5, layout=torch.sparse_coo,
       grad_fn=<SparseCooTensorWithDimsAndTensorsBackward0>) 3
 ---------------------------------------------------------------
r_exp.py:102: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  hor_graph, ver_graph = torch.tensor(sub_hor_graph, dtype=torch.float), torch.tensor(sub_ver_graph, dtype=torch.float)
wandb: WARNING Calling wandb.login() after wandb.init() has no effect.
node label: 0
29
num_high 111 len(mask) 111
mask_without_small tensor([0.7788, 0.7685, 0.7542, 0.6720, 0.7486, 0.6973, 0.7299, 0.6867, 0.7108,
        0.7723, 0.7206, 0.6924, 0.7114, 0.7160, 0.7103, 0.7507, 0.7721, 0.7268,
        0.7177, 0.7425, 0.7106, 0.7585, 0.7517, 0.7730, 0.7635, 0.7639, 0.7469,
        0.7648, 0.7249, 0.7322, 0.7244, 0.7531, 0.6930, 0.7075, 0.7251, 0.7739,
        0.7394, 0.7197, 0.7391, 0.7101, 0.6880, 0.7565, 0.7072, 0.7149, 0.6961,
        0.7833, 0.6973, 0.7180, 0.7063, 0.7133, 0.7331, 0.7447, 0.7180, 0.7613,
        0.7090, 0.7112, 0.6925, 0.7320, 0.7294, 0.7485, 0.7285, 0.7769, 0.6987,
        0.7660, 0.7675, 0.7531, 0.7855, 0.7446, 0.7401, 0.7258, 0.7023, 0.7634,
        0.7265, 0.7447, 0.7326, 0.7422, 0.7460, 0.7138, 0.6690, 0.7108, 0.7313,
        0.7220, 0.6942, 0.7153, 0.7450, 0.7447, 0.7601, 0.7324, 0.7502, 0.7182,
        0.7025, 0.7467, 0.6833, 0.7087, 0.7648, 0.7417, 0.6909, 0.7283, 0.7150,
        0.7435, 0.7498, 0.7335, 0.7207, 0.7635, 0.7307, 0.7374, 0.7345, 0.7507,
        0.7590, 0.7399, 0.7496], grad_fn=<IndexBackward0>)
pred_loss tensor([0.2617], grad_fn=<MulBackward0>)
size_loss tensor(-2.5902, grad_fn=<MulBackward0>)
size_num_loss 11.100000000000001
loss: tensor([17.4771], grad_fn=<AddBackward0>)
0
epoch:  0 ; loss:  17.477088928222656 ; pred:  tensor([0.9742, 0.0095, 0.0013, 0.0150], grad_fn=<SoftmaxBackward0>)
num_high 111 len(mask) 111
mask_without_small tensor([0.7955, 0.7858, 0.7722, 0.6496, 0.7293, 0.6757, 0.7098, 0.6648, 0.6898,
        0.7894, 0.7000, 0.6708, 0.6905, 0.6953, 0.6893, 0.7315, 0.7893, 0.7065,
        0.6970, 0.7229, 0.6896, 0.7764, 0.7699, 0.7901, 0.7810, 0.7814, 0.7275,
        0.7823, 0.7045, 0.7121, 0.7040, 0.7713, 0.6713, 0.6863, 0.7047, 0.7909,
        0.7197, 0.6991, 0.7193, 0.6891, 0.6662, 0.7745, 0.6861, 0.6941, 0.6746,
        0.7998, 0.6757, 0.6973, 0.6851, 0.6925, 0.7131, 0.7252, 0.6973, 0.7790,
        0.6880, 0.6902, 0.6708, 0.7119, 0.7092, 0.7292, 0.7082, 0.7937, 0.6772,
        0.7834, 0.7848, 0.7712, 0.8018, 0.7252, 0.7204, 0.7055, 0.6810, 0.7810,
        0.7062, 0.7252, 0.7125, 0.7226, 0.7265, 0.6929, 0.6465, 0.6898, 0.7112,
        0.7015, 0.6726, 0.6945, 0.7255, 0.7252, 0.7779, 0.7124, 0.7685, 0.6975,
        0.6812, 0.7273, 0.6612, 0.6876, 0.7823, 0.7221, 0.6692, 0.7081, 0.6941,
        0.7239, 0.7681, 0.7135, 0.7001, 0.7810, 0.7106, 0.7175, 0.7146, 0.7690,
        0.7768, 0.7202, 0.7304], grad_fn=<IndexBackward0>)
pred_loss tensor([0.2682], grad_fn=<MulBackward0>)
size_loss tensor(-4.0218, grad_fn=<MulBackward0>)
size_num_loss 11.100000000000001
loss: tensor([15.9425], grad_fn=<AddBackward0>)
1
epoch:  1 ; loss:  15.942475318908691 ; pred:  tensor([0.9735, 0.0098, 0.0014, 0.0153], grad_fn=<SoftmaxBackward0>)
num_high 111 len(mask) 111
mask_without_small tensor([0.8112, 0.8021, 0.7873, 0.6268, 0.7119, 0.6536, 0.6887, 0.6423, 0.6680,
        0.8055, 0.6786, 0.6484, 0.6687, 0.6737, 0.6675, 0.7158, 0.8054, 0.6853,
        0.6755, 0.7032, 0.6678, 0.7928, 0.7829, 0.8062, 0.7976, 0.7980, 0.7091,
        0.7988, 0.6833, 0.6912, 0.6827, 0.7857, 0.6490, 0.6645, 0.6835, 0.8070,
        0.6994, 0.6776, 0.6990, 0.6673, 0.6437, 0.7904, 0.6642, 0.6725, 0.6524,
        0.8152, 0.6536, 0.6758, 0.6632, 0.6708, 0.6923, 0.7061, 0.6758, 0.7956,
        0.6662, 0.6685, 0.6485, 0.6910, 0.6881, 0.7118, 0.6871, 0.8096, 0.6551,
        0.7999, 0.8012, 0.7867, 0.8171, 0.7061, 0.7003, 0.6843, 0.6590, 0.7976,
        0.6850, 0.7062, 0.6917, 0.7030, 0.7083, 0.6713, 0.6236, 0.6680, 0.6903,
        0.6801, 0.6504, 0.6729, 0.7063, 0.7078, 0.7946, 0.6915, 0.7838, 0.6760,
        0.6592, 0.7107, 0.6387, 0.6658, 0.7989, 0.7025, 0.6468, 0.6870, 0.6725,
        0.7049, 0.7813, 0.6927, 0.6787, 0.7976, 0.6896, 0.6970, 0.6939, 0.7849,
        0.7936, 0.7002, 0.7132], grad_fn=<IndexBackward0>)
pred_loss tensor([0.2777], grad_fn=<MulBackward0>)
size_loss tensor(-5.5577, grad_fn=<MulBackward0>)
size_num_loss 11.100000000000001
loss: tensor([14.2976], grad_fn=<AddBackward0>)
2
epoch:  2 ; loss:  14.297553062438965 ; pred:  tensor([0.9726, 0.0101, 0.0015, 0.0158], grad_fn=<SoftmaxBackward0>)
num_high 106 len(mask) 111
mask_without_small tensor([0.8257, 0.8175, 0.8024, 0.6036, 0.6927, 0.6308, 0.6668, 0.6194, 0.6455,
        0.8205, 0.6563, 0.6256, 0.6463, 0.6513, 0.6450, 0.6976, 0.8204, 0.6633,
        0.6532, 0.6825, 0.6454, 0.8085, 0.7972, 0.8211, 0.8133, 0.8136, 0.6894,
        0.8144, 0.6612, 0.6694, 0.6606, 0.8005, 0.6262, 0.6419, 0.6614, 0.8218,
        0.6782, 0.6554, 0.6778, 0.6449, 0.6209, 0.8060, 0.6417, 0.6501, 0.6296,
        0.8292, 0.6308, 0.6535, 0.6406, 0.6484, 0.6705, 0.6858, 0.6535, 0.8113,
        0.6437, 0.6460, 0.6256, 0.6693, 0.6662, 0.6926, 0.6652, 0.8242, 0.6324,
        0.8154, 0.8167, 0.8021, 0.8310, 0.6858, 0.6793, 0.6622, 0.6364, 0.8133,
        0.6629, 0.6860, 0.6699, 0.6823, 0.6886, 0.6489, 0.6005, 0.6456, 0.6685,
        0.6580, 0.6276, 0.6506, 0.6859, 0.6885, 0.8104, 0.6698, 0.7992, 0.6537,
        0.6366, 0.6919, 0.6157, 0.6432, 0.8144, 0.6817, 0.6240, 0.6651, 0.6501,
        0.6846, 0.7958, 0.6710, 0.6564, 0.8132, 0.6678, 0.6756, 0.6723, 0.8006,
        0.8096, 0.6793, 0.6942], grad_fn=<IndexBackward0>)
pred_loss tensor([0.2911], grad_fn=<MulBackward0>)
size_loss tensor(-7.1340, grad_fn=<MulBackward0>)
size_num_loss 10.600000000000001
loss: tensor([12.1057], grad_fn=<AddBackward0>)
3
epoch:  3 ; loss:  12.105669021606445 ; pred:  tensor([0.9713, 0.0107, 0.0016, 0.0164], grad_fn=<SoftmaxBackward0>)
num_high 87 len(mask) 111
mask_without_small tensor([0.8387, 0.8315, 0.8171, 0.5803, 0.6722, 0.6077, 0.6441, 0.5961, 0.6225,
        0.8342, 0.6334, 0.6024, 0.6232, 0.6283, 0.6220, 0.6778, 0.8341, 0.6405,
        0.6302, 0.6607, 0.6223, 0.8233, 0.8115, 0.8348, 0.8278, 0.8281, 0.6684,
        0.8288, 0.6383, 0.6468, 0.6377, 0.8151, 0.6030, 0.6188, 0.6386, 0.8354,
        0.6561, 0.6324, 0.6556, 0.6218, 0.5976, 0.8208, 0.6186, 0.6271, 0.6064,
        0.8419, 0.6077, 0.6305, 0.6175, 0.6253, 0.6480, 0.6643, 0.6305, 0.8260,
        0.6206, 0.6230, 0.6024, 0.6466, 0.6435, 0.6720, 0.6424, 0.8376, 0.6092,
        0.8297, 0.8309, 0.8169, 0.8436, 0.6644, 0.6572, 0.6394, 0.6132, 0.8278,
        0.6401, 0.6646, 0.6473, 0.6606, 0.6676, 0.6259, 0.5772, 0.6225, 0.6459,
        0.6350, 0.6045, 0.6276, 0.6645, 0.6678, 0.8252, 0.6472, 0.8141, 0.6307,
        0.6135, 0.6716, 0.5925, 0.6202, 0.8289, 0.6600, 0.6008, 0.6423, 0.6271,
        0.6632, 0.8103, 0.6485, 0.6335, 0.8278, 0.6451, 0.6533, 0.6498, 0.8157,
        0.8245, 0.6573, 0.6738], grad_fn=<IndexBackward0>)
pred_loss tensor([0.3092], grad_fn=<MulBackward0>)
size_loss tensor(-8.7157, grad_fn=<MulBackward0>)
size_num_loss 8.700000000000001
loss: tensor([8.5030], grad_fn=<AddBackward0>)
4
epoch:  4 ; loss:  8.502951622009277 ; pred:  tensor([0.9696, 0.0114, 0.0017, 0.0173], grad_fn=<SoftmaxBackward0>)
num_high 58 len(mask) 111
mask_without_small tensor([0.8502, 0.8442, 0.8309, 0.5569, 0.6505, 0.5842, 0.6207, 0.5727, 0.5989,
        0.8464, 0.6099, 0.5789, 0.5997, 0.6048, 0.5984, 0.6566, 0.8463, 0.6170,
        0.6067, 0.6380, 0.5987, 0.8369, 0.8252, 0.8469, 0.8409, 0.8412, 0.6463,
        0.8419, 0.6148, 0.6235, 0.6142, 0.8289, 0.5795, 0.5953, 0.6151, 0.8474,
        0.6331, 0.6089, 0.6326, 0.5982, 0.5742, 0.8345, 0.5950, 0.6035, 0.5829,
        0.8529, 0.5842, 0.6070, 0.5940, 0.6018, 0.6247, 0.6419, 0.6069, 0.8393,
        0.5970, 0.5994, 0.5789, 0.6233, 0.6201, 0.6503, 0.6190, 0.8494, 0.5858,
        0.8427, 0.8437, 0.8309, 0.8546, 0.6420, 0.6343, 0.6159, 0.5897, 0.8411,
        0.6167, 0.6423, 0.6240, 0.6379, 0.6455, 0.6023, 0.5539, 0.5990, 0.6225,
        0.6115, 0.5810, 0.6040, 0.6420, 0.6460, 0.8389, 0.6239, 0.8282, 0.6073,
        0.5902, 0.6501, 0.5691, 0.5966, 0.8420, 0.6373, 0.5774, 0.6189, 0.6036,
        0.6408, 0.8242, 0.6252, 0.6100, 0.8409, 0.6217, 0.6302, 0.6266, 0.8299,
        0.8383, 0.6344, 0.6522], grad_fn=<IndexBackward0>)
pred_loss tensor([0.3321], grad_fn=<MulBackward0>)
size_loss tensor(-10.2795, grad_fn=<MulBackward0>)
size_num_loss 5.800000000000001
loss: tensor([3.9128], grad_fn=<AddBackward0>)
5
epoch:  5 ; loss:  3.912837505340576 ; pred:  tensor([0.9673, 0.0123, 0.0019, 0.0185], grad_fn=<SoftmaxBackward0>)
num_high 37 len(mask) 111
mask_without_small tensor([0.8601, 0.8551, 0.8434, 0.5336, 0.6278, 0.5605, 0.5967, 0.5492, 0.5750,
        0.8569, 0.5858, 0.5553, 0.5757, 0.5808, 0.5745, 0.6344, 0.8569, 0.5930,
        0.5827, 0.6145, 0.5748, 0.8489, 0.8379, 0.8573, 0.8525, 0.8527, 0.6233,
        0.8532, 0.5908, 0.5995, 0.5901, 0.8414, 0.5559, 0.5714, 0.5910, 0.8577,
        0.6094, 0.5849, 0.6089, 0.5743, 0.5506, 0.8468, 0.5712, 0.5795, 0.5593,
        0.8623, 0.5605, 0.5830, 0.5701, 0.5778, 0.6007, 0.6187, 0.5829, 0.8511,
        0.5731, 0.5755, 0.5553, 0.5993, 0.5961, 0.6276, 0.5950, 0.8597, 0.5621,
        0.8540, 0.8549, 0.8437, 0.8641, 0.6188, 0.6107, 0.5918, 0.5660, 0.8529,
        0.5926, 0.6190, 0.6001, 0.6144, 0.6225, 0.5785, 0.5308, 0.5752, 0.5985,
        0.5876, 0.5575, 0.5801, 0.6187, 0.6232, 0.8513, 0.5999, 0.8413, 0.5834,
        0.5666, 0.6275, 0.5456, 0.5727, 0.8536, 0.6138, 0.5539, 0.5948, 0.5796,
        0.6175, 0.8372, 0.6012, 0.5859, 0.8523, 0.5978, 0.6063, 0.6027, 0.8432,
        0.8511, 0.6108, 0.6295], grad_fn=<IndexBackward0>)
pred_loss tensor([0.3602], grad_fn=<MulBackward0>)
size_loss tensor(-11.8064, grad_fn=<MulBackward0>)
size_num_loss 3.7
loss: tensor([0.1553], grad_fn=<AddBackward0>)
6
epoch:  6 ; loss:  0.15528106689453125 ; pred:  tensor([0.9646, 0.0134, 0.0021, 0.0198], grad_fn=<SoftmaxBackward0>)
num_high 29 len(mask) 111
mask_without_small tensor([0.8681, 0.8642, 0.8542, 0.5105, 0.6042, 0.5367, 0.5722, 0.5257, 0.5509,
        0.8656, 0.5615, 0.5317, 0.5516, 0.5565, 0.5504, 0.6112, 0.8655, 0.5685,
        0.5584, 0.5903, 0.5507, 0.8591, 0.8490, 0.8659, 0.8621, 0.8623, 0.5996,
        0.8627, 0.5663, 0.5751, 0.5657, 0.8524, 0.5322, 0.5474, 0.5666, 0.8662,
        0.5851, 0.5605, 0.5845, 0.5502, 0.5271, 0.8572, 0.5471, 0.5553, 0.5355,
        0.8699, 0.5367, 0.5586, 0.5461, 0.5536, 0.5762, 0.5946, 0.5586, 0.8610,
        0.5491, 0.5513, 0.5317, 0.5748, 0.5716, 0.6040, 0.5705, 0.8684, 0.5385,
        0.8635, 0.8644, 0.8550, 0.8720, 0.5948, 0.5864, 0.5674, 0.5421, 0.8630,
        0.5682, 0.5951, 0.5757, 0.5903, 0.5988, 0.5544, 0.5079, 0.5512, 0.5741,
        0.5632, 0.5339, 0.5560, 0.5947, 0.5996, 0.8623, 0.5755, 0.8531, 0.5593,
        0.5430, 0.6042, 0.5223, 0.5487, 0.8636, 0.5897, 0.5304, 0.5704, 0.5553,
        0.5935, 0.8488, 0.5768, 0.5615, 0.8617, 0.5734, 0.5819, 0.5783, 0.8552,
        0.8627, 0.5865, 0.6060], grad_fn=<IndexBackward0>)
pred_loss tensor([0.3944], grad_fn=<MulBackward0>)
size_loss tensor(-13.2791, grad_fn=<MulBackward0>)
size_num_loss 2.9000000000000004
loss: tensor([-2.2511], grad_fn=<AddBackward0>)
7
epoch:  7 ; loss:  -2.2510790824890137 ; pred:  tensor([0.9613, 0.0147, 0.0024, 0.0215], grad_fn=<SoftmaxBackward0>)
num_high 29 len(mask) 111
mask_without_small tensor([0.8743, 0.8712, 0.8628, 0.5799, 0.5130, 0.5474, 0.5024, 0.5266, 0.8723,
        0.5369, 0.5081, 0.5273, 0.5321, 0.5262, 0.5872, 0.8723, 0.5438, 0.5339,
        0.5656, 0.5265, 0.8671, 0.8582, 0.8726, 0.8696, 0.8697, 0.5751, 0.8701,
        0.5416, 0.5503, 0.5410, 0.8612, 0.5087, 0.5233, 0.5419, 0.8728, 0.5603,
        0.5360, 0.5597, 0.5260, 0.5037, 0.8655, 0.5230, 0.5309, 0.5118, 0.8758,
        0.5130, 0.5342, 0.5221, 0.5293, 0.5514, 0.5700, 0.5342, 0.8687, 0.5249,
        0.5271, 0.5082, 0.5500, 0.5468, 0.5797, 0.5457, 0.8754, 0.5149, 0.8710,
        0.8722, 0.8646, 0.8784, 0.5702, 0.5616, 0.5427, 0.5182, 0.8712, 0.5434,
        0.5705, 0.5510, 0.5657, 0.5744, 0.5302, 0.5271, 0.5493, 0.5388, 0.5105,
        0.5317, 0.5701, 0.5755, 0.8717, 0.5507, 0.8634, 0.5352, 0.5196, 0.5801,
        0.5245, 0.8719, 0.5650, 0.5071, 0.5457, 0.5309, 0.5690, 0.8589, 0.5520,
        0.5369, 0.8689, 0.5487, 0.5570, 0.5536, 0.8660, 0.8732, 0.5618, 0.5818],
       grad_fn=<IndexBackward0>)
pred_loss tensor([0.4353], grad_fn=<MulBackward0>)
size_loss tensor(-1470.8989, grad_fn=<MulBackward0>)
size_num_loss 2.9000000000000004
loss: tensor([-1466.2734], grad_fn=<AddBackward0>)
8
epoch:  8 ; loss:  -1466.2734375 ; pred:  tensor([0.9574, 0.0164, 0.0028, 0.0235], grad_fn=<SoftmaxBackward0>)
num_high 29 len(mask) 111
mask_without_small tensor([0.8797, 0.8767, 0.8686, 0.5671, 0.5345, 0.5136, 0.8777, 0.5239, 0.5143,
        0.5191, 0.5131, 0.5744, 0.8777, 0.5308, 0.5209, 0.5528, 0.5134, 0.8727,
        0.8641, 0.8780, 0.8751, 0.8752, 0.5623, 0.8755, 0.5286, 0.5374, 0.5281,
        0.8670, 0.5102, 0.5289, 0.8782, 0.5474, 0.5230, 0.5468, 0.5130, 0.8712,
        0.5100, 0.5179, 0.8811, 0.5211, 0.5090, 0.5163, 0.5384, 0.5572, 0.5211,
        0.8742, 0.5118, 0.5141, 0.5370, 0.5338, 0.5669, 0.5327, 0.8807, 0.5018,
        0.8765, 0.8776, 0.8702, 0.8836, 0.5574, 0.5488, 0.5298, 0.5052, 0.8767,
        0.5305, 0.5577, 0.5382, 0.5529, 0.5617, 0.5172, 0.5141, 0.5364, 0.5258,
        0.5187, 0.5572, 0.5629, 0.8772, 0.5379, 0.8691, 0.5223, 0.5067, 0.5675,
        0.5115, 0.8773, 0.5523, 0.5328, 0.5178, 0.5562, 0.8648, 0.5391, 0.5239,
        0.8744, 0.5359, 0.5440, 0.5408, 0.8717, 0.8787, 0.5490, 0.5690],
       grad_fn=<IndexBackward0>)
pred_loss tensor([0.4587], grad_fn=<MulBackward0>)
size_loss tensor(-1574.2438, grad_fn=<MulBackward0>)
size_num_loss 2.9000000000000004
loss: tensor([-1569.6545], grad_fn=<AddBackward0>)
9
epoch:  9 ; loss:  -1569.654541015625 ; pred:  tensor([0.9552, 0.0173, 0.0030, 0.0245], grad_fn=<SoftmaxBackward0>)
num_high 29 len(mask) 111
mask_without_small tensor([0.8864, 0.8836, 0.8758, 0.5507, 0.5178, 0.8846, 0.5072, 0.5023, 0.5581,
        0.8846, 0.5141, 0.5041, 0.5362, 0.8798, 0.8716, 0.8848, 0.8821, 0.8822,
        0.5458, 0.8825, 0.5119, 0.5208, 0.5113, 0.8744, 0.5122, 0.8850, 0.5308,
        0.5062, 0.5303, 0.8783, 0.5011, 0.8878, 0.5044, 0.5218, 0.5407, 0.5044,
        0.8812, 0.5204, 0.5171, 0.5505, 0.5161, 0.8874, 0.8834, 0.8845, 0.8775,
        0.8902, 0.5409, 0.5322, 0.5131, 0.8836, 0.5138, 0.5413, 0.5216, 0.5364,
        0.5453, 0.5005, 0.5198, 0.5091, 0.5020, 0.5407, 0.5466, 0.8841, 0.5213,
        0.8764, 0.5057, 0.5512, 0.8842, 0.5358, 0.5161, 0.5011, 0.5397, 0.8722,
        0.5225, 0.5071, 0.8814, 0.5193, 0.5274, 0.5242, 0.8788, 0.8856, 0.5325,
        0.5525], grad_fn=<IndexBackward0>)
pred_loss tensor([0.4892], grad_fn=<MulBackward0>)
size_loss tensor(-1728.0968, grad_fn=<MulBackward0>)
size_num_loss 2.9000000000000004
loss: tensor([-1723.5684], grad_fn=<AddBackward0>)
10
epoch:  10 ; loss:  -1723.568359375 ; pred:  tensor([0.9523, 0.0185, 0.0033, 0.0260], grad_fn=<SoftmaxBackward0>)
num_high 29 len(mask) 111
mask_without_small tensor([0.8938, 0.8911, 0.8838, 0.5322, 0.8921, 0.5398, 0.8921, 0.5175, 0.8876,
        0.8799, 0.8923, 0.8897, 0.8898, 0.5273, 0.8901, 0.5019, 0.8824, 0.8925,
        0.5120, 0.5114, 0.8862, 0.8951, 0.5029, 0.5220, 0.8889, 0.5015, 0.5320,
        0.8948, 0.8909, 0.8920, 0.8854, 0.8974, 0.5223, 0.5135, 0.8912, 0.5226,
        0.5027, 0.5177, 0.5267, 0.5009, 0.5220, 0.5281, 0.8916, 0.5024, 0.8844,
        0.5328, 0.8917, 0.5171, 0.5211, 0.8804, 0.5037, 0.8891, 0.5004, 0.5085,
        0.5054, 0.8867, 0.8931, 0.5138, 0.5341], grad_fn=<IndexBackward0>)
pred_loss tensor([0.5238], grad_fn=<MulBackward0>)
size_loss tensor(-1882.8358, grad_fn=<MulBackward0>)
size_num_loss 2.9000000000000004
loss: tensor([-1878.3970], grad_fn=<AddBackward0>)
11
epoch:  11 ; loss:  -1878.39697265625 ; pred:  tensor([0.9490, 0.0198, 0.0036, 0.0276], grad_fn=<SoftmaxBackward0>)
num_high 29 len(mask) 111
mask_without_small tensor([0.9015, 0.8989, 0.8921, 0.5135, 0.8998, 0.5213, 0.8998, 0.8956, 0.8883,
        0.9000, 0.8976, 0.8977, 0.5083, 0.8980, 0.8908, 0.9002, 0.8943, 0.9027,
        0.5030, 0.8969, 0.5132, 0.9024, 0.8988, 0.8997, 0.8935, 0.9048, 0.5032,
        0.8989, 0.5036, 0.5078, 0.5029, 0.5092, 0.8994, 0.8926, 0.5141, 0.8995,
        0.5020, 0.8889, 0.8970, 0.8948, 0.9008, 0.5154],
       grad_fn=<IndexBackward0>)
pred_loss tensor([0.5586], grad_fn=<MulBackward0>)
size_loss tensor(-1817.8671, grad_fn=<MulBackward0>)
size_num_loss 2.9000000000000004
loss: tensor([-1813.4849], grad_fn=<AddBackward0>)
12
epoch:  12 ; loss:  -1813.48486328125 ; pred:  tensor([0.9457, 0.0212, 0.0040, 0.0291], grad_fn=<SoftmaxBackward0>)
num_high 29 len(mask) 111
mask_without_small tensor([0.9090, 0.9066, 0.9002, 0.9075, 0.5032, 0.9075, 0.9035, 0.8967, 0.9077,
        0.9054, 0.9055, 0.9057, 0.8990, 0.9079, 0.9023, 0.9101, 0.9047, 0.9098,
        0.9065, 0.9074, 0.9016, 0.9121, 0.9067, 0.9071, 0.9007, 0.9072, 0.8973,
        0.9049, 0.9028, 0.9084], grad_fn=<IndexBackward0>)
pred_loss tensor([0.5906], grad_fn=<MulBackward0>)
size_loss tensor(-735.0125, grad_fn=<MulBackward0>)
size_num_loss 2.9000000000000004
loss: tensor([-730.6627], grad_fn=<AddBackward0>)
13
epoch:  13 ; loss:  -730.6627197265625 ; pred:  tensor([0.9426, 0.0224, 0.0043, 0.0306], grad_fn=<SoftmaxBackward0>)
num_high 29 len(mask) 111
mask_without_small tensor([0.9159, 0.9136, 0.9075, 0.9145, 0.9144, 0.9107, 0.9040, 0.9146, 0.9124,
        0.9126, 0.9128, 0.9063, 0.9148, 0.9095, 0.9170, 0.9118, 0.9167, 0.9135,
        0.9144, 0.9088, 0.9189, 0.9137, 0.9141, 0.9080, 0.9142, 0.9046, 0.9120,
        0.9099, 0.9154], grad_fn=<IndexBackward0>)
pred_loss tensor([0.6206], grad_fn=<MulBackward0>)
size_loss tensor(-36.8503, grad_fn=<MulBackward0>)
size_num_loss 2.9000000000000004
loss: tensor([-32.4794], grad_fn=<AddBackward0>)
14
epoch:  14 ; loss:  -32.479427337646484 ; pred:  tensor([0.9398, 0.0236, 0.0046, 0.0319], grad_fn=<SoftmaxBackward0>)
num_high 29 len(mask) 111
mask_without_small tensor([0.9227, 0.9203, 0.9089, 0.9213, 0.9213, 0.9151, 0.9034, 0.9215, 0.9185,
        0.9187, 0.9191, 0.9068, 0.9217, 0.9127, 0.9236, 0.9173, 0.9234, 0.9201,
        0.9212, 0.9113, 0.9251, 0.9203, 0.9209, 0.9098, 0.9210, 0.9042, 0.9176,
        0.9137, 0.9223], grad_fn=<IndexBackward0>)
pred_loss tensor([0.6512], grad_fn=<MulBackward0>)
size_loss tensor(-60.1969, grad_fn=<MulBackward0>)
size_num_loss 2.9000000000000004
loss: tensor([-55.7992], grad_fn=<AddBackward0>)
15
epoch:  15 ; loss:  -55.79916763305664 ; pred:  tensor([0.9370, 0.0248, 0.0049, 0.0333], grad_fn=<SoftmaxBackward0>)
num_high 29 len(mask) 111
mask_without_small tensor([0.9293, 0.9267, 0.9074, 0.9279, 0.9279, 0.9179, 0.9000, 0.9281, 0.9241,
        0.9244, 0.9250, 0.9045, 0.9283, 0.9135, 0.9301, 0.9221, 0.9299, 0.9264,
        0.9278, 0.9112, 0.9313, 0.9268, 0.9274, 0.9087, 0.9275, 0.9010, 0.9226,
        0.9152, 0.9289], grad_fn=<IndexBackward0>)
pred_loss tensor([0.6808], grad_fn=<MulBackward0>)
size_loss tensor(-94.5837, grad_fn=<MulBackward0>)
size_num_loss 2.9000000000000004
loss: tensor([-90.1600], grad_fn=<AddBackward0>)
16
epoch:  16 ; loss:  -90.15998077392578 ; pred:  tensor([0.9342, 0.0259, 0.0053, 0.0346], grad_fn=<SoftmaxBackward0>)
num_high 29 len(mask) 111
mask_without_small tensor([0.9356, 0.9328, 0.9039, 0.9342, 0.9341, 0.9194, 0.8948, 0.9344, 0.9293,
        0.9297, 0.9306, 0.9002, 0.9346, 0.9124, 0.9362, 0.9263, 0.9361, 0.9325,
        0.9341, 0.9091, 0.9372, 0.9329, 0.9336, 0.9057, 0.9337, 0.8960, 0.9271,
        0.9151, 0.9352], grad_fn=<IndexBackward0>)
pred_loss tensor([0.7094], grad_fn=<MulBackward0>)
size_loss tensor(-135.9171, grad_fn=<MulBackward0>)
size_num_loss 2.9000000000000004
loss: tensor([-131.4683], grad_fn=<AddBackward0>)
17
epoch:  17 ; loss:  -131.46832275390625 ; pred:  tensor([0.9315, 0.0270, 0.0056, 0.0358], grad_fn=<SoftmaxBackward0>)
num_high 29 len(mask) 111
mask_without_small tensor([0.9414, 0.9386, 0.8988, 0.9400, 0.9400, 0.9198, 0.8882, 0.9403, 0.9343,
        0.9348, 0.9359, 0.8944, 0.9405, 0.9098, 0.9419, 0.9301, 0.9418, 0.9382,
        0.9399, 0.9054, 0.9426, 0.9386, 0.9395, 0.9011, 0.9396, 0.8895, 0.9313,
        0.9135, 0.9411], grad_fn=<IndexBackward0>)
pred_loss tensor([0.7370], grad_fn=<MulBackward0>)
size_loss tensor(-182.4725, grad_fn=<MulBackward0>)
size_num_loss 2.9000000000000004
loss: tensor([-177.9994], grad_fn=<AddBackward0>)
18
epoch:  18 ; loss:  -177.9993896484375 ; pred:  tensor([0.9289, 0.0281, 0.0059, 0.0370], grad_fn=<SoftmaxBackward0>)
num_high 29 len(mask) 111
mask_without_small tensor([0.9467, 0.9439, 0.8924, 0.9455, 0.9454, 0.9193, 0.8801, 0.9457, 0.9390,
        0.9397, 0.9410, 0.8872, 0.9459, 0.9058, 0.9472, 0.9337, 0.9471, 0.9436,
        0.9454, 0.9003, 0.9476, 0.9440, 0.9449, 0.8950, 0.9450, 0.8816, 0.9353,
        0.9106, 0.9465], grad_fn=<IndexBackward0>)
pred_loss tensor([0.7640], grad_fn=<MulBackward0>)
size_loss tensor(-233.4811, grad_fn=<MulBackward0>)
size_num_loss 2.9000000000000004
loss: tensor([-228.9843], grad_fn=<AddBackward0>)
19
epoch:  19 ; loss:  -228.98426818847656 ; pred:  tensor([0.9264, 0.0291, 0.0062, 0.0382], grad_fn=<SoftmaxBackward0>)
Finished Training
sel masked ver tensor(indices=tensor([[7377, 7378, 7379, 7688, 5502, 5447, 5448, 5485, 5486,
                        5486, 5493, 7378, 7611, 5923, 5990, 7378, 5410, 5448,
                        5485, 5486, 5493, 5990, 5845, 5845, 5845, 5502, 5502,
                        5845, 5845],
                       [5845, 5845, 5845, 5845, 5990, 5990, 5990, 5990, 5923,
                        5990, 5990, 5990, 5990, 5486, 5350, 5410, 5845, 5845,
                        5845, 5845, 5845, 5845, 7378, 7384, 7688, 7578, 7614,
                        5941, 5990]]),
       values=tensor([0.9467, 0.9439, 0.8924, 0.9455, 0.9454, 0.9193, 0.8801,
                      0.9457, 0.9390, 0.9397, 0.9410, 0.8872, 0.9459, 0.9058,
                      0.9472, 0.9337, 0.9471, 0.9436, 0.9454, 0.9003, 0.9476,
                      0.9440, 0.9449, 0.8950, 0.9450, 0.8816, 0.9353, 0.9106,
                      0.9465]),
       size=(7689, 7689), nnz=29, layout=torch.sparse_coo,
       grad_fn=<SparseCooTensorWithDimsAndTensorsBackward0>)
Counter({'dealtWithIn': 6, 'isWorkedOnBy': 5, 'author': 4, 'publication': 3, 'isAbout': 3, 'publishes': 2, 'worksAtProject': 2, 'hasProject': 2, 'carriesOut': 1, 'member': 1})
dict index: {}
node_idx 5845
 node original label [0]
 node predicted label explain 0
 node prediction probability explain tensor([0.9264, 0.0291, 0.0062, 0.0382], grad_fn=<SoftmaxBackward0>)
 node predicted label full 0 most important relations  {'isWorkedOnBy': 5, 'dealtWithIn': 6, 'carriesOut': 1, 'publishes': 2, 'worksAtProject': 2, 'publication': 3, 'isAbout': 3, 'member': 1, 'author': 4, 'hasProject': 2, 'label': 0, 'node_idx': '5845'}
 final masks and lenght tensor(indices=tensor([[ 23947,  23948,  23949,  23954,  24148,  24181,  24183,
                         24184,  24202,  24258,  24523,  39063,  39081,  39130,
                         46927,  46927,  46927,  63345,  63405,  63405,  63405,
                         63442,  63443,  63480,  63481,  63481,  63488,  63488,
                         63489,  88695, 115082, 115083, 115084, 115089, 115283,
                        115316, 115337, 115393, 115393, 115658, 146768, 146768,
                        146768, 146786, 146786, 146835, 146835, 146835, 146835,
                        146835, 146835, 146835, 148222, 148223, 148229, 148423,
                        148456, 148477, 148533, 148798, 154480, 154540, 154577,
                        154578, 154615, 154616, 154623, 154624, 179487, 179908,
                        179926, 179975, 196400, 229540, 237825, 246188, 246188,
                        246206, 246255, 246255, 246255, 246255, 246255, 246255,
                        246255, 254395, 254395, 254395, 254395, 254395, 254395,
                        254395, 254395, 254395, 254395, 254395, 262337, 262337,
                        262337, 262337, 262337, 262337, 262337, 262337, 262337,
                        262337, 262337, 304105, 304105, 328960, 328960],
                       [  5845,   5845,   5845,   5845,   5845,   5845,   5845,
                          5845,   5845,   5845,   5845,   5502,   5502,   5502,
                          5923,   5941,   5990,   5990,   5923,   5941,   5990,
                          5990,   5990,   5990,   5923,   5990,   5923,   5990,
                          5941,     98,   5990,   5990,   5941,   5990,   5990,
                          5990,   5923,   5923,   5990,   5990,   5410,   5486,
                          5493,   5410,   5494,   5350,   5410,   5447,   5448,
                          5485,   5486,   5493,   5410,   5410,   5410,   5410,
                          5410,   5410,   5410,   5410,   5845,   5845,   5845,
                          5845,   5845,   5845,   5845,   5845,   5845,   5845,
                          5845,   5845,   1728,     86,   5627,   7632,   7688,
                          7379,   7377,   7378,   7384,   7578,   7611,   7688,
                          7953,   7377,   7378,   7379,   7384,   7578,   7611,
                          7613,   7614,   7632,   7688,   7953,   7377,   7378,
                          7379,   7384,   7578,   7611,   7613,   7614,   7632,
                          7688,   7953,   5941,   5990,   5230,   5231]]),
       values=tensor([0.9467, 0.9439, 0.8924, 0.3678, 0.4170, 0.4242, 0.4019,
                      0.4138, 0.4052, 0.9455, 0.3913, 0.4194, 0.4058, 0.3866,
                      0.4047, 0.4314, 0.9454, 0.3982, 0.3884, 0.4078, 0.4050,
                      0.9193, 0.8801, 0.9457, 0.9390, 0.9397, 0.4114, 0.9410,
                      0.3961, 0.3912, 0.3955, 0.8872, 0.4200, 0.4019, 0.3963,
                      0.9459, 0.4018, 0.3904, 0.4012, 0.4045, 0.4151, 0.9058,
                      0.4016, 0.3854, 0.4231, 0.9472, 0.4242, 0.3886, 0.4007,
                      0.4078, 0.3921, 0.4055, 0.3886, 0.9337, 0.4035, 0.4056,
                      0.4194, 0.3906, 0.4012, 0.4168, 0.4001, 0.9471, 0.3941,
                      0.9436, 0.9454, 0.9003, 0.9476, 0.4058, 0.4035, 0.3973,
                      0.3972, 0.9440, 0.3980, 0.4063, 0.3923, 0.4083, 0.4109,
                      0.3852, 0.3813, 0.4063, 0.3902, 0.3937, 0.4227, 0.3867,
                      0.4055, 0.4126, 0.9449, 0.3919, 0.8950, 0.3908, 0.3998,
                      0.4179, 0.3819, 0.4032, 0.9450, 0.4076, 0.4193, 0.4006,
                      0.3853, 0.4046, 0.8816, 0.3931, 0.3911, 0.9353, 0.3898,
                      0.3979, 0.3951, 0.9106, 0.9465, 0.4040, 0.4191]),
       size=(753935, 8285), nnz=111, layout=torch.sparse_coo,
       grad_fn=<SparseCooTensorWithDimsAndTensorsBackward0>) 29
 ---------------------------------------------------------------
r_exp.py:102: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  hor_graph, ver_graph = torch.tensor(sub_hor_graph, dtype=torch.float), torch.tensor(sub_ver_graph, dtype=torch.float)
wandb: WARNING Calling wandb.login() after wandb.init() has no effect.
node label: 0
13
num_high 21 len(mask) 23
mask_without_small tensor([0.8275, 0.8082, 0.7800, 0.5937, 0.7685, 0.6538, 0.7286, 0.8169, 0.7512,
        0.6894, 0.7015, 0.7448, 0.6620, 0.7363, 0.5785, 0.7779, 0.7127, 0.7075,
        0.7750, 0.6935, 0.6954, 0.7274, 0.6804], grad_fn=<IndexBackward0>)
pred_loss tensor([7.0159], grad_fn=<MulBackward0>)
size_loss tensor(-6.4320, grad_fn=<MulBackward0>)
size_num_loss 2.1
loss: tensor([4.9261], grad_fn=<AddBackward0>)
0
epoch:  0 ; loss:  4.926050186157227 ; pred:  tensor([0.4958, 0.1698, 0.1845, 0.1499], grad_fn=<SoftmaxBackward0>)
num_high 21 len(mask) 23
mask_without_small tensor([0.8413, 0.8233, 0.7967, 0.5693, 0.7858, 0.6309, 0.7479, 0.8314, 0.7694,
        0.6676, 0.6802, 0.7634, 0.6393, 0.7553, 0.5540, 0.7947, 0.6918, 0.6864,
        0.7920, 0.6719, 0.6738, 0.7468, 0.6583], grad_fn=<IndexBackward0>)
pred_loss tensor([7.0692], grad_fn=<MulBackward0>)
size_loss tensor(-8.1199, grad_fn=<MulBackward0>)
size_num_loss 2.1
loss: tensor([3.2833], grad_fn=<AddBackward0>)
1
epoch:  1 ; loss:  3.2832818031311035 ; pred:  tensor([0.4932, 0.1695, 0.1874, 0.1499], grad_fn=<SoftmaxBackward0>)
num_high 19 len(mask) 23
mask_without_small tensor([0.8541, 0.8372, 0.8124, 0.5447, 0.8022, 0.6073, 0.7660, 0.8448, 0.7864,
        0.6451, 0.6584, 0.7809, 0.6159, 0.7730, 0.5292, 0.8105, 0.6707, 0.6648,
        0.8080, 0.6498, 0.6519, 0.7648, 0.6355], grad_fn=<IndexBackward0>)
pred_loss tensor([7.1351], grad_fn=<MulBackward0>)
size_loss tensor(-9.9029, grad_fn=<MulBackward0>)
size_num_loss 1.9000000000000001
loss: tensor([1.3552], grad_fn=<AddBackward0>)
2
epoch:  2 ; loss:  1.3552006483078003 ; pred:  tensor([0.4899, 0.1694, 0.1907, 0.1500], grad_fn=<SoftmaxBackward0>)
num_high 17 len(mask) 23
mask_without_small tensor([0.8658, 0.8502, 0.8271, 0.5199, 0.8176, 0.5831, 0.7832, 0.8572, 0.8026,
        0.6219, 0.6358, 0.7975, 0.5919, 0.7899, 0.5045, 0.8253, 0.6487, 0.6424,
        0.8230, 0.6270, 0.6293, 0.7821, 0.6120], grad_fn=<IndexBackward0>)
pred_loss tensor([7.2133], grad_fn=<MulBackward0>)
size_loss tensor(-11.7281, grad_fn=<MulBackward0>)
size_num_loss 1.7000000000000002
loss: tensor([-0.6057], grad_fn=<AddBackward0>)
3
epoch:  3 ; loss:  -0.6056855916976929 ; pred:  tensor([0.4861, 0.1694, 0.1942, 0.1503], grad_fn=<SoftmaxBackward0>)
num_high 13 len(mask) 23
mask_without_small tensor([0.8765, 0.8621, 0.8408, 0.8320, 0.5585, 0.7997, 0.8686, 0.8179, 0.5980,
        0.6125, 0.8133, 0.5674, 0.8059, 0.8391, 0.6260, 0.6193, 0.8370, 0.6035,
        0.6059, 0.7986, 0.5879], grad_fn=<IndexBackward0>)
pred_loss tensor([7.3034], grad_fn=<MulBackward0>)
size_loss tensor(-1216.0311, grad_fn=<MulBackward0>)
size_num_loss 1.3
loss: tensor([-1206.7161], grad_fn=<AddBackward0>)
4
epoch:  4 ; loss:  -1206.716064453125 ; pred:  tensor([0.4817, 0.1696, 0.1979, 0.1507], grad_fn=<SoftmaxBackward0>)
num_high 12 len(mask) 23
mask_without_small tensor([0.8824, 0.8686, 0.8481, 0.8397, 0.5448, 0.8085, 0.8748, 0.8260, 0.5846,
        0.5993, 0.8216, 0.5538, 0.8145, 0.8466, 0.6129, 0.6061, 0.8446, 0.5903,
        0.5927, 0.8074, 0.5745], grad_fn=<IndexBackward0>)
pred_loss tensor([7.3628], grad_fn=<MulBackward0>)
size_loss tensor(-1319.5721, grad_fn=<MulBackward0>)
size_num_loss 1.2000000000000002
loss: tensor([-1310.3026], grad_fn=<AddBackward0>)
5
epoch:  5 ; loss:  -1310.3026123046875 ; pred:  tensor([0.4789, 0.1699, 0.2002, 0.1510], grad_fn=<SoftmaxBackward0>)
num_high 12 len(mask) 23
mask_without_small tensor([0.8896, 0.8765, 0.8570, 0.8490, 0.5272, 0.8193, 0.8824, 0.8359, 0.5673,
        0.5823, 0.8318, 0.5362, 0.8250, 0.8556, 0.5960, 0.5891, 0.8537, 0.5731,
        0.5755, 0.8183, 0.5572], grad_fn=<IndexBackward0>)
pred_loss tensor([7.4355], grad_fn=<MulBackward0>)
size_loss tensor(-1450.8987, grad_fn=<MulBackward0>)
size_num_loss 1.2000000000000002
loss: tensor([-1441.5632], grad_fn=<AddBackward0>)
6
epoch:  6 ; loss:  -1441.563232421875 ; pred:  tensor([0.4754, 0.1701, 0.2030, 0.1514], grad_fn=<SoftmaxBackward0>)
num_high 12 len(mask) 23
mask_without_small tensor([0.8972, 0.8849, 0.8665, 0.8589, 0.5073, 0.8309, 0.8904, 0.8466, 0.5477,
        0.5628, 0.8427, 0.5163, 0.8362, 0.8652, 0.5767, 0.5697, 0.8634, 0.5535,
        0.5560, 0.8299, 0.5375], grad_fn=<IndexBackward0>)
pred_loss tensor([7.5186], grad_fn=<MulBackward0>)
size_loss tensor(-1597.6812, grad_fn=<MulBackward0>)
size_num_loss 1.2000000000000002
loss: tensor([-1588.2709], grad_fn=<AddBackward0>)
7
epoch:  7 ; loss:  -1588.2708740234375 ; pred:  tensor([0.4715, 0.1704, 0.2062, 0.1519], grad_fn=<SoftmaxBackward0>)
num_high 12 len(mask) 23
mask_without_small tensor([0.9047, 0.8932, 0.8760, 0.8689, 0.8426, 0.8984, 0.8573, 0.5263, 0.5416,
        0.8537, 0.8476, 0.8748, 0.5556, 0.5485, 0.8732, 0.5322, 0.5348, 0.8417,
        0.5161], grad_fn=<IndexBackward0>)
pred_loss tensor([7.6105], grad_fn=<MulBackward0>)
size_loss tensor(-1660.2334, grad_fn=<MulBackward0>)
size_num_loss 1.2000000000000002
loss: tensor([-1650.7504], grad_fn=<AddBackward0>)
8
epoch:  8 ; loss:  -1650.7503662109375 ; pred:  tensor([0.4672, 0.1708, 0.2097, 0.1524], grad_fn=<SoftmaxBackward0>)
num_high 12 len(mask) 23
mask_without_small tensor([0.9120, 0.9013, 0.8853, 0.8787, 0.8542, 0.9062, 0.8679, 0.5038, 0.5190,
        0.8645, 0.8588, 0.8842, 0.5331, 0.5260, 0.8827, 0.5097, 0.5122, 0.8533],
       grad_fn=<IndexBackward0>)
pred_loss tensor([7.7082], grad_fn=<MulBackward0>)
size_loss tensor(-1763.2937, grad_fn=<MulBackward0>)
size_num_loss 1.2000000000000002
loss: tensor([-1753.7278], grad_fn=<AddBackward0>)
9
epoch:  9 ; loss:  -1753.727783203125 ; pred:  tensor([0.4626, 0.1712, 0.2133, 0.1529], grad_fn=<SoftmaxBackward0>)
num_high 12 len(mask) 23
mask_without_small tensor([0.9190, 0.9091, 0.8942, 0.8881, 0.8653, 0.9135, 0.8780, 0.8749, 0.8696,
        0.8932, 0.5097, 0.5026, 0.8918, 0.8645], grad_fn=<IndexBackward0>)
pred_loss tensor([7.8043], grad_fn=<MulBackward0>)
size_loss tensor(-1398.8021, grad_fn=<MulBackward0>)
size_num_loss 1.2000000000000002
loss: tensor([-1389.1700], grad_fn=<AddBackward0>)
10
epoch:  10 ; loss:  -1389.1700439453125 ; pred:  tensor([0.4582, 0.1715, 0.2170, 0.1533], grad_fn=<SoftmaxBackward0>)
num_high 12 len(mask) 23
mask_without_small tensor([0.9254, 0.9162, 0.9025, 0.8968, 0.8755, 0.9204, 0.8873, 0.8845, 0.8795,
        0.9015, 0.9002, 0.8747], grad_fn=<IndexBackward0>)
pred_loss tensor([7.8886], grad_fn=<MulBackward0>)
size_loss tensor(-173.2801, grad_fn=<MulBackward0>)
size_num_loss 1.2000000000000002
loss: tensor([-163.5832], grad_fn=<AddBackward0>)
11
epoch:  11 ; loss:  -163.58322143554688 ; pred:  tensor([0.4544, 0.1718, 0.2202, 0.1537], grad_fn=<SoftmaxBackward0>)
num_high 12 len(mask) 23
mask_without_small tensor([0.9316, 0.9232, 0.9100, 0.9040, 0.8774, 0.9270, 0.8928, 0.8892, 0.8827,
        0.9090, 0.9077, 0.8764], grad_fn=<IndexBackward0>)
pred_loss tensor([7.9877], grad_fn=<MulBackward0>)
size_loss tensor(-190.1769, grad_fn=<MulBackward0>)
size_num_loss 1.2000000000000002
loss: tensor([-180.3883], grad_fn=<AddBackward0>)
12
epoch:  12 ; loss:  -180.38832092285156 ; pred:  tensor([0.4499, 0.1726, 0.2233, 0.1543], grad_fn=<SoftmaxBackward0>)
num_high 12 len(mask) 23
mask_without_small tensor([0.9376, 0.9298, 0.9170, 0.9103, 0.8756, 0.9334, 0.8959, 0.8910, 0.8823,
        0.9160, 0.9145, 0.8743], grad_fn=<IndexBackward0>)
pred_loss tensor([8.0876], grad_fn=<MulBackward0>)
size_loss tensor(-222.2769, grad_fn=<MulBackward0>)
size_num_loss 1.2000000000000002
loss: tensor([-212.3946], grad_fn=<AddBackward0>)
13
epoch:  13 ; loss:  -212.39459228515625 ; pred:  tensor([0.4454, 0.1735, 0.2261, 0.1550], grad_fn=<SoftmaxBackward0>)
num_high 12 len(mask) 23
mask_without_small tensor([0.9431, 0.9360, 0.9235, 0.9160, 0.8713, 0.9393, 0.8972, 0.8907, 0.8795,
        0.9224, 0.9209, 0.8697], grad_fn=<IndexBackward0>)
pred_loss tensor([8.1853], grad_fn=<MulBackward0>)
size_loss tensor(-264.5702, grad_fn=<MulBackward0>)
size_num_loss 1.2000000000000002
loss: tensor([-254.5955], grad_fn=<AddBackward0>)
14
epoch:  14 ; loss:  -254.59552001953125 ; pred:  tensor([0.4411, 0.1746, 0.2286, 0.1558], grad_fn=<SoftmaxBackward0>)
num_high 12 len(mask) 23
mask_without_small tensor([0.9483, 0.9418, 0.9296, 0.9213, 0.8650, 0.9448, 0.8972, 0.8888, 0.8748,
        0.9285, 0.9268, 0.8631], grad_fn=<IndexBackward0>)
pred_loss tensor([8.2799], grad_fn=<MulBackward0>)
size_loss tensor(-314.5703, grad_fn=<MulBackward0>)
size_num_loss 1.2000000000000002
loss: tensor([-304.5057], grad_fn=<AddBackward0>)
15
epoch:  15 ; loss:  -304.50567626953125 ; pred:  tensor([0.4369, 0.1757, 0.2308, 0.1565], grad_fn=<SoftmaxBackward0>)
num_high 12 len(mask) 23
mask_without_small tensor([0.9530, 0.9470, 0.9353, 0.9264, 0.8569, 0.9498, 0.8961, 0.8855, 0.8684,
        0.9341, 0.9324, 0.8548], grad_fn=<IndexBackward0>)
pred_loss tensor([8.3716], grad_fn=<MulBackward0>)
size_loss tensor(-370.9894, grad_fn=<MulBackward0>)
size_num_loss 1.2000000000000002
loss: tensor([-360.8370], grad_fn=<AddBackward0>)
16
epoch:  16 ; loss:  -360.83697509765625 ; pred:  tensor([0.4329, 0.1769, 0.2329, 0.1573], grad_fn=<SoftmaxBackward0>)
num_high 12 len(mask) 23
mask_without_small tensor([0.9572, 0.9518, 0.9407, 0.9312, 0.8473, 0.9543, 0.8941, 0.8808, 0.8604,
        0.9394, 0.9377, 0.8449], grad_fn=<IndexBackward0>)
pred_loss tensor([8.4612], grad_fn=<MulBackward0>)
size_loss tensor(-433.2004, grad_fn=<MulBackward0>)
size_num_loss 1.2000000000000002
loss: tensor([-422.9616], grad_fn=<AddBackward0>)
17
epoch:  17 ; loss:  -422.96160888671875 ; pred:  tensor([0.4291, 0.1781, 0.2347, 0.1581], grad_fn=<SoftmaxBackward0>)
num_high 12 len(mask) 23
mask_without_small tensor([0.9610, 0.9561, 0.9456, 0.9358, 0.8361, 0.9584, 0.8913, 0.8750, 0.8510,
        0.9443, 0.9426, 0.8334], grad_fn=<IndexBackward0>)
pred_loss tensor([8.5496], grad_fn=<MulBackward0>)
size_loss tensor(-500.9667, grad_fn=<MulBackward0>)
size_num_loss 1.2000000000000002
loss: tensor([-490.6423], grad_fn=<AddBackward0>)
18
epoch:  18 ; loss:  -490.642333984375 ; pred:  tensor([0.4253, 0.1794, 0.2364, 0.1590], grad_fn=<SoftmaxBackward0>)
num_high 12 len(mask) 23
mask_without_small tensor([0.9644, 0.9599, 0.9501, 0.9402, 0.8234, 0.9620, 0.8877, 0.8679, 0.8400,
        0.9489, 0.9472, 0.8204], grad_fn=<IndexBackward0>)
pred_loss tensor([8.6376], grad_fn=<MulBackward0>)
size_loss tensor(-574.2897, grad_fn=<MulBackward0>)
size_num_loss 1.2000000000000002
loss: tensor([-563.8795], grad_fn=<AddBackward0>)
19
epoch:  19 ; loss:  -563.8794555664062 ; pred:  tensor([0.4216, 0.1808, 0.2379, 0.1598], grad_fn=<SoftmaxBackward0>)
Finished Training
sel masked ver tensor(indices=tensor([[6843, 7452, 7477, 7479, 5778, 5502, 5778, 5778, 5778,
                        5778, 5502, 5778],
                       [5778, 5778, 5778, 5778, 5181, 5778, 2965, 6843, 7477,
                        7479, 7477, 5230]]),
       values=tensor([0.9644, 0.9599, 0.9501, 0.9402, 0.8234, 0.9620, 0.8877,
                      0.8679, 0.8400, 0.9489, 0.9472, 0.8204]),
       size=(7480, 7480), nnz=12, layout=torch.sparse_coo,
       grad_fn=<SparseCooTensorWithDimsAndTensorsBackward0>)
Counter({'author': 4, 'publication': 3, 'member': 1, 'publishes': 1, 'name': 1, 'homepage': 1, 'type': 1})
dict index: {}
node_idx 5778
 node original label [0]
 node predicted label explain 0
 node prediction probability explain tensor([0.4216, 0.1808, 0.2379, 0.1598], grad_fn=<SoftmaxBackward0>)
 node predicted label full 0 most important relations  {'member': 1, 'publishes': 1, 'name': 1, 'homepage': 1, 'type': 1, 'publication': 3, 'author': 4, 'label': 0, 'node_idx': '5778'}
 final masks and lenght tensor(indices=tensor([[ 23413,  24022,  24047,  24048,  24049,  88628, 130053,
                        179487, 196333, 229473, 237758, 254328, 254328, 254328,
                        254328, 254328, 262337, 262337, 262337, 262337, 262337,
                        328893, 328893],
                       [  5778,   5778,   5778,   5778,   5778,      0,   5181,
                          5778,   2965,     62,   5579,   6843,   7452,   7477,
                          7478,   7479,   6843,   7452,   7477,   7478,   7479,
                          5230,   5231]]),
       values=tensor([0.9644, 0.9599, 0.9501, 0.3531, 0.9402, 0.3599, 0.8234,
                      0.9620, 0.8877, 0.3542, 0.3685, 0.8679, 0.3682, 0.8400,
                      0.3963, 0.9489, 0.3752, 0.3678, 0.9472, 0.3599, 0.3625,
                      0.8204, 0.3658]),
       size=(753935, 8285), nnz=23, layout=torch.sparse_coo,
       grad_fn=<SparseCooTensorWithDimsAndTensorsBackward0>) 12
 ---------------------------------------------------------------
node label: 1
30
r_exp.py:102: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  hor_graph, ver_graph = torch.tensor(sub_hor_graph, dtype=torch.float), torch.tensor(sub_ver_graph, dtype=torch.float)
wandb: WARNING Calling wandb.login() after wandb.init() has no effect.
num_high 61 len(mask) 61
mask_without_small tensor([0.7940, 0.7806, 0.7619, 0.6499, 0.7545, 0.6849, 0.7295, 0.6703, 0.7035,
        0.7856, 0.7169, 0.6783, 0.7044, 0.7107, 0.7028, 0.7573, 0.7854, 0.7253,
        0.7130, 0.7464, 0.7032, 0.7677, 0.7586, 0.7866, 0.7741, 0.7746, 0.7522,
        0.7759, 0.7227, 0.7325, 0.7220, 0.7605, 0.6790, 0.6989, 0.7230, 0.7877,
        0.7423, 0.7157, 0.7418, 0.7026, 0.6722, 0.7650, 0.6986, 0.7091, 0.6834,
        0.7373, 0.7774, 0.7149, 0.7793, 0.7604, 0.8024, 0.7493, 0.7432, 0.7506,
        0.7146, 0.7027, 0.7249, 0.7493, 0.7331, 0.7460, 0.7510],
       grad_fn=<IndexBackward0>)
pred_loss tensor([0.2362], grad_fn=<MulBackward0>)
size_loss tensor(-3.5539, grad_fn=<MulBackward0>)
size_num_loss 6.1000000000000005
loss: tensor([7.8413], grad_fn=<AddBackward0>)
0
epoch:  0 ; loss:  7.841347694396973 ; pred:  tensor([0.0061, 0.9767, 0.0078, 0.0094], grad_fn=<SoftmaxBackward0>)
num_high 61 len(mask) 61
mask_without_small tensor([0.8098, 0.7973, 0.7796, 0.6269, 0.7726, 0.6629, 0.7093, 0.6478, 0.6822,
        0.8020, 0.6961, 0.6561, 0.6831, 0.6897, 0.6815, 0.7752, 0.8018, 0.7050,
        0.6921, 0.7270, 0.6819, 0.7850, 0.7764, 0.8029, 0.7911, 0.7916, 0.7704,
        0.7928, 0.7023, 0.7125, 0.7015, 0.7783, 0.6568, 0.6775, 0.7026, 0.8039,
        0.7227, 0.6949, 0.7222, 0.6813, 0.6498, 0.7825, 0.6771, 0.6881, 0.6613,
        0.7174, 0.7943, 0.6941, 0.7960, 0.7782, 0.8178, 0.7676, 0.7618, 0.7688,
        0.6938, 0.6814, 0.7045, 0.7676, 0.7131, 0.7644, 0.7693],
       grad_fn=<IndexBackward0>)
pred_loss tensor([0.2374], grad_fn=<MulBackward0>)
size_loss tensor(-5.2632, grad_fn=<MulBackward0>)
size_num_loss 6.1000000000000005
loss: tensor([6.1089], grad_fn=<AddBackward0>)
1
epoch:  1 ; loss:  6.108919143676758 ; pred:  tensor([0.0061, 0.9765, 0.0078, 0.0096], grad_fn=<SoftmaxBackward0>)
num_high 60 len(mask) 61
mask_without_small tensor([0.8245, 0.8128, 0.7962, 0.6034, 0.7888, 0.6403, 0.6885, 0.6248, 0.6601,
        0.8172, 0.6746, 0.6332, 0.6611, 0.6679, 0.6594, 0.7919, 0.8170, 0.6838,
        0.6704, 0.7100, 0.6599, 0.8014, 0.7931, 0.8180, 0.8071, 0.8076, 0.7858,
        0.8086, 0.6810, 0.6919, 0.6802, 0.7950, 0.6340, 0.6553, 0.6813, 0.8190,
        0.7040, 0.6733, 0.7034, 0.6592, 0.6268, 0.7991, 0.6549, 0.6662, 0.6387,
        0.6974, 0.8100, 0.6724, 0.8117, 0.7949, 0.8320, 0.7827, 0.7752, 0.7852,
        0.6721, 0.6593, 0.6834, 0.7830, 0.6927, 0.7792, 0.7845],
       grad_fn=<IndexBackward0>)
pred_loss tensor([0.2437], grad_fn=<MulBackward0>)
size_loss tensor(-7.0400, grad_fn=<MulBackward0>)
size_num_loss 6.0
loss: tensor([4.2071], grad_fn=<AddBackward0>)
2
epoch:  2 ; loss:  4.2070794105529785 ; pred:  tensor([0.0061, 0.9759, 0.0080, 0.0100], grad_fn=<SoftmaxBackward0>)
num_high 54 len(mask) 61
mask_without_small tensor([0.8379, 0.8272, 0.8120, 0.5796, 0.8045, 0.6171, 0.6668, 0.6013, 0.6373,
        0.8312, 0.6522, 0.6099, 0.6383, 0.6453, 0.6366, 0.8077, 0.8310, 0.6618,
        0.6478, 0.6910, 0.6370, 0.8168, 0.8090, 0.8320, 0.8220, 0.8224, 0.8012,
        0.8234, 0.6588, 0.6704, 0.6580, 0.8108, 0.6107, 0.6323, 0.6592, 0.8329,
        0.6840, 0.6508, 0.6833, 0.6363, 0.6033, 0.8146, 0.6320, 0.6435, 0.6154,
        0.6764, 0.8247, 0.6500, 0.8262, 0.8108, 0.8448, 0.7980, 0.7898, 0.8010,
        0.6497, 0.6365, 0.6614, 0.7984, 0.6714, 0.7944, 0.7998],
       grad_fn=<IndexBackward0>)
pred_loss tensor([0.2517], grad_fn=<MulBackward0>)
size_loss tensor(-8.8493, grad_fn=<MulBackward0>)
size_num_loss 5.4
loss: tensor([1.7686], grad_fn=<AddBackward0>)
3
epoch:  3 ; loss:  1.7686119079589844 ; pred:  tensor([0.0062, 0.9751, 0.0082, 0.0104], grad_fn=<SoftmaxBackward0>)
num_high 44 len(mask) 61
mask_without_small tensor([0.8500, 0.8404, 0.8268, 0.5556, 0.8196, 0.5933, 0.6442, 0.5774, 0.6138,
        0.8439, 0.6290, 0.5861, 0.6148, 0.6219, 0.6131, 0.8227, 0.8438, 0.6390,
        0.6246, 0.6706, 0.6136, 0.8311, 0.8240, 0.8446, 0.8357, 0.8361, 0.8161,
        0.8370, 0.6359, 0.6480, 0.6350, 0.8257, 0.5869, 0.6088, 0.6362, 0.8455,
        0.6629, 0.6276, 0.6621, 0.6129, 0.5795, 0.8292, 0.6084, 0.6202, 0.5917,
        0.6545, 0.8381, 0.6267, 0.8396, 0.8257, 0.8564, 0.8130, 0.8046, 0.8162,
        0.6264, 0.6130, 0.6386, 0.8134, 0.6492, 0.8094, 0.8147],
       grad_fn=<IndexBackward0>)
pred_loss tensor([0.2622], grad_fn=<MulBackward0>)
size_loss tensor(-10.6636, grad_fn=<MulBackward0>)
size_num_loss 4.4
loss: tensor([-1.0783], grad_fn=<AddBackward0>)
4
epoch:  4 ; loss:  -1.0782737731933594 ; pred:  tensor([0.0064, 0.9741, 0.0085, 0.0110], grad_fn=<SoftmaxBackward0>)
num_high 33 len(mask) 61
mask_without_small tensor([0.8607, 0.8523, 0.8404, 0.5316, 0.8337, 0.5693, 0.6208, 0.5533, 0.5898,
        0.8554, 0.6052, 0.5620, 0.5908, 0.5980, 0.5891, 0.8367, 0.8553, 0.6154,
        0.6007, 0.6491, 0.5895, 0.8442, 0.8378, 0.8560, 0.8482, 0.8485, 0.8303,
        0.8493, 0.6122, 0.6248, 0.6113, 0.8394, 0.5629, 0.5848, 0.6126, 0.8568,
        0.6407, 0.6038, 0.6398, 0.5889, 0.5554, 0.8426, 0.5844, 0.5962, 0.5676,
        0.6316, 0.8503, 0.6029, 0.8517, 0.8394, 0.8667, 0.8273, 0.8190, 0.8306,
        0.6026, 0.5890, 0.6150, 0.8277, 0.6261, 0.8238, 0.8289],
       grad_fn=<IndexBackward0>)
pred_loss tensor([0.2756], grad_fn=<MulBackward0>)
size_loss tensor(-12.4651, grad_fn=<MulBackward0>)
size_num_loss 3.3000000000000003
loss: tensor([-4.0153], grad_fn=<AddBackward0>)
5
epoch:  5 ; loss:  -4.01528787612915 ; pred:  tensor([0.0067, 0.9728, 0.0089, 0.0117], grad_fn=<SoftmaxBackward0>)
num_high 28 len(mask) 61
mask_without_small tensor([0.8702, 0.8628, 0.8528, 0.5077, 0.8467, 0.5449, 0.5967, 0.5292, 0.5654,
        0.8655, 0.5808, 0.5377, 0.5664, 0.5736, 0.5646, 0.8495, 0.8654, 0.5912,
        0.5762, 0.6265, 0.5651, 0.8560, 0.8505, 0.8660, 0.8594, 0.8596, 0.8435,
        0.8603, 0.5879, 0.6009, 0.5870, 0.8519, 0.5386, 0.5604, 0.5883, 0.8669,
        0.6176, 0.5794, 0.6166, 0.5644, 0.5312, 0.8547, 0.5600, 0.5718, 0.5433,
        0.6080, 0.8612, 0.5785, 0.8626, 0.8521, 0.8758, 0.8407, 0.8328, 0.8440,
        0.5782, 0.5645, 0.5908, 0.8411, 0.6023, 0.8374, 0.8422],
       grad_fn=<IndexBackward0>)
pred_loss tensor([0.2924], grad_fn=<MulBackward0>)
size_loss tensor(-14.2400, grad_fn=<MulBackward0>)
size_num_loss 2.8000000000000003
loss: tensor([-6.3281], grad_fn=<AddBackward0>)
6
epoch:  6 ; loss:  -6.328050136566162 ; pred:  tensor([0.0070, 0.9712, 0.0093, 0.0125], grad_fn=<SoftmaxBackward0>)
num_high 27 len(mask) 61
mask_without_small tensor([0.8783, 0.8720, 0.8638, 0.8585, 0.5204, 0.5720, 0.5050, 0.5406, 0.8743,
        0.5559, 0.5134, 0.5416, 0.5487, 0.5398, 0.8610, 0.8742, 0.5664, 0.5514,
        0.6029, 0.5403, 0.8664, 0.8619, 0.8747, 0.8691, 0.8694, 0.8555, 0.8699,
        0.5631, 0.5763, 0.5622, 0.8631, 0.5143, 0.5357, 0.5635, 0.8756, 0.5936,
        0.5546, 0.5927, 0.5397, 0.5070, 0.8656, 0.5353, 0.5470, 0.5189, 0.5836,
        0.8707, 0.5537, 0.8722, 0.8635, 0.8836, 0.8531, 0.8458, 0.8564, 0.5534,
        0.5398, 0.5660, 0.8536, 0.5778, 0.8501, 0.8544],
       grad_fn=<IndexBackward0>)
pred_loss tensor([0.3130], grad_fn=<MulBackward0>)
size_loss tensor(-1588.5211, grad_fn=<MulBackward0>)
size_num_loss 2.7
loss: tensor([-1584.5350], grad_fn=<AddBackward0>)
7
epoch:  7 ; loss:  -1584.5350341796875 ; pred:  tensor([0.0074, 0.9692, 0.0099, 0.0135], grad_fn=<SoftmaxBackward0>)
num_high 27 len(mask) 61
mask_without_small tensor([0.8837, 0.8776, 0.8696, 0.8645, 0.5074, 0.5593, 0.5276, 0.8798, 0.5431,
        0.5003, 0.5287, 0.5358, 0.5268, 0.8670, 0.8797, 0.5536, 0.5385, 0.5905,
        0.5274, 0.8722, 0.8678, 0.8802, 0.8748, 0.8750, 0.8616, 0.8755, 0.5503,
        0.5636, 0.5494, 0.8690, 0.5012, 0.5227, 0.5507, 0.8811, 0.5811, 0.5417,
        0.5801, 0.5268, 0.8714, 0.5224, 0.5342, 0.5059, 0.5710, 0.8763, 0.5409,
        0.8778, 0.8693, 0.8888, 0.8593, 0.8522, 0.8625, 0.5406, 0.5269, 0.5533,
        0.8598, 0.5651, 0.8565, 0.8605], grad_fn=<IndexBackward0>)
pred_loss tensor([0.3242], grad_fn=<MulBackward0>)
size_loss tensor(-1669.9415, grad_fn=<MulBackward0>)
size_num_loss 2.7
loss: tensor([-1665.9603], grad_fn=<AddBackward0>)
8
epoch:  8 ; loss:  -1665.9603271484375 ; pred:  tensor([0.0076, 0.9681, 0.0103, 0.0140], grad_fn=<SoftmaxBackward0>)
num_high 27 len(mask) 61
mask_without_small tensor([0.8903, 0.8845, 0.8770, 0.8721, 0.5427, 0.5109, 0.8866, 0.5264, 0.5119,
        0.5191, 0.5100, 0.8744, 0.8865, 0.5370, 0.5218, 0.5742, 0.5106, 0.8794,
        0.8752, 0.8870, 0.8819, 0.8821, 0.8693, 0.8826, 0.5336, 0.5470, 0.5327,
        0.8764, 0.5060, 0.5341, 0.8879, 0.5647, 0.5250, 0.5638, 0.5100, 0.8786,
        0.5056, 0.5174, 0.5545, 0.8833, 0.5242, 0.8847, 0.8767, 0.8952, 0.8671,
        0.8604, 0.8702, 0.5239, 0.5101, 0.5367, 0.8676, 0.5486, 0.8644, 0.8683],
       grad_fn=<IndexBackward0>)
pred_loss tensor([0.3397], grad_fn=<MulBackward0>)
size_loss tensor(-1764.6667, grad_fn=<MulBackward0>)
size_num_loss 2.7
loss: tensor([-1760.6980], grad_fn=<AddBackward0>)
9
epoch:  9 ; loss:  -1760.697998046875 ; pred:  tensor([0.0079, 0.9666, 0.0107, 0.0148], grad_fn=<SoftmaxBackward0>)
num_high 27 len(mask) 61
mask_without_small tensor([0.8975, 0.8921, 0.8849, 0.8803, 0.5235, 0.8940, 0.5072, 0.8825, 0.8940,
        0.5178, 0.5025, 0.5554, 0.8872, 0.8833, 0.8944, 0.8896, 0.8898, 0.8777,
        0.8902, 0.5144, 0.5279, 0.5135, 0.8844, 0.5149, 0.8952, 0.5458, 0.5058,
        0.5448, 0.8865, 0.5354, 0.8909, 0.5050, 0.8923, 0.8847, 0.9022, 0.8757,
        0.8693, 0.8785, 0.5047, 0.5176, 0.8761, 0.5296, 0.8731, 0.8767],
       grad_fn=<IndexBackward0>)
pred_loss tensor([0.3584], grad_fn=<MulBackward0>)
size_loss tensor(-1799.2285, grad_fn=<MulBackward0>)
size_num_loss 2.7
loss: tensor([-1795.2998], grad_fn=<AddBackward0>)
10
epoch:  10 ; loss:  -1795.2998046875 ; pred:  tensor([0.0083, 0.9648, 0.0112, 0.0157], grad_fn=<SoftmaxBackward0>)
num_high 27 len(mask) 61
mask_without_small tensor([0.9049, 0.8998, 0.8931, 0.8887, 0.5031, 0.9016, 0.8908, 0.9016, 0.5354,
        0.8952, 0.8915, 0.9020, 0.8975, 0.8976, 0.8863, 0.8981, 0.5076, 0.8925,
        0.9027, 0.5256, 0.5247, 0.8946, 0.5151, 0.8987, 0.9000, 0.8929, 0.9092,
        0.8844, 0.8784, 0.8871, 0.8848, 0.5092, 0.8820, 0.8854],
       grad_fn=<IndexBackward0>)
pred_loss tensor([0.3777], grad_fn=<MulBackward0>)
size_loss tensor(-1549.1316, grad_fn=<MulBackward0>)
size_num_loss 2.7
loss: tensor([-1545.2416], grad_fn=<AddBackward0>)
11
epoch:  11 ; loss:  -1545.2415771484375 ; pred:  tensor([0.0087, 0.9629, 0.0118, 0.0166], grad_fn=<SoftmaxBackward0>)
num_high 27 len(mask) 61
mask_without_small tensor([0.9121, 0.9073, 0.9010, 0.8969, 0.9090, 0.8989, 0.9090, 0.5160, 0.9031,
        0.8996, 0.9094, 0.9051, 0.9053, 0.8946, 0.9057, 0.9005, 0.9101, 0.5061,
        0.5051, 0.9024, 0.9063, 0.9075, 0.9008, 0.9162, 0.8928, 0.8872, 0.8954,
        0.8932, 0.8906, 0.8938], grad_fn=<IndexBackward0>)
pred_loss tensor([0.3947], grad_fn=<MulBackward0>)
size_loss tensor(-1200.9952, grad_fn=<MulBackward0>)
size_num_loss 2.7
loss: tensor([-1197.1152], grad_fn=<AddBackward0>)
12
epoch:  12 ; loss:  -1197.115234375 ; pred:  tensor([0.0090, 0.9613, 0.0122, 0.0175], grad_fn=<SoftmaxBackward0>)
num_high 27 len(mask) 61
mask_without_small tensor([0.9189, 0.9144, 0.9085, 0.9046, 0.9160, 0.9064, 0.9159, 0.9104, 0.9071,
        0.9163, 0.9123, 0.9125, 0.9024, 0.9129, 0.9080, 0.9170, 0.9098, 0.9134,
        0.9146, 0.9083, 0.9227, 0.9007, 0.8953, 0.9031, 0.9011, 0.8986, 0.9016],
       grad_fn=<IndexBackward0>)
pred_loss tensor([0.4102], grad_fn=<MulBackward0>)
size_loss tensor(-68.1793, grad_fn=<MulBackward0>)
size_num_loss 2.7
loss: tensor([-64.3057], grad_fn=<AddBackward0>)
13
epoch:  13 ; loss:  -64.3056869506836 ; pred:  tensor([0.0093, 0.9598, 0.0127, 0.0183], grad_fn=<SoftmaxBackward0>)
num_high 27 len(mask) 61
mask_without_small tensor([0.9254, 0.9213, 0.9143, 0.9084, 0.9229, 0.9113, 0.9228, 0.9169, 0.9123,
        0.9232, 0.9192, 0.9194, 0.9050, 0.9198, 0.9136, 0.9237, 0.9161, 0.9204,
        0.9215, 0.9141, 0.9287, 0.9024, 0.8949, 0.9061, 0.9029, 0.8993, 0.9037],
       grad_fn=<IndexBackward0>)
pred_loss tensor([0.4335], grad_fn=<MulBackward0>)
size_loss tensor(-88.9390, grad_fn=<MulBackward0>)
size_num_loss 2.7
loss: tensor([-85.0476], grad_fn=<AddBackward0>)
14
epoch:  14 ; loss:  -85.04763793945312 ; pred:  tensor([0.0097, 0.9576, 0.0134, 0.0193], grad_fn=<SoftmaxBackward0>)
num_high 27 len(mask) 61
mask_without_small tensor([0.9318, 0.9280, 0.9193, 0.9099, 0.9295, 0.9145, 0.9294, 0.9230, 0.9161,
        0.9297, 0.9258, 0.9260, 0.9047, 0.9264, 0.9182, 0.9303, 0.9219, 0.9270,
        0.9282, 0.9189, 0.9346, 0.9010, 0.8915, 0.9064, 0.9018, 0.8970, 0.9029],
       grad_fn=<IndexBackward0>)
pred_loss tensor([0.4598], grad_fn=<MulBackward0>)
size_loss tensor(-121.7735, grad_fn=<MulBackward0>)
size_num_loss 2.7
loss: tensor([-117.8608], grad_fn=<AddBackward0>)
15
epoch:  15 ; loss:  -117.86079406738281 ; pred:  tensor([0.0103, 0.9551, 0.0142, 0.0205], grad_fn=<SoftmaxBackward0>)
num_high 27 len(mask) 61
mask_without_small tensor([0.9378, 0.9343, 0.9237, 0.9095, 0.9357, 0.9165, 0.9357, 0.9287, 0.9189,
        0.9360, 0.9320, 0.9322, 0.9023, 0.9327, 0.9221, 0.9365, 0.9274, 0.9334,
        0.9345, 0.9232, 0.9403, 0.8976, 0.8861, 0.9046, 0.8985, 0.8926, 0.9000],
       grad_fn=<IndexBackward0>)
pred_loss tensor([0.4881], grad_fn=<MulBackward0>)
size_loss tensor(-162.3059, grad_fn=<MulBackward0>)
size_num_loss 2.7
loss: tensor([-158.3694], grad_fn=<AddBackward0>)
16
epoch:  16 ; loss:  -158.36941528320312 ; pred:  tensor([0.0109, 0.9524, 0.0150, 0.0217], grad_fn=<SoftmaxBackward0>)
num_high 27 len(mask) 61
mask_without_small tensor([0.9434, 0.9402, 0.9278, 0.9077, 0.9415, 0.9175, 0.9415, 0.9342, 0.9211,
        0.9418, 0.9379, 0.9381, 0.8983, 0.9386, 0.9257, 0.9422, 0.9326, 0.9393,
        0.9404, 0.9271, 0.9455, 0.8925, 0.8791, 0.9013, 0.8937, 0.8865, 0.8954],
       grad_fn=<IndexBackward0>)
pred_loss tensor([0.5183], grad_fn=<MulBackward0>)
size_loss tensor(-208.5316, grad_fn=<MulBackward0>)
size_num_loss 2.7
loss: tensor([-204.5691], grad_fn=<AddBackward0>)
17
epoch:  17 ; loss:  -204.5691375732422 ; pred:  tensor([0.0115, 0.9495, 0.0160, 0.0230], grad_fn=<SoftmaxBackward0>)
num_high 27 len(mask) 61
mask_without_small tensor([0.9485, 0.9457, 0.9317, 0.9047, 0.9469, 0.9179, 0.9468, 0.9394, 0.9228,
        0.9471, 0.9434, 0.9436, 0.8928, 0.9441, 0.9289, 0.9475, 0.9375, 0.9448,
        0.9459, 0.9308, 0.9504, 0.8860, 0.8706, 0.8965, 0.8873, 0.8790, 0.8894],
       grad_fn=<IndexBackward0>)
pred_loss tensor([0.5504], grad_fn=<MulBackward0>)
size_loss tensor(-259.5389, grad_fn=<MulBackward0>)
size_num_loss 2.7
loss: tensor([-255.5481], grad_fn=<AddBackward0>)
18
epoch:  18 ; loss:  -255.54808044433594 ; pred:  tensor([0.0122, 0.9464, 0.0170, 0.0244], grad_fn=<SoftmaxBackward0>)
num_high 27 len(mask) 61
mask_without_small tensor([0.9532, 0.9507, 0.9355, 0.9004, 0.9518, 0.9175, 0.9517, 0.9443, 0.9241,
        0.9520, 0.9484, 0.9487, 0.8860, 0.9492, 0.9320, 0.9523, 0.9422, 0.9498,
        0.9508, 0.9343, 0.9547, 0.8780, 0.8606, 0.8904, 0.8795, 0.8701, 0.8820],
       grad_fn=<IndexBackward0>)
pred_loss tensor([0.5847], grad_fn=<MulBackward0>)
size_loss tensor(-314.9962, grad_fn=<MulBackward0>)
size_num_loss 2.7
loss: tensor([-310.9745], grad_fn=<AddBackward0>)
19
epoch:  19 ; loss:  -310.97454833984375 ; pred:  tensor([0.0129, 0.9432, 0.0181, 0.0258], grad_fn=<SoftmaxBackward0>)
Finished Training
sel masked ver tensor(indices=tensor([[6842, 6852, 6896, 7437, 7881, 5924, 6842, 7466, 7466,
                        7466, 7556, 7874, 7879, 7881, 5420, 5429, 5924, 5991,
                        5731, 5731, 5731, 5731, 5731, 5731, 5731, 5731, 5731],
                       [5731, 5731, 5731, 5731, 5731, 5455, 5451, 5437, 5451,
                        5457, 5451, 5451, 5451, 5423, 5731, 5731, 5731, 7556,
                        6852, 6896, 7361, 7437, 7466, 7556, 7902, 5991, 5230]]),
       values=tensor([0.9532, 0.9507, 0.9355, 0.9004, 0.9518, 0.9175, 0.9517,
                      0.9443, 0.9241, 0.9520, 0.9484, 0.9487, 0.8860, 0.9492,
                      0.9320, 0.9523, 0.9422, 0.9498, 0.9508, 0.9343, 0.9547,
                      0.8780, 0.8606, 0.8904, 0.8795, 0.8701, 0.8820]),
       size=(7882, 7903), nnz=27, layout=torch.sparse_coo,
       grad_fn=<SparseCooTensorWithDimsAndTensorsBackward0>)
Counter({'isAbout': 9, 'publication': 7, 'author': 5, 'isWorkedOnBy': 2, 'type': 1, 'worksAtProject': 1, 'member': 1, 'projectInfo': 1})
dict index: {}
node_idx 5731
 node original label [1]
 node predicted label explain 1
 node prediction probability explain tensor([0.0129, 0.9432, 0.0181, 0.0258], grad_fn=<SoftmaxBackward0>)
 node predicted label full 1 most important relations  {'isWorkedOnBy': 2, 'type': 1, 'worksAtProject': 1, 'publication': 7, 'isAbout': 9, 'member': 1, 'projectInfo': 1, 'author': 5, 'label': 1, 'node_idx': '5731'}
 final masks and lenght tensor(indices=tensor([[ 23412,  23422,  23466,  23931,  24007,  24036,  24126,
                         24444,  24449,  24451,  24472,  63450,  88581, 115261,
                        130006, 146769, 147687, 147697, 147741, 148311, 148311,
                        148311, 148311, 148311, 148401, 148719, 148724, 148726,
                        148726, 148747, 154542, 154550, 154551, 154552, 154553,
                        154559, 154567, 154581, 154585, 154587, 154594, 179909,
                        179976, 196286, 229426, 237711, 246256, 254281, 254281,
                        254281, 254281, 254281, 254281, 254281, 254281, 254281,
                        254281, 254281, 303991, 303991, 328846],
                       [  5731,   5731,   5731,   5731,   5731,   5731,   5731,
                          5731,   5731,   5731,   5731,   5924,      0,   5991,
                          5184,   5455,   5451,   5451,   5451,   5412,   5423,
                          5437,   5451,   5457,   5451,   5451,   5451,   5423,
                          5451,   5451,   5731,   5731,   5731,   5731,   5731,
                          5731,   5731,   5731,   5731,   5731,   5731,   5731,
                          5731,   3023,    108,   5543,   7556,   6842,   6852,
                          6896,   7361,   7437,   7466,   7556,   7874,   7879,
                          7881,   7902,   5924,   5991,   5230]]),
       values=tensor([0.9532, 0.9507, 0.9355, 0.3511, 0.9004, 0.3939, 0.3897,
                      0.4134, 0.3867, 0.9518, 0.3821, 0.3871, 0.3877, 0.3946,
                      0.3857, 0.9175, 0.9517, 0.3926, 0.3776, 0.4134, 0.3865,
                      0.9443, 0.9241, 0.9520, 0.9484, 0.9487, 0.8860, 0.9492,
                      0.3893, 0.3943, 0.3884, 0.9320, 0.3881, 0.3823, 0.3899,
                      0.9523, 0.4031, 0.3809, 0.4020, 0.3861, 0.4157, 0.9422,
                      0.3819, 0.3934, 0.3928, 0.4022, 0.9498, 0.3803, 0.9508,
                      0.9343, 0.9547, 0.8780, 0.8606, 0.8904, 0.3800, 0.3863,
                      0.3927, 0.8795, 0.3962, 0.8701, 0.8820]),
       size=(753935, 8285), nnz=61, layout=torch.sparse_coo,
       grad_fn=<SparseCooTensorWithDimsAndTensorsBackward0>) 27
 ---------------------------------------------------------------
node label: 1
6
num_high 6 len(mask) 7
mask_without_small tensor([0.7649, 0.7444, 0.7550, 0.7546, 0.5986, 0.7110, 0.8985],
       grad_fn=<IndexBackward0>)
pred_loss tensor([11.1614], grad_fn=<MulBackward0>)
size_loss tensor(-8.8219, grad_fn=<MulBackward0>)
size_num_loss 0.6000000000000001
loss: tensor([4.0094], grad_fn=<AddBackward0>)
r_exp.py:102: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  hor_graph, ver_graph = torch.tensor(sub_hor_graph, dtype=torch.float), torch.tensor(sub_ver_graph, dtype=torch.float)
wandb: WARNING Calling wandb.login() after wandb.init() has no effect.
0
epoch:  0 ; loss:  4.009417533874512 ; pred:  tensor([0.2758, 0.3275, 0.2286, 0.1681], grad_fn=<SoftmaxBackward0>)
num_high 6 len(mask) 7
mask_without_small tensor([0.7824, 0.7249, 0.7730, 0.7726, 0.5744, 0.6901, 0.9072],
       grad_fn=<IndexBackward0>)
pred_loss tensor([11.1363], grad_fn=<MulBackward0>)
size_loss tensor(-10.1476, grad_fn=<MulBackward0>)
size_num_loss 0.6000000000000001
loss: tensor([2.6532], grad_fn=<AddBackward0>)
1
epoch:  1 ; loss:  2.653181314468384 ; pred:  tensor([0.2755, 0.3284, 0.2282, 0.1679], grad_fn=<SoftmaxBackward0>)
num_high 6 len(mask) 7
mask_without_small tensor([0.7986, 0.7071, 0.7892, 0.7891, 0.5498, 0.6686, 0.9152],
       grad_fn=<IndexBackward0>)
pred_loss tensor([11.1183], grad_fn=<MulBackward0>)
size_loss tensor(-11.6299, grad_fn=<MulBackward0>)
size_num_loss 0.6000000000000001
loss: tensor([1.1459], grad_fn=<AddBackward0>)
2
epoch:  2 ; loss:  1.145944356918335 ; pred:  tensor([0.2754, 0.3290, 0.2279, 0.1677], grad_fn=<SoftmaxBackward0>)
num_high 6 len(mask) 7
mask_without_small tensor([0.8140, 0.6879, 0.8047, 0.8048, 0.5249, 0.6464, 0.9225],
       grad_fn=<IndexBackward0>)
pred_loss tensor([11.1055], grad_fn=<MulBackward0>)
size_loss tensor(-13.2183, grad_fn=<MulBackward0>)
size_num_loss 0.6000000000000001
loss: tensor([-0.4633], grad_fn=<AddBackward0>)
3
epoch:  3 ; loss:  -0.463312029838562 ; pred:  tensor([0.2753, 0.3294, 0.2277, 0.1676], grad_fn=<SoftmaxBackward0>)
num_high 6 len(mask) 7
mask_without_small tensor([0.8285, 0.6676, 0.8195, 0.8198, 0.6234, 0.9291],
       grad_fn=<IndexBackward0>)
pred_loss tensor([11.0981], grad_fn=<MulBackward0>)
size_loss tensor(-1139.1470, grad_fn=<MulBackward0>)
size_num_loss 0.6000000000000001
loss: tensor([-1126.8806], grad_fn=<AddBackward0>)
4
epoch:  4 ; loss:  -1126.880615234375 ; pred:  tensor([0.2753, 0.3296, 0.2276, 0.1675], grad_fn=<SoftmaxBackward0>)
num_high 5 len(mask) 7
mask_without_small tensor([0.8363, 0.6553, 0.8277, 0.8279, 0.6104, 0.9327],
       grad_fn=<IndexBackward0>)
pred_loss tensor([11.0889], grad_fn=<MulBackward0>)
size_loss tensor(-1227.7726, grad_fn=<MulBackward0>)
size_num_loss 0.5
loss: tensor([-1215.6202], grad_fn=<AddBackward0>)
5
epoch:  5 ; loss:  -1215.6202392578125 ; pred:  tensor([0.2751, 0.3299, 0.2275, 0.1675], grad_fn=<SoftmaxBackward0>)
num_high 5 len(mask) 7
mask_without_small tensor([0.8458, 0.6392, 0.8376, 0.8379, 0.5935, 0.9371],
       grad_fn=<IndexBackward0>)
pred_loss tensor([11.0872], grad_fn=<MulBackward0>)
size_loss tensor(-1343.4514, grad_fn=<MulBackward0>)
size_num_loss 0.5
loss: tensor([-1331.3071], grad_fn=<AddBackward0>)
6
epoch:  6 ; loss:  -1331.30712890625 ; pred:  tensor([0.2751, 0.3300, 0.2274, 0.1676], grad_fn=<SoftmaxBackward0>)
num_high 4 len(mask) 7
mask_without_small tensor([0.8560, 0.6207, 0.8482, 0.8485, 0.5742, 0.9416],
       grad_fn=<IndexBackward0>)
pred_loss tensor([11.0923], grad_fn=<MulBackward0>)
size_loss tensor(-1476.1625, grad_fn=<MulBackward0>)
size_num_loss 0.4
loss: tensor([-1464.1206], grad_fn=<AddBackward0>)
7
epoch:  7 ; loss:  -1464.12060546875 ; pred:  tensor([0.2752, 0.3298, 0.2273, 0.1677], grad_fn=<SoftmaxBackward0>)
num_high 4 len(mask) 7
mask_without_small tensor([0.8663, 0.6002, 0.8590, 0.8592, 0.5531, 0.9460],
       grad_fn=<IndexBackward0>)
pred_loss tensor([11.1036], grad_fn=<MulBackward0>)
size_loss tensor(-1620.4647, grad_fn=<MulBackward0>)
size_num_loss 0.4
loss: tensor([-1608.4199], grad_fn=<AddBackward0>)
8
epoch:  8 ; loss:  -1608.419921875 ; pred:  tensor([0.2754, 0.3294, 0.2272, 0.1679], grad_fn=<SoftmaxBackward0>)
num_high 4 len(mask) 7
mask_without_small tensor([0.8764, 0.5784, 0.8696, 0.8698, 0.5308, 0.9503],
       grad_fn=<IndexBackward0>)
pred_loss tensor([11.1205], grad_fn=<MulBackward0>)
size_loss tensor(-1772.7020, grad_fn=<MulBackward0>)
size_num_loss 0.4
loss: tensor([-1760.6494], grad_fn=<AddBackward0>)
9
epoch:  9 ; loss:  -1760.6494140625 ; pred:  tensor([0.2758, 0.3289, 0.2271, 0.1682], grad_fn=<SoftmaxBackward0>)
num_high 4 len(mask) 7
mask_without_small tensor([0.8861, 0.5553, 0.8798, 0.8801, 0.5075, 0.9543],
       grad_fn=<IndexBackward0>)
pred_loss tensor([11.1427], grad_fn=<MulBackward0>)
size_loss tensor(-1930.1556, grad_fn=<MulBackward0>)
size_num_loss 0.4
loss: tensor([-1918.0907], grad_fn=<AddBackward0>)
10
epoch:  10 ; loss:  -1918.0906982421875 ; pred:  tensor([0.2762, 0.3282, 0.2271, 0.1685], grad_fn=<SoftmaxBackward0>)
num_high 4 len(mask) 7
mask_without_small tensor([0.8953, 0.5314, 0.8896, 0.8898, 0.9580], grad_fn=<IndexBackward0>)
pred_loss tensor([11.1696], grad_fn=<MulBackward0>)
size_loss tensor(-1709.5891, grad_fn=<MulBackward0>)
size_num_loss 0.4
loss: tensor([-1697.5125], grad_fn=<AddBackward0>)
11
epoch:  11 ; loss:  -1697.512451171875 ; pred:  tensor([0.2767, 0.3273, 0.2270, 0.1690], grad_fn=<SoftmaxBackward0>)
num_high 4 len(mask) 7
mask_without_small tensor([0.9039, 0.5070, 0.8987, 0.8989, 0.9614], grad_fn=<IndexBackward0>)
pred_loss tensor([11.1931], grad_fn=<MulBackward0>)
size_loss tensor(-1846.8865, grad_fn=<MulBackward0>)
size_num_loss 0.4
loss: tensor([-1834.7965], grad_fn=<AddBackward0>)
12
epoch:  12 ; loss:  -1834.7965087890625 ; pred:  tensor([0.2772, 0.3265, 0.2269, 0.1694], grad_fn=<SoftmaxBackward0>)
num_high 4 len(mask) 7
mask_without_small tensor([0.9119, 0.9071, 0.9073, 0.9644], grad_fn=<IndexBackward0>)
pred_loss tensor([11.2136], grad_fn=<MulBackward0>)
size_loss tensor(-279.2787, grad_fn=<MulBackward0>)
size_num_loss 0.4
loss: tensor([-267.1837], grad_fn=<AddBackward0>)
13
epoch:  13 ; loss:  -267.1836853027344 ; pred:  tensor([0.2775, 0.3258, 0.2268, 0.1699], grad_fn=<SoftmaxBackward0>)
num_high 4 len(mask) 7
mask_without_small tensor([0.9171, 0.9117, 0.9120, 0.9674], grad_fn=<IndexBackward0>)
pred_loss tensor([11.2387], grad_fn=<MulBackward0>)
size_loss tensor(-270.3184, grad_fn=<MulBackward0>)
size_num_loss 0.4
loss: tensor([-258.2060], grad_fn=<AddBackward0>)
14
epoch:  14 ; loss:  -258.2060241699219 ; pred:  tensor([0.2778, 0.3250, 0.2266, 0.1705], grad_fn=<SoftmaxBackward0>)
num_high 4 len(mask) 7
mask_without_small tensor([0.9206, 0.9139, 0.9142, 0.9703], grad_fn=<IndexBackward0>)
pred_loss tensor([11.2657], grad_fn=<MulBackward0>)
size_loss tensor(-272.0688, grad_fn=<MulBackward0>)
size_num_loss 0.4
loss: tensor([-259.9353], grad_fn=<AddBackward0>)
15
epoch:  15 ; loss:  -259.9352722167969 ; pred:  tensor([0.2781, 0.3241, 0.2265, 0.1712], grad_fn=<SoftmaxBackward0>)
num_high 4 len(mask) 7
mask_without_small tensor([0.9229, 0.9143, 0.9147, 0.9730], grad_fn=<IndexBackward0>)
pred_loss tensor([11.2936], grad_fn=<MulBackward0>)
size_loss tensor(-281.3487, grad_fn=<MulBackward0>)
size_num_loss 0.4
loss: tensor([-269.1917], grad_fn=<AddBackward0>)
16
epoch:  16 ; loss:  -269.1916809082031 ; pred:  tensor([0.2785, 0.3232, 0.2264, 0.1719], grad_fn=<SoftmaxBackward0>)
num_high 4 len(mask) 7
mask_without_small tensor([0.9242, 0.9132, 0.9137, 0.9755], grad_fn=<IndexBackward0>)
pred_loss tensor([11.3220], grad_fn=<MulBackward0>)
size_loss tensor(-296.4247, grad_fn=<MulBackward0>)
size_num_loss 0.4
loss: tensor([-284.2425], grad_fn=<AddBackward0>)
17
epoch:  17 ; loss:  -284.2425231933594 ; pred:  tensor([0.2788, 0.3223, 0.2263, 0.1726], grad_fn=<SoftmaxBackward0>)
num_high 4 len(mask) 7
mask_without_small tensor([0.9248, 0.9109, 0.9115, 0.9777], grad_fn=<IndexBackward0>)
pred_loss tensor([11.3506], grad_fn=<MulBackward0>)
size_loss tensor(-316.3790, grad_fn=<MulBackward0>)
size_num_loss 0.4
loss: tensor([-304.1703], grad_fn=<AddBackward0>)
18
epoch:  18 ; loss:  -304.17034912109375 ; pred:  tensor([0.2791, 0.3214, 0.2263, 0.1733], grad_fn=<SoftmaxBackward0>)
num_high 4 len(mask) 7
mask_without_small tensor([0.9249, 0.9075, 0.9082, 0.9798], grad_fn=<IndexBackward0>)
pred_loss tensor([11.3795], grad_fn=<MulBackward0>)
size_loss tensor(-340.8182, grad_fn=<MulBackward0>)
size_num_loss 0.4
loss: tensor([-328.5817], grad_fn=<AddBackward0>)
19
epoch:  19 ; loss:  -328.58172607421875 ; pred:  tensor([0.2794, 0.3205, 0.2262, 0.1740], grad_fn=<SoftmaxBackward0>)
Finished Training
sel masked ver tensor(indices=tensor([[7693, 5905, 5905, 5905],
                       [5905, 2259,   28, 5230]]),
       values=tensor([0.9249, 0.9075, 0.9082, 0.9798]),
       size=(7694, 5906), nnz=4, layout=torch.sparse_coo,
       grad_fn=<SparseCooTensorWithDimsAndTensorsBackward0>)
Counter({'phone': 1, 'name': 1, 'type': 1, 'author': 1})
dict index: {}
node_idx 5905
 node original label [1]
 node predicted label explain 1
 node prediction probability explain tensor([0.2794, 0.3205, 0.2262, 0.1740], grad_fn=<SoftmaxBackward0>)
 node predicted label full 1 most important relations  {'phone': 1, 'name': 1, 'type': 1, 'author': 1, 'label': 1, 'node_idx': '5905'}
 final masks and lenght tensor(indices=tensor([[ 24263,  88755, 196460, 229600, 237885, 254455, 329020],
                       [  5905,     27,   2259,     28,   5670,   7693,   5230]]),
       values=tensor([0.9249, 0.3768, 0.9075, 0.9082, 0.3387, 0.3630, 0.9798]),
       size=(753935, 8285), nnz=7, layout=torch.sparse_coo,
       grad_fn=<SparseCooTensorWithDimsAndTensorsBackward0>) 4
 ---------------------------------------------------------------
r_exp.py:102: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  hor_graph, ver_graph = torch.tensor(sub_hor_graph, dtype=torch.float), torch.tensor(sub_ver_graph, dtype=torch.float)
wandb: WARNING Calling wandb.login() after wandb.init() has no effect.
node label: 1
13
num_high 30 len(mask) 31
mask_without_small tensor([0.8160, 0.7986, 0.7736, 0.6142, 0.7636, 0.6652, 0.7289, 0.6439, 0.6919,
        0.8051, 0.7110, 0.6555, 0.6932, 0.7022, 0.6910, 0.7201, 0.6566, 0.6854,
        0.7198, 0.8079, 0.7467, 0.7093, 0.7460, 0.7473, 0.6467, 0.7778, 0.6849,
        0.7000, 0.6629, 0.8233, 0.6652], grad_fn=<IndexBackward0>)
pred_loss tensor([7.5274], grad_fn=<MulBackward0>)
size_loss tensor(-5.6779, grad_fn=<MulBackward0>)
size_num_loss 3.0
loss: tensor([7.6620], grad_fn=<AddBackward0>)
0
epoch:  0 ; loss:  7.661965847015381 ; pred:  tensor([0.2795, 0.4711, 0.1444, 0.1050], grad_fn=<SoftmaxBackward0>)
num_high 29 len(mask) 31
mask_without_small tensor([0.8305, 0.8142, 0.7906, 0.5903, 0.7811, 0.6425, 0.7482, 0.6207, 0.6702,
        0.8204, 0.6900, 0.6326, 0.6715, 0.6809, 0.6692, 0.6995, 0.6337, 0.6634,
        0.7395, 0.8229, 0.7651, 0.6883, 0.7645, 0.7657, 0.6235, 0.7946, 0.6630,
        0.6786, 0.6849, 0.8374, 0.6425], grad_fn=<IndexBackward0>)
pred_loss tensor([7.3207], grad_fn=<MulBackward0>)
size_loss tensor(-7.2933, grad_fn=<MulBackward0>)
size_num_loss 2.9000000000000004
loss: tensor([5.7254], grad_fn=<AddBackward0>)
1
epoch:  1 ; loss:  5.725406646728516 ; pred:  tensor([0.2780, 0.4809, 0.1382, 0.1028], grad_fn=<SoftmaxBackward0>)
num_high 24 len(mask) 31
mask_without_small tensor([0.8440, 0.8288, 0.8067, 0.5660, 0.7978, 0.6192, 0.7621, 0.5969, 0.6477,
        0.8344, 0.6687, 0.6091, 0.6491, 0.6589, 0.6467, 0.6797, 0.6102, 0.6408,
        0.7581, 0.8369, 0.7826, 0.6669, 0.7821, 0.7832, 0.5998, 0.8104, 0.6404,
        0.6564, 0.7053, 0.8505, 0.6192], grad_fn=<IndexBackward0>)
pred_loss tensor([7.1339], grad_fn=<MulBackward0>)
size_loss tensor(-9.0337, grad_fn=<MulBackward0>)
size_num_loss 2.4000000000000004
loss: tensor([3.2800], grad_fn=<AddBackward0>)
2
epoch:  2 ; loss:  3.2799885272979736 ; pred:  tensor([0.2765, 0.4900, 0.1325, 0.1011], grad_fn=<SoftmaxBackward0>)
num_high 22 len(mask) 31
mask_without_small tensor([0.8564, 0.8423, 0.8218, 0.5414, 0.8135, 0.5953, 0.7773, 0.5726, 0.6245,
        0.8472, 0.6466, 0.5850, 0.6260, 0.6362, 0.6235, 0.6589, 0.5862, 0.6174,
        0.7760, 0.8498, 0.7992, 0.6448, 0.7987, 0.7997, 0.5756, 0.8252, 0.6171,
        0.6335, 0.7250, 0.8624, 0.5954], grad_fn=<IndexBackward0>)
pred_loss tensor([6.9617], grad_fn=<MulBackward0>)
size_loss tensor(-10.8332, grad_fn=<MulBackward0>)
size_num_loss 2.2
loss: tensor([1.0865], grad_fn=<AddBackward0>)
3
epoch:  3 ; loss:  1.0865297317504883 ; pred:  tensor([0.2750, 0.4985, 0.1271, 0.0994], grad_fn=<SoftmaxBackward0>)
num_high 16 len(mask) 31
mask_without_small tensor([0.8678, 0.8548, 0.8359, 0.5167, 0.8282, 0.5710, 0.7927, 0.5480, 0.6007,
        0.8585, 0.6237, 0.5605, 0.6022, 0.6127, 0.5996, 0.6370, 0.5617, 0.5934,
        0.7929, 0.8616, 0.8148, 0.6218, 0.8143, 0.8152, 0.5512, 0.8391, 0.5931,
        0.6098, 0.7442, 0.8734, 0.5710], grad_fn=<IndexBackward0>)
pred_loss tensor([6.8035], grad_fn=<MulBackward0>)
size_loss tensor(-12.6549, grad_fn=<MulBackward0>)
size_num_loss 1.6
loss: tensor([-1.5186], grad_fn=<AddBackward0>)
4
epoch:  4 ; loss:  -1.5185668468475342 ; pred:  tensor([0.2735, 0.5064, 0.1222, 0.0979], grad_fn=<SoftmaxBackward0>)
num_high 14 len(mask) 31
mask_without_small tensor([0.8781, 0.8662, 0.8489, 0.8418, 0.5462, 0.8078, 0.5233, 0.5762, 0.8684,
        0.6000, 0.5357, 0.5777, 0.5886, 0.5752, 0.6142, 0.5369, 0.5688, 0.8089,
        0.8723, 0.8295, 0.5981, 0.8291, 0.8298, 0.5267, 0.8518, 0.5686, 0.5855,
        0.7627, 0.8833, 0.5463], grad_fn=<IndexBackward0>)
pred_loss tensor([6.6591], grad_fn=<MulBackward0>)
size_loss tensor(-1425.0607, grad_fn=<MulBackward0>)
size_num_loss 1.4000000000000001
loss: tensor([-1416.2223], grad_fn=<AddBackward0>)
5
epoch:  5 ; loss:  -1416.2222900390625 ; pred:  tensor([0.2720, 0.5138, 0.1175, 0.0966], grad_fn=<SoftmaxBackward0>)
num_high 14 len(mask) 31
mask_without_small tensor([0.8837, 0.8723, 0.8556, 0.8489, 0.5328, 0.8159, 0.5097, 0.5630, 0.8743,
        0.5871, 0.5223, 0.5646, 0.5755, 0.5620, 0.6014, 0.5234, 0.5556, 0.8173,
        0.8782, 0.8370, 0.5851, 0.8366, 0.8375, 0.5134, 0.8586, 0.5555, 0.5723,
        0.7729, 0.8888, 0.5328], grad_fn=<IndexBackward0>)
pred_loss tensor([6.5844], grad_fn=<MulBackward0>)
size_loss tensor(-1524.8596, grad_fn=<MulBackward0>)
size_num_loss 1.4000000000000001
loss: tensor([-1516.1010], grad_fn=<AddBackward0>)
6
epoch:  6 ; loss:  -1516.1009521484375 ; pred:  tensor([0.2713, 0.5177, 0.1151, 0.0959], grad_fn=<SoftmaxBackward0>)
num_high 14 len(mask) 31
mask_without_small tensor([0.8907, 0.8798, 0.8640, 0.8575, 0.5155, 0.8260, 0.5460, 0.8816, 0.5703,
        0.5050, 0.5475, 0.5586, 0.5449, 0.5848, 0.5062, 0.5386, 0.8274, 0.8853,
        0.8462, 0.5683, 0.8459, 0.8469, 0.8668, 0.5385, 0.5552, 0.7851, 0.8955,
        0.5155], grad_fn=<IndexBackward0>)
pred_loss tensor([6.4990], grad_fn=<MulBackward0>)
size_loss tensor(-1624.5521, grad_fn=<MulBackward0>)
size_num_loss 1.4000000000000001
loss: tensor([-1615.8956], grad_fn=<AddBackward0>)
7
epoch:  7 ; loss:  -1615.8956298828125 ; pred:  tensor([0.2705, 0.5221, 0.1123, 0.0951], grad_fn=<SoftmaxBackward0>)
num_high 14 len(mask) 31
mask_without_small tensor([0.8980, 0.8878, 0.8729, 0.8668, 0.8369, 0.5265, 0.8895, 0.5510, 0.5280,
        0.5392, 0.5254, 0.5657, 0.5191, 0.8384, 0.8930, 0.8561, 0.5490, 0.8558,
        0.8569, 0.8756, 0.5191, 0.5357, 0.7983, 0.9026],
       grad_fn=<IndexBackward0>)
pred_loss tensor([6.4041], grad_fn=<MulBackward0>)
size_loss tensor(-1681.0006, grad_fn=<MulBackward0>)
size_num_loss 1.4000000000000001
loss: tensor([-1672.4667], grad_fn=<AddBackward0>)
8
epoch:  8 ; loss:  -1672.4666748046875 ; pred:  tensor([0.2694, 0.5271, 0.1093, 0.0942], grad_fn=<SoftmaxBackward0>)
num_high 14 len(mask) 31
mask_without_small tensor([0.9055, 0.8959, 0.8819, 0.8762, 0.8481, 0.5055, 0.8974, 0.5303, 0.5071,
        0.5183, 0.5044, 0.5453, 0.8495, 0.9008, 0.8661, 0.5283, 0.8659, 0.8670,
        0.8845, 0.5148, 0.8117, 0.9098], grad_fn=<IndexBackward0>)
pred_loss tensor([6.3087], grad_fn=<MulBackward0>)
size_loss tensor(-1769.9757, grad_fn=<MulBackward0>)
size_num_loss 1.4000000000000001
loss: tensor([-1761.5555], grad_fn=<AddBackward0>)
9
epoch:  9 ; loss:  -1761.5555419921875 ; pred:  tensor([0.2682, 0.5321, 0.1063, 0.0934], grad_fn=<SoftmaxBackward0>)
num_high 14 len(mask) 31
mask_without_small tensor([0.9127, 0.9038, 0.8908, 0.8854, 0.8590, 0.9052, 0.5085, 0.5237, 0.8605,
        0.9084, 0.8760, 0.5065, 0.8758, 0.8769, 0.8933, 0.8250, 0.9167],
       grad_fn=<IndexBackward0>)
pred_loss tensor([6.2090], grad_fn=<MulBackward0>)
size_loss tensor(-1480.1428, grad_fn=<MulBackward0>)
size_num_loss 1.4000000000000001
loss: tensor([-1471.8560], grad_fn=<AddBackward0>)
10
epoch:  10 ; loss:  -1471.85595703125 ; pred:  tensor([0.2667, 0.5375, 0.1034, 0.0925], grad_fn=<SoftmaxBackward0>)
num_high 14 len(mask) 31
mask_without_small tensor([0.9196, 0.9114, 0.8992, 0.8941, 0.8693, 0.9126, 0.5032, 0.8707, 0.9156,
        0.8854, 0.8852, 0.8862, 0.9015, 0.8366, 0.9234],
       grad_fn=<IndexBackward0>)
pred_loss tensor([6.1184], grad_fn=<MulBackward0>)
size_loss tensor(-1033.7797, grad_fn=<MulBackward0>)
size_num_loss 1.4000000000000001
loss: tensor([-1025.6016], grad_fn=<AddBackward0>)
11
epoch:  11 ; loss:  -1025.6015625 ; pred:  tensor([0.2652, 0.5424, 0.1008, 0.0916], grad_fn=<SoftmaxBackward0>)
num_high 14 len(mask) 31
mask_without_small tensor([0.9261, 0.9183, 0.9069, 0.9021, 0.8781, 0.9195, 0.8795, 0.9223, 0.8937,
        0.8936, 0.8946, 0.9091, 0.8433, 0.9296], grad_fn=<IndexBackward0>)
pred_loss tensor([6.0544], grad_fn=<MulBackward0>)
size_loss tensor(-233.6992, grad_fn=<MulBackward0>)
size_num_loss 1.4000000000000001
loss: tensor([-225.5980], grad_fn=<AddBackward0>)
12
epoch:  12 ; loss:  -225.59799194335938 ; pred:  tensor([0.2641, 0.5458, 0.0990, 0.0911], grad_fn=<SoftmaxBackward0>)
num_high 14 len(mask) 31
mask_without_small tensor([0.9323, 0.9251, 0.9139, 0.9089, 0.8803, 0.9261, 0.8822, 0.9287, 0.8996,
        0.8995, 0.9007, 0.9161, 0.8389, 0.9355], grad_fn=<IndexBackward0>)
pred_loss tensor([6.0618], grad_fn=<MulBackward0>)
size_loss tensor(-261.0669, grad_fn=<MulBackward0>)
size_num_loss 1.4000000000000001
loss: tensor([-252.9648], grad_fn=<AddBackward0>)
13
epoch:  13 ; loss:  -252.9647979736328 ; pred:  tensor([0.2642, 0.5454, 0.0988, 0.0916], grad_fn=<SoftmaxBackward0>)
num_high 14 len(mask) 31
mask_without_small tensor([0.9382, 0.9314, 0.9203, 0.9149, 0.8792, 0.9324, 0.8817, 0.9349, 0.9039,
        0.9037, 0.9053, 0.9226, 0.8312, 0.9412], grad_fn=<IndexBackward0>)
pred_loss tensor([6.0961], grad_fn=<MulBackward0>)
size_loss tensor(-299.8915, grad_fn=<MulBackward0>)
size_num_loss 1.4000000000000001
loss: tensor([-291.7609], grad_fn=<AddBackward0>)
14
epoch:  14 ; loss:  -291.76092529296875 ; pred:  tensor([0.2647, 0.5436, 0.0993, 0.0924], grad_fn=<SoftmaxBackward0>)
num_high 14 len(mask) 31
mask_without_small tensor([0.9437, 0.9373, 0.9262, 0.9204, 0.8757, 0.9382, 0.8788, 0.9406, 0.9071,
        0.9068, 0.9089, 0.9287, 0.8213, 0.9465], grad_fn=<IndexBackward0>)
pred_loss tensor([6.1489], grad_fn=<MulBackward0>)
size_loss tensor(-346.3801, grad_fn=<MulBackward0>)
size_num_loss 1.4000000000000001
loss: tensor([-338.2020], grad_fn=<AddBackward0>)
15
epoch:  15 ; loss:  -338.2019958496094 ; pred:  tensor([0.2656, 0.5407, 0.1002, 0.0935], grad_fn=<SoftmaxBackward0>)
num_high 14 len(mask) 31
mask_without_small tensor([0.9488, 0.9427, 0.9317, 0.9254, 0.8702, 0.9436, 0.8740, 0.9459, 0.9094,
        0.9091, 0.9118, 0.9343, 0.8093, 0.9513], grad_fn=<IndexBackward0>)
pred_loss tensor([6.2165], grad_fn=<MulBackward0>)
size_loss tensor(-398.9797, grad_fn=<MulBackward0>)
size_num_loss 1.4000000000000001
loss: tensor([-390.7388], grad_fn=<AddBackward0>)
16
epoch:  16 ; loss:  -390.7388000488281 ; pred:  tensor([0.2667, 0.5371, 0.1015, 0.0947], grad_fn=<SoftmaxBackward0>)
num_high 14 len(mask) 31
mask_without_small tensor([0.9534, 0.9477, 0.9368, 0.9301, 0.8630, 0.9486, 0.8675, 0.9507, 0.9111,
        0.9108, 0.9141, 0.9395, 0.7955, 0.9557], grad_fn=<IndexBackward0>)
pred_loss tensor([6.2968], grad_fn=<MulBackward0>)
size_loss tensor(-457.0086, grad_fn=<MulBackward0>)
size_num_loss 1.4000000000000001
loss: tensor([-448.6918], grad_fn=<AddBackward0>)
17
epoch:  17 ; loss:  -448.6917724609375 ; pred:  tensor([0.2680, 0.5328, 0.1031, 0.0962], grad_fn=<SoftmaxBackward0>)
num_high 14 len(mask) 31
mask_without_small tensor([0.9575, 0.9523, 0.9416, 0.9345, 0.8542, 0.9531, 0.8594, 0.9550, 0.9124,
        0.9121, 0.9161, 0.9443, 0.7800, 0.9597], grad_fn=<IndexBackward0>)
pred_loss tensor([6.3889], grad_fn=<MulBackward0>)
size_loss tensor(-520.2151, grad_fn=<MulBackward0>)
size_num_loss 1.4000000000000001
loss: tensor([-511.8100], grad_fn=<AddBackward0>)
18
epoch:  18 ; loss:  -511.8100280761719 ; pred:  tensor([0.2693, 0.5279, 0.1050, 0.0978], grad_fn=<SoftmaxBackward0>)
num_high 14 len(mask) 31
mask_without_small tensor([0.9613, 0.9564, 0.9461, 0.9387, 0.8438, 0.9571, 0.8497, 0.9589, 0.9134,
        0.9130, 0.9179, 0.9487, 0.7628, 0.9633], grad_fn=<IndexBackward0>)
pred_loss tensor([6.4923], grad_fn=<MulBackward0>)
size_loss tensor(-588.5803, grad_fn=<MulBackward0>)
size_num_loss 1.4000000000000001
loss: tensor([-580.0754], grad_fn=<AddBackward0>)
19
epoch:  19 ; loss:  -580.0753784179688 ; pred:  tensor([0.2708, 0.5224, 0.1072, 0.0996], grad_fn=<SoftmaxBackward0>)
Finished Training
sel masked ver tensor(indices=tensor([[6927, 7069, 7081, 5503, 5808, 5808, 5808, 5808, 5970,
                        5808, 5808, 5503, 5808, 5808],
                       [5808, 5808, 5808, 5970,    0, 8159,   44, 8158, 7069,
                        6927, 7069, 6927, 5970, 5230]]),
       values=tensor([0.9613, 0.9564, 0.9461, 0.9387, 0.8438, 0.9571, 0.8497,
                      0.9589, 0.9134, 0.9130, 0.9179, 0.9487, 0.7628, 0.9633]),
       size=(7082, 8160), nnz=14, layout=torch.sparse_coo,
       grad_fn=<SparseCooTensorWithDimsAndTensorsBackward0>)
Counter({'author': 3, 'publication': 2, 'carriesOut': 1, 'publishes': 1, 'fax': 1, 'phone': 1, 'type': 1, 'worksAtProject': 1, 'photo': 1, 'homepage': 1, 'projectInfo': 1})
dict index: {}
node_idx 5808
 node original label [1]
 node predicted label explain 1
 node prediction probability explain tensor([0.2708, 0.5224, 0.1072, 0.0996], grad_fn=<SoftmaxBackward0>)
 node predicted label full 1 most important relations  {'carriesOut': 1, 'publishes': 1, 'fax': 1, 'phone': 1, 'type': 1, 'worksAtProject': 1, 'publication': 2, 'photo': 1, 'homepage': 1, 'projectInfo': 1, 'author': 3, 'label': 1, 'node_idx': '5808'}
 final masks and lenght tensor(indices=tensor([[ 23497,  23639,  23651,  39110,  46928,  63380,  88658,
                        114774, 114786, 130083, 146815, 147772, 147914, 147926,
                        154515, 179488, 179955, 196363, 229503, 237788, 246235,
                        246235, 254358, 254358, 254358, 262338, 262338, 262338,
                        304068, 328923, 328923],
                       [  5808,   5808,   5808,   5503,   5970,   5970,      0,
                          5970,   5970,   8159,   5385,   5385,   5385,   5385,
                          5808,   5808,   5808,   2855,     44,   8158,   7069,
                          7081,   6927,   7069,   7081,   6927,   7069,   7081,
                          5970,   5230,   5231]]),
       values=tensor([0.9613, 0.9564, 0.9461, 0.3499, 0.9387, 0.3790, 0.8438,
                      0.3866, 0.3644, 0.9571, 0.3821, 0.3691, 0.3659, 0.3770,
                      0.3633, 0.3914, 0.3702, 0.3784, 0.8497, 0.9589, 0.9134,
                      0.3800, 0.9130, 0.9179, 0.3930, 0.9487, 0.3790, 0.3730,
                      0.7628, 0.9633, 0.3783]),
       size=(753935, 8285), nnz=31, layout=torch.sparse_coo,
       grad_fn=<SparseCooTensorWithDimsAndTensorsBackward0>) 14
 ---------------------------------------------------------------
node label: 1
20
r_exp.py:102: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  hor_graph, ver_graph = torch.tensor(sub_hor_graph, dtype=torch.float), torch.tensor(sub_ver_graph, dtype=torch.float)
wandb: WARNING Calling wandb.login() after wandb.init() has no effect.
num_high 55 len(mask) 55
mask_without_small tensor([0.7970, 0.7831, 0.7635, 0.6453, 0.7557, 0.6823, 0.7294, 0.6669, 0.7019,
        0.7882, 0.7161, 0.6753, 0.7029, 0.7096, 0.7013, 0.7587, 0.7880, 0.7250,
        0.7120, 0.7472, 0.7017, 0.7695, 0.7600, 0.7893, 0.7762, 0.7768, 0.7533,
        0.7781, 0.7223, 0.7326, 0.7215, 0.7621, 0.6761, 0.6972, 0.7226, 0.7904,
        0.7428, 0.7148, 0.7424, 0.7026, 0.6985, 0.6951, 0.7934, 0.7370, 0.7446,
        0.7376, 0.7797, 0.7797, 0.6837, 0.7568, 0.6889, 0.7507, 0.7728, 0.7516,
        0.7137], grad_fn=<IndexBackward0>)
pred_loss tensor([0.5431], grad_fn=<MulBackward0>)
size_loss tensor(-3.8010, grad_fn=<MulBackward0>)
size_num_loss 5.5
loss: tensor([6.8625], grad_fn=<AddBackward0>)
0
epoch:  0 ; loss:  6.862457752227783 ; pred:  tensor([0.0116, 0.9471, 0.0180, 0.0232], grad_fn=<SoftmaxBackward0>)
num_high 54 len(mask) 55
mask_without_small tensor([0.8127, 0.7996, 0.7810, 0.6221, 0.7737, 0.6603, 0.7093, 0.6443, 0.6806,
        0.8045, 0.6953, 0.6530, 0.6816, 0.6885, 0.6799, 0.7765, 0.8043, 0.7047,
        0.6911, 0.7279, 0.6804, 0.7868, 0.7778, 0.8054, 0.7931, 0.7937, 0.7714,
        0.7949, 0.7018, 0.7126, 0.7010, 0.7797, 0.6538, 0.6757, 0.7021, 0.8065,
        0.7233, 0.6940, 0.7610, 0.6813, 0.6771, 0.6735, 0.8093, 0.7172, 0.7632,
        0.7178, 0.7964, 0.7964, 0.6617, 0.7747, 0.6670, 0.7689, 0.7899, 0.7698,
        0.6929], grad_fn=<IndexBackward0>)
pred_loss tensor([0.5407], grad_fn=<MulBackward0>)
size_loss tensor(-5.5285, grad_fn=<MulBackward0>)
size_num_loss 5.4
loss: tensor([5.0119], grad_fn=<AddBackward0>)
1
epoch:  1 ; loss:  5.011900424957275 ; pred:  tensor([0.0115, 0.9474, 0.0179, 0.0233], grad_fn=<SoftmaxBackward0>)
num_high 53 len(mask) 55
mask_without_small tensor([0.8272, 0.8150, 0.7977, 0.5985, 0.7901, 0.6376, 0.6884, 0.6211, 0.6585,
        0.8195, 0.6737, 0.6301, 0.6595, 0.6667, 0.6578, 0.7932, 0.8194, 0.6835,
        0.6693, 0.7113, 0.6582, 0.8031, 0.7944, 0.8204, 0.8090, 0.8095, 0.7872,
        0.8106, 0.6805, 0.6920, 0.6796, 0.7964, 0.6309, 0.6534, 0.6808, 0.8215,
        0.7058, 0.6724, 0.7759, 0.6592, 0.6548, 0.6512, 0.8241, 0.6976, 0.7792,
        0.6982, 0.8121, 0.8120, 0.6390, 0.7914, 0.6445, 0.7848, 0.8060, 0.7848,
        0.6712], grad_fn=<IndexBackward0>)
pred_loss tensor([0.5450], grad_fn=<MulBackward0>)
size_loss tensor(-7.3202, grad_fn=<MulBackward0>)
size_num_loss 5.300000000000001
loss: tensor([3.0980], grad_fn=<AddBackward0>)
2
epoch:  2 ; loss:  3.098029613494873 ; pred:  tensor([0.0116, 0.9470, 0.0179, 0.0235], grad_fn=<SoftmaxBackward0>)
num_high 48 len(mask) 55
mask_without_small tensor([0.8405, 0.8293, 0.8133, 0.5746, 0.8059, 0.6142, 0.6667, 0.5975, 0.6356,
        0.8334, 0.6513, 0.6066, 0.6367, 0.6440, 0.6349, 0.8090, 0.8333, 0.6615,
        0.6467, 0.6926, 0.6353, 0.8184, 0.8103, 0.8343, 0.8238, 0.8243, 0.8027,
        0.8253, 0.6583, 0.6706, 0.6575, 0.8122, 0.6075, 0.6304, 0.6587, 0.8353,
        0.6865, 0.6499, 0.7913, 0.6363, 0.6319, 0.6281, 0.8377, 0.6770, 0.7950,
        0.6776, 0.8266, 0.8266, 0.6157, 0.8074, 0.6213, 0.8004, 0.8210, 0.8000,
        0.6487], grad_fn=<IndexBackward0>)
pred_loss tensor([0.5539], grad_fn=<MulBackward0>)
size_loss tensor(-9.1380, grad_fn=<MulBackward0>)
size_num_loss 4.800000000000001
loss: tensor([0.7571], grad_fn=<AddBackward0>)
3
epoch:  3 ; loss:  0.7571368217468262 ; pred:  tensor([0.0118, 0.9461, 0.0181, 0.0240], grad_fn=<SoftmaxBackward0>)
num_high 39 len(mask) 55
mask_without_small tensor([0.8525, 0.8424, 0.8280, 0.5505, 0.8209, 0.5904, 0.6441, 0.5736, 0.6121,
        0.8461, 0.6281, 0.5828, 0.6132, 0.6207, 0.6113, 0.8239, 0.8460, 0.6387,
        0.6234, 0.6725, 0.6118, 0.8326, 0.8252, 0.8469, 0.8375, 0.8379, 0.8177,
        0.8388, 0.6354, 0.6482, 0.6345, 0.8270, 0.5837, 0.6068, 0.6357, 0.8479,
        0.6658, 0.6267, 0.8064, 0.6128, 0.6083, 0.6045, 0.8500, 0.6553, 0.8103,
        0.6559, 0.8400, 0.8400, 0.5919, 0.8224, 0.5976, 0.8154, 0.8351, 0.8148,
        0.6254], grad_fn=<IndexBackward0>)
pred_loss tensor([0.5678], grad_fn=<MulBackward0>)
size_loss tensor(-10.9579, grad_fn=<MulBackward0>)
size_num_loss 3.9000000000000004
loss: tensor([-1.9863], grad_fn=<AddBackward0>)
4
epoch:  4 ; loss:  -1.9863357543945312 ; pred:  tensor([0.0121, 0.9448, 0.0185, 0.0246], grad_fn=<SoftmaxBackward0>)
num_high 30 len(mask) 55
mask_without_small tensor([0.8632, 0.8542, 0.8416, 0.5264, 0.8350, 0.5663, 0.6207, 0.5494, 0.5880,
        0.8575, 0.6043, 0.5586, 0.5891, 0.5967, 0.5872, 0.8379, 0.8574, 0.6151,
        0.5995, 0.6511, 0.5877, 0.8457, 0.8390, 0.8582, 0.8499, 0.8503, 0.8318,
        0.8511, 0.6117, 0.6250, 0.6108, 0.8407, 0.5596, 0.5827, 0.6121, 0.8592,
        0.6440, 0.6028, 0.8210, 0.5888, 0.5842, 0.5804, 0.8612, 0.6326, 0.8249,
        0.6333, 0.8522, 0.8521, 0.5678, 0.8365, 0.5735, 0.8297, 0.8481, 0.8290,
        0.6015], grad_fn=<IndexBackward0>)
pred_loss tensor([0.5871], grad_fn=<MulBackward0>)
size_loss tensor(-12.7642, grad_fn=<MulBackward0>)
size_num_loss 3.0
loss: tensor([-4.7162], grad_fn=<AddBackward0>)
5
epoch:  5 ; loss:  -4.71623420715332 ; pred:  tensor([0.0125, 0.9430, 0.0190, 0.0255], grad_fn=<SoftmaxBackward0>)
num_high 26 len(mask) 55
mask_without_small tensor([0.8727, 0.8648, 0.8540, 0.5024, 0.8480, 0.5418, 0.5966, 0.5251, 0.5635,
        0.8677, 0.5798, 0.5342, 0.5646, 0.5722, 0.5627, 0.8507, 0.8676, 0.5908,
        0.5750, 0.6286, 0.5632, 0.8575, 0.8517, 0.8683, 0.8611, 0.8614, 0.8451,
        0.8621, 0.5874, 0.6011, 0.5864, 0.8533, 0.5353, 0.5582, 0.5878, 0.8694,
        0.6212, 0.5784, 0.8349, 0.5643, 0.5598, 0.5560, 0.8712, 0.6091, 0.8387,
        0.6098, 0.8632, 0.8631, 0.5434, 0.8495, 0.5491, 0.8432, 0.8600, 0.8423,
        0.5771], grad_fn=<IndexBackward0>)
pred_loss tensor([0.6121], grad_fn=<MulBackward0>)
size_loss tensor(-14.5447, grad_fn=<MulBackward0>)
size_num_loss 2.6
loss: tensor([-6.9198], grad_fn=<AddBackward0>)
6
epoch:  6 ; loss:  -6.919831275939941 ; pred:  tensor([0.0131, 0.9406, 0.0197, 0.0266], grad_fn=<SoftmaxBackward0>)
num_high 25 len(mask) 55
mask_without_small tensor([0.8809, 0.8741, 0.8652, 0.8599, 0.5173, 0.5720, 0.5009, 0.5387, 0.8766,
        0.5550, 0.5098, 0.5398, 0.5473, 0.5379, 0.8623, 0.8765, 0.5660, 0.5501,
        0.6052, 0.5384, 0.8680, 0.8632, 0.8771, 0.8710, 0.8713, 0.8572, 0.8718,
        0.5625, 0.5765, 0.5616, 0.8648, 0.5109, 0.5335, 0.5629, 0.8784, 0.5975,
        0.5535, 0.8479, 0.5396, 0.5350, 0.5314, 0.8801, 0.5849, 0.8516, 0.5856,
        0.8729, 0.8728, 0.5189, 0.8614, 0.5245, 0.8556, 0.8709, 0.8546, 0.5522],
       grad_fn=<IndexBackward0>)
pred_loss tensor([0.6430], grad_fn=<MulBackward0>)
size_loss tensor(-1617.2924, grad_fn=<MulBackward0>)
size_num_loss 2.5
loss: tensor([-1613.2208], grad_fn=<AddBackward0>)
7
epoch:  7 ; loss:  -1613.2208251953125 ; pred:  tensor([0.0138, 0.9377, 0.0206, 0.0279], grad_fn=<SoftmaxBackward0>)
num_high 25 len(mask) 55
mask_without_small tensor([0.8862, 0.8796, 0.8710, 0.8659, 0.5043, 0.5593, 0.5257, 0.8820, 0.5421,
        0.5268, 0.5344, 0.5250, 0.8682, 0.8819, 0.5533, 0.5373, 0.5928, 0.5255,
        0.8737, 0.8691, 0.8825, 0.8766, 0.8769, 0.8633, 0.8774, 0.5498, 0.5639,
        0.5488, 0.8706, 0.5206, 0.5502, 0.8838, 0.5851, 0.5407, 0.8544, 0.5268,
        0.5222, 0.5186, 0.8854, 0.5723, 0.8579, 0.5730, 0.8785, 0.8784, 0.5059,
        0.8673, 0.5115, 0.8617, 0.8767, 0.8607, 0.5393],
       grad_fn=<IndexBackward0>)
pred_loss tensor([0.6600], grad_fn=<MulBackward0>)
size_loss tensor(-1688.5291, grad_fn=<MulBackward0>)
size_num_loss 2.5
loss: tensor([-1684.4614], grad_fn=<AddBackward0>)
8
epoch:  8 ; loss:  -1684.46142578125 ; pred:  tensor([0.0141, 0.9361, 0.0211, 0.0287], grad_fn=<SoftmaxBackward0>)
num_high 25 len(mask) 55
mask_without_small tensor([0.8927, 0.8865, 0.8782, 0.8734, 0.5427, 0.5090, 0.8887, 0.5254, 0.5101,
        0.5177, 0.5082, 0.8756, 0.8886, 0.5367, 0.5206, 0.5766, 0.5087, 0.8809,
        0.8764, 0.8892, 0.8836, 0.8839, 0.8709, 0.8844, 0.5331, 0.5473, 0.5321,
        0.8779, 0.5039, 0.5335, 0.8905, 0.5689, 0.5240, 0.8624, 0.5101, 0.5054,
        0.5018, 0.8920, 0.5559, 0.8658, 0.5566, 0.8854, 0.8853, 0.8747, 0.8694,
        0.8838, 0.8685, 0.5226], grad_fn=<IndexBackward0>)
pred_loss tensor([0.6828], grad_fn=<MulBackward0>)
size_loss tensor(-1784.3877, grad_fn=<MulBackward0>)
size_num_loss 2.5
loss: tensor([-1780.3201], grad_fn=<AddBackward0>)
9
epoch:  9 ; loss:  -1780.320068359375 ; pred:  tensor([0.0147, 0.9340, 0.0217, 0.0296], grad_fn=<SoftmaxBackward0>)
num_high 25 len(mask) 55
mask_without_small tensor([0.8998, 0.8939, 0.8861, 0.8816, 0.5236, 0.8961, 0.5062, 0.8837, 0.8960,
        0.5175, 0.5013, 0.5578, 0.8886, 0.8844, 0.8965, 0.8912, 0.8914, 0.8792,
        0.8919, 0.5139, 0.5283, 0.5130, 0.8858, 0.5143, 0.8977, 0.5500, 0.5049,
        0.8712, 0.8991, 0.5369, 0.8744, 0.5376, 0.8929, 0.8928, 0.8828, 0.8778,
        0.8915, 0.8769, 0.5034], grad_fn=<IndexBackward0>)
pred_loss tensor([0.7096], grad_fn=<MulBackward0>)
size_loss tensor(-1783.2375, grad_fn=<MulBackward0>)
size_num_loss 2.5
loss: tensor([-1779.1964], grad_fn=<AddBackward0>)
10
epoch:  10 ; loss:  -1779.1964111328125 ; pred:  tensor([0.0153, 0.9315, 0.0225, 0.0307], grad_fn=<SoftmaxBackward0>)
num_high 25 len(mask) 55
mask_without_small tensor([0.9070, 0.9015, 0.8942, 0.8899, 0.5032, 0.9035, 0.8919, 0.9034, 0.5380,
        0.8965, 0.8926, 0.9039, 0.8990, 0.8992, 0.8877, 0.8996, 0.5079, 0.8939,
        0.9051, 0.5300, 0.8802, 0.9064, 0.5168, 0.8832, 0.5175, 0.9006, 0.9005,
        0.8911, 0.8864, 0.8993, 0.8855], grad_fn=<IndexBackward0>)
pred_loss tensor([0.7335], grad_fn=<MulBackward0>)
size_loss tensor(-1517.2977, grad_fn=<MulBackward0>)
size_num_loss 2.5
loss: tensor([-1513.2805], grad_fn=<AddBackward0>)
11
epoch:  11 ; loss:  -1513.280517578125 ; pred:  tensor([0.0158, 0.9293, 0.0233, 0.0316], grad_fn=<SoftmaxBackward0>)
num_high 25 len(mask) 55
mask_without_small tensor([0.9140, 0.9089, 0.9021, 0.8980, 0.9108, 0.8999, 0.9107, 0.5185, 0.9043,
        0.9006, 0.9111, 0.9065, 0.9067, 0.8959, 0.9072, 0.9018, 0.9122, 0.5104,
        0.8889, 0.9135, 0.8918, 0.9080, 0.9079, 0.8992, 0.8947, 0.9068, 0.8939],
       grad_fn=<IndexBackward0>)
pred_loss tensor([0.7544], grad_fn=<MulBackward0>)
size_loss tensor(-1041.4059, grad_fn=<MulBackward0>)
size_num_loss 2.5
loss: tensor([-1037.3953], grad_fn=<AddBackward0>)
12
epoch:  12 ; loss:  -1037.395263671875 ; pred:  tensor([0.0163, 0.9273, 0.0239, 0.0324], grad_fn=<SoftmaxBackward0>)
num_high 25 len(mask) 55
mask_without_small tensor([0.9207, 0.9158, 0.9093, 0.9055, 0.9176, 0.9072, 0.9175, 0.9114, 0.9079,
        0.9179, 0.9136, 0.9138, 0.9035, 0.9142, 0.9091, 0.9190, 0.8967, 0.9201,
        0.8995, 0.9150, 0.9149, 0.9066, 0.9023, 0.9139, 0.9015],
       grad_fn=<IndexBackward0>)
pred_loss tensor([0.7735], grad_fn=<MulBackward0>)
size_loss tensor(-67.7107, grad_fn=<MulBackward0>)
size_num_loss 2.5
loss: tensor([-63.6981], grad_fn=<AddBackward0>)
13
epoch:  13 ; loss:  -63.69807815551758 ; pred:  tensor([0.0168, 0.9256, 0.0245, 0.0332], grad_fn=<SoftmaxBackward0>)
num_high 25 len(mask) 55
mask_without_small tensor([0.9270, 0.9226, 0.9147, 0.9087, 0.9243, 0.9115, 0.9242, 0.9176, 0.9125,
        0.9246, 0.9202, 0.9204, 0.9055, 0.9209, 0.9143, 0.9255, 0.8961, 0.9266,
        0.8998, 0.9218, 0.9217, 0.9104, 0.9039, 0.9206, 0.9026],
       grad_fn=<IndexBackward0>)
pred_loss tensor([0.8001], grad_fn=<MulBackward0>)
size_loss tensor(-90.1080, grad_fn=<MulBackward0>)
size_num_loss 2.5
loss: tensor([-86.0743], grad_fn=<AddBackward0>)
14
epoch:  14 ; loss:  -86.0743179321289 ; pred:  tensor([0.0174, 0.9231, 0.0253, 0.0342], grad_fn=<SoftmaxBackward0>)
num_high 25 len(mask) 55
mask_without_small tensor([0.9332, 0.9291, 0.9189, 0.9093, 0.9307, 0.9137, 0.9307, 0.9232, 0.9154,
        0.9310, 0.9266, 0.9268, 0.9047, 0.9273, 0.9183, 0.9319, 0.8925, 0.9328,
        0.8971, 0.9283, 0.9282, 0.9120, 0.9024, 0.9271, 0.9008],
       grad_fn=<IndexBackward0>)
pred_loss tensor([0.8285], grad_fn=<MulBackward0>)
size_loss tensor(-123.9923, grad_fn=<MulBackward0>)
size_num_loss 2.5
loss: tensor([-119.9353], grad_fn=<AddBackward0>)
15
epoch:  15 ; loss:  -119.93528747558594 ; pred:  tensor([0.0181, 0.9205, 0.0262, 0.0353], grad_fn=<SoftmaxBackward0>)
num_high 25 len(mask) 55
mask_without_small tensor([0.9391, 0.9353, 0.9223, 0.9081, 0.9369, 0.9145, 0.9368, 0.9286, 0.9170,
        0.9371, 0.9326, 0.9329, 0.9019, 0.9335, 0.9215, 0.9379, 0.8870, 0.9387,
        0.8924, 0.9345, 0.9344, 0.9120, 0.8989, 0.9332, 0.8969],
       grad_fn=<IndexBackward0>)
pred_loss tensor([0.8577], grad_fn=<MulBackward0>)
size_loss tensor(-165.0960, grad_fn=<MulBackward0>)
size_num_loss 2.5
loss: tensor([-161.0144], grad_fn=<AddBackward0>)
16
epoch:  16 ; loss:  -161.014404296875 ; pred:  tensor([0.0188, 0.9178, 0.0271, 0.0363], grad_fn=<SoftmaxBackward0>)
num_high 25 len(mask) 55
mask_without_small tensor([0.9445, 0.9411, 0.9253, 0.9054, 0.9425, 0.9141, 0.9425, 0.9336, 0.9177,
        0.9428, 0.9383, 0.9386, 0.8975, 0.9392, 0.9242, 0.9435, 0.8800, 0.9442,
        0.8861, 0.9402, 0.9401, 0.9106, 0.8939, 0.9389, 0.8914],
       grad_fn=<IndexBackward0>)
pred_loss tensor([0.8876], grad_fn=<MulBackward0>)
size_loss tensor(-211.5471, grad_fn=<MulBackward0>)
size_num_loss 2.5
loss: tensor([-207.4399], grad_fn=<AddBackward0>)
17
epoch:  17 ; loss:  -207.43988037109375 ; pred:  tensor([0.0195, 0.9151, 0.0280, 0.0374], grad_fn=<SoftmaxBackward0>)
num_high 25 len(mask) 55
mask_without_small tensor([0.9495, 0.9464, 0.9280, 0.9012, 0.9478, 0.9126, 0.9477, 0.9384, 0.9175,
        0.9480, 0.9437, 0.9440, 0.8916, 0.9446, 0.9264, 0.9486, 0.8714, 0.9493,
        0.8784, 0.9456, 0.9455, 0.9080, 0.8873, 0.9443, 0.8844],
       grad_fn=<IndexBackward0>)
pred_loss tensor([0.9183], grad_fn=<MulBackward0>)
size_loss tensor(-262.4963, grad_fn=<MulBackward0>)
size_num_loss 2.5
loss: tensor([-258.3621], grad_fn=<AddBackward0>)
18
epoch:  18 ; loss:  -258.36212158203125 ; pred:  tensor([0.0203, 0.9123, 0.0290, 0.0385], grad_fn=<SoftmaxBackward0>)
num_high 25 len(mask) 55
mask_without_small tensor([0.9541, 0.9513, 0.9304, 0.8958, 0.9526, 0.9102, 0.9525, 0.9430, 0.9166,
        0.9528, 0.9486, 0.9489, 0.8843, 0.9495, 0.9285, 0.9533, 0.8615, 0.9539,
        0.8693, 0.9505, 0.9504, 0.9043, 0.8794, 0.9493, 0.8761],
       grad_fn=<IndexBackward0>)
pred_loss tensor([0.9501], grad_fn=<MulBackward0>)
size_loss tensor(-317.6328, grad_fn=<MulBackward0>)
size_num_loss 2.5
loss: tensor([-313.4701], grad_fn=<AddBackward0>)
19
epoch:  19 ; loss:  -313.4700927734375 ; pred:  tensor([0.0210, 0.9094, 0.0300, 0.0396], grad_fn=<SoftmaxBackward0>)
Finished Training
sel masked ver tensor(indices=tensor([[6898, 6918, 6929, 6973, 5420, 7163, 5965, 6972, 7163,
                        7163, 7163, 5403, 5404, 5420, 5965, 5965, 5785, 5785,
                        5785, 5503, 5503, 5503, 5503, 5785, 5785],
                       [5785, 5785, 5785, 5785, 5965, 5965, 5420, 5420, 5403,
                        5446, 5477, 5785, 5785, 5785, 5785, 6898, 6898, 6973,
                        7163, 6918, 6929, 6973, 7163, 5965, 5230]]),
       values=tensor([0.9541, 0.9513, 0.9304, 0.8958, 0.9526, 0.9102, 0.9525,
                      0.9430, 0.9166, 0.9528, 0.9486, 0.9489, 0.8843, 0.9495,
                      0.9285, 0.9533, 0.8615, 0.9539, 0.8693, 0.9505, 0.9504,
                      0.9043, 0.8794, 0.9493, 0.8761]),
       size=(7164, 7164), nnz=25, layout=torch.sparse_coo,
       grad_fn=<SparseCooTensorWithDimsAndTensorsBackward0>)
Counter({'isAbout': 5, 'publishes': 4, 'author': 4, 'isWorkedOnBy': 3, 'publication': 3, 'dealtWithIn': 1, 'type': 1, 'worksAtProject': 1, 'member': 1, 'projectInfo': 1, 'hasProject': 1})
dict index: {}
node_idx 5785
 node original label [1]
 node predicted label explain 1
 node prediction probability explain tensor([0.0210, 0.9094, 0.0300, 0.0396], grad_fn=<SoftmaxBackward0>)
 node predicted label full 1 most important relations  {'isWorkedOnBy': 3, 'dealtWithIn': 1, 'publishes': 4, 'type': 1, 'worksAtProject': 1, 'publication': 3, 'isAbout': 5, 'member': 1, 'projectInfo': 1, 'author': 4, 'hasProject': 1, 'label': 1, 'node_idx': '5785'}
 final masks and lenght tensor(indices=tensor([[ 23468,  23488,  23499,  23542,  23543,  23690,  23733,
                         39105,  46928,  63415,  63441,  63472,  88635, 114603,
                        114623, 114868, 146810, 146810, 146810, 147763, 147763,
                        147817, 148008, 148008, 148008, 154533, 154534, 154550,
                        154576, 154607, 179488, 179950, 196340, 229480, 237765,
                        246230, 246230, 246230, 254335, 254335, 254335, 254335,
                        254335, 254335, 254335, 262338, 262338, 262338, 262338,
                        262338, 262338, 262338, 304045, 328900, 328900],
                       [  5785,   5785,   5785,   5785,   5785,   5785,   5785,
                          5503,   5965,   5965,   5965,   5965,     81,   5965,
                          5965,   5965,   5420,   5446,   5477,   5446,   5477,
                          5420,   5403,   5446,   5477,   5785,   5785,   5785,
                          5785,   5785,   5785,   5785,   2489,     41,   5585,
                          6898,   6918,   7163,   6898,   6918,   6929,   6972,
                          6973,   7120,   7163,   6898,   6918,   6929,   6972,
                          6973,   7120,   7163,   5965,   5230,   5231]]),
       values=tensor([0.9541, 0.9513, 0.9304, 0.3454, 0.8958, 0.3910, 0.3893,
                      0.4095, 0.3850, 0.9526, 0.3816, 0.4183, 0.3861, 0.3934,
                      0.3843, 0.9102, 0.9525, 0.3928, 0.3768, 0.4184, 0.3847,
                      0.9430, 0.9166, 0.9528, 0.9486, 0.9489, 0.8843, 0.9495,
                      0.3892, 0.3941, 0.3882, 0.9285, 0.4205, 0.3806, 0.3896,
                      0.9533, 0.4100, 0.3805, 0.8615, 0.3871, 0.3822, 0.3791,
                      0.9539, 0.4034, 0.8693, 0.4040, 0.9505, 0.9504, 0.3931,
                      0.9043, 0.3985, 0.8794, 0.9493, 0.8761, 0.3788]),
       size=(753935, 8285), nnz=55, layout=torch.sparse_coo,
       grad_fn=<SparseCooTensorWithDimsAndTensorsBackward0>) 25
 ---------------------------------------------------------------
node label: 2
14
num_high 22 len(mask) 24
mask_without_small tensor([0.8258, 0.8068, 0.7790, 0.5968, 0.7678, 0.6556, 0.7286, 0.6311, 0.7508,
        0.6904, 0.7022, 0.7445, 0.6636, 0.7362, 0.5820, 0.7186, 0.7131, 0.7080,
        0.7742, 0.6944, 0.6962, 0.7275, 0.6815, 0.7494],
       grad_fn=<IndexBackward0>)
pred_loss tensor([4.1189], grad_fn=<MulBackward0>)
size_loss tensor(-5.9990, grad_fn=<MulBackward0>)
size_num_loss 2.2
loss: tensor([2.6229], grad_fn=<AddBackward0>)
r_exp.py:102: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  hor_graph, ver_graph = torch.tensor(sub_hor_graph, dtype=torch.float), torch.tensor(sub_ver_graph, dtype=torch.float)
wandb: WARNING Calling wandb.login() after wandb.init() has no effect.
0
epoch:  0 ; loss:  2.62287974357605 ; pred:  tensor([0.0677, 0.1524, 0.6624, 0.1175], grad_fn=<SoftmaxBackward0>)
num_high 21 len(mask) 24
mask_without_small tensor([0.8397, 0.8219, 0.7958, 0.5725, 0.7851, 0.6327, 0.7479, 0.6075, 0.7690,
        0.6686, 0.6808, 0.7631, 0.6409, 0.7552, 0.5575, 0.7384, 0.7331, 0.7283,
        0.7912, 0.6727, 0.6746, 0.7468, 0.6594, 0.7677],
       grad_fn=<IndexBackward0>)
pred_loss tensor([4.0704], grad_fn=<MulBackward0>)
size_loss tensor(-7.6824, grad_fn=<MulBackward0>)
size_num_loss 2.1
loss: tensor([0.7871], grad_fn=<AddBackward0>)
1
epoch:  1 ; loss:  0.7871107459068298 ; pred:  tensor([0.0668, 0.1508, 0.6656, 0.1168], grad_fn=<SoftmaxBackward0>)
num_high 19 len(mask) 24
mask_without_small tensor([0.8526, 0.8359, 0.8115, 0.5479, 0.8015, 0.6091, 0.7655, 0.5834, 0.7863,
        0.6462, 0.6599, 0.7807, 0.6176, 0.7732, 0.5328, 0.7569, 0.7517, 0.7465,
        0.8071, 0.6536, 0.6562, 0.7652, 0.6371, 0.7851],
       grad_fn=<IndexBackward0>)
pred_loss tensor([4.0272], grad_fn=<MulBackward0>)
size_loss tensor(-9.4612, grad_fn=<MulBackward0>)
size_num_loss 1.9000000000000001
loss: tensor([-1.2412], grad_fn=<AddBackward0>)
2
epoch:  2 ; loss:  -1.241180181503296 ; pred:  tensor([0.0659, 0.1494, 0.6685, 0.1162], grad_fn=<SoftmaxBackward0>)
num_high 18 len(mask) 24
mask_without_small tensor([0.8643, 0.8488, 0.8261, 0.5231, 0.8168, 0.5850, 0.7824, 0.5589, 0.8027,
        0.6232, 0.6381, 0.7974, 0.5936, 0.7903, 0.5080, 0.7747, 0.7695, 0.7642,
        0.8221, 0.6329, 0.6359, 0.7828, 0.6141, 0.8015],
       grad_fn=<IndexBackward0>)
pred_loss tensor([4.0007], grad_fn=<MulBackward0>)
size_loss tensor(-11.2921, grad_fn=<MulBackward0>)
size_num_loss 1.8
loss: tensor([-3.2079], grad_fn=<AddBackward0>)
3
epoch:  3 ; loss:  -3.207857370376587 ; pred:  tensor([0.0653, 0.1484, 0.6703, 0.1160], grad_fn=<SoftmaxBackward0>)
num_high 14 len(mask) 24
mask_without_small tensor([0.8749, 0.8606, 0.8396, 0.8310, 0.5603, 0.7987, 0.5341, 0.8180, 0.5995,
        0.6155, 0.8131, 0.5691, 0.8065, 0.7916, 0.7865, 0.7814, 0.8360, 0.6109,
        0.6141, 0.7994, 0.5903, 0.8169], grad_fn=<IndexBackward0>)
pred_loss tensor([3.9891], grad_fn=<MulBackward0>)
size_loss tensor(-1172.4919, grad_fn=<MulBackward0>)
size_num_loss 1.4000000000000001
loss: tensor([-1166.3829], grad_fn=<AddBackward0>)
4
epoch:  4 ; loss:  -1166.3829345703125 ; pred:  tensor([0.0650, 0.1479, 0.6711, 0.1161], grad_fn=<SoftmaxBackward0>)
num_high 14 len(mask) 24
mask_without_small tensor([0.8809, 0.8672, 0.8470, 0.8388, 0.5466, 0.8074, 0.5201, 0.8262, 0.5861,
        0.6023, 0.8215, 0.5554, 0.8151, 0.8008, 0.7959, 0.7909, 0.8436, 0.5977,
        0.6010, 0.8083, 0.5769, 0.8251], grad_fn=<IndexBackward0>)
pred_loss tensor([3.9977], grad_fn=<MulBackward0>)
size_loss tensor(-1275.1636, grad_fn=<MulBackward0>)
size_num_loss 1.4000000000000001
loss: tensor([-1269.0519], grad_fn=<AddBackward0>)
5
epoch:  5 ; loss:  -1269.0518798828125 ; pred:  tensor([0.0651, 0.1480, 0.6705, 0.1164], grad_fn=<SoftmaxBackward0>)
num_high 14 len(mask) 24
mask_without_small tensor([0.8882, 0.8752, 0.8560, 0.8481, 0.5290, 0.8182, 0.5024, 0.8361, 0.5688,
        0.5853, 0.8317, 0.5378, 0.8255, 0.8119, 0.8072, 0.8024, 0.8528, 0.5807,
        0.5840, 0.8191, 0.5596, 0.8351], grad_fn=<IndexBackward0>)
pred_loss tensor([3.9993], grad_fn=<MulBackward0>)
size_loss tensor(-1405.3311, grad_fn=<MulBackward0>)
size_num_loss 1.4000000000000001
loss: tensor([-1399.2257], grad_fn=<AddBackward0>)
6
epoch:  6 ; loss:  -1399.2257080078125 ; pred:  tensor([0.0650, 0.1479, 0.6704, 0.1167], grad_fn=<SoftmaxBackward0>)
num_high 14 len(mask) 24
mask_without_small tensor([0.8958, 0.8836, 0.8655, 0.8581, 0.5091, 0.8298, 0.8468, 0.5491, 0.5658,
        0.8426, 0.5179, 0.8367, 0.8238, 0.8194, 0.8149, 0.8626, 0.5612, 0.5646,
        0.8307, 0.5399, 0.8458], grad_fn=<IndexBackward0>)
pred_loss tensor([3.9992], grad_fn=<MulBackward0>)
size_loss tensor(-1481.2444, grad_fn=<MulBackward0>)
size_num_loss 1.4000000000000001
loss: tensor([-1475.1533], grad_fn=<AddBackward0>)
7
epoch:  7 ; loss:  -1475.1533203125 ; pred:  tensor([0.0649, 0.1477, 0.6704, 0.1170], grad_fn=<SoftmaxBackward0>)
num_high 14 len(mask) 24
mask_without_small tensor([0.9034, 0.8920, 0.8751, 0.8681, 0.8415, 0.8575, 0.5278, 0.5446, 0.8536,
        0.8481, 0.8360, 0.8318, 0.8275, 0.8724, 0.5399, 0.5433, 0.8424, 0.5185,
        0.8566], grad_fn=<IndexBackward0>)
pred_loss tensor([4.0008], grad_fn=<MulBackward0>)
size_loss tensor(-1473.5056, grad_fn=<MulBackward0>)
size_num_loss 1.4000000000000001
loss: tensor([-1467.4330], grad_fn=<AddBackward0>)
8
epoch:  8 ; loss:  -1467.4329833984375 ; pred:  tensor([0.0649, 0.1475, 0.6703, 0.1173], grad_fn=<SoftmaxBackward0>)
num_high 14 len(mask) 24
mask_without_small tensor([0.9108, 0.9002, 0.8844, 0.8779, 0.8530, 0.8680, 0.5053, 0.5222, 0.8643,
        0.8592, 0.8478, 0.8439, 0.8399, 0.8819, 0.5175, 0.5210, 0.8538, 0.8671],
       grad_fn=<IndexBackward0>)
pred_loss tensor([4.0088], grad_fn=<MulBackward0>)
size_loss tensor(-1515.2943, grad_fn=<MulBackward0>)
size_num_loss 1.4000000000000001
loss: tensor([-1509.2295], grad_fn=<AddBackward0>)
9
epoch:  9 ; loss:  -1509.2294921875 ; pred:  tensor([0.0649, 0.1476, 0.6697, 0.1178], grad_fn=<SoftmaxBackward0>)
num_high 14 len(mask) 24
mask_without_small tensor([0.9178, 0.9080, 0.8933, 0.8873, 0.8640, 0.8780, 0.8746, 0.8698, 0.8592,
        0.8556, 0.8518, 0.8909, 0.8648, 0.8772], grad_fn=<IndexBackward0>)
pred_loss tensor([4.0181], grad_fn=<MulBackward0>)
size_loss tensor(-195.4570, grad_fn=<MulBackward0>)
size_num_loss 1.4000000000000001
loss: tensor([-189.4138], grad_fn=<AddBackward0>)
10
epoch:  10 ; loss:  -189.41383361816406 ; pred:  tensor([0.0649, 0.1477, 0.6691, 0.1183], grad_fn=<SoftmaxBackward0>)
num_high 14 len(mask) 24
mask_without_small tensor([0.9244, 0.9154, 0.9019, 0.8961, 0.8686, 0.8863, 0.8825, 0.8764, 0.8620,
        0.8571, 0.8520, 0.8997, 0.8698, 0.8855], grad_fn=<IndexBackward0>)
pred_loss tensor([4.0767], grad_fn=<MulBackward0>)
size_loss tensor(-216.1618, grad_fn=<MulBackward0>)
size_num_loss 1.4000000000000001
loss: tensor([-210.0676], grad_fn=<AddBackward0>)
11
epoch:  11 ; loss:  -210.0675506591797 ; pred:  tensor([0.0658, 0.1492, 0.6652, 0.1198], grad_fn=<SoftmaxBackward0>)
num_high 14 len(mask) 24
mask_without_small tensor([0.9308, 0.9226, 0.9101, 0.9044, 0.8695, 0.8938, 0.8889, 0.8805, 0.8607,
        0.8542, 0.8480, 0.9080, 0.8712, 0.8927], grad_fn=<IndexBackward0>)
pred_loss tensor([4.1523], grad_fn=<MulBackward0>)
size_loss tensor(-254.3753, grad_fn=<MulBackward0>)
size_num_loss 1.4000000000000001
loss: tensor([-248.2115], grad_fn=<AddBackward0>)
12
epoch:  12 ; loss:  -248.2114715576172 ; pred:  tensor([0.0670, 0.1513, 0.6602, 0.1215], grad_fn=<SoftmaxBackward0>)
num_high 14 len(mask) 24
mask_without_small tensor([0.9368, 0.9294, 0.9178, 0.9123, 0.8678, 0.9006, 0.8944, 0.8830, 0.8564,
        0.8486, 0.8412, 0.9158, 0.8701, 0.8994], grad_fn=<IndexBackward0>)
pred_loss tensor([4.2368], grad_fn=<MulBackward0>)
size_loss tensor(-304.3860, grad_fn=<MulBackward0>)
size_num_loss 1.4000000000000001
loss: tensor([-298.1428], grad_fn=<AddBackward0>)
13
epoch:  13 ; loss:  -298.14276123046875 ; pred:  tensor([0.0684, 0.1536, 0.6546, 0.1233], grad_fn=<SoftmaxBackward0>)
num_high 14 len(mask) 24
mask_without_small tensor([0.9424, 0.9356, 0.9250, 0.9197, 0.8640, 0.9072, 0.8996, 0.8843, 0.8500,
        0.8408, 0.8322, 0.9231, 0.8671, 0.9057], grad_fn=<IndexBackward0>)
pred_loss tensor([4.3274], grad_fn=<MulBackward0>)
size_loss tensor(-363.3177, grad_fn=<MulBackward0>)
size_num_loss 1.4000000000000001
loss: tensor([-356.9882], grad_fn=<AddBackward0>)
14
epoch:  14 ; loss:  -356.9881591796875 ; pred:  tensor([0.0700, 0.1561, 0.6487, 0.1252], grad_fn=<SoftmaxBackward0>)
num_high 14 len(mask) 24
mask_without_small tensor([0.9476, 0.9414, 0.9316, 0.9266, 0.8585, 0.9135, 0.9045, 0.8846, 0.8417,
        0.8311, 0.8214, 0.9299, 0.8623, 0.9118], grad_fn=<IndexBackward0>)
pred_loss tensor([4.4230], grad_fn=<MulBackward0>)
size_loss tensor(-429.6706, grad_fn=<MulBackward0>)
size_num_loss 1.4000000000000001
loss: tensor([-423.2493], grad_fn=<AddBackward0>)
15
epoch:  15 ; loss:  -423.249267578125 ; pred:  tensor([0.0716, 0.1587, 0.6426, 0.1271], grad_fn=<SoftmaxBackward0>)
num_high 14 len(mask) 24
mask_without_small tensor([0.9522, 0.9466, 0.9377, 0.9330, 0.8513, 0.9196, 0.9094, 0.8843, 0.8317,
        0.8196, 0.8089, 0.9361, 0.8559, 0.9177], grad_fn=<IndexBackward0>)
pred_loss tensor([4.5233], grad_fn=<MulBackward0>)
size_loss tensor(-502.6912, grad_fn=<MulBackward0>)
size_num_loss 1.4000000000000001
loss: tensor([-496.1727], grad_fn=<AddBackward0>)
16
epoch:  16 ; loss:  -496.1727294921875 ; pred:  tensor([0.0733, 0.1615, 0.6361, 0.1290], grad_fn=<SoftmaxBackward0>)
num_high 14 len(mask) 24
mask_without_small tensor([0.9564, 0.9513, 0.9433, 0.9389, 0.8425, 0.9255, 0.9143, 0.8833, 0.8199,
        0.8064, 0.7946, 0.9418, 0.8480, 0.9234], grad_fn=<IndexBackward0>)
pred_loss tensor([4.6286], grad_fn=<MulBackward0>)
size_loss tensor(-582.0453, grad_fn=<MulBackward0>)
size_num_loss 1.4000000000000001
loss: tensor([-575.4241], grad_fn=<AddBackward0>)
17
epoch:  17 ; loss:  -575.424072265625 ; pred:  tensor([0.0752, 0.1644, 0.6295, 0.1310], grad_fn=<SoftmaxBackward0>)
num_high 14 len(mask) 24
mask_without_small tensor([0.9601, 0.9555, 0.9483, 0.9443, 0.8321, 0.9311, 0.9192, 0.8820, 0.8065,
        0.7916, 0.7787, 0.9470, 0.8386, 0.9290], grad_fn=<IndexBackward0>)
pred_loss tensor([4.7395], grad_fn=<MulBackward0>)
size_loss tensor(-667.6363, grad_fn=<MulBackward0>)
size_num_loss 1.4000000000000001
loss: tensor([-660.9064], grad_fn=<AddBackward0>)
18
epoch:  18 ; loss:  -660.9063720703125 ; pred:  tensor([0.0771, 0.1674, 0.6225, 0.1330], grad_fn=<SoftmaxBackward0>)
num_high 14 len(mask) 24
mask_without_small tensor([0.9634, 0.9593, 0.9529, 0.9492, 0.8202, 0.9364, 0.9241, 0.8803, 0.7915,
        0.7751, 0.7612, 0.9516, 0.8276, 0.9343], grad_fn=<IndexBackward0>)
pred_loss tensor([4.8566], grad_fn=<MulBackward0>)
size_loss tensor(-759.4982, grad_fn=<MulBackward0>)
size_num_loss 1.4000000000000001
loss: tensor([-752.6530], grad_fn=<AddBackward0>)
19
epoch:  19 ; loss:  -752.6529541015625 ; pred:  tensor([0.0792, 0.1706, 0.6153, 0.1350], grad_fn=<SoftmaxBackward0>)
Finished Training
sel masked ver tensor(indices=tensor([[6860, 6874, 6920, 7731, 7857, 7933, 5757, 5757, 5757,
                        5757, 5757, 5757, 5757, 5757],
                       [5757, 5757, 5757, 5757, 5757, 5757, 2227, 6860, 6920,
                        6976, 7731, 7837, 7933, 5230]]),
       values=tensor([0.9634, 0.9593, 0.9529, 0.9492, 0.8202, 0.9364, 0.9241,
                      0.8803, 0.7915, 0.7751, 0.7612, 0.9516, 0.8276, 0.9343]),
       size=(7934, 7934), nnz=14, layout=torch.sparse_coo,
       grad_fn=<SparseCooTensorWithDimsAndTensorsBackward0>)
Counter({'publication': 6, 'author': 6, 'name': 1, 'type': 1})
dict index: {}
node_idx 5757
 node original label [2]
 node predicted label explain 2
 node prediction probability explain tensor([0.0792, 0.1706, 0.6153, 0.1350], grad_fn=<SoftmaxBackward0>)
 node predicted label full 2 most important relations  {'name': 1, 'type': 1, 'publication': 6, 'author': 6, 'label': 2, 'node_idx': '5757'}
 final masks and lenght tensor(indices=tensor([[ 23430,  23444,  23490,  23546,  24301,  24407,  24427,
                         24475,  24503,  24543,  88607, 196312, 229452, 254307,
                        254307, 254307, 254307, 254307, 254307, 254307, 254307,
                        254307, 254307, 328872],
                       [  5757,   5757,   5757,   5757,   5757,   5757,   5757,
                          5757,   5757,   5757,    908,   2227,   1002,   6860,
                          6874,   6920,   6976,   7731,   7837,   7857,   7905,
                          7933,   7973,   5230]]),
       values=tensor([0.9634, 0.9593, 0.9529, 0.3556, 0.9492, 0.3607, 0.8202,
                      0.3618, 0.9364, 0.3565, 0.3729, 0.9241, 0.3683, 0.8803,
                      0.4561, 0.7915, 0.7751, 0.7612, 0.9516, 0.3686, 0.3720,
                      0.8276, 0.3681, 0.9343]),
       size=(753935, 8285), nnz=24, layout=torch.sparse_coo,
       grad_fn=<SparseCooTensorWithDimsAndTensorsBackward0>) 14
 ---------------------------------------------------------------
r_exp.py:102: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  hor_graph, ver_graph = torch.tensor(sub_hor_graph, dtype=torch.float), torch.tensor(sub_ver_graph, dtype=torch.float)
wandb: WARNING Calling wandb.login() after wandb.init() has no effect.
node label: 2
41
num_high 233 len(mask) 233
mask_without_small tensor([0.7647, 0.7573, 0.7471, 0.6910, 0.7432, 0.7080, 0.7303, 0.7008, 0.7171,
        0.7600, 0.7238, 0.7047, 0.7176, 0.7207, 0.7168, 0.7447, 0.7599, 0.7281,
        0.7219, 0.7390, 0.7170, 0.7502, 0.7454, 0.7606, 0.7537, 0.7540, 0.7420,
        0.7547, 0.7268, 0.7318, 0.7265, 0.7464, 0.7051, 0.7149, 0.7270, 0.7612,
        0.7368, 0.7233, 0.7366, 0.7167, 0.7018, 0.7488, 0.7147, 0.7200, 0.7072,
        0.7679, 0.7080, 0.7221, 0.7141, 0.7189, 0.7325, 0.7405, 0.7221, 0.7522,
        0.7160, 0.7174, 0.7047, 0.7317, 0.7299, 0.7432, 0.7293, 0.7633, 0.7089,
        0.7555, 0.7566, 0.7464, 0.7695, 0.7405, 0.7373, 0.7274, 0.7114, 0.7537,
        0.7279, 0.7405, 0.7321, 0.7388, 0.7414, 0.7192, 0.6890, 0.7172, 0.7313,
        0.7248, 0.7059, 0.7203, 0.7407, 0.7405, 0.7513, 0.7320, 0.7444, 0.7222,
        0.7115, 0.7419, 0.6986, 0.7157, 0.7547, 0.7398, 0.6830, 0.7399, 0.7451,
        0.7316, 0.7426, 0.7415, 0.7500, 0.7228, 0.7277, 0.7445, 0.7384, 0.7343,
        0.7359, 0.7536, 0.7310, 0.7255, 0.7037, 0.7292, 0.7200, 0.7397, 0.7441,
        0.7327, 0.7239, 0.7406, 0.7308, 0.7354, 0.7335, 0.7448, 0.7505, 0.7372,
        0.7440, 0.7385, 0.7648, 0.7491, 0.7041, 0.7100, 0.7286, 0.7598, 0.7428,
        0.7414, 0.7513, 0.7314, 0.6969, 0.7476, 0.7242, 0.7495, 0.7184, 0.7425,
        0.7130, 0.7482, 0.7595, 0.7566, 0.7359, 0.7272, 0.7175, 0.7330, 0.7374,
        0.7483, 0.7225, 0.7593, 0.6836, 0.7234, 0.7087, 0.7456, 0.6951, 0.7352,
        0.7315, 0.7247, 0.7363, 0.7175, 0.7342, 0.7107, 0.7009, 0.7550, 0.7539,
        0.7320, 0.7020, 0.7446, 0.7449, 0.7663, 0.7317, 0.7332, 0.7161, 0.7273,
        0.7137, 0.7011, 0.7099, 0.7214, 0.7215, 0.7029, 0.6946, 0.7334, 0.7493,
        0.7208, 0.7437, 0.7438, 0.7621, 0.7139, 0.7482, 0.7249, 0.7091, 0.7375,
        0.7397, 0.7550, 0.7405, 0.7678, 0.7215, 0.7137, 0.7344, 0.7501, 0.7542,
        0.7394, 0.7160, 0.7121, 0.7219, 0.7201, 0.7339, 0.7390, 0.7283, 0.6868,
        0.7237, 0.7299, 0.7054, 0.7370, 0.7128, 0.7365, 0.7343, 0.7287, 0.7138,
        0.7700, 0.7489, 0.7552, 0.7424, 0.7384, 0.7372, 0.7270, 0.7533],
       grad_fn=<IndexBackward0>)
pred_loss tensor([2.3842e-06], grad_fn=<MulBackward0>)
size_loss tensor(-1.8115, grad_fn=<MulBackward0>)
size_num_loss 23.3
loss: tensor([39.1205], grad_fn=<AddBackward0>)
0
epoch:  0 ; loss:  39.120548248291016 ; pred:  tensor([3.5507e-11, 1.9416e-07, 1.0000e+00, 3.6828e-08],
       grad_fn=<SoftmaxBackward0>)
num_high 233 len(mask) 233
mask_without_small tensor([0.7822, 0.7384, 0.7278, 0.6693, 0.7237, 0.6869, 0.7101, 0.6795, 0.6964,
        0.7413, 0.7034, 0.6835, 0.6969, 0.7002, 0.6961, 0.7252, 0.7412, 0.7079,
        0.7014, 0.7192, 0.6963, 0.7310, 0.7260, 0.7419, 0.7347, 0.7350, 0.7224,
        0.7357, 0.7065, 0.7117, 0.7061, 0.7270, 0.6839, 0.6941, 0.7067, 0.7425,
        0.7170, 0.7028, 0.7167, 0.6960, 0.6804, 0.7295, 0.6939, 0.6994, 0.6861,
        0.7853, 0.6869, 0.7016, 0.6932, 0.6983, 0.7124, 0.7209, 0.7016, 0.7331,
        0.6952, 0.6967, 0.6835, 0.7116, 0.7097, 0.7236, 0.7091, 0.7809, 0.6879,
        0.7366, 0.7377, 0.7270, 0.7868, 0.7208, 0.7175, 0.7072, 0.6905, 0.7347,
        0.7077, 0.7208, 0.7120, 0.7190, 0.7218, 0.6986, 0.6672, 0.6964, 0.7112,
        0.7045, 0.6848, 0.6997, 0.7211, 0.7208, 0.7322, 0.7119, 0.7249, 0.7017,
        0.6906, 0.7223, 0.6771, 0.6949, 0.7357, 0.7201, 0.6609, 0.7202, 0.7257,
        0.7115, 0.7230, 0.7219, 0.7308, 0.7023, 0.7074, 0.7251, 0.7186, 0.7143,
        0.7160, 0.7346, 0.7109, 0.7051, 0.6824, 0.7090, 0.6994, 0.7199, 0.7246,
        0.7127, 0.7035, 0.7209, 0.7107, 0.7155, 0.7135, 0.7253, 0.7313, 0.7174,
        0.7245, 0.7187, 0.7823, 0.7298, 0.6829, 0.6890, 0.7084, 0.7411, 0.7233,
        0.7218, 0.7322, 0.7113, 0.6754, 0.7282, 0.7038, 0.7302, 0.6977, 0.7229,
        0.6921, 0.7289, 0.7408, 0.7378, 0.7161, 0.7069, 0.6968, 0.7129, 0.7175,
        0.7290, 0.7020, 0.7405, 0.6616, 0.7029, 0.6877, 0.7262, 0.6735, 0.7153,
        0.7114, 0.7043, 0.7164, 0.6968, 0.7143, 0.6897, 0.6795, 0.7360, 0.7349,
        0.7119, 0.6806, 0.7251, 0.7255, 0.7838, 0.7116, 0.7132, 0.6954, 0.7070,
        0.6929, 0.6797, 0.6889, 0.7009, 0.7010, 0.6816, 0.6729, 0.7134, 0.7300,
        0.7003, 0.7242, 0.7243, 0.7435, 0.6931, 0.7289, 0.7045, 0.6881, 0.7177,
        0.7200, 0.7360, 0.7209, 0.7851, 0.7009, 0.6929, 0.7145, 0.7309, 0.7352,
        0.7196, 0.6952, 0.6911, 0.7014, 0.6995, 0.7139, 0.7193, 0.7081, 0.6649,
        0.7033, 0.7097, 0.6843, 0.7172, 0.6919, 0.7167, 0.7143, 0.7085, 0.6929,
        0.7873, 0.7296, 0.7362, 0.7229, 0.7186, 0.7174, 0.7067, 0.7343],
       grad_fn=<IndexBackward0>)
pred_loss tensor([4.7684e-06], grad_fn=<MulBackward0>)
size_loss tensor(-2.2164, grad_fn=<MulBackward0>)
size_num_loss 23.3
loss: tensor([38.2950], grad_fn=<AddBackward0>)
1
epoch:  1 ; loss:  38.295005798339844 ; pred:  tensor([1.0086e-10, 3.8829e-07, 1.0000e+00, 7.9725e-08],
       grad_fn=<SoftmaxBackward0>)
num_high 233 len(mask) 233
mask_without_small tensor([0.7953, 0.7187, 0.7075, 0.6469, 0.7032, 0.6651, 0.6891, 0.6574, 0.6749,
        0.7221, 0.6822, 0.6616, 0.6754, 0.6788, 0.6746, 0.7049, 0.7220, 0.6868,
        0.6801, 0.6986, 0.6748, 0.7109, 0.7056, 0.7229, 0.7147, 0.7150, 0.7019,
        0.7158, 0.6854, 0.6908, 0.6850, 0.7067, 0.6620, 0.6725, 0.6856, 0.7240,
        0.6963, 0.6815, 0.6960, 0.6745, 0.6584, 0.7093, 0.6723, 0.6780, 0.6642,
        0.7993, 0.6651, 0.6803, 0.6716, 0.6768, 0.6915, 0.7003, 0.6803, 0.7131,
        0.6737, 0.6753, 0.6616, 0.6907, 0.6887, 0.7032, 0.6881, 0.7935, 0.6661,
        0.7167, 0.7179, 0.7067, 0.8011, 0.7002, 0.6968, 0.6861, 0.6688, 0.7147,
        0.6866, 0.7003, 0.6911, 0.6984, 0.7013, 0.6772, 0.6447, 0.6750, 0.6902,
        0.6833, 0.6629, 0.6783, 0.7005, 0.7003, 0.7121, 0.6910, 0.7045, 0.6804,
        0.6689, 0.7018, 0.6549, 0.6734, 0.7158, 0.6995, 0.6383, 0.6996, 0.7053,
        0.6906, 0.7025, 0.7014, 0.7107, 0.6810, 0.6863, 0.7047, 0.6979, 0.6935,
        0.6952, 0.7146, 0.6900, 0.6840, 0.6605, 0.6880, 0.6780, 0.6994, 0.7042,
        0.6918, 0.6822, 0.7003, 0.6897, 0.6947, 0.6926, 0.7049, 0.7112, 0.6967,
        0.7040, 0.6981, 0.7954, 0.7096, 0.6609, 0.6672, 0.6873, 0.7218, 0.7028,
        0.7013, 0.7121, 0.6904, 0.6532, 0.7080, 0.6825, 0.7100, 0.6763, 0.7024,
        0.6704, 0.7086, 0.7214, 0.7180, 0.6953, 0.6858, 0.6753, 0.6921, 0.6969,
        0.7088, 0.6807, 0.7211, 0.6389, 0.6817, 0.6659, 0.7058, 0.6512, 0.6945,
        0.6905, 0.6831, 0.6956, 0.6754, 0.6934, 0.6679, 0.6574, 0.7161, 0.7149,
        0.6910, 0.6586, 0.7048, 0.7051, 0.7974, 0.6907, 0.6924, 0.6738, 0.6859,
        0.6713, 0.6577, 0.6671, 0.6796, 0.6796, 0.6595, 0.6506, 0.6925, 0.7098,
        0.6789, 0.7037, 0.7039, 0.7263, 0.6715, 0.7087, 0.6833, 0.6663, 0.6970,
        0.6994, 0.7161, 0.7003, 0.7991, 0.6796, 0.6713, 0.6936, 0.7107, 0.7152,
        0.6990, 0.6737, 0.6694, 0.6801, 0.6782, 0.6930, 0.6986, 0.6871, 0.6424,
        0.6821, 0.6887, 0.6623, 0.6965, 0.6702, 0.6959, 0.6935, 0.6874, 0.6713,
        0.8017, 0.7094, 0.7163, 0.7024, 0.6979, 0.6967, 0.6856, 0.7143],
       grad_fn=<IndexBackward0>)
pred_loss tensor([9.5367e-06], grad_fn=<MulBackward0>)
size_loss tensor(-2.6959, grad_fn=<MulBackward0>)
size_num_loss 23.3
loss: tensor([37.3723], grad_fn=<AddBackward0>)
2
epoch:  2 ; loss:  37.37232971191406 ; pred:  tensor([2.9668e-10, 7.9394e-07, 1.0000e+00, 1.7674e-07],
       grad_fn=<SoftmaxBackward0>)
num_high 229 len(mask) 233
mask_without_small tensor([0.8092, 0.6981, 0.6864, 0.6239, 0.6820, 0.6426, 0.6674, 0.6347, 0.6528,
        0.7020, 0.6602, 0.6390, 0.6533, 0.6568, 0.6524, 0.6836, 0.7018, 0.6650,
        0.6581, 0.6772, 0.6526, 0.6899, 0.6844, 0.7029, 0.6938, 0.6942, 0.6806,
        0.6949, 0.6636, 0.6691, 0.6631, 0.6856, 0.6394, 0.6503, 0.6637, 0.7043,
        0.6748, 0.6596, 0.6745, 0.6523, 0.6357, 0.6882, 0.6501, 0.6559, 0.6418,
        0.8135, 0.6426, 0.6583, 0.6494, 0.6547, 0.6699, 0.6789, 0.6583, 0.6921,
        0.6515, 0.6531, 0.6390, 0.6690, 0.6670, 0.6819, 0.6663, 0.8072, 0.6437,
        0.6959, 0.6972, 0.6855, 0.8154, 0.6789, 0.6753, 0.6643, 0.6464, 0.6938,
        0.6648, 0.6789, 0.6694, 0.6769, 0.6799, 0.6551, 0.6218, 0.6528, 0.6685,
        0.6613, 0.6404, 0.6562, 0.6791, 0.6789, 0.6911, 0.6693, 0.6833, 0.6584,
        0.6465, 0.6805, 0.6322, 0.6512, 0.6949, 0.6781, 0.6151, 0.6782, 0.6841,
        0.6689, 0.6812, 0.6801, 0.6896, 0.6590, 0.6645, 0.6834, 0.6765, 0.6719,
        0.6737, 0.6937, 0.6683, 0.6621, 0.6379, 0.6662, 0.6560, 0.6779, 0.6829,
        0.6702, 0.6603, 0.6790, 0.6680, 0.6732, 0.6710, 0.6837, 0.6902, 0.6752,
        0.6828, 0.6766, 0.8093, 0.6886, 0.6383, 0.6448, 0.6655, 0.7016, 0.6815,
        0.6799, 0.6911, 0.6687, 0.6304, 0.6869, 0.6606, 0.6890, 0.6541, 0.6811,
        0.6481, 0.6875, 0.7012, 0.6973, 0.6738, 0.6640, 0.6532, 0.6704, 0.6754,
        0.6877, 0.6587, 0.7008, 0.6158, 0.6597, 0.6434, 0.6846, 0.6284, 0.6729,
        0.6688, 0.6612, 0.6741, 0.6532, 0.6719, 0.6456, 0.6348, 0.6953, 0.6940,
        0.6694, 0.6360, 0.6835, 0.6839, 0.8114, 0.6690, 0.6708, 0.6516, 0.6640,
        0.6490, 0.6350, 0.6447, 0.6575, 0.6576, 0.6369, 0.6278, 0.6709, 0.6888,
        0.6569, 0.6825, 0.6826, 0.7072, 0.6492, 0.6876, 0.6614, 0.6439, 0.6756,
        0.6780, 0.6953, 0.6789, 0.8132, 0.6576, 0.6490, 0.6721, 0.6897, 0.6944,
        0.6776, 0.6515, 0.6471, 0.6581, 0.6561, 0.6714, 0.6772, 0.6653, 0.6193,
        0.6601, 0.6670, 0.6398, 0.6750, 0.6479, 0.6744, 0.6719, 0.6657, 0.6491,
        0.8160, 0.6883, 0.6955, 0.6811, 0.6765, 0.6752, 0.6638, 0.6934],
       grad_fn=<IndexBackward0>)
pred_loss tensor([2.0266e-05], grad_fn=<MulBackward0>)
size_loss tensor(-3.2397, grad_fn=<MulBackward0>)
size_num_loss 22.900000000000002
loss: tensor([35.9671], grad_fn=<AddBackward0>)
3
epoch:  3 ; loss:  35.96709060668945 ; pred:  tensor([8.8696e-10, 1.6426e-06, 1.0000e+00, 3.9643e-07],
       grad_fn=<SoftmaxBackward0>)
num_high 203 len(mask) 233
mask_without_small tensor([0.8231, 0.6765, 0.6644, 0.6007, 0.6599, 0.6197, 0.6451, 0.6117, 0.6301,
        0.6808, 0.6377, 0.6160, 0.6306, 0.6342, 0.6297, 0.6616, 0.6806, 0.6426,
        0.6355, 0.6550, 0.6300, 0.6680, 0.6624, 0.6819, 0.6720, 0.6724, 0.6585,
        0.6732, 0.6411, 0.6468, 0.6407, 0.6636, 0.6165, 0.6275, 0.6413, 0.6835,
        0.6526, 0.6370, 0.6523, 0.6296, 0.6127, 0.6663, 0.6274, 0.6333, 0.6189,
        0.8274, 0.6197, 0.6357, 0.6266, 0.6321, 0.6476, 0.6568, 0.6357, 0.6703,
        0.6288, 0.6304, 0.6161, 0.6467, 0.6446, 0.6599, 0.6439, 0.8210, 0.6208,
        0.6742, 0.6756, 0.6635, 0.8293, 0.6568, 0.6531, 0.6418, 0.6236, 0.6720,
        0.6424, 0.6568, 0.6471, 0.6548, 0.6578, 0.6324, 0.5985, 0.6301, 0.6462,
        0.6389, 0.6174, 0.6336, 0.6570, 0.6568, 0.6692, 0.6470, 0.6612, 0.6358,
        0.6237, 0.6584, 0.6091, 0.6285, 0.6732, 0.6559, 0.5917, 0.6560, 0.6621,
        0.6466, 0.6592, 0.6580, 0.6678, 0.6365, 0.6421, 0.6614, 0.6543, 0.6497,
        0.6515, 0.6719, 0.6459, 0.6396, 0.6149, 0.6438, 0.6333, 0.6558, 0.6609,
        0.6479, 0.6378, 0.6569, 0.6457, 0.6510, 0.6487, 0.6617, 0.6683, 0.6530,
        0.6608, 0.6545, 0.8232, 0.6666, 0.6153, 0.6220, 0.6431, 0.6804, 0.6594,
        0.6578, 0.6693, 0.6463, 0.6073, 0.6649, 0.6381, 0.6671, 0.6315, 0.6591,
        0.6254, 0.6656, 0.6799, 0.6757, 0.6515, 0.6416, 0.6305, 0.6481, 0.6532,
        0.6658, 0.6362, 0.6795, 0.5924, 0.6372, 0.6206, 0.6626, 0.6052, 0.6507,
        0.6465, 0.6387, 0.6519, 0.6305, 0.6496, 0.6227, 0.6117, 0.6736, 0.6722,
        0.6470, 0.6129, 0.6615, 0.6619, 0.8253, 0.6467, 0.6485, 0.6289, 0.6416,
        0.6262, 0.6120, 0.6218, 0.6350, 0.6350, 0.6139, 0.6046, 0.6486, 0.6669,
        0.6343, 0.6604, 0.6606, 0.6869, 0.6265, 0.6657, 0.6389, 0.6210, 0.6534,
        0.6558, 0.6736, 0.6568, 0.8272, 0.6350, 0.6262, 0.6498, 0.6678, 0.6726,
        0.6555, 0.6287, 0.6243, 0.6356, 0.6335, 0.6492, 0.6551, 0.6429, 0.5960,
        0.6376, 0.6446, 0.6168, 0.6528, 0.6251, 0.6522, 0.6497, 0.6433, 0.6263,
        0.8299, 0.6664, 0.6738, 0.6590, 0.6543, 0.6530, 0.6413, 0.6716],
       grad_fn=<IndexBackward0>)
pred_loss tensor([4.2915e-05], grad_fn=<MulBackward0>)
size_loss tensor(-3.8179, grad_fn=<MulBackward0>)
size_num_loss 20.3
loss: tensor([32.3110], grad_fn=<AddBackward0>)
4
epoch:  4 ; loss:  32.311012268066406 ; pred:  tensor([2.6858e-09, 3.4257e-06, 1.0000e+00, 8.9377e-07],
       grad_fn=<SoftmaxBackward0>)
num_high 130 len(mask) 233
mask_without_small tensor([0.8365, 0.6541, 0.6418, 0.5773, 0.6372, 0.5965, 0.6222, 0.5884, 0.6070,
        0.6587, 0.6148, 0.5928, 0.6076, 0.6112, 0.6067, 0.6390, 0.6585, 0.6197,
        0.6125, 0.6323, 0.6069, 0.6454, 0.6397, 0.6600, 0.6495, 0.6498, 0.6358,
        0.6506, 0.6182, 0.6240, 0.6178, 0.6409, 0.5932, 0.6044, 0.6184, 0.6617,
        0.6298, 0.6141, 0.6295, 0.6065, 0.5894, 0.6437, 0.6043, 0.6103, 0.5957,
        0.8407, 0.5965, 0.6127, 0.6035, 0.6091, 0.6248, 0.6341, 0.6127, 0.6477,
        0.6057, 0.6074, 0.5928, 0.6239, 0.6218, 0.6372, 0.6211, 0.8345, 0.5976,
        0.6517, 0.6531, 0.6409, 0.8426, 0.6340, 0.6304, 0.6189, 0.6005, 0.6494,
        0.6195, 0.6341, 0.6243, 0.6320, 0.6351, 0.6094, 0.5750, 0.6071, 0.6234,
        0.6159, 0.5942, 0.6106, 0.6343, 0.6341, 0.6466, 0.6242, 0.6386, 0.6129,
        0.6006, 0.6357, 0.5858, 0.6054, 0.6506, 0.6332, 0.5682, 0.6333, 0.6394,
        0.6237, 0.6365, 0.6353, 0.6451, 0.6135, 0.6192, 0.6388, 0.6316, 0.6269,
        0.6287, 0.6493, 0.6231, 0.6167, 0.5916, 0.6210, 0.6103, 0.6331, 0.6382,
        0.6250, 0.6148, 0.6341, 0.6229, 0.6282, 0.6259, 0.6390, 0.6457, 0.6302,
        0.6381, 0.6317, 0.8366, 0.6440, 0.5921, 0.5988, 0.6202, 0.6582, 0.6367,
        0.6351, 0.6466, 0.6235, 0.5839, 0.6423, 0.6151, 0.6444, 0.6084, 0.6364,
        0.6022, 0.6429, 0.6577, 0.6532, 0.6288, 0.6187, 0.6074, 0.6253, 0.6304,
        0.6431, 0.6132, 0.6572, 0.5689, 0.6142, 0.5974, 0.6400, 0.5818, 0.6279,
        0.6236, 0.6158, 0.6291, 0.6075, 0.6268, 0.5996, 0.5884, 0.6510, 0.6497,
        0.6242, 0.5897, 0.6388, 0.6392, 0.8387, 0.6239, 0.6257, 0.6059, 0.6187,
        0.6031, 0.5887, 0.5987, 0.6120, 0.6121, 0.5907, 0.5812, 0.6258, 0.6442,
        0.6113, 0.6378, 0.6379, 0.6655, 0.6034, 0.6430, 0.6160, 0.5978, 0.6306,
        0.6331, 0.6510, 0.6341, 0.8405, 0.6120, 0.6031, 0.6270, 0.6452, 0.6500,
        0.6327, 0.6057, 0.6012, 0.6126, 0.6105, 0.6264, 0.6323, 0.6200, 0.5725,
        0.6147, 0.6218, 0.5936, 0.6300, 0.6020, 0.6295, 0.6269, 0.6204, 0.6032,
        0.8432, 0.6438, 0.6513, 0.6363, 0.6316, 0.6303, 0.6184, 0.6490],
       grad_fn=<IndexBackward0>)
pred_loss tensor([9.1792e-05], grad_fn=<MulBackward0>)
size_loss tensor(-4.4115, grad_fn=<MulBackward0>)
size_num_loss 13.0
loss: tensor([23.9255], grad_fn=<AddBackward0>)
5
epoch:  5 ; loss:  23.925491333007812 ; pred:  tensor([8.1552e-09, 7.1483e-06, 9.9999e-01, 2.0156e-06],
       grad_fn=<SoftmaxBackward0>)
num_high 36 len(mask) 233
mask_without_small tensor([0.8492, 0.6309, 0.6186, 0.5538, 0.6141, 0.5732, 0.5990, 0.5650, 0.5838,
        0.6358, 0.5916, 0.5695, 0.5843, 0.5880, 0.5834, 0.6158, 0.6355, 0.5966,
        0.5893, 0.6092, 0.5837, 0.6221, 0.6165, 0.6371, 0.6262, 0.6266, 0.6127,
        0.6274, 0.5950, 0.6008, 0.5946, 0.6177, 0.5699, 0.5812, 0.5952, 0.6389,
        0.6067, 0.5909, 0.6064, 0.5833, 0.5661, 0.6205, 0.5810, 0.5871, 0.5723,
        0.8533, 0.5732, 0.5895, 0.5803, 0.5858, 0.6016, 0.6109, 0.5895, 0.6244,
        0.5824, 0.5841, 0.5695, 0.6007, 0.5986, 0.6140, 0.5979, 0.8473, 0.5743,
        0.6284, 0.6298, 0.6177, 0.8550, 0.6109, 0.6072, 0.5958, 0.5772, 0.6262,
        0.5963, 0.6109, 0.6012, 0.6089, 0.6120, 0.5862, 0.5516, 0.5838, 0.6002,
        0.5927, 0.5709, 0.5874, 0.6112, 0.6109, 0.6234, 0.6010, 0.6154, 0.5896,
        0.5773, 0.6125, 0.5624, 0.5821, 0.6274, 0.6101, 0.5448, 0.6102, 0.6162,
        0.6006, 0.6133, 0.6121, 0.6219, 0.5903, 0.5960, 0.6156, 0.6084, 0.6037,
        0.6055, 0.6261, 0.5999, 0.5935, 0.5683, 0.5978, 0.5871, 0.6099, 0.6150,
        0.6019, 0.5916, 0.6110, 0.5997, 0.6050, 0.6028, 0.6158, 0.6225, 0.6071,
        0.6149, 0.6086, 0.8493, 0.6208, 0.5688, 0.5755, 0.5971, 0.6352, 0.6136,
        0.6120, 0.6234, 0.6003, 0.5605, 0.6191, 0.5919, 0.6212, 0.5852, 0.6132,
        0.5790, 0.6197, 0.6347, 0.6300, 0.6056, 0.5955, 0.5842, 0.6022, 0.6073,
        0.6199, 0.5900, 0.6342, 0.5454, 0.5910, 0.5741, 0.6168, 0.5584, 0.6048,
        0.6005, 0.5926, 0.6060, 0.5842, 0.6036, 0.5763, 0.5651, 0.6278, 0.6264,
        0.6011, 0.5663, 0.6157, 0.6160, 0.8514, 0.6007, 0.6025, 0.5826, 0.5955,
        0.5799, 0.5653, 0.5754, 0.5888, 0.5888, 0.5673, 0.5578, 0.6027, 0.6210,
        0.5880, 0.6146, 0.6147, 0.6431, 0.5801, 0.6198, 0.5928, 0.5745, 0.6075,
        0.6100, 0.6278, 0.6109, 0.8531, 0.5888, 0.5799, 0.6039, 0.6219, 0.6268,
        0.6096, 0.5824, 0.5779, 0.5894, 0.5873, 0.6032, 0.6092, 0.5968, 0.5491,
        0.5915, 0.5986, 0.5703, 0.6069, 0.5787, 0.6063, 0.6037, 0.5972, 0.5799,
        0.8556, 0.6206, 0.6280, 0.6131, 0.6084, 0.6071, 0.5952, 0.6257],
       grad_fn=<IndexBackward0>)
pred_loss tensor([0.0002], grad_fn=<MulBackward0>)
size_loss tensor(-5.0078, grad_fn=<MulBackward0>)
size_num_loss 3.6
loss: tensor([13.4266], grad_fn=<AddBackward0>)
6
epoch:  6 ; loss:  13.42659854888916 ; pred:  tensor([2.4644e-08, 1.4857e-05, 9.9998e-01, 4.5138e-06],
       grad_fn=<SoftmaxBackward0>)
num_high 8 len(mask) 233
mask_without_small tensor([0.8611, 0.6070, 0.5950, 0.5306, 0.5906, 0.5500, 0.5757, 0.5418, 0.5605,
        0.6121, 0.5683, 0.5462, 0.5610, 0.5647, 0.5602, 0.5923, 0.6118, 0.5733,
        0.5660, 0.5858, 0.5604, 0.5985, 0.5930, 0.6135, 0.6024, 0.6028, 0.5892,
        0.6036, 0.5717, 0.5775, 0.5713, 0.5942, 0.5467, 0.5579, 0.5719, 0.6155,
        0.5833, 0.5676, 0.5830, 0.5600, 0.5428, 0.5969, 0.5577, 0.5638, 0.5491,
        0.8649, 0.5500, 0.5662, 0.5570, 0.5626, 0.5783, 0.5875, 0.5662, 0.6007,
        0.5592, 0.5609, 0.5462, 0.5774, 0.5753, 0.5905, 0.5746, 0.8592, 0.5511,
        0.6046, 0.6060, 0.5941, 0.8665, 0.5875, 0.5839, 0.5725, 0.5539, 0.6024,
        0.5730, 0.5875, 0.5778, 0.5855, 0.5885, 0.5629, 0.5284, 0.5605, 0.5769,
        0.5694, 0.5476, 0.5641, 0.5877, 0.5875, 0.5997, 0.5777, 0.5919, 0.5664,
        0.5540, 0.5891, 0.5392, 0.5589, 0.6036, 0.5867, 0.5216, 0.5867, 0.5927,
        0.5772, 0.5898, 0.5887, 0.5982, 0.5670, 0.5727, 0.5921, 0.5850, 0.5804,
        0.5822, 0.6023, 0.5766, 0.5702, 0.5451, 0.5745, 0.5638, 0.5865, 0.5915,
        0.5785, 0.5684, 0.5876, 0.5764, 0.5817, 0.5794, 0.5923, 0.5988, 0.5837,
        0.5914, 0.5852, 0.8612, 0.5972, 0.5455, 0.5523, 0.5738, 0.6116, 0.5901,
        0.5885, 0.5997, 0.5770, 0.5373, 0.5955, 0.5687, 0.5976, 0.5619, 0.5898,
        0.5557, 0.5961, 0.6110, 0.6061, 0.5823, 0.5722, 0.5609, 0.5788, 0.5839,
        0.5963, 0.5667, 0.6105, 0.5223, 0.5677, 0.5508, 0.5933, 0.5352, 0.5814,
        0.5772, 0.5693, 0.5826, 0.5610, 0.5803, 0.5531, 0.5418, 0.6040, 0.6026,
        0.5777, 0.5431, 0.5922, 0.5925, 0.8631, 0.5774, 0.5792, 0.5594, 0.5722,
        0.5566, 0.5421, 0.5521, 0.5655, 0.5656, 0.5441, 0.5346, 0.5793, 0.5974,
        0.5648, 0.5911, 0.5912, 0.6200, 0.5568, 0.5962, 0.5695, 0.5513, 0.5841,
        0.5866, 0.6040, 0.5875, 0.8647, 0.5655, 0.5566, 0.5805, 0.5983, 0.6030,
        0.5862, 0.5592, 0.5546, 0.5661, 0.5640, 0.5799, 0.5858, 0.5735, 0.5259,
        0.5682, 0.5753, 0.5470, 0.5835, 0.5555, 0.5829, 0.5804, 0.5739, 0.5567,
        0.8671, 0.5970, 0.6042, 0.5897, 0.5851, 0.5838, 0.5719, 0.6020],
       grad_fn=<IndexBackward0>)
pred_loss tensor([0.0004], grad_fn=<MulBackward0>)
size_loss tensor(-5.5972, grad_fn=<MulBackward0>)
size_num_loss 0.8
loss: tensor([9.5278], grad_fn=<AddBackward0>)
7
epoch:  7 ; loss:  9.52778434753418 ; pred:  tensor([7.3165e-08, 3.0554e-05, 9.9996e-01, 9.9819e-06],
       grad_fn=<SoftmaxBackward0>)
num_high 8 len(mask) 233
mask_without_small tensor([0.8719, 0.5828, 0.5713, 0.5078, 0.5670, 0.5270, 0.5525, 0.5189, 0.5374,
        0.5880, 0.5451, 0.5233, 0.5380, 0.5416, 0.5371, 0.5686, 0.5877, 0.5500,
        0.5429, 0.5623, 0.5373, 0.5746, 0.5694, 0.5895, 0.5784, 0.5787, 0.5657,
        0.5795, 0.5485, 0.5542, 0.5481, 0.5705, 0.5237, 0.5349, 0.5487, 0.5915,
        0.5599, 0.5444, 0.5596, 0.5369, 0.5199, 0.5730, 0.5347, 0.5407, 0.5261,
        0.8754, 0.5270, 0.5431, 0.5339, 0.5394, 0.5550, 0.5640, 0.5431, 0.5767,
        0.5361, 0.5378, 0.5233, 0.5541, 0.5520, 0.5670, 0.5513, 0.8702, 0.5281,
        0.5805, 0.5818, 0.5704, 0.8769, 0.5640, 0.5605, 0.5492, 0.5309, 0.5783,
        0.5498, 0.5640, 0.5545, 0.5620, 0.5650, 0.5398, 0.5056, 0.5375, 0.5536,
        0.5463, 0.5246, 0.5410, 0.5642, 0.5640, 0.5758, 0.5544, 0.5683, 0.5432,
        0.5310, 0.5656, 0.5163, 0.5358, 0.5795, 0.5632, 0.5633, 0.5691, 0.5540,
        0.5663, 0.5652, 0.5744, 0.5439, 0.5495, 0.5684, 0.5616, 0.5570, 0.5588,
        0.5782, 0.5533, 0.5470, 0.5221, 0.5512, 0.5407, 0.5631, 0.5679, 0.5552,
        0.5452, 0.5641, 0.5531, 0.5583, 0.5561, 0.5687, 0.5749, 0.5603, 0.5678,
        0.5618, 0.8720, 0.5733, 0.5226, 0.5292, 0.5505, 0.5874, 0.5666, 0.5650,
        0.5758, 0.5537, 0.5144, 0.5717, 0.5455, 0.5737, 0.5388, 0.5662, 0.5327,
        0.5723, 0.5868, 0.5820, 0.5589, 0.5490, 0.5378, 0.5555, 0.5605, 0.5725,
        0.5436, 0.5863, 0.5446, 0.5278, 0.5696, 0.5123, 0.5581, 0.5539, 0.5461,
        0.5592, 0.5379, 0.5570, 0.5300, 0.5189, 0.5798, 0.5786, 0.5544, 0.5201,
        0.5685, 0.5689, 0.8737, 0.5541, 0.5559, 0.5363, 0.5490, 0.5335, 0.5192,
        0.5291, 0.5424, 0.5424, 0.5211, 0.5118, 0.5560, 0.5736, 0.5416, 0.5675,
        0.5676, 0.5965, 0.5338, 0.5724, 0.5463, 0.5283, 0.5607, 0.5631, 0.5799,
        0.5640, 0.8752, 0.5424, 0.5335, 0.5572, 0.5744, 0.5789, 0.5627, 0.5361,
        0.5316, 0.5429, 0.5409, 0.5565, 0.5623, 0.5503, 0.5031, 0.5450, 0.5520,
        0.5241, 0.5601, 0.5324, 0.5595, 0.5570, 0.5507, 0.5336, 0.8774, 0.5731,
        0.5801, 0.5661, 0.5616, 0.5604, 0.5487, 0.5779],
       grad_fn=<IndexBackward0>)
pred_loss tensor([0.0008], grad_fn=<MulBackward0>)
size_loss tensor(-616.9019, grad_fn=<MulBackward0>)
size_num_loss 0.8
loss: tensor([-614.1212], grad_fn=<AddBackward0>)
8
epoch:  8 ; loss:  -614.1212158203125 ; pred:  tensor([2.1063e-07, 6.1648e-05, 9.9992e-01, 2.1638e-05],
       grad_fn=<SoftmaxBackward0>)
num_high 8 len(mask) 233
mask_without_small tensor([0.8773, 0.5941, 0.5802, 0.5696, 0.5133, 0.5376, 0.5053, 0.5236, 0.5994,
        0.5309, 0.5097, 0.5241, 0.5275, 0.5232, 0.5752, 0.5992, 0.5355, 0.5288,
        0.5413, 0.5234, 0.5848, 0.5768, 0.6010, 0.5892, 0.5896, 0.5597, 0.5904,
        0.5341, 0.5390, 0.5337, 0.5790, 0.5101, 0.5210, 0.5342, 0.6031, 0.5423,
        0.5303, 0.5423, 0.5231, 0.5063, 0.5828, 0.5209, 0.5267, 0.5124, 0.8807,
        0.5133, 0.5290, 0.5201, 0.5255, 0.5396, 0.5400, 0.5290, 0.5873, 0.5223,
        0.5239, 0.5097, 0.5389, 0.5372, 0.5693, 0.5366, 0.8757, 0.5144, 0.5915,
        0.5930, 0.5788, 0.8822, 0.5398, 0.5423, 0.5347, 0.5172, 0.5892, 0.5352,
        0.5398, 0.5393, 0.5416, 0.5502, 0.5258, 0.5236, 0.5385, 0.5320, 0.5110,
        0.5270, 0.5410, 0.5399, 0.5862, 0.5392, 0.5742, 0.5291, 0.5173, 0.5582,
        0.5027, 0.5220, 0.5904, 0.5399, 0.5398, 0.5762, 0.5388, 0.5652, 0.5527,
        0.5845, 0.5298, 0.5350, 0.5747, 0.5419, 0.5411, 0.5420, 0.5891, 0.5383,
        0.5327, 0.5085, 0.5365, 0.5267, 0.5401, 0.5732, 0.5398, 0.5310, 0.5401,
        0.5381, 0.5418, 0.5404, 0.5752, 0.5852, 0.5423, 0.5728, 0.5418, 0.8774,
        0.5832, 0.5089, 0.5155, 0.5359, 0.5989, 0.5671, 0.5505, 0.5862, 0.5387,
        0.5009, 0.5809, 0.5313, 0.5837, 0.5249, 0.5646, 0.5189, 0.5818, 0.5982,
        0.5931, 0.5420, 0.5345, 0.5240, 0.5400, 0.5423, 0.5821, 0.5295, 0.5977,
        0.5304, 0.5141, 0.5773, 0.5417, 0.5388, 0.5319, 0.5422, 0.5240, 0.5410,
        0.5163, 0.5053, 0.5909, 0.5894, 0.5392, 0.5065, 0.5749, 0.5758, 0.8791,
        0.5389, 0.5403, 0.5224, 0.5346, 0.5198, 0.5056, 0.5154, 0.5283, 0.5284,
        0.5075, 0.5404, 0.5835, 0.5276, 0.5718, 0.5722, 0.6081, 0.5200, 0.5820,
        0.5320, 0.5146, 0.5423, 0.5401, 0.5909, 0.5400, 0.8806, 0.5283, 0.5198,
        0.5412, 0.5846, 0.5898, 0.5407, 0.5222, 0.5179, 0.5289, 0.5269, 0.5408,
        0.5412, 0.5357, 0.5308, 0.5372, 0.5104, 0.5423, 0.5187, 0.5422, 0.5411,
        0.5360, 0.5198, 0.8827, 0.5829, 0.5911, 0.5641, 0.5419, 0.5423, 0.5343,
        0.5887], grad_fn=<IndexBackward0>)
pred_loss tensor([0.0011], grad_fn=<MulBackward0>)
size_loss tensor(-671.6343, grad_fn=<MulBackward0>)
size_num_loss 0.8
loss: tensor([-668.8966], grad_fn=<AddBackward0>)
9
epoch:  9 ; loss:  -668.8966064453125 ; pred:  tensor([2.9365e-07, 7.6955e-05, 9.9989e-01, 2.7980e-05],
       grad_fn=<SoftmaxBackward0>)
num_high 9 len(mask) 233
mask_without_small tensor([0.8842, 0.6090, 0.5938, 0.5817, 0.5202, 0.5063, 0.6146, 0.5136, 0.5068,
        0.5102, 0.5059, 0.5880, 0.6143, 0.5181, 0.5115, 0.5263, 0.5062, 0.5990,
        0.5900, 0.6161, 0.6039, 0.6043, 0.5654, 0.6052, 0.5167, 0.5217, 0.5163,
        0.5924, 0.5038, 0.5169, 0.6182, 0.5259, 0.5130, 0.5258, 0.5058, 0.5968,
        0.5036, 0.5094, 0.8874, 0.5117, 0.5029, 0.5082, 0.5223, 0.5263, 0.5117,
        0.6018, 0.5050, 0.5066, 0.5216, 0.5199, 0.5813, 0.5192, 0.8827, 0.6063,
        0.6079, 0.5923, 0.8888, 0.5261, 0.5262, 0.5174, 0.6038, 0.5179, 0.5261,
        0.5220, 0.5264, 0.5360, 0.5086, 0.5063, 0.5212, 0.5147, 0.5097, 0.5273,
        0.5262, 0.6006, 0.5219, 0.5869, 0.5118, 0.5001, 0.5558, 0.5047, 0.6052,
        0.5257, 0.5256, 0.5892, 0.5215, 0.5763, 0.5381, 0.5987, 0.5124, 0.5176,
        0.5875, 0.5265, 0.5240, 0.5252, 0.6037, 0.5210, 0.5153, 0.5192, 0.5094,
        0.5258, 0.5858, 0.5226, 0.5137, 0.5264, 0.5208, 0.5249, 0.5232, 0.5881,
        0.5994, 0.5261, 0.5853, 0.5265, 0.8843, 0.5972, 0.5185, 0.6140, 0.5787,
        0.5363, 0.6006, 0.5213, 0.5947, 0.5139, 0.5978, 0.5076, 0.5755, 0.5017,
        0.5957, 0.6133, 0.6080, 0.5253, 0.5171, 0.5067, 0.5228, 0.5262, 0.5960,
        0.5121, 0.6128, 0.5131, 0.5905, 0.5247, 0.5214, 0.5145, 0.5255, 0.5067,
        0.5239, 0.6056, 0.6041, 0.5219, 0.5877, 0.5887, 0.8859, 0.5216, 0.5230,
        0.5052, 0.5172, 0.5025, 0.5110, 0.5111, 0.5232, 0.5975, 0.5103, 0.5841,
        0.5846, 0.6233, 0.5028, 0.5958, 0.5147, 0.5263, 0.5258, 0.6056, 0.5263,
        0.8873, 0.5110, 0.5025, 0.5241, 0.5988, 0.6045, 0.5261, 0.5050, 0.5007,
        0.5115, 0.5096, 0.5236, 0.5263, 0.5183, 0.5135, 0.5199, 0.5260, 0.5015,
        0.5257, 0.5240, 0.5187, 0.5026, 0.8893, 0.5969, 0.6059, 0.5749, 0.5265,
        0.5261, 0.5169, 0.6033], grad_fn=<IndexBackward0>)
pred_loss tensor([0.0013], grad_fn=<MulBackward0>)
size_loss tensor(-771.8560, grad_fn=<MulBackward0>)
size_num_loss 0.9
loss: tensor([-769.1589], grad_fn=<AddBackward0>)
10
epoch:  10 ; loss:  -769.1588745117188 ; pred:  tensor([3.7471e-07, 9.0770e-05, 9.9987e-01, 3.4086e-05],
       grad_fn=<SoftmaxBackward0>)
num_high 23 len(mask) 233
mask_without_small tensor([0.8917, 0.6262, 0.6102, 0.5969, 0.5016, 0.6319, 0.6039, 0.6316, 0.5093,
        0.6157, 0.6060, 0.6334, 0.6208, 0.6213, 0.5782, 0.6222, 0.5033, 0.6086,
        0.6356, 0.5084, 0.5082, 0.6133, 0.8948, 0.5040, 0.5097, 0.6187, 0.5032,
        0.5012, 0.5966, 0.5005, 0.8903, 0.6234, 0.6250, 0.6084, 0.8961, 0.5095,
        0.5088, 0.6208, 0.5095, 0.5036, 0.5094, 0.5204, 0.5027, 0.5108, 0.5096,
        0.6174, 0.5035, 0.6027, 0.5464, 0.6222, 0.5089, 0.5088, 0.6051, 0.5030,
        0.5910, 0.5230, 0.6154, 0.6033, 0.5093, 0.5059, 0.5075, 0.6207, 0.5024,
        0.5004, 0.5090, 0.6014, 0.5043, 0.5099, 0.5022, 0.5071, 0.5051, 0.6040,
        0.6161, 0.5087, 0.6010, 0.5093, 0.8918, 0.6138, 0.6313, 0.5936, 0.5207,
        0.6174, 0.5028, 0.6110, 0.6144, 0.5901, 0.6121, 0.6306, 0.6252, 0.5076,
        0.5045, 0.5088, 0.6124, 0.6300, 0.6065, 0.5069, 0.5030, 0.5079, 0.5059,
        0.6227, 0.6211, 0.5035, 0.6035, 0.6046, 0.8933, 0.5032, 0.5048, 0.5050,
        0.6141, 0.5997, 0.6001, 0.6407, 0.6123, 0.5089, 0.5089, 0.6227, 0.5097,
        0.8946, 0.5061, 0.6154, 0.6215, 0.5092, 0.5055, 0.5093, 0.5012, 0.5086,
        0.5081, 0.5059, 0.8965, 0.6135, 0.6230, 0.5893, 0.5093, 0.5087, 0.6203],
       grad_fn=<IndexBackward0>)
pred_loss tensor([0.0015], grad_fn=<MulBackward0>)
size_loss tensor(-948.5490, grad_fn=<MulBackward0>)
size_num_loss 2.3000000000000003
loss: tensor([-944.7910], grad_fn=<AddBackward0>)
11
epoch:  11 ; loss:  -944.791015625 ; pred:  tensor([4.7330e-07, 1.0620e-04, 9.9985e-01, 4.1285e-05],
       grad_fn=<SoftmaxBackward0>)
num_high 55 len(mask) 233
mask_without_small tensor([0.8995, 0.6450, 0.6285, 0.6147, 0.6507, 0.6220, 0.6504, 0.6342, 0.6241,
        0.6523, 0.6395, 0.6400, 0.5859, 0.6409, 0.6269, 0.6544, 0.6318, 0.9023,
        0.6373, 0.6143, 0.8981, 0.6422, 0.6438, 0.6267, 0.9036, 0.6395, 0.5047,
        0.6360, 0.6207, 0.5340, 0.6409, 0.6233, 0.6082, 0.5077, 0.6339, 0.6213,
        0.6394, 0.6194, 0.6221, 0.6347, 0.6189, 0.8996, 0.6323, 0.6501, 0.6111,
        0.5051, 0.6360, 0.6294, 0.6329, 0.6071, 0.6306, 0.6494, 0.6440, 0.6309,
        0.6489, 0.6247, 0.6414, 0.6398, 0.6216, 0.6227, 0.9010, 0.6326, 0.6176,
        0.6181, 0.6595, 0.6307, 0.6414, 0.9022, 0.6340, 0.6402, 0.9039, 0.6320,
        0.6417, 0.6062, 0.6389], grad_fn=<IndexBackward0>)
pred_loss tensor([0.0017], grad_fn=<MulBackward0>)
size_loss tensor(-909.1180, grad_fn=<MulBackward0>)
size_num_loss 5.5
loss: tensor([-902.4558], grad_fn=<AddBackward0>)
12
epoch:  12 ; loss:  -902.455810546875 ; pred:  tensor([5.6130e-07, 1.1901e-04, 9.9983e-01, 4.7718e-05],
       grad_fn=<SoftmaxBackward0>)
num_high 54 len(mask) 233
mask_without_small tensor([0.9072, 0.6585, 0.6319, 0.6096, 0.6663, 0.6210, 0.6659, 0.6417, 0.6246,
        0.6683, 0.6503, 0.6510, 0.5746, 0.6525, 0.6292, 0.6710, 0.6375, 0.9099,
        0.6468, 0.6091, 0.9060, 0.6544, 0.6568, 0.6289, 0.9110, 0.6503, 0.6446,
        0.6190, 0.5205, 0.6525, 0.6231, 0.6004, 0.6411, 0.6200, 0.6501, 0.6169,
        0.6212, 0.6425, 0.6161, 0.9073, 0.6384, 0.6655, 0.6045, 0.6446, 0.6335,
        0.6395, 0.5991, 0.6354, 0.6647, 0.6571, 0.6360, 0.6639, 0.6255, 0.6532,
        0.6507, 0.6204, 0.6223, 0.9086, 0.6390, 0.6140, 0.6148, 0.6769, 0.6357,
        0.6533, 0.9098, 0.6412, 0.6514, 0.9114, 0.6378, 0.6537, 0.5979, 0.6494],
       grad_fn=<IndexBackward0>)
pred_loss tensor([0.0021], grad_fn=<MulBackward0>)
size_loss tensor(-896.9156, grad_fn=<MulBackward0>)
size_num_loss 5.4
loss: tensor([-890.3668], grad_fn=<AddBackward0>)
13
epoch:  13 ; loss:  -890.3667602539062 ; pred:  tensor([7.8627e-07, 1.4914e-04, 9.9979e-01, 6.1607e-05],
       grad_fn=<SoftmaxBackward0>)
num_high 49 len(mask) 233
mask_without_small tensor([0.9148, 0.6683, 0.6262, 0.5976, 0.6801, 0.6115, 0.6796, 0.6407, 0.6162,
        0.6829, 0.6547, 0.6559, 0.5589, 0.6584, 0.6223, 0.6865, 0.6343, 0.9173,
        0.6488, 0.5969, 0.9137, 0.6615, 0.6655, 0.6220, 0.9183, 0.6547, 0.6453,
        0.6090, 0.5034, 0.6584, 0.6143, 0.5869, 0.6398, 0.6102, 0.6543, 0.6064,
        0.6118, 0.6419, 0.6054, 0.9149, 0.6356, 0.6790, 0.5915, 0.6453, 0.6284,
        0.6373, 0.5853, 0.6312, 0.6777, 0.6658, 0.6320, 0.6766, 0.6174, 0.6595,
        0.6554, 0.6108, 0.6132, 0.9161, 0.6365, 0.6028, 0.6038, 0.6940, 0.6316,
        0.6596, 0.9172, 0.6400, 0.6565, 0.9187, 0.6348, 0.6603, 0.5839, 0.6532],
       grad_fn=<IndexBackward0>)
pred_loss tensor([0.0028], grad_fn=<MulBackward0>)
size_loss tensor(-947.0297, grad_fn=<MulBackward0>)
size_num_loss 4.9
loss: tensor([-940.9826], grad_fn=<AddBackward0>)
14
epoch:  14 ; loss:  -940.9826049804688 ; pred:  tensor([1.1677e-06, 1.9399e-04, 9.9972e-01, 8.2515e-05],
       grad_fn=<SoftmaxBackward0>)
num_high 45 len(mask) 233
mask_without_small tensor([0.9221, 0.6775, 0.6150, 0.5814, 0.6948, 0.5973, 0.6941, 0.6344, 0.6028,
        0.6985, 0.6557, 0.6575, 0.5403, 0.6615, 0.6102, 0.7031, 0.6255, 0.9244,
        0.6464, 0.5807, 0.9210, 0.6666, 0.6731, 0.6098, 0.9253, 0.6556, 0.6411,
        0.5944, 0.6615, 0.6005, 0.5697, 0.6332, 0.5958, 0.6550, 0.5914, 0.5976,
        0.6362, 0.5903, 0.9222, 0.6273, 0.6933, 0.5748, 0.6411, 0.6178, 0.6296,
        0.5680, 0.6215, 0.6915, 0.6736, 0.6225, 0.6899, 0.6043, 0.6634, 0.6567,
        0.5964, 0.5992, 0.9233, 0.6285, 0.5873, 0.5884, 0.7119, 0.6220, 0.6636,
        0.9243, 0.6334, 0.6586, 0.9256, 0.6262, 0.6647, 0.5665, 0.6532],
       grad_fn=<IndexBackward0>)
pred_loss tensor([0.0037], grad_fn=<MulBackward0>)
size_loss tensor(-1001.0261, grad_fn=<MulBackward0>)
size_num_loss 4.5
loss: tensor([-995.3877], grad_fn=<AddBackward0>)
15
epoch:  15 ; loss:  -995.3876953125 ; pred:  tensor([1.7579e-06, 2.5432e-04, 9.9963e-01, 1.1122e-04],
       grad_fn=<SoftmaxBackward0>)
num_high 37 len(mask) 233
mask_without_small tensor([0.9290, 0.6882, 0.5998, 0.5624, 0.7111, 0.5797, 0.7102, 0.6236, 0.5858,
        0.7155, 0.6542, 0.6571, 0.5194, 0.6635, 0.5943, 0.7207, 0.6124, 0.9310,
        0.6402, 0.5617, 0.9280, 0.6716, 0.6816, 0.5938, 0.9319, 0.6541, 0.6327,
        0.5765, 0.6635, 0.5833, 0.5499, 0.6221, 0.5781, 0.6533, 0.5732, 0.5800,
        0.6260, 0.5720, 0.9290, 0.6146, 0.7093, 0.5553, 0.6327, 0.6031, 0.6174,
        0.5481, 0.6075, 0.7071, 0.6825, 0.6087, 0.7051, 0.5875, 0.6665, 0.6559,
        0.5788, 0.5819, 0.9301, 0.6161, 0.5688, 0.5700, 0.7303, 0.6080, 0.6667,
        0.9309, 0.6223, 0.6588, 0.9322, 0.6132, 0.6686, 0.5465, 0.6504],
       grad_fn=<IndexBackward0>)
pred_loss tensor([0.0048], grad_fn=<MulBackward0>)
size_loss tensor(-1086.5404, grad_fn=<MulBackward0>)
size_num_loss 3.7
loss: tensor([-1081.7068], grad_fn=<AddBackward0>)
16
epoch:  16 ; loss:  -1081.706787109375 ; pred:  tensor([2.6399e-06, 3.3271e-04, 9.9952e-01, 1.4929e-04],
       grad_fn=<SoftmaxBackward0>)
num_high 33 len(mask) 233
mask_without_small tensor([0.9353, 0.7015, 0.5816, 0.5412, 0.7285, 0.5596, 0.7275, 0.6094, 0.5661,
        0.7332, 0.6517, 0.6561, 0.6661, 0.5754, 0.7388, 0.5959, 0.9372, 0.6311,
        0.5404, 0.9344, 0.6786, 0.6929, 0.5749, 0.9380, 0.6514, 0.6209, 0.5561,
        0.6661, 0.5634, 0.5280, 0.6075, 0.5578, 0.6502, 0.5526, 0.5599, 0.6123,
        0.5513, 0.9354, 0.5985, 0.7265, 0.5336, 0.6209, 0.5853, 0.6019, 0.5261,
        0.5902, 0.7240, 0.6941, 0.5916, 0.7218, 0.5680, 0.6708, 0.6542, 0.5585,
        0.5619, 0.9364, 0.6003, 0.5479, 0.5491, 0.7489, 0.5909, 0.6711, 0.9371,
        0.6078, 0.6587, 0.9383, 0.5968, 0.6739, 0.5245, 0.6458],
       grad_fn=<IndexBackward0>)
pred_loss tensor([0.0063], grad_fn=<MulBackward0>)
size_loss tensor(-1182.1455, grad_fn=<MulBackward0>)
size_num_loss 3.3000000000000003
loss: tensor([-1177.7225], grad_fn=<AddBackward0>)
17
epoch:  17 ; loss:  -1177.7225341796875 ; pred:  tensor([3.9113e-06, 4.3147e-04, 9.9937e-01, 1.9817e-04],
       grad_fn=<SoftmaxBackward0>)
num_high 32 len(mask) 233
mask_without_small tensor([0.9412, 0.7171, 0.5608, 0.5181, 0.7463, 0.5373, 0.7453, 0.5921, 0.5443,
        0.7513, 0.6486, 0.6554, 0.6706, 0.5542, 0.7570, 0.5766, 0.9429, 0.6190,
        0.5173, 0.9404, 0.6885, 0.7069, 0.5536, 0.9437, 0.6482, 0.6059, 0.5337,
        0.6706, 0.5414, 0.5045, 0.5898, 0.5355, 0.6463, 0.5300, 0.5377, 0.5955,
        0.5287, 0.9412, 0.5795, 0.7443, 0.5103, 0.6059, 0.5649, 0.5834, 0.5026,
        0.5703, 0.7417, 0.7083, 0.5719, 0.7393, 0.5463, 0.6776, 0.6524, 0.5363,
        0.5398, 0.9421, 0.5815, 0.5251, 0.5264, 0.7672, 0.5710, 0.6780, 0.9428,
        0.5902, 0.6594, 0.9439, 0.5777, 0.6820, 0.5009, 0.6397],
       grad_fn=<IndexBackward0>)
pred_loss tensor([0.0082], grad_fn=<MulBackward0>)
size_loss tensor(-1299.0079, grad_fn=<MulBackward0>)
size_num_loss 3.2
loss: tensor([-1294.6912], grad_fn=<AddBackward0>)
18
epoch:  18 ; loss:  -1294.691162109375 ; pred:  tensor([5.6827e-06, 5.5228e-04, 9.9918e-01, 2.5911e-04],
       grad_fn=<SoftmaxBackward0>)
num_high 32 len(mask) 233
mask_without_small tensor([0.9465, 0.7340, 0.5381, 0.7643, 0.5135, 0.7633, 0.5721, 0.5207, 0.7693,
        0.6459, 0.6561, 0.6783, 0.5311, 0.7751, 0.5551, 0.9481, 0.6042, 0.9458,
        0.7016, 0.7229, 0.5305, 0.9488, 0.6452, 0.5881, 0.5097, 0.6783, 0.5177,
        0.5696, 0.5116, 0.6425, 0.5059, 0.5139, 0.5761, 0.5045, 0.9466, 0.5582,
        0.7622, 0.5882, 0.5424, 0.5624, 0.5483, 0.7596, 0.7245, 0.5499, 0.7572,
        0.5228, 0.6878, 0.6517, 0.5124, 0.5160, 0.9474, 0.5604, 0.5008, 0.5022,
        0.7852, 0.5490, 0.6883, 0.9480, 0.5700, 0.6622, 0.9490, 0.5562, 0.6936,
        0.6326], grad_fn=<IndexBackward0>)
pred_loss tensor([0.0104], grad_fn=<MulBackward0>)
size_loss tensor(-1410.4052, grad_fn=<MulBackward0>)
size_num_loss 3.2
loss: tensor([-1406.1241], grad_fn=<AddBackward0>)
19
epoch:  19 ; loss:  -1406.1241455078125 ; pred:  tensor([8.0660e-06, 6.9604e-04, 9.9896e-01, 3.3296e-04],
       grad_fn=<SoftmaxBackward0>)
Finished Training
sel masked ver tensor(indices=tensor([[6868, 6872, 6936, 7075, 7931, 7934, 5948, 5954, 5956,
                        5974, 5978, 5504, 5504, 5444, 5471, 5471, 6936, 7021,
                        7065, 7075, 7075, 7089, 7931, 5974, 5978, 6868, 6936,
                        6937, 7015, 7065, 7107, 7945, 7945, 7945, 7958, 7958,
                        8035, 5426, 5449, 5504, 5974, 5978, 5989, 5948, 5948,
                        5948, 5956, 5956, 5956, 5956, 5956, 5797, 5797, 5797,
                        5797, 5797, 5797, 5797, 5504, 5504, 5797, 5797, 5797,
                        5797],
                       [5797, 5797, 5797, 5797, 5797, 5797, 5504, 5504, 5504,
                        5504, 5504, 5948, 5978, 5989, 5948, 5978, 5956, 5974,
                        5956, 5948, 5974, 5956, 5948, 5471, 5449, 5444, 5381,
                        5471, 5471, 5449, 5382, 5381, 5426, 5471, 5382, 5426,
                        5471, 5797, 5797, 5797, 5797, 5797, 5797, 7065, 7883,
                        8004, 7065, 7089, 7931, 7934, 7945, 7006, 7021, 7065,
                        7075, 7104, 7934, 7958, 6872, 6936, 5948, 5954, 5956,
                        5231]]),
       values=tensor([0.9465, 0.7340, 0.5381, 0.7643, 0.5135, 0.7633, 0.5721,
                      0.5207, 0.7693, 0.6459, 0.6561, 0.6783, 0.5311, 0.7751,
                      0.5551, 0.9481, 0.6042, 0.9458, 0.7016, 0.7229, 0.5305,
                      0.9488, 0.6452, 0.5881, 0.5097, 0.6783, 0.5177, 0.5696,
                      0.5116, 0.6425, 0.5059, 0.5139, 0.5761, 0.5045, 0.9466,
                      0.5582, 0.7622, 0.5882, 0.5424, 0.5624, 0.5483, 0.7596,
                      0.7245, 0.5499, 0.7572, 0.5228, 0.6878, 0.6517, 0.5124,
                      0.5160, 0.9474, 0.5604, 0.5008, 0.5022, 0.7852, 0.5490,
                      0.6883, 0.9480, 0.5700, 0.6622, 0.9490, 0.5562, 0.6936,
                      0.6326]),
       size=(8036, 8005), nnz=64, layout=torch.sparse_coo,
       grad_fn=<SparseCooTensorWithDimsAndTensorsBackward0>)
Counter({'isAbout': 14, 'projectInfo': 8, 'publication': 7, 'hasProject': 7, 'author': 6, 'carriedOutBy': 5, 'member': 4, 'dealtWithIn': 3, 'worksAtProject': 3, 'isWorkedOnBy': 2, 'carriesOut': 2, 'publishes': 2, 'type': 1})
dict index: {}
node_idx 5797
 node original label [2]
 node predicted label explain 2
 node prediction probability explain tensor([8.0660e-06, 6.9604e-04, 9.9896e-01, 3.3296e-04],
       grad_fn=<SoftmaxBackward0>)
 node predicted label full 2 most important relations  {'isWorkedOnBy': 2, 'dealtWithIn': 3, 'member': 4, 'carriesOut': 2, 'publishes': 2, 'type': 1, 'worksAtProject': 3, 'publication': 7, 'carriedOutBy': 5, 'projectInfo': 8, 'isAbout': 14, 'author': 6, 'hasProject': 7, 'label': 2, 'node_idx': '5797'}
 final masks and lenght tensor(indices=tensor([[ 23438,  23442,  23506,  23507,  23527,  23576,  23585,
                         23591,  23635,  23645,  23659,  23674,  23677,  24397,
                         24453,  24501,  24504,  24515,  24528,  24574,  24605,
                         39088,  39094,  39096,  39114,  39118,  39129,  46929,
                         46929,  46929,  46929,  46929,  46929,  63439,  63439,
                         63439,  63441,  63441,  63441,  63444,  63444,  63466,
                         63466,  63466,  63466,  63466,  63466,  88647, 114573,
                        114573, 114573, 114577, 114577, 114641, 114642, 114711,
                        114720, 114720, 114720, 114726, 114726, 114726, 114770,
                        114770, 114780, 114780, 114794, 114809, 114809, 115532,
                        115588, 115636, 115636, 115639, 115639, 115650, 115650,
                        115709, 115740, 115740, 146793, 146799, 146799, 146799,
                        146801, 146819, 146819, 146823, 146823, 146823, 146834,
                        146834, 146834, 146834, 147713, 147713, 147717, 147717,
                        147781, 147781, 147781, 147781, 147782, 147802, 147851,
                        147860, 147866, 147910, 147910, 147910, 147910, 147920,
                        147934, 147934, 147949, 147952, 147952, 148672, 148672,
                        148672, 148728, 148776, 148779, 148790, 148790, 148790,
                        148790, 148803, 148803, 148803, 148803, 148849, 148880,
                        148880, 154511, 154512, 154556, 154574, 154576, 154579,
                        154601, 179489, 179933, 179939, 179941, 179959, 179963,
                        179974, 196352, 229492, 237777, 246213, 246213, 246213,
                        246213, 246213, 246213, 246213, 246213, 246213, 246213,
                        246219, 246219, 246221, 246221, 246221, 246221, 246221,
                        246221, 246221, 246221, 246221, 246221, 246221, 246221,
                        246221, 246221, 246239, 246239, 246239, 246239, 246243,
                        246243, 254347, 254347, 254347, 254347, 254347, 254347,
                        254347, 254347, 254347, 254347, 254347, 254347, 254347,
                        254347, 254347, 254347, 254347, 254347, 254347, 254347,
                        254347, 262339, 262339, 262339, 262339, 262339, 262339,
                        262339, 262339, 262339, 262339, 262339, 262339, 262339,
                        262339, 262339, 262339, 262339, 262339, 262339, 262339,
                        262339, 304057, 304057, 304057, 304057, 304057, 304057,
                        328912, 328912],
                       [  5797,   5797,   5797,   5797,   5797,   5797,   5797,
                          5797,   5797,   5797,   5797,   5797,   5797,   5797,
                          5797,   5797,   5797,   5797,   5797,   5797,   5797,
                          5504,   5504,   5504,   5504,   5504,   5504,   5948,
                          5954,   5956,   5974,   5978,   5989,   5954,   5974,
                          5989,   5954,   5978,   5989,   5978,   5989,   5948,
                          5954,   5956,   5974,   5978,   5989,     67,   5948,
                          5956,   5974,   5956,   5978,   5956,   5948,   5956,
                          5954,   5956,   5974,   5954,   5956,   5974,   5948,
                          5956,   5948,   5974,   5956,   5956,   5978,   5956,
                          5948,   5948,   5956,   5948,   5956,   5948,   5956,
                          5948,   5948,   5956,   5471,   5444,   5446,   5471,
                          5471,   5444,   5471,   5446,   5449,   5471,   5444,
                          5446,   5449,   5471,   5444,   5471,   5444,   5471,
                          5381,   5426,   5446,   5449,   5471,   5426,   5471,
                          5471,   5471,   5444,   5446,   5449,   5471,   5471,
                          5444,   5471,   5449,   5381,   5382,   5382,   5426,
                          5449,   5471,   5471,   5471,   5381,   5426,   5449,
                          5471,   5381,   5382,   5426,   5449,   5471,   5444,
                          5471,   5797,   5797,   5797,   5797,   5797,   5797,
                          5797,   5797,   5797,   5797,   5797,   5797,   5797,
                          5797,   2968,     90,   5591,   6868,   6937,   7065,
                          7075,   7883,   7931,   7934,   7945,   8004,   8035,
                          7015,   7021,   6868,   6872,   6936,   7006,   7015,
                          7021,   7065,   7089,   7104,   7827,   7931,   7934,
                          7945,   8035,   6868,   7015,   7021,   7075,   6872,
                          7104,   6868,   6872,   6936,   6937,   6957,   7006,
                          7015,   7021,   7065,   7075,   7089,   7104,   7107,
                          7827,   7883,   7931,   7934,   7945,   7958,   8004,
                          8035,   6868,   6872,   6936,   6937,   6957,   7006,
                          7015,   7021,   7065,   7075,   7089,   7104,   7107,
                          7827,   7883,   7931,   7934,   7945,   7958,   8004,
                          8035,   5948,   5954,   5956,   5974,   5978,   5989,
                          5230,   5231]]),
       values=tensor([0.9465, 0.7340, 0.5381, 0.4170, 0.4936, 0.4027, 0.3995,
                      0.3954, 0.3893, 0.7643, 0.3974, 0.3994, 0.3898, 0.3936,
                      0.3889, 0.5135, 0.7633, 0.4034, 0.3950, 0.4127, 0.3891,
                      0.5721, 0.5207, 0.7693, 0.6459, 0.6561, 0.4575, 0.6783,
                      0.4015, 0.4023, 0.4009, 0.5311, 0.3998, 0.3867, 0.4017,
                      0.7751, 0.4110, 0.3967, 0.4106, 0.3888, 0.3963, 0.5551,
                      0.3865, 0.3926, 0.4020, 0.9481, 0.4027, 0.3952, 0.3858,
                      0.3913, 0.4035, 0.4134, 0.3952, 0.6042, 0.3879, 0.3896,
                      0.3994, 0.4021, 0.3989, 0.4928, 0.3978, 0.9458, 0.4037,
                      0.7016, 0.7229, 0.5305, 0.9488, 0.4131, 0.4116, 0.4024,
                      0.4063, 0.6452, 0.4030, 0.4131, 0.4028, 0.4127, 0.4268,
                      0.3917, 0.4149, 0.3893, 0.4013, 0.3988, 0.4006, 0.3930,
                      0.4151, 0.4132, 0.5881, 0.4026, 0.5097, 0.3953, 0.3829,
                      0.4224, 0.3930, 0.3876, 0.6783, 0.4121, 0.3821, 0.4121,
                      0.5177, 0.4019, 0.4797, 0.4305, 0.5696, 0.3961, 0.4027,
                      0.5116, 0.4126, 0.4068, 0.4095, 0.6425, 0.4009, 0.3996,
                      0.3983, 0.3976, 0.3927, 0.4122, 0.5059, 0.4039, 0.3975,
                      0.4136, 0.4005, 0.4087, 0.4053, 0.5139, 0.5761, 0.4115,
                      0.5045, 0.4126, 0.9466, 0.5582, 0.3987, 0.4048, 0.4040,
                      0.7622, 0.4857, 0.4273, 0.5882, 0.4015, 0.3913, 0.5424,
                      0.3979, 0.5624, 0.3907, 0.4778, 0.3845, 0.5483, 0.7596,
                      0.7245, 0.4096, 0.4020, 0.3897, 0.4043, 0.4117, 0.5499,
                      0.3957, 0.7572, 0.3826, 0.3969, 0.4035, 0.5228, 0.4212,
                      0.4084, 0.4017, 0.3986, 0.4101, 0.3897, 0.4066, 0.4055,
                      0.3954, 0.6878, 0.6517, 0.4026, 0.3965, 0.5124, 0.5160,
                      0.9474, 0.4021, 0.4049, 0.3881, 0.4021, 0.3854, 0.3957,
                      0.4047, 0.3944, 0.3945, 0.3975, 0.4206, 0.4051, 0.5604,
                      0.3937, 0.5008, 0.5022, 0.7852, 0.3856, 0.5490, 0.3988,
                      0.4039, 0.4119, 0.4122, 0.6883, 0.4135, 0.9480, 0.3945,
                      0.3854, 0.4070, 0.5700, 0.6622, 0.4125, 0.3879, 0.3835,
                      0.3950, 0.3928, 0.4060, 0.4127, 0.4037, 0.4126, 0.3973,
                      0.3989, 0.4001, 0.4113, 0.3843, 0.4105, 0.4068, 0.4042,
                      0.3854, 0.9490, 0.5562, 0.6936, 0.4761, 0.4126, 0.4115,
                      0.4017, 0.6326]),
       size=(753935, 8285), nnz=233, layout=torch.sparse_coo,
       grad_fn=<SparseCooTensorWithDimsAndTensorsBackward0>) 64
 ---------------------------------------------------------------
node label: 2
66
r_exp.py:102: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  hor_graph, ver_graph = torch.tensor(sub_hor_graph, dtype=torch.float), torch.tensor(sub_ver_graph, dtype=torch.float)
wandb: WARNING Calling wandb.login() after wandb.init() has no effect.
num_high 410 len(mask) 410
mask_without_small tensor([0.7567, 0.7510, 0.7432, 0.7012, 0.7403, 0.7138, 0.7305, 0.7085, 0.7206,
        0.7531, 0.7256, 0.7114, 0.7209, 0.7233, 0.7204, 0.7414, 0.7530, 0.7289,
        0.7242, 0.7371, 0.7205, 0.7456, 0.7419, 0.7535, 0.7483, 0.7485, 0.7394,
        0.7490, 0.7279, 0.7316, 0.7276, 0.7427, 0.7116, 0.7189, 0.7280, 0.7540,
        0.7354, 0.7252, 0.7352, 0.7203, 0.7091, 0.7445, 0.7188, 0.7227, 0.7132,
        0.7592, 0.7138, 0.7243, 0.7183, 0.7219, 0.7321, 0.7382, 0.7243, 0.7471,
        0.7197, 0.7208, 0.7114, 0.7316, 0.7302, 0.7402, 0.7297, 0.7556, 0.7145,
        0.7496, 0.7504, 0.7427, 0.7604, 0.7382, 0.7358, 0.7283, 0.7163, 0.7482,
        0.7287, 0.7382, 0.7318, 0.7369, 0.7389, 0.7222, 0.6997, 0.7206, 0.7312,
        0.7264, 0.7123, 0.7229, 0.7384, 0.7382, 0.7464, 0.7318, 0.7412, 0.7244,
        0.7164, 0.7393, 0.7068, 0.7195, 0.7490, 0.7376, 0.6952, 0.7377, 0.7417,
        0.7315, 0.7398, 0.7390, 0.7455, 0.7248, 0.7285, 0.7413, 0.7366, 0.7335,
        0.7347, 0.7482, 0.7310, 0.7269, 0.7106, 0.7297, 0.7228, 0.7376, 0.7409,
        0.7323, 0.7257, 0.7382, 0.7309, 0.7344, 0.7329, 0.7414, 0.7458, 0.7357,
        0.7408, 0.7367, 0.7567, 0.7447, 0.7109, 0.7153, 0.7292, 0.7529, 0.7400,
        0.7389, 0.7464, 0.7313, 0.7056, 0.7436, 0.7259, 0.7450, 0.7215, 0.7397,
        0.7175, 0.7440, 0.7527, 0.7505, 0.7347, 0.7282, 0.7209, 0.7325, 0.7358,
        0.7441, 0.7246, 0.7525, 0.6957, 0.7253, 0.7143, 0.7421, 0.7042, 0.7342,
        0.7314, 0.7263, 0.7350, 0.7209, 0.7335, 0.7158, 0.7085, 0.7492, 0.7484,
        0.7318, 0.7093, 0.7413, 0.7416, 0.7580, 0.7316, 0.7327, 0.7199, 0.7282,
        0.7181, 0.7087, 0.7152, 0.7238, 0.7239, 0.7100, 0.7038, 0.7328, 0.7449,
        0.7234, 0.7406, 0.7407, 0.7547, 0.7182, 0.7441, 0.7264, 0.7146, 0.7359,
        0.7376, 0.7492, 0.7382, 0.7591, 0.7238, 0.7181, 0.7336, 0.7455, 0.7486,
        0.7373, 0.7197, 0.7168, 0.7242, 0.7228, 0.7332, 0.7371, 0.7290, 0.6981,
        0.7256, 0.7456, 0.7059, 0.7513, 0.7353, 0.7241, 0.7450, 0.7536, 0.7310,
        0.7533, 0.7332, 0.7163, 0.7231, 0.7322, 0.7365, 0.7575, 0.7301, 0.7184,
        0.7020, 0.7160, 0.7313, 0.7321, 0.7337, 0.7366, 0.7181, 0.7348, 0.7236,
        0.7374, 0.7189, 0.7307, 0.7262, 0.7507, 0.7479, 0.7211, 0.7426, 0.7380,
        0.7384, 0.7388, 0.7379, 0.7341, 0.7215, 0.7387, 0.7099, 0.7084, 0.7103,
        0.7370, 0.7293, 0.7417, 0.7085, 0.7296, 0.7408, 0.7316, 0.7486, 0.7344,
        0.7031, 0.7313, 0.7117, 0.7396, 0.6941, 0.7307, 0.7294, 0.7207, 0.7539,
        0.7319, 0.7471, 0.7568, 0.7409, 0.7443, 0.7367, 0.7466, 0.7347, 0.7306,
        0.7442, 0.7169, 0.7236, 0.7250, 0.7267, 0.7293, 0.7211, 0.7304, 0.7579,
        0.7345, 0.7437, 0.7407, 0.7306, 0.7325, 0.7491, 0.7407, 0.7366, 0.7211,
        0.7424, 0.7177, 0.7372, 0.7478, 0.6982, 0.7130, 0.7335, 0.7009, 0.7292,
        0.7165, 0.7204, 0.7303, 0.7473, 0.7174, 0.7370, 0.7211, 0.7453, 0.7106,
        0.7374, 0.7361, 0.7310, 0.7321, 0.7362, 0.7406, 0.7067, 0.7193, 0.7370,
        0.7347, 0.7229, 0.7322, 0.7430, 0.7336, 0.7417, 0.7303, 0.7388, 0.7212,
        0.7243, 0.7317, 0.7393, 0.7529, 0.7299, 0.7457, 0.7439, 0.7217, 0.7231,
        0.7265, 0.7200, 0.7362, 0.7323, 0.7137, 0.7266, 0.7193, 0.7234, 0.7575,
        0.7563, 0.7537, 0.7314, 0.7286, 0.7060, 0.7213, 0.7256, 0.7562, 0.7280,
        0.7333, 0.7595, 0.7538, 0.7358, 0.7398, 0.7282, 0.7404, 0.7291, 0.7145,
        0.7131, 0.7372, 0.7229, 0.7426, 0.7243, 0.7261, 0.7401, 0.7300, 0.7282,
        0.7333, 0.7508, 0.7180, 0.7227, 0.7297, 0.7173, 0.6956, 0.7374, 0.7365,
        0.6923, 0.7226, 0.7497, 0.7341, 0.7403, 0.7125, 0.7420, 0.7413, 0.7367,
        0.7494, 0.7498, 0.7208, 0.7244, 0.7208], grad_fn=<IndexBackward0>)
pred_loss tensor([-0.], grad_fn=<MulBackward0>)
size_loss tensor(-1.3611, grad_fn=<MulBackward0>)
size_num_loss 41.0
loss: tensor([190.1647], grad_fn=<AddBackward0>)
0
epoch:  0 ; loss:  190.1646728515625 ; pred:  tensor([5.3914e-22, 1.1321e-14, 1.0000e+00, 1.6410e-16],
       grad_fn=<SoftmaxBackward0>)
num_high 410 len(mask) 410
mask_without_small tensor([0.7378, 0.7318, 0.7237, 0.6798, 0.7206, 0.6929, 0.7103, 0.6874, 0.7000,
        0.7340, 0.7053, 0.6904, 0.7004, 0.7029, 0.6998, 0.7218, 0.7339, 0.7087,
        0.7038, 0.7172, 0.6999, 0.7262, 0.7223, 0.7345, 0.7290, 0.7292, 0.7196,
        0.7297, 0.7076, 0.7115, 0.7073, 0.7231, 0.6907, 0.6983, 0.7077, 0.7350,
        0.7155, 0.7048, 0.7153, 0.6997, 0.6881, 0.7250, 0.6982, 0.7022, 0.6923,
        0.7404, 0.6929, 0.7039, 0.6977, 0.7014, 0.7121, 0.7184, 0.7039, 0.7277,
        0.6991, 0.7003, 0.6904, 0.7115, 0.7100, 0.7205, 0.7095, 0.7367, 0.6937,
        0.7304, 0.7312, 0.7231, 0.7417, 0.7184, 0.7159, 0.7081, 0.6956, 0.7289,
        0.7085, 0.7184, 0.7118, 0.7170, 0.7191, 0.7017, 0.6783, 0.7001, 0.7111,
        0.7061, 0.6913, 0.7025, 0.7186, 0.7184, 0.7270, 0.7117, 0.7215, 0.7040,
        0.6957, 0.7195, 0.6856, 0.6989, 0.7297, 0.7178, 0.6736, 0.7179, 0.7221,
        0.7114, 0.7201, 0.7192, 0.7260, 0.7044, 0.7083, 0.7216, 0.7167, 0.7135,
        0.7147, 0.7289, 0.7109, 0.7066, 0.6896, 0.7095, 0.7023, 0.7177, 0.7213,
        0.7123, 0.7053, 0.7185, 0.7108, 0.7144, 0.7128, 0.7218, 0.7264, 0.7158,
        0.7212, 0.7168, 0.7379, 0.7253, 0.6899, 0.6945, 0.7090, 0.7338, 0.7203,
        0.7191, 0.7271, 0.7112, 0.6844, 0.7240, 0.7055, 0.7255, 0.7010, 0.7200,
        0.6968, 0.7245, 0.7336, 0.7313, 0.7148, 0.7079, 0.7003, 0.7124, 0.7159,
        0.7246, 0.7042, 0.7334, 0.6741, 0.7049, 0.6935, 0.7225, 0.6829, 0.7142,
        0.7113, 0.7060, 0.7150, 0.7003, 0.7135, 0.6950, 0.6874, 0.7300, 0.7291,
        0.7117, 0.6883, 0.7217, 0.7220, 0.7391, 0.7115, 0.7127, 0.6993, 0.7080,
        0.6974, 0.6876, 0.6944, 0.7034, 0.7034, 0.6889, 0.6825, 0.7128, 0.7254,
        0.7029, 0.7210, 0.7210, 0.7357, 0.6975, 0.7246, 0.7061, 0.6938, 0.7161,
        0.7178, 0.7300, 0.7184, 0.7403, 0.7034, 0.6974, 0.7136, 0.7260, 0.7293,
        0.7175, 0.6991, 0.6961, 0.7038, 0.7024, 0.7132, 0.7172, 0.7088, 0.6766,
        0.7052, 0.7262, 0.6847, 0.7321, 0.7154, 0.7037, 0.7256, 0.7346, 0.7109,
        0.7343, 0.7132, 0.6955, 0.7027, 0.7121, 0.7167, 0.7386, 0.7099, 0.6978,
        0.6807, 0.6952, 0.7112, 0.7121, 0.7137, 0.7168, 0.6974, 0.7149, 0.7032,
        0.7175, 0.6983, 0.7106, 0.7059, 0.7315, 0.7286, 0.7006, 0.7230, 0.7183,
        0.7186, 0.7190, 0.7182, 0.7141, 0.7010, 0.7190, 0.6888, 0.6873, 0.6893,
        0.7171, 0.7092, 0.7220, 0.6875, 0.7094, 0.7211, 0.7115, 0.7293, 0.7145,
        0.6818, 0.7112, 0.6908, 0.7198, 0.6725, 0.7106, 0.7092, 0.7001, 0.7349,
        0.7118, 0.7278, 0.7379, 0.7213, 0.7248, 0.7169, 0.7273, 0.7148, 0.7104,
        0.7247, 0.6962, 0.7031, 0.7046, 0.7064, 0.7091, 0.7006, 0.7103, 0.7391,
        0.7146, 0.7242, 0.7211, 0.7105, 0.7124, 0.7298, 0.7211, 0.7167, 0.7006,
        0.7228, 0.6970, 0.7174, 0.7285, 0.6768, 0.6921, 0.7135, 0.6795, 0.7090,
        0.6958, 0.6999, 0.7102, 0.7279, 0.6966, 0.7171, 0.7005, 0.7259, 0.6897,
        0.7176, 0.7163, 0.7109, 0.7121, 0.7163, 0.7210, 0.6856, 0.6987, 0.7172,
        0.7148, 0.7025, 0.7121, 0.7235, 0.7136, 0.7221, 0.7101, 0.7190, 0.7007,
        0.7039, 0.7117, 0.7196, 0.7338, 0.7097, 0.7263, 0.7244, 0.7011, 0.7027,
        0.7062, 0.6994, 0.7163, 0.7122, 0.6928, 0.7063, 0.6987, 0.7030, 0.7386,
        0.7374, 0.7347, 0.7114, 0.7084, 0.6848, 0.7007, 0.7053, 0.7373, 0.7078,
        0.7133, 0.7407, 0.7348, 0.7159, 0.7201, 0.7080, 0.7207, 0.7089, 0.6937,
        0.6922, 0.7173, 0.7024, 0.7231, 0.7039, 0.7058, 0.7204, 0.7099, 0.7079,
        0.7133, 0.7316, 0.6973, 0.7022, 0.7095, 0.6966, 0.6740, 0.7176, 0.7166,
        0.6706, 0.7021, 0.7305, 0.7141, 0.7207, 0.6916, 0.7224, 0.7217, 0.7169,
        0.7302, 0.7305, 0.7002, 0.7040, 0.7002], grad_fn=<IndexBackward0>)
pred_loss tensor([-0.], grad_fn=<MulBackward0>)
size_loss tensor(-1.4222, grad_fn=<MulBackward0>)
size_num_loss 41.0
loss: tensor([186.0078], grad_fn=<AddBackward0>)
1
epoch:  1 ; loss:  186.00784301757812 ; pred:  tensor([6.3158e-21, 5.7964e-14, 1.0000e+00, 1.0701e-15],
       grad_fn=<SoftmaxBackward0>)
num_high 410 len(mask) 410
mask_without_small tensor([0.7181, 0.7118, 0.7033, 0.6577, 0.7000, 0.6712, 0.6893, 0.6655, 0.6786,
        0.7141, 0.6841, 0.6686, 0.6790, 0.6816, 0.6784, 0.7013, 0.7140, 0.6876,
        0.6825, 0.6965, 0.6785, 0.7059, 0.7018, 0.7146, 0.7088, 0.7090, 0.6990,
        0.7096, 0.6865, 0.6906, 0.6862, 0.7027, 0.6689, 0.6768, 0.6866, 0.7151,
        0.6947, 0.6836, 0.6945, 0.6783, 0.6662, 0.7047, 0.6767, 0.6809, 0.6706,
        0.7208, 0.6712, 0.6827, 0.6762, 0.6801, 0.6912, 0.6978, 0.6826, 0.7075,
        0.6777, 0.6789, 0.6686, 0.6905, 0.6890, 0.7000, 0.6885, 0.7169, 0.6720,
        0.7103, 0.7112, 0.7027, 0.7222, 0.6978, 0.6952, 0.6870, 0.6740, 0.7088,
        0.6874, 0.6978, 0.6908, 0.6963, 0.6985, 0.6803, 0.6561, 0.6787, 0.6902,
        0.6849, 0.6696, 0.6812, 0.6980, 0.6978, 0.7068, 0.6908, 0.7010, 0.6827,
        0.6741, 0.6989, 0.6637, 0.6775, 0.7096, 0.6972, 0.6513, 0.6972, 0.7016,
        0.6904, 0.6995, 0.6986, 0.7057, 0.6832, 0.6872, 0.7011, 0.6960, 0.6927,
        0.6939, 0.7087, 0.6900, 0.6854, 0.6678, 0.6885, 0.6810, 0.6971, 0.7007,
        0.6914, 0.6841, 0.6978, 0.6898, 0.6936, 0.6920, 0.7013, 0.7061, 0.6951,
        0.7007, 0.6961, 0.7181, 0.7049, 0.6681, 0.6729, 0.6880, 0.7139, 0.6997,
        0.6985, 0.7068, 0.6903, 0.6624, 0.7037, 0.6844, 0.7052, 0.6796, 0.6994,
        0.6753, 0.7041, 0.7137, 0.7113, 0.6940, 0.6868, 0.6789, 0.6915, 0.6952,
        0.7043, 0.6830, 0.7135, 0.6518, 0.6837, 0.6718, 0.7020, 0.6609, 0.6934,
        0.6904, 0.6848, 0.6943, 0.6790, 0.6926, 0.6734, 0.6655, 0.7099, 0.7089,
        0.6908, 0.6664, 0.7012, 0.7015, 0.7195, 0.6905, 0.6918, 0.6778, 0.6869,
        0.6759, 0.6657, 0.6728, 0.6821, 0.6822, 0.6671, 0.6605, 0.6919, 0.7051,
        0.6816, 0.7004, 0.7005, 0.7159, 0.6761, 0.7042, 0.6849, 0.6722, 0.6953,
        0.6971, 0.7099, 0.6978, 0.7207, 0.6822, 0.6759, 0.6928, 0.7057, 0.7092,
        0.6968, 0.6777, 0.6745, 0.6825, 0.6811, 0.6923, 0.6965, 0.6878, 0.6543,
        0.6840, 0.7059, 0.6628, 0.7121, 0.6946, 0.6825, 0.7053, 0.7147, 0.6899,
        0.7144, 0.6923, 0.6739, 0.6814, 0.6912, 0.6959, 0.7189, 0.6889, 0.6763,
        0.6586, 0.6736, 0.6903, 0.6912, 0.6929, 0.6961, 0.6759, 0.6941, 0.6819,
        0.6969, 0.6768, 0.6896, 0.6847, 0.7115, 0.7085, 0.6792, 0.7026, 0.6976,
        0.6980, 0.6984, 0.6975, 0.6933, 0.6797, 0.6984, 0.6670, 0.6654, 0.6675,
        0.6964, 0.6881, 0.7016, 0.6656, 0.6884, 0.7006, 0.6906, 0.7092, 0.6937,
        0.6597, 0.6903, 0.6690, 0.6993, 0.6501, 0.6896, 0.6882, 0.6787, 0.7150,
        0.6909, 0.7075, 0.7182, 0.7008, 0.7045, 0.6962, 0.7070, 0.6940, 0.6894,
        0.7044, 0.6746, 0.6818, 0.6834, 0.6853, 0.6881, 0.6792, 0.6893, 0.7194,
        0.6938, 0.7038, 0.7005, 0.6895, 0.6915, 0.7097, 0.7006, 0.6960, 0.6792,
        0.7023, 0.6755, 0.6967, 0.7083, 0.6545, 0.6704, 0.6927, 0.6573, 0.6880,
        0.6742, 0.6784, 0.6892, 0.7077, 0.6751, 0.6964, 0.6792, 0.7056, 0.6679,
        0.6969, 0.6955, 0.6899, 0.6912, 0.6956, 0.7004, 0.6637, 0.6772, 0.6965,
        0.6940, 0.6811, 0.6912, 0.7031, 0.6927, 0.7016, 0.6891, 0.6984, 0.6793,
        0.6827, 0.6907, 0.6990, 0.7138, 0.6887, 0.7060, 0.7040, 0.6798, 0.6814,
        0.6850, 0.6780, 0.6956, 0.6913, 0.6711, 0.6852, 0.6772, 0.6817, 0.7189,
        0.7177, 0.7148, 0.6904, 0.6874, 0.6629, 0.6794, 0.6841, 0.7175, 0.6867,
        0.6925, 0.7211, 0.7149, 0.6951, 0.6995, 0.6869, 0.7001, 0.6879, 0.6721,
        0.6705, 0.6967, 0.6811, 0.7026, 0.6826, 0.6846, 0.6999, 0.6889, 0.6868,
        0.6925, 0.7115, 0.6758, 0.6809, 0.6885, 0.6751, 0.6516, 0.6969, 0.6959,
        0.6481, 0.6808, 0.7104, 0.6933, 0.7001, 0.6699, 0.7020, 0.7012, 0.6962,
        0.7101, 0.7105, 0.6788, 0.6827, 0.6788], grad_fn=<IndexBackward0>)
pred_loss tensor([-0.], grad_fn=<MulBackward0>)
size_loss tensor(-1.4813, grad_fn=<MulBackward0>)
size_num_loss 41.0
loss: tensor([181.6791], grad_fn=<AddBackward0>)
2
epoch:  2 ; loss:  181.67909240722656 ; pred:  tensor([7.6836e-20, 3.0381e-13, 1.0000e+00, 7.1744e-15],
       grad_fn=<SoftmaxBackward0>)
num_high 410 len(mask) 410
mask_without_small tensor([0.6976, 0.6910, 0.6821, 0.6348, 0.6787, 0.6488, 0.6676, 0.6429, 0.6565,
        0.6934, 0.6621, 0.6461, 0.6569, 0.6595, 0.6562, 0.6800, 0.6933, 0.6658,
        0.6605, 0.6750, 0.6564, 0.6848, 0.6806, 0.6939, 0.6878, 0.6881, 0.6777,
        0.6887, 0.6646, 0.6689, 0.6643, 0.6815, 0.6464, 0.6546, 0.6648, 0.6944,
        0.6732, 0.6616, 0.6730, 0.6561, 0.6437, 0.6836, 0.6545, 0.6589, 0.6482,
        0.7005, 0.6488, 0.6606, 0.6539, 0.6580, 0.6695, 0.6764, 0.6606, 0.6865,
        0.6555, 0.6567, 0.6461, 0.6688, 0.6673, 0.6787, 0.6667, 0.6963, 0.6496,
        0.6894, 0.6903, 0.6814, 0.7019, 0.6763, 0.6736, 0.6652, 0.6517, 0.6878,
        0.6656, 0.6763, 0.6691, 0.6748, 0.6771, 0.6582, 0.6332, 0.6565, 0.6684,
        0.6630, 0.6471, 0.6591, 0.6765, 0.6764, 0.6858, 0.6691, 0.6797, 0.6607,
        0.6518, 0.6776, 0.6410, 0.6553, 0.6887, 0.6757, 0.6283, 0.6758, 0.6803,
        0.6687, 0.6781, 0.6773, 0.6846, 0.6612, 0.6654, 0.6798, 0.6745, 0.6710,
        0.6724, 0.6878, 0.6682, 0.6635, 0.6453, 0.6667, 0.6589, 0.6756, 0.6794,
        0.6697, 0.6622, 0.6764, 0.6681, 0.6720, 0.6703, 0.6800, 0.6851, 0.6735,
        0.6793, 0.6746, 0.6976, 0.6838, 0.6456, 0.6505, 0.6661, 0.6932, 0.6784,
        0.6771, 0.6858, 0.6685, 0.6397, 0.6825, 0.6624, 0.6841, 0.6575, 0.6781,
        0.6530, 0.6830, 0.6930, 0.6904, 0.6724, 0.6650, 0.6568, 0.6699, 0.6737,
        0.6831, 0.6610, 0.6927, 0.6287, 0.6617, 0.6495, 0.6808, 0.6381, 0.6718,
        0.6686, 0.6629, 0.6727, 0.6568, 0.6710, 0.6511, 0.6430, 0.6890, 0.6880,
        0.6691, 0.6438, 0.6799, 0.6802, 0.6991, 0.6688, 0.6701, 0.6556, 0.6650,
        0.6536, 0.6431, 0.6504, 0.6601, 0.6602, 0.6446, 0.6377, 0.6702, 0.6840,
        0.6596, 0.6791, 0.6792, 0.6953, 0.6538, 0.6830, 0.6630, 0.6498, 0.6738,
        0.6757, 0.6890, 0.6764, 0.7003, 0.6601, 0.6536, 0.6711, 0.6847, 0.6883,
        0.6754, 0.6555, 0.6522, 0.6605, 0.6590, 0.6706, 0.6751, 0.6659, 0.6314,
        0.6620, 0.6848, 0.6401, 0.6913, 0.6730, 0.6604, 0.6841, 0.6940, 0.6682,
        0.6937, 0.6706, 0.6516, 0.6593, 0.6696, 0.6744, 0.6985, 0.6671, 0.6540,
        0.6358, 0.6513, 0.6685, 0.6695, 0.6712, 0.6746, 0.6537, 0.6725, 0.6598,
        0.6754, 0.6546, 0.6678, 0.6627, 0.6906, 0.6875, 0.6571, 0.6814, 0.6762,
        0.6766, 0.6770, 0.6761, 0.6717, 0.6575, 0.6770, 0.6445, 0.6428, 0.6449,
        0.6749, 0.6663, 0.6803, 0.6430, 0.6666, 0.6793, 0.6689, 0.6882, 0.6721,
        0.6369, 0.6685, 0.6465, 0.6779, 0.6270, 0.6679, 0.6664, 0.6566, 0.6943,
        0.6692, 0.6865, 0.6977, 0.6795, 0.6833, 0.6747, 0.6860, 0.6724, 0.6677,
        0.6832, 0.6523, 0.6598, 0.6614, 0.6634, 0.6662, 0.6571, 0.6675, 0.6990,
        0.6722, 0.6827, 0.6792, 0.6678, 0.6698, 0.6888, 0.6792, 0.6745, 0.6571,
        0.6811, 0.6532, 0.6753, 0.6873, 0.6316, 0.6480, 0.6710, 0.6345, 0.6662,
        0.6519, 0.6563, 0.6674, 0.6867, 0.6528, 0.6750, 0.6570, 0.6845, 0.6453,
        0.6754, 0.6740, 0.6682, 0.6695, 0.6741, 0.6791, 0.6410, 0.6550, 0.6750,
        0.6724, 0.6591, 0.6695, 0.6819, 0.6711, 0.6804, 0.6674, 0.6770, 0.6571,
        0.6606, 0.6690, 0.6776, 0.6931, 0.6669, 0.6849, 0.6828, 0.6577, 0.6593,
        0.6631, 0.6558, 0.6741, 0.6696, 0.6487, 0.6633, 0.6550, 0.6596, 0.6985,
        0.6972, 0.6941, 0.6687, 0.6655, 0.6402, 0.6572, 0.6621, 0.6970, 0.6649,
        0.6708, 0.7008, 0.6942, 0.6736, 0.6782, 0.6651, 0.6788, 0.6661, 0.6497,
        0.6481, 0.6752, 0.6590, 0.6814, 0.6606, 0.6627, 0.6785, 0.6671, 0.6650,
        0.6708, 0.6907, 0.6535, 0.6589, 0.6667, 0.6528, 0.6286, 0.6755, 0.6744,
        0.6250, 0.6587, 0.6895, 0.6717, 0.6788, 0.6474, 0.6807, 0.6799, 0.6747,
        0.6892, 0.6896, 0.6567, 0.6607, 0.6567], grad_fn=<IndexBackward0>)
pred_loss tensor([-0.], grad_fn=<MulBackward0>)
size_loss tensor(-1.5387, grad_fn=<MulBackward0>)
size_num_loss 41.0
loss: tensor([177.1928], grad_fn=<AddBackward0>)
3
epoch:  3 ; loss:  177.19281005859375 ; pred:  tensor([9.5464e-19, 1.6158e-12, 1.0000e+00, 4.8727e-14],
       grad_fn=<SoftmaxBackward0>)
num_high 378 len(mask) 410
mask_without_small tensor([0.6764, 0.6695, 0.6602, 0.6114, 0.6567, 0.6258, 0.6451, 0.6197, 0.6337,
        0.6721, 0.6395, 0.6230, 0.6340, 0.6368, 0.6334, 0.6580, 0.6720, 0.6433,
        0.6378, 0.6529, 0.6336, 0.6630, 0.6586, 0.6726, 0.6662, 0.6665, 0.6556,
        0.6671, 0.6421, 0.6465, 0.6418, 0.6596, 0.6233, 0.6317, 0.6422, 0.6731,
        0.6509, 0.6390, 0.6507, 0.6333, 0.6205, 0.6617, 0.6316, 0.6361, 0.6251,
        0.6795, 0.6258, 0.6379, 0.6310, 0.6352, 0.6471, 0.6543, 0.6379, 0.6648,
        0.6326, 0.6339, 0.6230, 0.6464, 0.6448, 0.6566, 0.6443, 0.6751, 0.6266,
        0.6679, 0.6688, 0.6595, 0.6810, 0.6542, 0.6514, 0.6426, 0.6287, 0.6662,
        0.6431, 0.6542, 0.6467, 0.6527, 0.6550, 0.6354, 0.6097, 0.6337, 0.6460,
        0.6404, 0.6240, 0.6364, 0.6544, 0.6542, 0.6640, 0.6467, 0.6577, 0.6380,
        0.6288, 0.6555, 0.6177, 0.6324, 0.6671, 0.6536, 0.6046, 0.6536, 0.6584,
        0.6463, 0.6561, 0.6552, 0.6629, 0.6386, 0.6428, 0.6579, 0.6523, 0.6487,
        0.6501, 0.6661, 0.6458, 0.6409, 0.6221, 0.6442, 0.6361, 0.6535, 0.6574,
        0.6473, 0.6395, 0.6543, 0.6456, 0.6497, 0.6480, 0.6581, 0.6633, 0.6513,
        0.6573, 0.6524, 0.6765, 0.6620, 0.6225, 0.6275, 0.6436, 0.6718, 0.6563,
        0.6551, 0.6640, 0.6461, 0.6164, 0.6606, 0.6398, 0.6623, 0.6347, 0.6560,
        0.6301, 0.6611, 0.6716, 0.6689, 0.6502, 0.6424, 0.6340, 0.6475, 0.6514,
        0.6613, 0.6383, 0.6714, 0.6051, 0.6391, 0.6264, 0.6588, 0.6148, 0.6495,
        0.6462, 0.6402, 0.6504, 0.6340, 0.6486, 0.6281, 0.6197, 0.6674, 0.6664,
        0.6467, 0.6206, 0.6579, 0.6582, 0.6780, 0.6464, 0.6478, 0.6328, 0.6425,
        0.6307, 0.6199, 0.6274, 0.6374, 0.6374, 0.6214, 0.6144, 0.6479, 0.6622,
        0.6368, 0.6571, 0.6572, 0.6740, 0.6309, 0.6612, 0.6404, 0.6267, 0.6516,
        0.6535, 0.6674, 0.6543, 0.6794, 0.6374, 0.6307, 0.6488, 0.6629, 0.6666,
        0.6532, 0.6326, 0.6293, 0.6378, 0.6363, 0.6483, 0.6529, 0.6434, 0.6078,
        0.6394, 0.6631, 0.6168, 0.6698, 0.6508, 0.6377, 0.6623, 0.6727, 0.6458,
        0.6723, 0.6483, 0.6286, 0.6366, 0.6472, 0.6522, 0.6774, 0.6447, 0.6311,
        0.6123, 0.6283, 0.6461, 0.6471, 0.6489, 0.6524, 0.6308, 0.6503, 0.6371,
        0.6532, 0.6317, 0.6454, 0.6401, 0.6691, 0.6658, 0.6343, 0.6595, 0.6540,
        0.6545, 0.6549, 0.6539, 0.6494, 0.6347, 0.6549, 0.6213, 0.6196, 0.6217,
        0.6528, 0.6438, 0.6583, 0.6198, 0.6441, 0.6573, 0.6465, 0.6666, 0.6498,
        0.6135, 0.6461, 0.6234, 0.6558, 0.6034, 0.6454, 0.6439, 0.6337, 0.6730,
        0.6468, 0.6649, 0.6766, 0.6575, 0.6615, 0.6525, 0.6643, 0.6501, 0.6452,
        0.6614, 0.6294, 0.6371, 0.6387, 0.6408, 0.6437, 0.6343, 0.6451, 0.6779,
        0.6499, 0.6608, 0.6572, 0.6453, 0.6475, 0.6672, 0.6572, 0.6523, 0.6343,
        0.6592, 0.6303, 0.6531, 0.6657, 0.6080, 0.6249, 0.6487, 0.6110, 0.6437,
        0.6289, 0.6334, 0.6449, 0.6650, 0.6299, 0.6528, 0.6342, 0.6627, 0.6222,
        0.6533, 0.6518, 0.6458, 0.6471, 0.6519, 0.6571, 0.6177, 0.6322, 0.6528,
        0.6501, 0.6363, 0.6472, 0.6600, 0.6488, 0.6584, 0.6449, 0.6549, 0.6343,
        0.6380, 0.6466, 0.6556, 0.6718, 0.6445, 0.6631, 0.6610, 0.6349, 0.6366,
        0.6405, 0.6330, 0.6518, 0.6472, 0.6257, 0.6407, 0.6321, 0.6369, 0.6774,
        0.6760, 0.6728, 0.6463, 0.6430, 0.6169, 0.6344, 0.6395, 0.6758, 0.6423,
        0.6485, 0.6799, 0.6729, 0.6514, 0.6561, 0.6425, 0.6568, 0.6436, 0.6266,
        0.6250, 0.6530, 0.6363, 0.6595, 0.6379, 0.6400, 0.6565, 0.6446, 0.6424,
        0.6485, 0.6692, 0.6306, 0.6361, 0.6442, 0.6299, 0.6050, 0.6533, 0.6522,
        0.6013, 0.6359, 0.6680, 0.6494, 0.6568, 0.6243, 0.6588, 0.6579, 0.6525,
        0.6676, 0.6680, 0.6339, 0.6380, 0.6338], grad_fn=<IndexBackward0>)
pred_loss tensor([-0.], grad_fn=<MulBackward0>)
size_loss tensor(-1.5950, grad_fn=<MulBackward0>)
size_num_loss 37.800000000000004
loss: tensor([169.3670], grad_fn=<AddBackward0>)
4
epoch:  4 ; loss:  169.3670196533203 ; pred:  tensor([1.1896e-17, 8.6248e-12, 1.0000e+00, 3.3101e-13],
       grad_fn=<SoftmaxBackward0>)
num_high 225 len(mask) 410
mask_without_small tensor([0.6548, 0.6475, 0.6377, 0.5874, 0.6341, 0.6022, 0.6221, 0.5959, 0.6103,
        0.6502, 0.6163, 0.5993, 0.6107, 0.6135, 0.6100, 0.6355, 0.6501, 0.6201,
        0.6145, 0.6301, 0.6102, 0.6407, 0.6361, 0.6507, 0.6440, 0.6443, 0.6329,
        0.6449, 0.6189, 0.6235, 0.6186, 0.6371, 0.5996, 0.6083, 0.6191, 0.6513,
        0.6281, 0.6157, 0.6279, 0.6099, 0.5967, 0.6393, 0.6081, 0.6128, 0.6015,
        0.6582, 0.6022, 0.6147, 0.6075, 0.6118, 0.6241, 0.6315, 0.6147, 0.6426,
        0.6092, 0.6105, 0.5993, 0.6234, 0.6217, 0.6340, 0.6212, 0.6535, 0.6030,
        0.6458, 0.6468, 0.6370, 0.6598, 0.6315, 0.6286, 0.6195, 0.6052, 0.6440,
        0.6199, 0.6315, 0.6237, 0.6299, 0.6324, 0.6121, 0.5857, 0.6103, 0.6230,
        0.6172, 0.6004, 0.6130, 0.6317, 0.6315, 0.6417, 0.6237, 0.6352, 0.6148,
        0.6053, 0.6328, 0.5939, 0.6090, 0.6449, 0.6308, 0.5806, 0.6309, 0.6358,
        0.6233, 0.6334, 0.6325, 0.6405, 0.6153, 0.6197, 0.6353, 0.6295, 0.6258,
        0.6272, 0.6439, 0.6228, 0.6177, 0.5984, 0.6211, 0.6128, 0.6307, 0.6349,
        0.6243, 0.6163, 0.6316, 0.6226, 0.6268, 0.6250, 0.6355, 0.6410, 0.6285,
        0.6348, 0.6296, 0.6549, 0.6396, 0.5988, 0.6039, 0.6205, 0.6500, 0.6337,
        0.6324, 0.6417, 0.6231, 0.5925, 0.6382, 0.6166, 0.6399, 0.6114, 0.6334,
        0.6066, 0.6387, 0.6497, 0.6469, 0.6273, 0.6193, 0.6106, 0.6245, 0.6286,
        0.6389, 0.6150, 0.6494, 0.5811, 0.6158, 0.6028, 0.6363, 0.5909, 0.6266,
        0.6232, 0.6170, 0.6276, 0.6106, 0.6257, 0.6045, 0.5960, 0.6452, 0.6442,
        0.6237, 0.5969, 0.6354, 0.6357, 0.6565, 0.6234, 0.6248, 0.6094, 0.6193,
        0.6072, 0.5961, 0.6038, 0.6141, 0.6142, 0.5977, 0.5905, 0.6249, 0.6398,
        0.6135, 0.6345, 0.6346, 0.6523, 0.6074, 0.6388, 0.6172, 0.6032, 0.6288,
        0.6308, 0.6453, 0.6315, 0.6580, 0.6141, 0.6072, 0.6259, 0.6405, 0.6445,
        0.6304, 0.6092, 0.6057, 0.6146, 0.6129, 0.6254, 0.6301, 0.6203, 0.5838,
        0.6162, 0.6407, 0.5929, 0.6478, 0.6279, 0.6145, 0.6400, 0.6509, 0.6227,
        0.6505, 0.6254, 0.6051, 0.6133, 0.6242, 0.6294, 0.6559, 0.6216, 0.6077,
        0.5884, 0.6047, 0.6231, 0.6241, 0.6260, 0.6296, 0.6073, 0.6274, 0.6138,
        0.6305, 0.6082, 0.6224, 0.6169, 0.6471, 0.6436, 0.6109, 0.6370, 0.6313,
        0.6318, 0.6322, 0.6312, 0.6265, 0.6114, 0.6322, 0.5976, 0.5958, 0.5980,
        0.6300, 0.6207, 0.6358, 0.5960, 0.6210, 0.6347, 0.6235, 0.6445, 0.6269,
        0.5896, 0.6231, 0.5997, 0.6332, 0.5793, 0.6224, 0.6208, 0.6103, 0.6512,
        0.6238, 0.6426, 0.6550, 0.6349, 0.6391, 0.6297, 0.6420, 0.6273, 0.6222,
        0.6389, 0.6058, 0.6138, 0.6155, 0.6176, 0.6207, 0.6109, 0.6220, 0.6564,
        0.6270, 0.6383, 0.6346, 0.6223, 0.6245, 0.6451, 0.6346, 0.6295, 0.6109,
        0.6366, 0.6068, 0.6303, 0.6435, 0.5840, 0.6013, 0.6258, 0.5871, 0.6206,
        0.6054, 0.6100, 0.6219, 0.6428, 0.6064, 0.6300, 0.6108, 0.6403, 0.5985,
        0.6305, 0.6290, 0.6227, 0.6241, 0.6291, 0.6345, 0.5939, 0.6087, 0.6300,
        0.6272, 0.6130, 0.6242, 0.6375, 0.6258, 0.6359, 0.6218, 0.6322, 0.6110,
        0.6147, 0.6236, 0.6329, 0.6499, 0.6214, 0.6408, 0.6385, 0.6115, 0.6133,
        0.6173, 0.6096, 0.6290, 0.6243, 0.6020, 0.6175, 0.6087, 0.6136, 0.6559,
        0.6544, 0.6509, 0.6233, 0.6199, 0.5931, 0.6110, 0.6162, 0.6542, 0.6192,
        0.6256, 0.6585, 0.6511, 0.6286, 0.6335, 0.6194, 0.6342, 0.6205, 0.6031,
        0.6013, 0.6303, 0.6130, 0.6370, 0.6146, 0.6168, 0.6339, 0.6216, 0.6193,
        0.6256, 0.6472, 0.6071, 0.6128, 0.6211, 0.6063, 0.5809, 0.6306, 0.6294,
        0.5771, 0.6126, 0.6459, 0.6265, 0.6342, 0.6006, 0.6363, 0.6353, 0.6297,
        0.6455, 0.6459, 0.6105, 0.6147, 0.6105], grad_fn=<IndexBackward0>)
pred_loss tensor([-0.], grad_fn=<MulBackward0>)
size_loss tensor(-1.6512, grad_fn=<MulBackward0>)
size_num_loss 22.5
loss: tensor([47.0730], grad_fn=<AddBackward0>)
5
epoch:  5 ; loss:  47.073020935058594 ; pred:  tensor([1.4647e-16, 4.5815e-11, 1.0000e+00, 2.2253e-12],
       grad_fn=<SoftmaxBackward0>)
num_high 53 len(mask) 410
mask_without_small tensor([0.6367, 0.6279, 0.6168, 0.5637, 0.6128, 0.5789, 0.5998, 0.5725, 0.5874,
        0.6311, 0.5937, 0.5760, 0.5878, 0.5908, 0.5871, 0.6143, 0.6309, 0.5978,
        0.5918, 0.6084, 0.5873, 0.6201, 0.6150, 0.6317, 0.6239, 0.6242, 0.6115,
        0.6249, 0.5965, 0.6014, 0.5962, 0.6161, 0.5763, 0.5853, 0.5967, 0.6324,
        0.6063, 0.5931, 0.6060, 0.5870, 0.5733, 0.6186, 0.5851, 0.5900, 0.5782,
        0.6409, 0.5789, 0.5920, 0.5845, 0.5890, 0.6020, 0.6100, 0.5920, 0.6222,
        0.5863, 0.5877, 0.5760, 0.6012, 0.5995, 0.6127, 0.5989, 0.6350, 0.5798,
        0.6259, 0.6271, 0.6160, 0.6430, 0.6099, 0.6068, 0.5971, 0.5821, 0.6239,
        0.5976, 0.6100, 0.6016, 0.6082, 0.6109, 0.5893, 0.5620, 0.5874, 0.6008,
        0.5946, 0.5771, 0.5903, 0.6102, 0.6100, 0.6213, 0.6015, 0.6139, 0.5921,
        0.5822, 0.6114, 0.5704, 0.5861, 0.6249, 0.6092, 0.5567, 0.6093, 0.6147,
        0.6011, 0.6121, 0.6110, 0.6199, 0.5927, 0.5973, 0.6141, 0.6078, 0.6038,
        0.6053, 0.6238, 0.6006, 0.5952, 0.5751, 0.5988, 0.5901, 0.6091, 0.6136,
        0.6022, 0.5937, 0.6100, 0.6004, 0.6049, 0.6030, 0.6143, 0.6204, 0.6067,
        0.6135, 0.6079, 0.6368, 0.6189, 0.5754, 0.5808, 0.5982, 0.6308, 0.6123,
        0.6109, 0.6213, 0.6009, 0.5690, 0.6173, 0.5940, 0.6193, 0.5885, 0.6120,
        0.5835, 0.6179, 0.6305, 0.6272, 0.6054, 0.5969, 0.5877, 0.6025, 0.6068,
        0.6181, 0.5924, 0.6302, 0.5572, 0.5932, 0.5796, 0.6152, 0.5673, 0.6047,
        0.6011, 0.5945, 0.6057, 0.5877, 0.6037, 0.5814, 0.5725, 0.6253, 0.6241,
        0.6015, 0.5735, 0.6142, 0.6145, 0.6388, 0.6012, 0.6028, 0.5864, 0.5969,
        0.5842, 0.5727, 0.5807, 0.5914, 0.5915, 0.5743, 0.5669, 0.6029, 0.6191,
        0.5908, 0.6132, 0.6133, 0.6336, 0.5844, 0.6180, 0.5947, 0.5800, 0.6070,
        0.6092, 0.6253, 0.6100, 0.6407, 0.5914, 0.5842, 0.6039, 0.6199, 0.6244,
        0.6088, 0.5863, 0.5827, 0.5919, 0.5902, 0.6033, 0.6085, 0.5980, 0.5600,
        0.5936, 0.6201, 0.5694, 0.6283, 0.6061, 0.5918, 0.6193, 0.6319, 0.6005,
        0.6314, 0.6033, 0.5820, 0.5905, 0.6021, 0.6077, 0.6380, 0.5993, 0.5847,
        0.5647, 0.5816, 0.6009, 0.6020, 0.6040, 0.6079, 0.5843, 0.6055, 0.5911,
        0.6089, 0.5853, 0.6001, 0.5944, 0.6274, 0.6234, 0.5880, 0.6160, 0.6098,
        0.6102, 0.6107, 0.6096, 0.6046, 0.5885, 0.6107, 0.5742, 0.5724, 0.5747,
        0.6083, 0.5984, 0.6146, 0.5726, 0.5987, 0.6134, 0.6013, 0.6244, 0.6050,
        0.5660, 0.6009, 0.5764, 0.6118, 0.5554, 0.6002, 0.5984, 0.5875, 0.6323,
        0.6016, 0.6223, 0.6370, 0.6137, 0.6183, 0.6080, 0.6216, 0.6054, 0.6000,
        0.6182, 0.5828, 0.5911, 0.5928, 0.5951, 0.5983, 0.5880, 0.5998, 0.6387,
        0.6051, 0.6175, 0.6134, 0.6000, 0.6024, 0.6251, 0.6134, 0.6078, 0.5880,
        0.6156, 0.5838, 0.6087, 0.6233, 0.5602, 0.5780, 0.6038, 0.5634, 0.5982,
        0.5823, 0.5872, 0.5996, 0.6225, 0.5833, 0.6083, 0.5880, 0.6197, 0.5751,
        0.6089, 0.6072, 0.6005, 0.6020, 0.6073, 0.6132, 0.5704, 0.5858, 0.6084,
        0.6053, 0.5903, 0.6021, 0.6165, 0.6039, 0.6147, 0.5996, 0.6107, 0.5881,
        0.5920, 0.6015, 0.6115, 0.6307, 0.5991, 0.6202, 0.6177, 0.5887, 0.5905,
        0.5948, 0.5866, 0.6073, 0.6022, 0.5788, 0.5950, 0.5858, 0.5909, 0.6380,
        0.6362, 0.6320, 0.6011, 0.5975, 0.5695, 0.5882, 0.5936, 0.6359, 0.5968,
        0.6036, 0.6414, 0.6322, 0.6068, 0.6121, 0.5970, 0.6129, 0.5981, 0.5799,
        0.5781, 0.6086, 0.5902, 0.6160, 0.5920, 0.5943, 0.6125, 0.5993, 0.5969,
        0.6036, 0.6276, 0.5841, 0.5900, 0.5988, 0.5833, 0.5571, 0.6089, 0.6077,
        0.5532, 0.5898, 0.6260, 0.6046, 0.6129, 0.5774, 0.6152, 0.6141, 0.6080,
        0.6255, 0.6261, 0.5876, 0.5921, 0.5876], grad_fn=<IndexBackward0>)
pred_loss tensor([-0.], grad_fn=<MulBackward0>)
size_loss tensor(-1.7775, grad_fn=<MulBackward0>)
size_num_loss 5.300000000000001
loss: tensor([28.8617], grad_fn=<AddBackward0>)
6
epoch:  6 ; loss:  28.861682891845703 ; pred:  tensor([1.4793e-15, 2.1373e-10, 1.0000e+00, 1.2870e-11],
       grad_fn=<SoftmaxBackward0>)
num_high 10 len(mask) 410
mask_without_small tensor([0.6224, 0.6108, 0.5974, 0.5403, 0.5927, 0.5561, 0.5784, 0.5494, 0.5650,
        0.6149, 0.5717, 0.5530, 0.5654, 0.5686, 0.5647, 0.5945, 0.6147, 0.5761,
        0.5697, 0.5878, 0.5649, 0.6013, 0.5953, 0.6157, 0.6059, 0.6063, 0.5913,
        0.6072, 0.5747, 0.5800, 0.5744, 0.5965, 0.5534, 0.5628, 0.5749, 0.6167,
        0.5854, 0.5711, 0.5852, 0.5646, 0.5502, 0.5995, 0.5626, 0.5678, 0.5554,
        0.6283, 0.5561, 0.5699, 0.5620, 0.5667, 0.5807, 0.5896, 0.5699, 0.6038,
        0.5638, 0.5653, 0.5530, 0.5799, 0.5780, 0.5927, 0.5773, 0.6201, 0.5570,
        0.6083, 0.6098, 0.5965, 0.6313, 0.5895, 0.5860, 0.5754, 0.5594, 0.6058,
        0.5759, 0.5896, 0.5803, 0.5876, 0.5906, 0.5670, 0.5385, 0.5650, 0.5794,
        0.5727, 0.5542, 0.5681, 0.5898, 0.5896, 0.6027, 0.5802, 0.5941, 0.5700,
        0.5595, 0.5912, 0.5472, 0.5636, 0.6072, 0.5887, 0.5331, 0.5888, 0.5949,
        0.5797, 0.5919, 0.5908, 0.6010, 0.5706, 0.5756, 0.5943, 0.5871, 0.5827,
        0.5844, 0.6057, 0.5792, 0.5734, 0.5521, 0.5772, 0.5678, 0.5886, 0.5937,
        0.5810, 0.5718, 0.5896, 0.5789, 0.5839, 0.5818, 0.5945, 0.6017, 0.5859,
        0.5936, 0.5873, 0.6226, 0.5998, 0.5524, 0.5580, 0.5766, 0.6145, 0.5922,
        0.5906, 0.6027, 0.5795, 0.5457, 0.5979, 0.5720, 0.6003, 0.5662, 0.5919,
        0.5609, 0.5987, 0.6141, 0.6099, 0.5845, 0.5752, 0.5653, 0.5812, 0.5860,
        0.5989, 0.5703, 0.6138, 0.5336, 0.5712, 0.5568, 0.5955, 0.5440, 0.5836,
        0.5797, 0.5726, 0.5848, 0.5654, 0.5826, 0.5587, 0.5494, 0.6076, 0.6061,
        0.5802, 0.5504, 0.5944, 0.5948, 0.6253, 0.5799, 0.5815, 0.5640, 0.5752,
        0.5617, 0.5496, 0.5579, 0.5693, 0.5693, 0.5513, 0.5436, 0.5817, 0.6001,
        0.5686, 0.5933, 0.5934, 0.6182, 0.5618, 0.5988, 0.5727, 0.5572, 0.5862,
        0.5886, 0.6076, 0.5896, 0.6279, 0.5693, 0.5617, 0.5828, 0.6011, 0.6065,
        0.5883, 0.5638, 0.5600, 0.5698, 0.5680, 0.5822, 0.5879, 0.5763, 0.5365,
        0.5716, 0.6013, 0.5462, 0.6113, 0.5852, 0.5697, 0.6003, 0.6160, 0.5791,
        0.6154, 0.5822, 0.5593, 0.5683, 0.5808, 0.5870, 0.6242, 0.5778, 0.5621,
        0.5414, 0.5589, 0.5795, 0.5807, 0.5829, 0.5872, 0.5617, 0.5846, 0.5690,
        0.5883, 0.5628, 0.5787, 0.5724, 0.6103, 0.6053, 0.5657, 0.5964, 0.5893,
        0.5899, 0.5904, 0.5892, 0.5835, 0.5662, 0.5904, 0.5511, 0.5493, 0.5517,
        0.5877, 0.5768, 0.5949, 0.5495, 0.5771, 0.5935, 0.5799, 0.6065, 0.5840,
        0.5426, 0.5795, 0.5535, 0.5916, 0.5318, 0.5787, 0.5768, 0.5651, 0.6165,
        0.5803, 0.6039, 0.6227, 0.5938, 0.5992, 0.5873, 0.6030, 0.5844, 0.5785,
        0.5990, 0.5601, 0.5689, 0.5708, 0.5732, 0.5767, 0.5657, 0.5783, 0.6252,
        0.5841, 0.5982, 0.5934, 0.5786, 0.5812, 0.6074, 0.5935, 0.5871, 0.5657,
        0.5960, 0.5612, 0.5881, 0.6051, 0.5367, 0.5552, 0.5827, 0.5400, 0.5766,
        0.5597, 0.5647, 0.5781, 0.6041, 0.5607, 0.5877, 0.5656, 0.6008, 0.5521,
        0.5884, 0.5865, 0.5791, 0.5807, 0.5866, 0.5933, 0.5472, 0.5633, 0.5878,
        0.5844, 0.5680, 0.5808, 0.5971, 0.5827, 0.5950, 0.5781, 0.5904, 0.5658,
        0.5699, 0.5802, 0.5913, 0.6144, 0.5776, 0.6014, 0.5984, 0.5664, 0.5683,
        0.5729, 0.5642, 0.5866, 0.5809, 0.5560, 0.5731, 0.5633, 0.5687, 0.6242,
        0.6217, 0.6161, 0.5797, 0.5758, 0.5463, 0.5659, 0.5717, 0.6213, 0.5750,
        0.5824, 0.6289, 0.6163, 0.5860, 0.5920, 0.5753, 0.5929, 0.5765, 0.5571,
        0.5552, 0.5880, 0.5680, 0.5964, 0.5699, 0.5723, 0.5925, 0.5778, 0.5752,
        0.5824, 0.6104, 0.5615, 0.5678, 0.5773, 0.5607, 0.5335, 0.5884, 0.5870,
        0.5295, 0.5676, 0.6085, 0.5835, 0.5928, 0.5545, 0.5955, 0.5943, 0.5873,
        0.6079, 0.6086, 0.5652, 0.5700, 0.5652], grad_fn=<IndexBackward0>)
pred_loss tensor([-0.], grad_fn=<MulBackward0>)
size_loss tensor(-1.9809, grad_fn=<MulBackward0>)
size_num_loss 1.0
loss: tensor([23.5156], grad_fn=<AddBackward0>)
7
epoch:  7 ; loss:  23.515625 ; pred:  tensor([1.2350e-14, 8.7973e-10, 1.0000e+00, 6.4373e-11],
       grad_fn=<SoftmaxBackward0>)
num_high 1 len(mask) 410
mask_without_small tensor([0.6123, 0.5964, 0.5795, 0.5173, 0.5740, 0.5337, 0.5576, 0.5267, 0.5431,
        0.6019, 0.5503, 0.5305, 0.5436, 0.5469, 0.5428, 0.5760, 0.6016, 0.5551,
        0.5482, 0.5683, 0.5430, 0.5843, 0.5770, 0.6030, 0.5900, 0.5905, 0.5723,
        0.5917, 0.5536, 0.5594, 0.5532, 0.5785, 0.5308, 0.5407, 0.5538, 0.6043,
        0.5655, 0.5497, 0.5652, 0.5426, 0.5276, 0.5820, 0.5406, 0.5461, 0.5329,
        0.6205, 0.5337, 0.5484, 0.5399, 0.5450, 0.5602, 0.5703, 0.5484, 0.5875,
        0.5419, 0.5434, 0.5305, 0.5593, 0.5572, 0.5739, 0.5564, 0.6090, 0.5347,
        0.5932, 0.5951, 0.5784, 0.6248, 0.5702, 0.5662, 0.5543, 0.5372, 0.5900,
        0.5549, 0.5702, 0.5597, 0.5680, 0.5715, 0.5453, 0.5154, 0.5431, 0.5588,
        0.5514, 0.5317, 0.5464, 0.5705, 0.5703, 0.5860, 0.5596, 0.5756, 0.5485,
        0.5373, 0.5722, 0.5245, 0.5416, 0.5917, 0.5693, 0.5099, 0.5694, 0.5766,
        0.5591, 0.5730, 0.5717, 0.5840, 0.5491, 0.5546, 0.5758, 0.5675, 0.5624,
        0.5643, 0.5898, 0.5585, 0.5521, 0.5295, 0.5563, 0.5461, 0.5692, 0.5751,
        0.5605, 0.5504, 0.5703, 0.5582, 0.5638, 0.5614, 0.5761, 0.5847, 0.5660,
        0.5750, 0.5676, 0.6124, 0.5825, 0.5298, 0.5357, 0.5556, 0.6014, 0.5734,
        0.5715, 0.5860, 0.5589, 0.5229, 0.5802, 0.5507, 0.5831, 0.5444, 0.5729,
        0.5388, 0.5811, 0.6008, 0.5953, 0.5644, 0.5541, 0.5435, 0.5608, 0.5662,
        0.5813, 0.5488, 0.6003, 0.5104, 0.5498, 0.5344, 0.5773, 0.5211, 0.5635,
        0.5591, 0.5513, 0.5648, 0.5435, 0.5623, 0.5364, 0.5267, 0.5922, 0.5903,
        0.5596, 0.5278, 0.5759, 0.5764, 0.6164, 0.5593, 0.5611, 0.5420, 0.5541,
        0.5396, 0.5269, 0.5356, 0.5477, 0.5477, 0.5286, 0.5206, 0.5613, 0.5828,
        0.5470, 0.5746, 0.5747, 0.6064, 0.5398, 0.5812, 0.5515, 0.5348, 0.5664,
        0.5692, 0.5922, 0.5703, 0.6200, 0.5477, 0.5396, 0.5625, 0.5840, 0.5908,
        0.5688, 0.5419, 0.5378, 0.5482, 0.5463, 0.5619, 0.5683, 0.5554, 0.5134,
        0.5502, 0.5843, 0.5234, 0.5971, 0.5653, 0.5481, 0.5831, 0.6033, 0.5584,
        0.6025, 0.5619, 0.5371, 0.5467, 0.5603, 0.5674, 0.6148, 0.5570, 0.5401,
        0.5184, 0.5367, 0.5589, 0.5602, 0.5627, 0.5676, 0.5396, 0.5646, 0.5473,
        0.5688, 0.5407, 0.5579, 0.5511, 0.5957, 0.5893, 0.5438, 0.5783, 0.5700,
        0.5706, 0.5712, 0.5698, 0.5634, 0.5444, 0.5712, 0.5285, 0.5265, 0.5290,
        0.5681, 0.5559, 0.5765, 0.5268, 0.5562, 0.5749, 0.5594, 0.5908, 0.5639,
        0.5197, 0.5589, 0.5309, 0.5727, 0.5085, 0.5580, 0.5559, 0.5432, 0.6040,
        0.5598, 0.5875, 0.6127, 0.5752, 0.5817, 0.5677, 0.5865, 0.5644, 0.5577,
        0.5814, 0.5379, 0.5473, 0.5493, 0.5519, 0.5558, 0.5438, 0.5575, 0.6161,
        0.5641, 0.5805, 0.5748, 0.5578, 0.5607, 0.5919, 0.5748, 0.5675, 0.5438,
        0.5778, 0.5390, 0.5686, 0.5890, 0.5136, 0.5327, 0.5624, 0.5169, 0.5557,
        0.5374, 0.5428, 0.5573, 0.5878, 0.5386, 0.5681, 0.5438, 0.5837, 0.5295,
        0.5689, 0.5667, 0.5584, 0.5602, 0.5669, 0.5746, 0.5244, 0.5413, 0.5682,
        0.5643, 0.5464, 0.5603, 0.5791, 0.5625, 0.5766, 0.5573, 0.5713, 0.5439,
        0.5484, 0.5596, 0.5723, 0.6012, 0.5567, 0.5845, 0.5808, 0.5446, 0.5467,
        0.5516, 0.5423, 0.5668, 0.5604, 0.5336, 0.5518, 0.5413, 0.5471, 0.6148,
        0.6112, 0.6035, 0.5591, 0.5548, 0.5235, 0.5440, 0.5503, 0.6107, 0.5539,
        0.5621, 0.6215, 0.6038, 0.5661, 0.5731, 0.5542, 0.5741, 0.5556, 0.5347,
        0.5328, 0.5685, 0.5463, 0.5783, 0.5483, 0.5510, 0.5737, 0.5569, 0.5541,
        0.5621, 0.5959, 0.5394, 0.5461, 0.5564, 0.5385, 0.5103, 0.5689, 0.5673,
        0.5062, 0.5459, 0.5934, 0.5634, 0.5741, 0.5320, 0.5772, 0.5758, 0.5677,
        0.5926, 0.5935, 0.5433, 0.5485, 0.5433], grad_fn=<IndexBackward0>)
pred_loss tensor([-0.], grad_fn=<MulBackward0>)
size_loss tensor(-2.2686, grad_fn=<MulBackward0>)
size_num_loss 0.1
loss: tensor([21.5293], grad_fn=<AddBackward0>)
8
epoch:  8 ; loss:  21.529260635375977 ; pred:  tensor([8.6259e-14, 3.2107e-09, 1.0000e+00, 2.7993e-10],
       grad_fn=<SoftmaxBackward0>)
num_high 1 len(mask) 410
mask_without_small tensor([0.6063, 0.5848, 0.5632, 0.5564, 0.5118, 0.5376, 0.5044, 0.5217, 0.5921,
        0.5296, 0.5084, 0.5223, 0.5259, 0.5214, 0.5589, 0.5918, 0.5349, 0.5272,
        0.5497, 0.5216, 0.5691, 0.5601, 0.5936, 0.5764, 0.5771, 0.5545, 0.5786,
        0.5332, 0.5396, 0.5327, 0.5619, 0.5088, 0.5192, 0.5334, 0.5954, 0.5466,
        0.5288, 0.5462, 0.5213, 0.5054, 0.5663, 0.5191, 0.5250, 0.5110, 0.6177,
        0.5118, 0.5274, 0.5184, 0.5238, 0.5405, 0.5521, 0.5274, 0.5731, 0.5204,
        0.5221, 0.5084, 0.5395, 0.5371, 0.5563, 0.5363, 0.6019, 0.5128, 0.5805,
        0.5830, 0.5618, 0.6236, 0.5520, 0.5473, 0.5340, 0.5154, 0.5763, 0.5346,
        0.5520, 0.5400, 0.5494, 0.5535, 0.5241, 0.5218, 0.5389, 0.5308, 0.5097,
        0.5253, 0.5524, 0.5521, 0.5713, 0.5399, 0.5584, 0.5276, 0.5156, 0.5543,
        0.5021, 0.5202, 0.5786, 0.5509, 0.5511, 0.5596, 0.5393, 0.5553, 0.5537,
        0.5687, 0.5283, 0.5343, 0.5586, 0.5488, 0.5430, 0.5452, 0.5762, 0.5386,
        0.5316, 0.5073, 0.5362, 0.5250, 0.5508, 0.5578, 0.5408, 0.5296, 0.5522,
        0.5383, 0.5446, 0.5418, 0.5590, 0.5697, 0.5471, 0.5577, 0.5490, 0.6066,
        0.5668, 0.5078, 0.5139, 0.5354, 0.5915, 0.5557, 0.5535, 0.5713, 0.5391,
        0.5005, 0.5640, 0.5299, 0.5675, 0.5231, 0.5552, 0.5171, 0.5651, 0.5907,
        0.5833, 0.5453, 0.5337, 0.5222, 0.5411, 0.5473, 0.5654, 0.5279, 0.5901,
        0.5290, 0.5126, 0.5604, 0.5442, 0.5392, 0.5306, 0.5457, 0.5222, 0.5429,
        0.5146, 0.5045, 0.5793, 0.5768, 0.5399, 0.5056, 0.5587, 0.5593, 0.6120,
        0.5395, 0.5415, 0.5206, 0.5337, 0.5180, 0.5047, 0.5138, 0.5267, 0.5268,
        0.5065, 0.5417, 0.5672, 0.5260, 0.5572, 0.5574, 0.5983, 0.5182, 0.5652,
        0.5308, 0.5130, 0.5476, 0.5508, 0.5793, 0.5521, 0.6171, 0.5267, 0.5180,
        0.5432, 0.5688, 0.5774, 0.5503, 0.5204, 0.5161, 0.5273, 0.5252, 0.5424,
        0.5498, 0.5351, 0.5294, 0.5692, 0.5010, 0.5857, 0.5463, 0.5272, 0.5676,
        0.5941, 0.5385, 0.5930, 0.5424, 0.5153, 0.5256, 0.5406, 0.5487, 0.6098,
        0.5369, 0.5185, 0.5149, 0.5390, 0.5405, 0.5434, 0.5489, 0.5180, 0.5455,
        0.5263, 0.5504, 0.5192, 0.5380, 0.5304, 0.5838, 0.5755, 0.5225, 0.5617,
        0.5517, 0.5525, 0.5532, 0.5515, 0.5441, 0.5232, 0.5531, 0.5064, 0.5043,
        0.5069, 0.5496, 0.5357, 0.5595, 0.5045, 0.5360, 0.5575, 0.5396, 0.5774,
        0.5447, 0.5390, 0.5089, 0.5549, 0.5380, 0.5357, 0.5219, 0.5950, 0.5400,
        0.5732, 0.6069, 0.5579, 0.5658, 0.5491, 0.5718, 0.5453, 0.5377, 0.5655,
        0.5163, 0.5263, 0.5285, 0.5313, 0.5356, 0.5225, 0.5375, 0.6117, 0.5449,
        0.5643, 0.5574, 0.5379, 0.5411, 0.5789, 0.5575, 0.5488, 0.5226, 0.5611,
        0.5174, 0.5501, 0.5751, 0.5107, 0.5430, 0.5354, 0.5157, 0.5215, 0.5373,
        0.5736, 0.5169, 0.5496, 0.5225, 0.5683, 0.5074, 0.5504, 0.5479, 0.5385,
        0.5405, 0.5481, 0.5572, 0.5021, 0.5198, 0.5496, 0.5452, 0.5253, 0.5406,
        0.5627, 0.5431, 0.5596, 0.5372, 0.5532, 0.5226, 0.5275, 0.5398, 0.5544,
        0.5913, 0.5366, 0.5693, 0.5647, 0.5234, 0.5256, 0.5309, 0.5209, 0.5480,
        0.5407, 0.5116, 0.5312, 0.5198, 0.5261, 0.6098, 0.6049, 0.5943, 0.5393,
        0.5345, 0.5011, 0.5228, 0.5295, 0.6042, 0.5335, 0.5427, 0.6190, 0.5948,
        0.5473, 0.5554, 0.5338, 0.5566, 0.5353, 0.5129, 0.5108, 0.5500, 0.5252,
        0.5617, 0.5274, 0.5303, 0.5561, 0.5368, 0.5337, 0.5427, 0.5841, 0.5178,
        0.5250, 0.5363, 0.5169, 0.5505, 0.5486, 0.5247, 0.5808, 0.5441, 0.5566,
        0.5100, 0.5604, 0.5587, 0.5491, 0.5798, 0.5810, 0.5220, 0.5275, 0.5220],
       grad_fn=<IndexBackward0>)
pred_loss tensor([-0.], grad_fn=<MulBackward0>)
size_loss tensor(-250.5398, grad_fn=<MulBackward0>)
size_num_loss 0.1
loss: tensor([-247.5949], grad_fn=<AddBackward0>)
9
epoch:  9 ; loss:  -247.59494018554688 ; pred:  tensor([5.0581e-13, 1.0424e-08, 1.0000e+00, 1.0646e-09],
       grad_fn=<SoftmaxBackward0>)
num_high 6 len(mask) 410
mask_without_small tensor([0.6178, 0.5963, 0.5740, 0.5664, 0.5215, 0.5079, 0.6036, 0.5152, 0.5084,
        0.5118, 0.5076, 0.5693, 0.6033, 0.5197, 0.5131, 0.5564, 0.5078, 0.5802,
        0.5706, 0.6051, 0.5877, 0.5884, 0.5639, 0.5899, 0.5183, 0.5225, 0.5180,
        0.5726, 0.5055, 0.5185, 0.6069, 0.5445, 0.5145, 0.5419, 0.5075, 0.5773,
        0.5053, 0.5110, 0.6291, 0.5133, 0.5047, 0.5098, 0.5227, 0.5606, 0.5133,
        0.5844, 0.5067, 0.5082, 0.5225, 0.5213, 0.5663, 0.5207, 0.6133, 0.5919,
        0.5945, 0.5725, 0.6349, 0.5605, 0.5486, 0.5190, 0.5018, 0.5877, 0.5194,
        0.5605, 0.5226, 0.5555, 0.5626, 0.5102, 0.5079, 0.5222, 0.5163, 0.5113,
        0.5610, 0.5605, 0.5825, 0.5226, 0.5687, 0.5134, 0.5019, 0.5637, 0.5064,
        0.5899, 0.5587, 0.5589, 0.5700, 0.5224, 0.5650, 0.5629, 0.5798, 0.5140,
        0.5192, 0.5690, 0.5541, 0.5222, 0.5317, 0.5875, 0.5221, 0.5169, 0.5206,
        0.5110, 0.5584, 0.5680, 0.5227, 0.5153, 0.5607, 0.5219, 0.5261, 0.5226,
        0.5694, 0.5808, 0.5477, 0.5678, 0.5546, 0.6180, 0.5779, 0.5003, 0.5201,
        0.6029, 0.5655, 0.5626, 0.5825, 0.5223, 0.5749, 0.5155, 0.5786, 0.5092,
        0.5649, 0.5035, 0.5760, 0.6022, 0.5947, 0.5329, 0.5187, 0.5083, 0.5227,
        0.5489, 0.5763, 0.5137, 0.6015, 0.5147, 0.5710, 0.5242, 0.5224, 0.5161,
        0.5376, 0.5083, 0.5222, 0.5011, 0.5906, 0.5881, 0.5226, 0.5691, 0.5697,
        0.6234, 0.5225, 0.5227, 0.5068, 0.5188, 0.5043, 0.5002, 0.5126, 0.5126,
        0.5226, 0.5782, 0.5119, 0.5673, 0.5675, 0.6097, 0.5045, 0.5761, 0.5163,
        0.5499, 0.5584, 0.5907, 0.5606, 0.6284, 0.5126, 0.5043, 0.5222, 0.5799,
        0.5888, 0.5575, 0.5067, 0.5025, 0.5131, 0.5112, 0.5224, 0.5564, 0.5199,
        0.5151, 0.5803, 0.5972, 0.5427, 0.5130, 0.5787, 0.6055, 0.5220, 0.6044,
        0.5224, 0.5017, 0.5116, 0.5227, 0.5537, 0.6212, 0.5211, 0.5048, 0.5013,
        0.5223, 0.5227, 0.5223, 0.5544, 0.5043, 0.5348, 0.5122, 0.5576, 0.5055,
        0.5218, 0.5160, 0.5953, 0.5868, 0.5087, 0.5724, 0.5600, 0.5612, 0.5622,
        0.5597, 0.5236, 0.5093, 0.5621, 0.5559, 0.5203, 0.5700, 0.5205, 0.5677,
        0.5225, 0.5888, 0.5272, 0.5223, 0.5645, 0.5218, 0.5203, 0.5080, 0.6065,
        0.5226, 0.5844, 0.6183, 0.5681, 0.5768, 0.5548, 0.5831, 0.5328, 0.5216,
        0.5765, 0.5026, 0.5122, 0.5142, 0.5168, 0.5202, 0.5087, 0.5215, 0.6230,
        0.5289, 0.5752, 0.5676, 0.5217, 0.5227, 0.5903, 0.5676, 0.5540, 0.5087,
        0.5717, 0.5038, 0.5571, 0.5864, 0.5222, 0.5201, 0.5021, 0.5077, 0.5214,
        0.5849, 0.5033, 0.5560, 0.5086, 0.5794, 0.5578, 0.5513, 0.5220, 0.5227,
        0.5519, 0.5673, 0.5061, 0.5561, 0.5320, 0.5113, 0.5227, 0.5735, 0.5222,
        0.5701, 0.5213, 0.5623, 0.5088, 0.5133, 0.5226, 0.5639, 0.6027, 0.5209,
        0.5804, 0.5756, 0.5094, 0.5115, 0.5164, 0.5071, 0.5517, 0.5227, 0.5166,
        0.5061, 0.5120, 0.6212, 0.6163, 0.6058, 0.5224, 0.5194, 0.5089, 0.5151,
        0.6156, 0.5186, 0.5223, 0.6303, 0.6062, 0.5485, 0.5651, 0.5189, 0.5666,
        0.5200, 0.5569, 0.5112, 0.5724, 0.5132, 0.5158, 0.5660, 0.5211, 0.5187,
        0.5223, 0.5955, 0.5041, 0.5110, 0.5207, 0.5032, 0.5578, 0.5534, 0.5107,
        0.5922, 0.5236, 0.5666, 0.5709, 0.5690, 0.5548, 0.5912, 0.5923, 0.5082,
        0.5134, 0.5082], grad_fn=<IndexBackward0>)
pred_loss tensor([-0.], grad_fn=<MulBackward0>)
size_loss tensor(-342.3679, grad_fn=<MulBackward0>)
size_num_loss 0.6000000000000001
loss: tensor([-339.1092], grad_fn=<AddBackward0>)
10
epoch:  10 ; loss:  -339.10919189453125 ; pred:  tensor([8.3604e-13, 1.4345e-08, 1.0000e+00, 1.5413e-09],
       grad_fn=<SoftmaxBackward0>)
num_high 15 len(mask) 410
mask_without_small tensor([0.6329, 0.6116, 0.5892, 0.5811, 0.5050, 0.6188, 0.5842, 0.6185, 0.5028,
        0.5696, 0.5955, 0.5856, 0.6203, 0.6031, 0.6037, 0.5784, 0.6053, 0.5013,
        0.5065, 0.5009, 0.5877, 0.5015, 0.6221, 0.5418, 0.5307, 0.5925, 0.6440,
        0.5070, 0.5746, 0.5997, 0.5064, 0.5047, 0.5810, 0.5040, 0.6285, 0.6072,
        0.6098, 0.5876, 0.6497, 0.5745, 0.5581, 0.5020, 0.6030, 0.5025, 0.5745,
        0.5067, 0.5686, 0.5769, 0.5060, 0.5751, 0.5746, 0.5978, 0.5067, 0.5836,
        0.5781, 0.6053, 0.5724, 0.5726, 0.5850, 0.5063, 0.5796, 0.5773, 0.5951,
        0.5023, 0.5839, 0.5667, 0.5078, 0.5179, 0.6028, 0.5058, 0.5039, 0.5720,
        0.5829, 0.5072, 0.5747, 0.5056, 0.5123, 0.5075, 0.5843, 0.5961, 0.5559,
        0.5827, 0.5673, 0.6331, 0.5931, 0.5032, 0.6182, 0.5802, 0.5770, 0.5978,
        0.5061, 0.5900, 0.5939, 0.5795, 0.5912, 0.6175, 0.6100, 0.5191, 0.5018,
        0.5073, 0.5586, 0.5915, 0.6168, 0.5860, 0.5104, 0.5062, 0.5238, 0.5078,
        0.6060, 0.6034, 0.5067, 0.5840, 0.5847, 0.6384, 0.5064, 0.5075, 0.5018,
        0.5075, 0.5935, 0.5821, 0.5823, 0.6249, 0.5914, 0.5607, 0.5721, 0.6060,
        0.5746, 0.6433, 0.5079, 0.5952, 0.6041, 0.5710, 0.5077, 0.5697, 0.5030,
        0.5956, 0.6125, 0.5331, 0.5940, 0.6208, 0.5057, 0.6197, 0.5077, 0.5071,
        0.5663, 0.6363, 0.5045, 0.5061, 0.5070, 0.5081, 0.5672, 0.5209, 0.5711,
        0.5053, 0.6106, 0.6021, 0.5875, 0.5740, 0.5753, 0.5765, 0.5736, 0.5097,
        0.5764, 0.5690, 0.5035, 0.5849, 0.5038, 0.5825, 0.5065, 0.6041, 0.5135,
        0.5061, 0.5790, 0.5054, 0.5035, 0.6217, 0.5068, 0.5998, 0.6334, 0.5830,
        0.5920, 0.5676, 0.5984, 0.5190, 0.5052, 0.5917, 0.5034, 0.5050, 0.6381,
        0.5151, 0.5904, 0.5824, 0.5052, 0.5073, 0.6056, 0.5824, 0.5666, 0.5868,
        0.5705, 0.6018, 0.5078, 0.5033, 0.5048, 0.6002, 0.5691, 0.5947, 0.5713,
        0.5629, 0.5057, 0.5070, 0.5638, 0.5821, 0.5693, 0.5182, 0.5071, 0.5886,
        0.5079, 0.5851, 0.5048, 0.5765, 0.5066, 0.5783, 0.6180, 0.5042, 0.5957,
        0.5908, 0.5634, 0.5071, 0.6363, 0.6314, 0.6210, 0.5063, 0.5025, 0.6307,
        0.5016, 0.5077, 0.6452, 0.6215, 0.5579, 0.5797, 0.5019, 0.5814, 0.5032,
        0.5702, 0.5875, 0.5806, 0.5044, 0.5018, 0.5077, 0.6109, 0.5040, 0.5714,
        0.5658, 0.6076, 0.5097, 0.5813, 0.5860, 0.5839, 0.5676, 0.6065, 0.6077],
       grad_fn=<IndexBackward0>)
pred_loss tensor([-0.], grad_fn=<MulBackward0>)
size_loss tensor(-440.7004, grad_fn=<MulBackward0>)
size_num_loss 1.5
loss: tensor([-437.0482], grad_fn=<AddBackward0>)
11
epoch:  11 ; loss:  -437.0482482910156 ; pred:  tensor([1.1798e-12, 1.7527e-08, 1.0000e+00, 1.9592e-09],
       grad_fn=<SoftmaxBackward0>)
num_high 43 len(mask) 410
mask_without_small tensor([0.6501, 0.6292, 0.6069, 0.5986, 0.6364, 0.6019, 0.6361, 0.5857, 0.6133,
        0.6033, 0.6378, 0.6208, 0.6215, 0.5957, 0.6230, 0.6054, 0.6396, 0.5297,
        0.5177, 0.6103, 0.6610, 0.5916, 0.6175, 0.5985, 0.6458, 0.6249, 0.6275,
        0.6053, 0.6666, 0.5915, 0.5574, 0.6208, 0.5915, 0.5845, 0.5941, 0.5922,
        0.5915, 0.6156, 0.6012, 0.5955, 0.6230, 0.5891, 0.5894, 0.6027, 0.5971,
        0.5945, 0.6129, 0.6015, 0.5819, 0.5030, 0.6206, 0.5886, 0.6005, 0.5917,
        0.6019, 0.6139, 0.5505, 0.6002, 0.5828, 0.6503, 0.6109, 0.6357, 0.5976,
        0.5942, 0.6156, 0.6078, 0.6117, 0.5969, 0.6090, 0.6350, 0.6277, 0.5044,
        0.5595, 0.6093, 0.6344, 0.6037, 0.5098, 0.6237, 0.6212, 0.6016, 0.6023,
        0.6555, 0.6113, 0.5996, 0.5998, 0.6423, 0.6091, 0.5674, 0.5888, 0.6237,
        0.5916, 0.6604, 0.6129, 0.6219, 0.5874, 0.5858, 0.6134, 0.6301, 0.5203,
        0.6117, 0.6383, 0.6372, 0.5812, 0.6535, 0.5826, 0.5065, 0.5876, 0.6282,
        0.6199, 0.6052, 0.5909, 0.5924, 0.5937, 0.5905, 0.5936, 0.5851, 0.6026,
        0.6001, 0.6218, 0.5964, 0.6392, 0.6175, 0.6507, 0.6006, 0.6098, 0.5832,
        0.6162, 0.5043, 0.6095, 0.6552, 0.6082, 0.5999, 0.6233, 0.6000, 0.5818,
        0.6045, 0.5869, 0.6195, 0.6180, 0.5852, 0.6125, 0.5878, 0.5745, 0.5767,
        0.5997, 0.5853, 0.5033, 0.6063, 0.6027, 0.5937, 0.5957, 0.6355, 0.6135,
        0.6086, 0.5758, 0.6535, 0.6487, 0.6385, 0.6480, 0.6622, 0.6390, 0.5566,
        0.5971, 0.5989, 0.5865, 0.6052, 0.5981, 0.6285, 0.5879, 0.5805, 0.6253,
        0.5988, 0.6036, 0.6016, 0.5832, 0.6242, 0.6254],
       grad_fn=<IndexBackward0>)
pred_loss tensor([-0.], grad_fn=<MulBackward0>)
size_loss tensor(-307.7659, grad_fn=<MulBackward0>)
size_num_loss 4.3
loss: tensor([-301.7151], grad_fn=<AddBackward0>)
12
epoch:  12 ; loss:  -301.7150573730469 ; pred:  tensor([1.4714e-12, 1.9699e-08, 1.0000e+00, 2.2705e-09],
       grad_fn=<SoftmaxBackward0>)
num_high 80 len(mask) 410
mask_without_small tensor([0.6688, 0.6483, 0.6241, 0.6095, 0.6553, 0.6161, 0.6551, 0.5800, 0.6319,
        0.6186, 0.6568, 0.6399, 0.6406, 0.6027, 0.6421, 0.6220, 0.6585, 0.5163,
        0.5038, 0.6285, 0.6793, 0.5924, 0.6364, 0.6092, 0.6646, 0.6441, 0.6466,
        0.6218, 0.6848, 0.5921, 0.5454, 0.6399, 0.5922, 0.5778, 0.5986, 0.5938,
        0.5923, 0.6345, 0.6148, 0.6020, 0.6421, 0.5866, 0.5873, 0.6176, 0.6059,
        0.5996, 0.6315, 0.6154, 0.5737, 0.6397, 0.5857, 0.6135, 0.5927, 0.6162,
        0.6326, 0.5383, 0.6130, 0.5750, 0.6690, 0.6292, 0.6547, 0.6073, 0.5987,
        0.6345, 0.6253, 0.6301, 0.6055, 0.6268, 0.6540, 0.6468, 0.5476, 0.6272,
        0.6534, 0.6193, 0.6428, 0.6403, 0.6157, 0.6170, 0.6740, 0.6296, 0.6117,
        0.6122, 0.6612, 0.6270, 0.5561, 0.5860, 0.6429, 0.5925, 0.6787, 0.6315,
        0.6410, 0.5832, 0.5802, 0.6320, 0.6492, 0.5065, 0.6301, 0.6572, 0.6562,
        0.5726, 0.6720, 0.5747, 0.5836, 0.6474, 0.6390, 0.6217, 0.5907, 0.5942,
        0.5975, 0.5897, 0.5972, 0.5788, 0.6174, 0.6126, 0.6410, 0.6043, 0.6581,
        0.6365, 0.6693, 0.6137, 0.6278, 0.5757, 0.6351, 0.6275, 0.6737, 0.6258,
        0.6124, 0.6424, 0.6124, 0.5735, 0.6206, 0.5822, 0.6386, 0.6370, 0.5790,
        0.6310, 0.5840, 0.5640, 0.5667, 0.6118, 0.5793, 0.6233, 0.6177, 0.5976,
        0.6025, 0.6545, 0.6322, 0.6263, 0.5656, 0.6720, 0.6674, 0.6574, 0.6667,
        0.6805, 0.6579, 0.5446, 0.6061, 0.6101, 0.5814, 0.6217, 0.6084, 0.6476,
        0.5842, 0.5716, 0.6444, 0.6100, 0.6192, 0.6156, 0.5756, 0.6434, 0.6445],
       grad_fn=<IndexBackward0>)
pred_loss tensor([-0.], grad_fn=<MulBackward0>)
size_loss tensor(-343.1847, grad_fn=<MulBackward0>)
size_num_loss 8.0
loss: tensor([-333.4499], grad_fn=<AddBackward0>)
13
epoch:  13 ; loss:  -333.4498596191406 ; pred:  tensor([2.0728e-12, 2.4192e-08, 1.0000e+00, 2.8940e-09],
       grad_fn=<SoftmaxBackward0>)
num_high 99 len(mask) 410
mask_without_small tensor([0.6882, 0.6683, 0.6418, 0.6133, 0.6752, 0.6279, 0.6749, 0.5671, 0.6515,
        0.6328, 0.6766, 0.6599, 0.6606, 0.5995, 0.6622, 0.6386, 0.6783, 0.6475,
        0.6984, 0.5832, 0.6564, 0.6128, 0.6842, 0.6641, 0.6666, 0.6383, 0.7037,
        0.5827, 0.5294, 0.6599, 0.5828, 0.5645, 0.5925, 0.5851, 0.5829, 0.6543,
        0.6251, 0.5982, 0.6622, 0.5754, 0.5762, 0.6308, 0.6057, 0.5941, 0.6510,
        0.6265, 0.5597, 0.6597, 0.5742, 0.6222, 0.5836, 0.6281, 0.6523, 0.5220,
        0.6211, 0.5613, 0.6884, 0.6484, 0.6746, 0.6086, 0.5927, 0.6543, 0.6435,
        0.6494, 0.6049, 0.6455, 0.6739, 0.6668, 0.5317, 0.6460, 0.6733, 0.6341,
        0.6629, 0.6603, 0.6270, 0.6297, 0.6933, 0.6489, 0.6183, 0.6193, 0.6809,
        0.6457, 0.5405, 0.5745, 0.6629, 0.5832, 0.6978, 0.6511, 0.6610, 0.5710,
        0.5674, 0.6516, 0.6692, 0.6495, 0.6770, 0.6760, 0.5585, 0.6914, 0.5609,
        0.5715, 0.6674, 0.6590, 0.6381, 0.5808, 0.5858, 0.5906, 0.5795, 0.5903,
        0.5657, 0.6305, 0.6203, 0.6610, 0.6027, 0.6779, 0.6565, 0.6887, 0.6226,
        0.6467, 0.5620, 0.6549, 0.6463, 0.6930, 0.6442, 0.6198, 0.6625, 0.6199,
        0.5595, 0.6363, 0.5699, 0.6586, 0.6569, 0.5659, 0.6505, 0.5720, 0.5489,
        0.5518, 0.6185, 0.5663, 0.6407, 0.6311, 0.5909, 0.5991, 0.6744, 0.6518,
        0.6449, 0.5506, 0.6913, 0.6869, 0.6773, 0.6862, 0.6996, 0.6777, 0.5286,
        0.6061, 0.6147, 0.5689, 0.6381, 0.6109, 0.6676, 0.5723, 0.5574, 0.6645,
        0.6144, 0.6339, 0.6267, 0.5619, 0.6634, 0.6646],
       grad_fn=<IndexBackward0>)
pred_loss tensor([-0.], grad_fn=<MulBackward0>)
size_loss tensor(-441.7998, grad_fn=<MulBackward0>)
size_num_loss 9.9
loss: tensor([-430.1707], grad_fn=<AddBackward0>)
14
epoch:  14 ; loss:  -430.170654296875 ; pred:  tensor([2.9651e-12, 3.0128e-08, 1.0000e+00, 3.7381e-09],
       grad_fn=<SoftmaxBackward0>)
num_high 104 len(mask) 410
mask_without_small tensor([0.7080, 0.6888, 0.6606, 0.6105, 0.6955, 0.6386, 0.6952, 0.5504, 0.6719,
        0.6474, 0.6968, 0.6806, 0.6812, 0.5892, 0.6828, 0.6564, 0.6984, 0.6675,
        0.7177, 0.5687, 0.6770, 0.6095, 0.7041, 0.6847, 0.6871, 0.6560, 0.7228,
        0.5681, 0.5104, 0.6805, 0.5682, 0.5474, 0.5800, 0.5709, 0.5684, 0.6748,
        0.6332, 0.5874, 0.6828, 0.5597, 0.5606, 0.6441, 0.5981, 0.5820, 0.6714,
        0.6358, 0.5422, 0.6803, 0.5583, 0.6274, 0.5691, 0.6391, 0.6727, 0.5029,
        0.6252, 0.5439, 0.7082, 0.6685, 0.6949, 0.6025, 0.5802, 0.6748, 0.6627,
        0.6696, 0.5969, 0.6651, 0.6942, 0.6874, 0.5127, 0.6658, 0.6936, 0.6496,
        0.6835, 0.6809, 0.6370, 0.6419, 0.7128, 0.6691, 0.6197, 0.6217, 0.7009,
        0.6654, 0.5219, 0.5587, 0.6835, 0.5687, 0.7172, 0.6715, 0.6817, 0.5547,
        0.5506, 0.6720, 0.6896, 0.6697, 0.6972, 0.6962, 0.5409, 0.7110, 0.5435,
        0.5553, 0.6879, 0.6796, 0.6557, 0.5659, 0.5717, 0.5777, 0.5643, 0.5772,
        0.5488, 0.6435, 0.6236, 0.6816, 0.5936, 0.6980, 0.6770, 0.7085, 0.6282,
        0.6666, 0.5447, 0.6755, 0.6661, 0.7126, 0.6636, 0.6225, 0.6831, 0.6229,
        0.5419, 0.6530, 0.5534, 0.6792, 0.6775, 0.5490, 0.6708, 0.5558, 0.5306,
        0.5337, 0.6200, 0.5494, 0.6592, 0.6445, 0.5780, 0.5887, 0.6947, 0.6722,
        0.6644, 0.5324, 0.7110, 0.7067, 0.6974, 0.7061, 0.7189, 0.6978, 0.5096,
        0.5986, 0.6130, 0.5523, 0.6557, 0.6064, 0.6881, 0.5562, 0.5397, 0.6850,
        0.6124, 0.6493, 0.6364, 0.5447, 0.6840, 0.6851],
       grad_fn=<IndexBackward0>)
pred_loss tensor([-0.], grad_fn=<MulBackward0>)
size_loss tensor(-595.6581, grad_fn=<MulBackward0>)
size_num_loss 10.4
loss: tensor([-583.5227], grad_fn=<AddBackward0>)
15
epoch:  15 ; loss:  -583.522705078125 ; pred:  tensor([4.2015e-12, 3.7413e-08, 1.0000e+00, 4.8016e-09],
       grad_fn=<SoftmaxBackward0>)
num_high 102 len(mask) 410
mask_without_small tensor([0.7276, 0.7093, 0.6804, 0.6022, 0.7157, 0.6497, 0.7154, 0.5309, 0.6927,
        0.6635, 0.7170, 0.7014, 0.7020, 0.5744, 0.7035, 0.6753, 0.7185, 0.6881,
        0.7368, 0.5507, 0.6978, 0.6008, 0.7239, 0.7054, 0.7077, 0.6748, 0.7416,
        0.5501, 0.7013, 0.5503, 0.5277, 0.5635, 0.5533, 0.5505, 0.6957, 0.6402,
        0.5722, 0.7035, 0.5409, 0.5419, 0.6585, 0.5853, 0.5659, 0.6921, 0.6450,
        0.5221, 0.7011, 0.5394, 0.6298, 0.5513, 0.6505, 0.6935, 0.6260, 0.5239,
        0.7278, 0.6891, 0.7151, 0.5911, 0.5638, 0.6957, 0.6828, 0.6903, 0.5838,
        0.6855, 0.7145, 0.7079, 0.6862, 0.7139, 0.6665, 0.7042, 0.7017, 0.6469,
        0.6552, 0.7322, 0.6897, 0.6165, 0.6199, 0.7209, 0.6858, 0.5010, 0.5398,
        0.7042, 0.5508, 0.7363, 0.6922, 0.7024, 0.5355, 0.5311, 0.6928, 0.7101,
        0.6904, 0.7173, 0.7164, 0.5207, 0.7304, 0.5235, 0.5361, 0.7084, 0.7004,
        0.6745, 0.5477, 0.5542, 0.5609, 0.5460, 0.5604, 0.5291, 0.6576, 0.6231,
        0.7024, 0.5797, 0.7181, 0.6979, 0.7280, 0.6314, 0.6871, 0.5248, 0.6963,
        0.6866, 0.7319, 0.6837, 0.6213, 0.7038, 0.6219, 0.5219, 0.6710, 0.5341,
        0.7001, 0.6984, 0.5294, 0.6915, 0.5367, 0.5100, 0.5132, 0.6170, 0.5298,
        0.6787, 0.6591, 0.5612, 0.5737, 0.7150, 0.6930, 0.6847, 0.5119, 0.7304,
        0.7264, 0.7176, 0.7258, 0.7379, 0.7179, 0.5860, 0.6058, 0.5329, 0.6745,
        0.5963, 0.7087, 0.5371, 0.5194, 0.7057, 0.6050, 0.6661, 0.6459, 0.5247,
        0.7047, 0.7058], grad_fn=<IndexBackward0>)
pred_loss tensor([-0.], grad_fn=<MulBackward0>)
size_loss tensor(-738.9868, grad_fn=<MulBackward0>)
size_num_loss 10.200000000000001
loss: tensor([-727.0674], grad_fn=<AddBackward0>)
16
epoch:  16 ; loss:  -727.0673828125 ; pred:  tensor([5.8419e-12, 4.6048e-08, 1.0000e+00, 6.0947e-09],
       grad_fn=<SoftmaxBackward0>)
num_high 100 len(mask) 410
mask_without_small tensor([0.7467, 0.7296, 0.7006, 0.5890, 0.7355, 0.6614, 0.7353, 0.5092, 0.7135,
        0.6809, 0.7368, 0.7220, 0.7227, 0.5561, 0.7241, 0.6951, 0.7382, 0.7088,
        0.7554, 0.5303, 0.7186, 0.5872, 0.7432, 0.7259, 0.7281, 0.6945, 0.7598,
        0.5296, 0.7220, 0.5298, 0.5059, 0.5442, 0.5330, 0.5300, 0.7165, 0.6460,
        0.5537, 0.7241, 0.5198, 0.5209, 0.6743, 0.5686, 0.5468, 0.7130, 0.6539,
        0.5001, 0.7218, 0.5182, 0.6285, 0.5309, 0.6626, 0.7144, 0.6224, 0.5020,
        0.7469, 0.7098, 0.7350, 0.5754, 0.5445, 0.7165, 0.7032, 0.7111, 0.5668,
        0.7061, 0.7344, 0.7283, 0.7068, 0.7339, 0.6847, 0.7248, 0.7224, 0.6571,
        0.6696, 0.7510, 0.7105, 0.6081, 0.6130, 0.7404, 0.7064, 0.5186, 0.7248,
        0.5304, 0.7549, 0.7131, 0.7231, 0.5141, 0.5095, 0.7137, 0.7304, 0.7112,
        0.7371, 0.7362, 0.7494, 0.5015, 0.5148, 0.7288, 0.7211, 0.6941, 0.5270,
        0.5340, 0.5413, 0.5252, 0.5407, 0.5074, 0.6731, 0.6179, 0.7230, 0.5621,
        0.7379, 0.7187, 0.7471, 0.6311, 0.7078, 0.5028, 0.7172, 0.7072, 0.7507,
        0.7043, 0.6151, 0.7244, 0.6161, 0.6901, 0.5126, 0.7208, 0.7192, 0.5077,
        0.7124, 0.5154, 0.6088, 0.5081, 0.6988, 0.6751, 0.5416, 0.5554, 0.7349,
        0.7139, 0.7052, 0.7493, 0.7455, 0.7373, 0.7450, 0.7564, 0.7377, 0.5694,
        0.5936, 0.5114, 0.6941, 0.5817, 0.7290, 0.5158, 0.7262, 0.5926, 0.6842,
        0.6554, 0.5028, 0.7252, 0.7263], grad_fn=<IndexBackward0>)
pred_loss tensor([-0.], grad_fn=<MulBackward0>)
size_loss tensor(-880.5003, grad_fn=<MulBackward0>)
size_num_loss 10.0
loss: tensor([-868.8135], grad_fn=<AddBackward0>)
17
epoch:  17 ; loss:  -868.8134765625 ; pred:  tensor([7.9671e-12, 5.6044e-08, 1.0000e+00, 7.6262e-09],
       grad_fn=<SoftmaxBackward0>)
num_high 99 len(mask) 410
mask_without_small tensor([0.7651, 0.7493, 0.7211, 0.5718, 0.7548, 0.6734, 0.7546, 0.7342, 0.6993,
        0.7559, 0.7423, 0.7429, 0.5354, 0.7442, 0.7151, 0.7572, 0.7294, 0.7731,
        0.5079, 0.7391, 0.5698, 0.7619, 0.7459, 0.7479, 0.7145, 0.7772, 0.5072,
        0.7423, 0.5074, 0.5226, 0.5108, 0.5076, 0.7370, 0.6493, 0.5328, 0.7442,
        0.6912, 0.5490, 0.5253, 0.7336, 0.6619, 0.7421, 0.6222, 0.5085, 0.6753,
        0.7350, 0.6135, 0.7652, 0.7305, 0.7543, 0.5565, 0.5229, 0.7370, 0.7238,
        0.7318, 0.5470, 0.7267, 0.7538, 0.7481, 0.7274, 0.7533, 0.7036, 0.7448,
        0.7426, 0.6669, 0.6851, 0.7691, 0.7312, 0.5947, 0.6009, 0.7593, 0.7270,
        0.7449, 0.5080, 0.7726, 0.7337, 0.7433, 0.7343, 0.7500, 0.7319, 0.7562,
        0.7554, 0.7675, 0.7486, 0.7415, 0.7141, 0.5045, 0.5118, 0.5195, 0.5026,
        0.5189, 0.6896, 0.6073, 0.7432, 0.5419, 0.7569, 0.7391, 0.7655, 0.6259,
        0.7284, 0.7377, 0.7279, 0.7688, 0.7248, 0.6037, 0.7445, 0.6049, 0.7097,
        0.7411, 0.7396, 0.7330, 0.5956, 0.7191, 0.6922, 0.5199, 0.5346, 0.7542,
        0.7345, 0.7258, 0.7675, 0.7640, 0.7564, 0.7635, 0.7740, 0.7568, 0.5499,
        0.5772, 0.7141, 0.5635, 0.7488, 0.7462, 0.5760, 0.7031, 0.6643, 0.7453,
        0.7463], grad_fn=<IndexBackward0>)
pred_loss tensor([1.1921e-06], grad_fn=<MulBackward0>)
size_loss tensor(-893.2755, grad_fn=<MulBackward0>)
size_num_loss 9.9
loss: tensor([-881.7899], grad_fn=<AddBackward0>)
18
epoch:  18 ; loss:  -881.7898559570312 ; pred:  tensor([1.0653e-11, 6.7440e-08, 1.0000e+00, 9.4067e-09],
       grad_fn=<SoftmaxBackward0>)
num_high 98 len(mask) 410
mask_without_small tensor([0.7826, 0.7683, 0.7412, 0.5525, 0.7733, 0.6813, 0.7731, 0.7542, 0.7174,
        0.7743, 0.7619, 0.7624, 0.5134, 0.7637, 0.7350, 0.7754, 0.7496, 0.7899,
        0.7589, 0.5503, 0.7797, 0.7652, 0.7670, 0.7344, 0.7937, 0.7618, 0.7570,
        0.6442, 0.5107, 0.7637, 0.7073, 0.5279, 0.5028, 0.7537, 0.6630, 0.7617,
        0.6092, 0.6843, 0.7550, 0.5990, 0.7827, 0.7506, 0.7728, 0.5359, 0.5002,
        0.7570, 0.7439, 0.7519, 0.5258, 0.7469, 0.7723, 0.7672, 0.7476, 0.7719,
        0.7224, 0.7642, 0.7622, 0.6710, 0.6990, 0.7862, 0.7513, 0.5777, 0.5846,
        0.7773, 0.7472, 0.7642, 0.7895, 0.7537, 0.7628, 0.7543, 0.7689, 0.7520,
        0.7745, 0.7738, 0.7848, 0.7676, 0.7611, 0.7339, 0.7053, 0.5919, 0.7627,
        0.5203, 0.7752, 0.7589, 0.7829, 0.6137, 0.7486, 0.7576, 0.7480, 0.7860,
        0.7450, 0.5877, 0.7639, 0.5892, 0.7293, 0.7608, 0.7594, 0.7531, 0.5787,
        0.7392, 0.7086, 0.5125, 0.7727, 0.7545, 0.7460, 0.7848, 0.7816, 0.7747,
        0.7811, 0.7907, 0.7750, 0.5288, 0.5584, 0.7339, 0.5435, 0.7678, 0.7654,
        0.5571, 0.7218, 0.6668, 0.7646, 0.7655], grad_fn=<IndexBackward0>)
pred_loss tensor([1.1921e-06], grad_fn=<MulBackward0>)
size_loss tensor(-857.6819, grad_fn=<MulBackward0>)
size_num_loss 9.8
loss: tensor([-846.3608], grad_fn=<AddBackward0>)
19
epoch:  19 ; loss:  -846.36083984375 ; pred:  tensor([1.3900e-11, 8.0071e-08, 1.0000e+00, 1.1404e-08],
       grad_fn=<SoftmaxBackward0>)
Finished Training
sel masked ver tensor(indices=tensor([[6815, 6839, 6848, 6860, 6919, 7054, 7061, 7381, 7382,
                        7402, 7410, 7463, 7468, 7486, 7777, 7835, 7878, 7960,
                        5462, 6815, 6848, 6860, 6860, 6861, 6861, 6928, 7463,
                        7468, 7486, 7487, 7737, 7777, 7778, 7778, 7791, 7837,
                        7905, 5900, 5936, 5947, 5947, 5954, 6815, 6815, 6839,
                        6839, 6848, 6848, 6850, 6860, 6860, 6861, 6866, 6866,
                        6874, 6928, 6928, 6952, 6952, 6952, 7063, 7091, 7091,
                        7168, 7380, 7402, 7410, 7463, 7463, 7487, 7487, 7731,
                        7731, 7777, 7778, 7838, 7838, 7839, 7933, 7951, 7951,
                        7970, 7973, 5352, 5371, 5396, 5459, 5462, 5936, 5936,
                        5936, 5936, 5936, 5936, 5936, 5936, 5936, 5947, 5947,
                        5947, 5947, 5954, 5954, 5954, 5954, 5900, 5900, 5900,
                        5900, 5900, 5900, 5900, 5900, 5900, 5900, 5900, 5900,
                        5900, 5900, 5900, 5900, 5900],
                       [5900, 5900, 5900, 5900, 5900, 5900, 5900, 5900, 5900,
                        5900, 5900, 5900, 5900, 5900, 5900, 5900, 5900, 5900,
                        5947, 5947, 5947, 5947, 5954, 5947, 5954, 5947, 5947,
                        5936, 5954, 5954, 5936, 5954, 5947, 5954, 5954, 5936,
                        5954, 8049, 5459, 5459, 5462, 5352, 5371, 5471, 5371,
                        5471, 5460, 5471, 5471, 5460, 5462, 5396, 5460, 5471,
                        5462, 5396, 5459, 5396, 5459, 5460, 5460, 5460, 5462,
                        5471, 5460, 5471, 5471, 5462, 5471, 5371, 5492, 5460,
                        5462, 5352, 5460, 5371, 5459, 5371, 5462, 5371, 5460,
                        5471, 5471, 5900, 5900, 5900, 5900, 5900, 5900, 6850,
                        6976, 7054, 7382, 7402, 7468, 7731, 7970, 6861, 7063,
                        7778, 7835, 6860, 6861, 6928, 7463, 6815, 6839, 6848,
                        6874, 6928, 6952, 6979, 7061, 7382, 7463, 7731, 7839,
                        7878, 7933, 7951, 7970, 7973]]),
       values=tensor([0.7826, 0.7683, 0.7412, 0.5525, 0.7733, 0.6813, 0.7731,
                      0.7542, 0.7174, 0.7743, 0.7619, 0.7624, 0.5134, 0.7637,
                      0.7350, 0.7754, 0.7496, 0.7899, 0.7589, 0.5503, 0.7797,
                      0.7652, 0.7670, 0.7344, 0.7937, 0.7618, 0.7570, 0.6442,
                      0.5107, 0.7637, 0.7073, 0.5279, 0.5028, 0.7537, 0.6630,
                      0.7617, 0.6092, 0.6843, 0.7550, 0.5990, 0.7827, 0.7506,
                      0.7728, 0.5359, 0.5002, 0.7570, 0.7439, 0.7519, 0.5258,
                      0.7469, 0.7723, 0.7672, 0.7476, 0.7719, 0.7224, 0.7642,
                      0.7622, 0.6710, 0.6990, 0.7862, 0.7513, 0.5777, 0.5846,
                      0.7773, 0.7472, 0.7642, 0.7895, 0.7537, 0.7628, 0.7543,
                      0.7689, 0.7520, 0.7745, 0.7738, 0.7848, 0.7676, 0.7611,
                      0.7339, 0.7053, 0.5919, 0.7627, 0.5203, 0.7752, 0.7589,
                      0.7829, 0.6137, 0.7486, 0.7576, 0.7480, 0.7860, 0.7450,
                      0.5877, 0.7639, 0.5892, 0.7293, 0.7608, 0.7594, 0.7531,
                      0.5787, 0.7392, 0.7086, 0.5125, 0.7727, 0.7545, 0.7460,
                      0.7848, 0.7816, 0.7747, 0.7811, 0.7907, 0.7750, 0.5288,
                      0.5584, 0.7339, 0.5435, 0.7678, 0.7654, 0.5571, 0.7218,
                      0.6668, 0.7646, 0.7655]),
       size=(7974, 8050), nnz=122, layout=torch.sparse_coo,
       grad_fn=<SparseCooTensorWithDimsAndTensorsBackward0>)
Counter({'isAbout': 45, 'author': 18, 'hasProject': 18, 'publication': 17, 'projectInfo': 16, 'isWorkedOnBy': 5, 'dealtWithIn': 1, 'homepage': 1, 'member': 1})
dict index: {}
node_idx 5900
 node original label [2]
 node predicted label explain 2
 node prediction probability explain tensor([1.3900e-11, 8.0071e-08, 1.0000e+00, 1.1404e-08],
       grad_fn=<SoftmaxBackward0>)
 node predicted label full 2 most important relations  {'isWorkedOnBy': 5, 'dealtWithIn': 1, 'publication': 17, 'homepage': 1, 'isAbout': 45, 'member': 1, 'projectInfo': 16, 'author': 18, 'hasProject': 18, 'label': 2, 'node_idx': '5900'}
 final masks and lenght tensor(indices=tensor([[ 23385,  23409,  23418,  23420,  23430,  23431,  23436,
                         23440,  23444,  23489,  23490,  23498,  23522,  23546,
                         23549,  23624,  23631,  23633,  23661,  23738,  23950,
                         23951,  23952,  23972,  23980,  24033,  24038,  24056,
                         24057,  24301,  24307,  24347,  24348,  24350,  24361,
                         24405,  24406,  24407,  24408,  24409,  24427,  24448,
                         24475,  24503,  24521,  24530,  24540,  24543,  63347,
                         63454,  63454,  63455,  63455,  63457,  63457,  63466,
                         63466,  88750, 114520, 114520, 114544, 114553, 114555,
                        114565, 114565, 114566, 114566, 114575, 114579, 114624,
                        114625, 114633, 114633, 114657, 114681, 114681, 114759,
                        114766, 114768, 114796, 115085, 115086, 115087, 115107,
                        115115, 115168, 115168, 115168, 115173, 115191, 115191,
                        115191, 115192, 115192, 115192, 115436, 115436, 115436,
                        115442, 115482, 115482, 115483, 115483, 115496, 115496,
                        115496, 115540, 115540, 115541, 115542, 115543, 115544,
                        115562, 115562, 115583, 115610, 115610, 115638, 115656,
                        115665, 115675, 115678, 115678, 130175, 146781, 146781,
                        146792, 146792, 146792, 146799, 146799, 146799, 146799,
                        147660, 147660, 147684, 147684, 147693, 147693, 147693,
                        147693, 147693, 147695, 147695, 147705, 147705, 147705,
                        147706, 147706, 147706, 147706, 147711, 147711, 147711,
                        147711, 147711, 147715, 147715, 147719, 147719, 147764,
                        147764, 147764, 147765, 147765, 147765, 147765, 147773,
                        147773, 147773, 147773, 147773, 147773, 147797, 147797,
                        147797, 147797, 147821, 147821, 147821, 147821, 147899,
                        147899, 147906, 147906, 147906, 147906, 147908, 147908,
                        147908, 147936, 147936, 148013, 148225, 148225, 148225,
                        148227, 148227, 148227, 148247, 148255, 148255, 148308,
                        148308, 148308, 148308, 148308, 148308, 148313, 148313,
                        148313, 148331, 148331, 148331, 148331, 148331, 148331,
                        148332, 148332, 148332, 148576, 148576, 148576, 148576,
                        148582, 148622, 148622, 148622, 148622, 148622, 148623,
                        148623, 148623, 148623, 148625, 148636, 148636, 148636,
                        148680, 148680, 148680, 148681, 148682, 148682, 148682,
                        148682, 148682, 148683, 148683, 148683, 148684, 148702,
                        148702, 148702, 148723, 148723, 148750, 148750, 148750,
                        148750, 148750, 148778, 148778, 148778, 148778, 148796,
                        148796, 148796, 148796, 148805, 148805, 148815, 148815,
                        148815, 148818, 148818, 148818, 148818, 148818, 154479,
                        154482, 154501, 154526, 154589, 154590, 154592, 154601,
                        154622, 179921, 179932, 179939, 196455, 229595, 237880,
                        246201, 246201, 246201, 246201, 246201, 246201, 246201,
                        246201, 246201, 246201, 246201, 246201, 246201, 246201,
                        246201, 246201, 246201, 246201, 246201, 246201, 246201,
                        246201, 246201, 246201, 246201, 246212, 246212, 246212,
                        246212, 246212, 246212, 246212, 246212, 246212, 246212,
                        246212, 246212, 246212, 246212, 246212, 246212, 246212,
                        246212, 246212, 246212, 246212, 246212, 246212, 246212,
                        246212, 246219, 246219, 246219, 246219, 246219, 246219,
                        246219, 246219, 246219, 246219, 246219, 246219, 246219,
                        246219, 246219, 254450, 254450, 254450, 254450, 254450,
                        254450, 254450, 254450, 254450, 254450, 254450, 254450,
                        254450, 254450, 254450, 254450, 254450, 254450, 254450,
                        254450, 254450, 254450, 254450, 254450, 254450, 254450,
                        254450, 254450, 254450, 254450, 254450, 254450, 254450,
                        254450, 254450, 254450, 254450, 254450, 254450, 254450,
                        254450, 254450, 254450, 254450, 254450, 254450, 254450,
                        254450, 304160, 304160, 329015],
                       [  5900,   5900,   5900,   5900,   5900,   5900,   5900,
                          5900,   5900,   5900,   5900,   5900,   5900,   5900,
                          5900,   5900,   5900,   5900,   5900,   5900,   5900,
                          5900,   5900,   5900,   5900,   5900,   5900,   5900,
                          5900,   5900,   5900,   5900,   5900,   5900,   5900,
                          5900,   5900,   5900,   5900,   5900,   5900,   5900,
                          5900,   5900,   5900,   5900,   5900,   5900,   5954,
                          5936,   5947,   5947,   5954,   5947,   5954,   5936,
                          5954,     98,   5936,   5947,   5936,   5947,   5936,
                          5947,   5954,   5947,   5954,   5936,   5954,   5947,
                          5947,   5947,   5954,   5947,   5936,   5947,   5936,
                          5947,   5947,   5947,   5936,   5936,   5936,   5936,
                          5936,   5936,   5947,   5954,   5936,   5936,   5947,
                          5954,   5936,   5947,   5954,   5936,   5947,   5954,
                          5936,   5947,   5954,   5947,   5954,   5936,   5947,
                          5954,   5947,   5954,   5936,   5936,   5936,   5936,
                          5947,   5954,   5936,   5947,   5954,   5947,   5947,
                          5936,   5936,   5947,   5954,   8049,   5459,   5471,
                          5459,   5460,   5462,   5352,   5460,   5462,   5471,
                          5371,   5471,   5371,   5471,   5371,   5396,   5460,
                          5462,   5471,   5371,   5471,   5396,   5460,   5462,
                          5396,   5459,   5460,   5462,   5371,   5396,   5460,
                          5462,   5471,   5371,   5471,   5460,   5462,   5396,
                          5460,   5462,   5396,   5460,   5462,   5471,   5349,
                          5352,   5396,   5459,   5460,   5462,   5396,   5459,
                          5460,   5462,   5371,   5396,   5460,   5462,   5371,
                          5471,   5396,   5460,   5462,   5471,   5396,   5460,
                          5462,   5460,   5462,   5471,   5459,   5460,   5471,
                          5459,   5471,   5492,   5471,   5459,   5471,   5371,
                          5459,   5460,   5462,   5471,   5492,   5371,   5459,
                          5471,   5352,   5371,   5459,   5460,   5462,   5492,
                          5371,   5459,   5492,   5371,   5459,   5460,   5462,
                          5371,   5352,   5459,   5460,   5462,   5471,   5352,
                          5460,   5462,   5471,   5352,   5459,   5460,   5462,
                          5459,   5460,   5462,   5371,   5371,   5459,   5460,
                          5462,   5492,   5371,   5459,   5471,   5371,   5459,
                          5460,   5462,   5371,   5459,   5352,   5459,   5460,
                          5462,   5471,   5459,   5460,   5462,   5471,   5352,
                          5371,   5459,   5460,   5371,   5471,   5371,   5459,
                          5471,   5352,   5459,   5460,   5462,   5471,   5900,
                          5900,   5900,   5900,   5900,   5900,   5900,   5900,
                          5900,   5900,   5900,   5900,   1571,     90,   5665,
                          6815,   6839,   6850,   6870,   6976,   7054,   7380,
                          7381,   7382,   7402,   7410,   7463,   7468,   7486,
                          7487,   7731,   7737,   7791,   7836,   7837,   7838,
                          7839,   7878,   7960,   7970,   6815,   6848,   6860,
                          6861,   6919,   6920,   6928,   6952,   6976,   7061,
                          7063,   7091,   7463,   7486,   7487,   7731,   7777,
                          7778,   7791,   7835,   7857,   7905,   7933,   7951,
                          7973,   6860,   6861,   6874,   6928,   7463,   7486,
                          7487,   7731,   7777,   7778,   7791,   7835,   7857,
                          7905,   7973,   6815,   6839,   6848,   6850,   6860,
                          6861,   6866,   6870,   6874,   6919,   6920,   6928,
                          6952,   6976,   6979,   7054,   7061,   7063,   7091,
                          7168,   7380,   7381,   7382,   7402,   7410,   7463,
                          7468,   7486,   7487,   7731,   7737,   7777,   7778,
                          7780,   7791,   7835,   7836,   7837,   7838,   7839,
                          7857,   7878,   7905,   7933,   7951,   7960,   7970,
                          7973,   5947,   5954,   5230]]),
       values=tensor([0.7826, 0.7683, 0.7412, 0.3810, 0.5525, 0.4244, 0.4098,
                      0.4181, 0.4036, 0.7733, 0.4108, 0.4215, 0.4041, 0.4073,
                      0.4033, 0.6813, 0.7731, 0.4055, 0.4086, 0.4648, 0.4035,
                      0.7542, 0.7174, 0.7743, 0.7619, 0.7624, 0.5134, 0.7637,
                      0.4029, 0.4130, 0.4022, 0.7350, 0.4219, 0.4014, 0.4032,
                      0.7754, 0.4359, 0.4101, 0.4221, 0.4032, 0.4189, 0.7496,
                      0.4012, 0.4065, 0.4238, 0.7899, 0.4244, 0.4087, 0.4006,
                      0.4054, 0.4142, 0.4845, 0.4087, 0.7589, 0.4024, 0.4039,
                      0.4216, 0.4128, 0.4090, 0.5503, 0.4077, 0.7797, 0.4253,
                      0.7652, 0.7670, 0.7344, 0.7937, 0.4837, 0.4374, 0.4041,
                      0.3980, 0.7618, 0.4050, 0.4839, 0.4135, 0.4613, 0.4998,
                      0.4057, 0.3791, 0.4036, 0.4118, 0.4119, 0.4226, 0.4068,
                      0.4875, 0.4842, 0.7570, 0.4133, 0.6442, 0.4089, 0.3981,
                      0.5107, 0.4161, 0.4022, 0.7637, 0.4761, 0.3735, 0.4773,
                      0.7073, 0.4125, 0.5279, 0.5028, 0.7537, 0.4095, 0.4045,
                      0.6630, 0.4552, 0.4171, 0.4263, 0.7617, 0.4114, 0.4128,
                      0.4206, 0.4076, 0.4065, 0.4744, 0.6092, 0.4147, 0.4108,
                      0.4851, 0.4110, 0.4240, 0.4159, 0.6843, 0.7550, 0.4297,
                      0.5990, 0.4572, 0.7827, 0.7506, 0.4210, 0.3967, 0.4063,
                      0.7728, 0.5359, 0.5002, 0.7570, 0.4121, 0.4146, 0.7439,
                      0.4111, 0.7519, 0.4048, 0.5258, 0.3995, 0.7469, 0.7723,
                      0.7672, 0.4280, 0.4037, 0.4040, 0.4151, 0.4398, 0.7476,
                      0.4092, 0.7719, 0.3740, 0.4102, 0.4251, 0.7224, 0.3849,
                      0.4213, 0.4124, 0.4118, 0.4347, 0.4040, 0.4170, 0.3973,
                      0.4181, 0.7642, 0.7622, 0.4133, 0.4191, 0.6710, 0.6990,
                      0.7862, 0.4128, 0.4155, 0.4026, 0.4038, 0.4003, 0.4183,
                      0.3965, 0.4080, 0.4081, 0.4199, 0.3844, 0.4157, 0.7513,
                      0.4074, 0.5777, 0.5846, 0.7773, 0.4004, 0.7472, 0.4120,
                      0.4254, 0.4396, 0.4749, 0.7642, 0.4846, 0.7895, 0.4081,
                      0.4003, 0.4173, 0.7537, 0.7628, 0.4700, 0.4024, 0.3986,
                      0.4086, 0.4067, 0.4164, 0.4652, 0.4059, 0.3770, 0.4106,
                      0.7543, 0.4150, 0.7689, 0.4251, 0.4085, 0.7520, 0.7745,
                      0.4113, 0.7738, 0.4164, 0.3979, 0.4070, 0.4144, 0.4604,
                      0.7848, 0.4087, 0.4007, 0.3821, 0.3975, 0.4121, 0.4142,
                      0.4177, 0.4567, 0.4003, 0.4305, 0.4077, 0.4707, 0.4014,
                      0.4104, 0.4116, 0.7676, 0.7611, 0.4043, 0.7339, 0.4809,
                      0.4885, 0.4966, 0.4789, 0.4202, 0.4049, 0.4960, 0.4198,
                      0.4180, 0.4203, 0.4629, 0.4067, 0.7053, 0.4182, 0.4073,
                      0.5919, 0.4129, 0.7627, 0.4257, 0.3834, 0.4121, 0.4220,
                      0.5203, 0.3721, 0.4105, 0.4068, 0.4037, 0.7752, 0.4135,
                      0.7589, 0.7829, 0.6137, 0.7486, 0.4581, 0.7576, 0.4278,
                      0.4101, 0.7480, 0.3987, 0.4077, 0.4097, 0.4125, 0.4066,
                      0.4043, 0.4096, 0.7860, 0.4282, 0.7450, 0.5877, 0.4102,
                      0.4150, 0.7639, 0.5892, 0.4617, 0.4043, 0.7293, 0.3998,
                      0.4685, 0.7608, 0.3772, 0.4235, 0.4171, 0.3806, 0.4064,
                      0.3983, 0.4034, 0.4093, 0.7594, 0.3993, 0.4632, 0.4043,
                      0.7531, 0.4207, 0.4714, 0.4489, 0.4113, 0.4143, 0.4523,
                      0.5787, 0.4160, 0.4019, 0.4637, 0.4266, 0.4068, 0.4144,
                      0.7392, 0.4172, 0.7086, 0.4092, 0.4970, 0.4044, 0.4088,
                      0.4133, 0.5125, 0.7727, 0.4082, 0.7545, 0.7460, 0.4050,
node label: 2
43
r_exp.py:102: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  hor_graph, ver_graph = torch.tensor(sub_hor_graph, dtype=torch.float), torch.tensor(sub_ver_graph, dtype=torch.float)
wandb: WARNING Calling wandb.login() after wandb.init() has no effect.
num_high 82 len(mask) 82
mask_without_small tensor([0.7860, 0.7742, 0.7578, 0.6618, 0.7514, 0.6915, 0.7297, 0.6790, 0.7073,
        0.7786, 0.7188, 0.6859, 0.7081, 0.7135, 0.7068, 0.7538, 0.7784, 0.7261,
        0.7155, 0.7443, 0.7072, 0.7629, 0.7549, 0.7795, 0.7685, 0.7690, 0.7494,
        0.7700, 0.7239, 0.7323, 0.7233, 0.7566, 0.6865, 0.7035, 0.7241, 0.7804,
        0.7407, 0.7178, 0.7403, 0.7066, 0.6806, 0.7605, 0.7032, 0.7122, 0.6902,
        0.7911, 0.6915, 0.7158, 0.7021, 0.7104, 0.7334, 0.7469, 0.7158, 0.7660,
        0.7053, 0.7079, 0.6859, 0.7322, 0.7291, 0.7513, 0.7280, 0.7838, 0.6932,
        0.7714, 0.7731, 0.7565, 0.6880, 0.7127, 0.7472, 0.7469, 0.7646, 0.7326,
        0.7528, 0.7087, 0.6976, 0.7492, 0.6750, 0.7049, 0.7700, 0.7456, 0.7249,
        0.7682], grad_fn=<IndexBackward0>)
pred_loss tensor([0.0125], grad_fn=<MulBackward0>)
size_loss tensor(-3.1538, grad_fn=<MulBackward0>)
size_num_loss 8.200000000000001
loss: tensor([11.6371], grad_fn=<AddBackward0>)
0
epoch:  0 ; loss:  11.637073516845703 ; pred:  tensor([2.3137e-05, 4.3929e-04, 9.9875e-01, 7.8696e-04],
       grad_fn=<SoftmaxBackward0>)
num_high 82 len(mask) 82
mask_without_small tensor([0.8023, 0.7912, 0.7757, 0.6390, 0.7696, 0.6698, 0.7096, 0.6569, 0.6862,
        0.7954, 0.6982, 0.6639, 0.6870, 0.6927, 0.6857, 0.7719, 0.7952, 0.7058,
        0.6947, 0.7249, 0.6860, 0.7805, 0.7730, 0.7962, 0.7858, 0.7862, 0.7677,
        0.7873, 0.7035, 0.7123, 0.7028, 0.7746, 0.6646, 0.6822, 0.7037, 0.7971,
        0.7211, 0.6971, 0.7207, 0.6855, 0.6585, 0.7782, 0.6819, 0.6913, 0.6684,
        0.8071, 0.6698, 0.6950, 0.6808, 0.6894, 0.7134, 0.7275, 0.6950, 0.7835,
        0.6841, 0.6868, 0.6639, 0.7121, 0.7089, 0.7695, 0.7078, 0.8003, 0.6715,
        0.7885, 0.7901, 0.7745, 0.6661, 0.6918, 0.7278, 0.7275, 0.7821, 0.7126,
        0.7710, 0.6876, 0.6761, 0.7675, 0.6527, 0.6837, 0.7873, 0.7262, 0.7046,
        0.7855], grad_fn=<IndexBackward0>)
pred_loss tensor([0.0155], grad_fn=<MulBackward0>)
size_loss tensor(-4.8044, grad_fn=<MulBackward0>)
size_num_loss 8.200000000000001
loss: tensor([9.9333], grad_fn=<AddBackward0>)
1
epoch:  1 ; loss:  9.933320999145508 ; pred:  tensor([3.0066e-05, 5.5293e-04, 9.9845e-01, 9.6990e-04],
       grad_fn=<SoftmaxBackward0>)
num_high 81 len(mask) 82
mask_without_small tensor([0.8175, 0.8072, 0.7924, 0.6159, 0.7849, 0.6474, 0.6887, 0.6341, 0.6643,
        0.8110, 0.6767, 0.6413, 0.6651, 0.6710, 0.6637, 0.7881, 0.8109, 0.6847,
        0.6731, 0.7068, 0.6641, 0.7971, 0.7894, 0.8118, 0.8021, 0.8026, 0.7815,
        0.8035, 0.6822, 0.6916, 0.6815, 0.7912, 0.6420, 0.6602, 0.6825, 0.8126,
        0.7017, 0.6756, 0.7012, 0.6635, 0.6358, 0.7950, 0.6599, 0.6695, 0.6460,
        0.8220, 0.6474, 0.6734, 0.6587, 0.6676, 0.6929, 0.7112, 0.6734, 0.8000,
        0.6621, 0.6649, 0.6414, 0.6914, 0.6880, 0.7849, 0.6868, 0.8156, 0.6491,
        0.8047, 0.8062, 0.7911, 0.6436, 0.6701, 0.7117, 0.7111, 0.7987, 0.6919,
        0.7870, 0.6657, 0.6539, 0.7813, 0.6299, 0.6617, 0.8035, 0.7090, 0.6834,
        0.8019], grad_fn=<IndexBackward0>)
pred_loss tensor([0.0195], grad_fn=<MulBackward0>)
size_loss tensor(-6.5230, grad_fn=<MulBackward0>)
size_num_loss 8.1
loss: tensor([8.0550], grad_fn=<AddBackward0>)
2
epoch:  2 ; loss:  8.055025100708008 ; pred:  tensor([3.9593e-05, 7.0371e-04, 9.9805e-01, 1.2073e-03],
       grad_fn=<SoftmaxBackward0>)
num_high 74 len(mask) 82
mask_without_small tensor([0.8314, 0.8220, 0.8083, 0.5924, 0.8002, 0.6244, 0.6669, 0.6109, 0.6416,
        0.8255, 0.6544, 0.6182, 0.6425, 0.6485, 0.6410, 0.8039, 0.8253, 0.6626,
        0.6506, 0.6872, 0.6414, 0.8128, 0.8053, 0.8262, 0.8174, 0.8178, 0.7963,
        0.8187, 0.6601, 0.6700, 0.6594, 0.8071, 0.6189, 0.6374, 0.6604, 0.8269,
        0.6812, 0.6532, 0.6807, 0.6408, 0.6126, 0.8108, 0.6371, 0.6470, 0.6229,
        0.8354, 0.6244, 0.6510, 0.6359, 0.6450, 0.6714, 0.6926, 0.6510, 0.8155,
        0.6394, 0.6422, 0.6183, 0.6698, 0.6661, 0.8002, 0.6649, 0.8296, 0.6262,
        0.8197, 0.8211, 0.8070, 0.6205, 0.6475, 0.6933, 0.6925, 0.8143, 0.6704,
        0.8026, 0.6431, 0.6310, 0.7961, 0.6066, 0.6389, 0.8187, 0.6899, 0.6613,
        0.8172], grad_fn=<IndexBackward0>)
pred_loss tensor([0.0249], grad_fn=<MulBackward0>)
size_loss tensor(-8.2668, grad_fn=<MulBackward0>)
size_num_loss 7.4
loss: tensor([5.5448], grad_fn=<AddBackward0>)
3
epoch:  3 ; loss:  5.54481315612793 ; pred:  tensor([5.3156e-05, 9.1032e-04, 9.9751e-01, 1.5243e-03],
       grad_fn=<SoftmaxBackward0>)
num_high 56 len(mask) 82
mask_without_small tensor([0.8439, 0.8355, 0.8232, 0.5687, 0.8151, 0.6009, 0.6442, 0.5873, 0.6183,
        0.8386, 0.6313, 0.5947, 0.6192, 0.6253, 0.6177, 0.8189, 0.8385, 0.6398,
        0.6275, 0.6664, 0.6181, 0.8274, 0.8203, 0.8392, 0.8315, 0.8319, 0.8109,
        0.8326, 0.6371, 0.6475, 0.6364, 0.8221, 0.5954, 0.6140, 0.6375, 0.8399,
        0.6597, 0.6301, 0.6590, 0.6175, 0.5891, 0.8256, 0.6137, 0.6238, 0.5995,
        0.8475, 0.6009, 0.6278, 0.6125, 0.6217, 0.6490, 0.6724, 0.6278, 0.8298,
        0.6161, 0.6189, 0.5947, 0.6473, 0.6434, 0.8151, 0.6421, 0.8423, 0.6027,
        0.8336, 0.8348, 0.8220, 0.5970, 0.6243, 0.6732, 0.6723, 0.8288, 0.6479,
        0.8176, 0.6198, 0.6076, 0.8107, 0.5830, 0.6156, 0.8326, 0.6694, 0.6384,
        0.8313], grad_fn=<IndexBackward0>)
pred_loss tensor([0.0322], grad_fn=<MulBackward0>)
size_loss tensor(-10.0089, grad_fn=<MulBackward0>)
size_num_loss 5.6000000000000005
loss: tensor([1.9304], grad_fn=<AddBackward0>)
4
epoch:  4 ; loss:  1.9303865432739258 ; pred:  tensor([7.2602e-05, 1.1945e-03, 9.9678e-01, 1.9488e-03],
       grad_fn=<SoftmaxBackward0>)
num_high 40 len(mask) 82
mask_without_small tensor([0.8549, 0.8477, 0.8370, 0.5450, 0.8292, 0.5770, 0.6208, 0.5635, 0.5945,
        0.8504, 0.6075, 0.5709, 0.5954, 0.6015, 0.5939, 0.8329, 0.8503, 0.6163,
        0.6037, 0.6444, 0.5943, 0.8408, 0.8343, 0.8509, 0.8443, 0.8446, 0.8249,
        0.8452, 0.6135, 0.6243, 0.6128, 0.8360, 0.5715, 0.5902, 0.6138, 0.8515,
        0.6372, 0.6064, 0.6365, 0.5937, 0.5652, 0.8391, 0.5899, 0.6000, 0.5756,
        0.8582, 0.5770, 0.6041, 0.5887, 0.5979, 0.6258, 0.6510, 0.6041, 0.8428,
        0.5923, 0.5951, 0.5709, 0.6240, 0.6200, 0.8293, 0.6186, 0.8536, 0.5789,
        0.8461, 0.8471, 0.8360, 0.5732, 0.6005, 0.6519, 0.6509, 0.8419, 0.6247,
        0.8317, 0.5960, 0.5838, 0.8248, 0.5592, 0.5918, 0.8453, 0.6477, 0.6148,
        0.8441], grad_fn=<IndexBackward0>)
pred_loss tensor([0.0421], grad_fn=<MulBackward0>)
size_loss tensor(-11.7303, grad_fn=<MulBackward0>)
size_num_loss 4.0
loss: tensor([-1.4686], grad_fn=<AddBackward0>)
5
epoch:  5 ; loss:  -1.4685897827148438 ; pred:  tensor([1.0067e-04, 1.5866e-03, 9.9579e-01, 2.5183e-03],
       grad_fn=<SoftmaxBackward0>)
num_high 33 len(mask) 82
mask_without_small tensor([0.8645, 0.8584, 0.8495, 0.5212, 0.8422, 0.5529, 0.5967, 0.5395, 0.5702,
        0.8606, 0.5833, 0.5468, 0.5711, 0.5772, 0.5696, 0.8458, 0.8605, 0.5921,
        0.5794, 0.6214, 0.5700, 0.8527, 0.8470, 0.8611, 0.8556, 0.8558, 0.8381,
        0.8563, 0.5893, 0.6003, 0.5885, 0.8486, 0.5475, 0.5659, 0.5896, 0.8616,
        0.6138, 0.5821, 0.6130, 0.5694, 0.5413, 0.8513, 0.5657, 0.5757, 0.5515,
        0.8674, 0.5530, 0.5798, 0.5645, 0.5736, 0.6019, 0.6285, 0.5798, 0.8544,
        0.5680, 0.5708, 0.5469, 0.6000, 0.5959, 0.8423, 0.5945, 0.8635, 0.5548,
        0.8571, 0.8580, 0.8486, 0.5491, 0.5763, 0.6295, 0.6284, 0.8537, 0.6007,
        0.8447, 0.5717, 0.5596, 0.8379, 0.5353, 0.5675, 0.8564, 0.6249, 0.5906,
        0.8554], grad_fn=<IndexBackward0>)
pred_loss tensor([0.0557], grad_fn=<MulBackward0>)
size_loss tensor(-13.4153, grad_fn=<MulBackward0>)
size_num_loss 3.3000000000000003
loss: tensor([-3.9351], grad_fn=<AddBackward0>)
6
epoch:  6 ; loss:  -3.9350996017456055 ; pred:  tensor([1.4140e-04, 2.1285e-03, 9.9445e-01, 3.2826e-03],
       grad_fn=<SoftmaxBackward0>)
num_high 29 len(mask) 82
mask_without_small tensor([0.8726, 0.8675, 0.8604, 0.8539, 0.5287, 0.5721, 0.5156, 0.5457, 0.8694,
        0.5586, 0.5227, 0.5465, 0.5525, 0.5451, 0.8571, 0.8693, 0.5674, 0.5547,
        0.5976, 0.5454, 0.8630, 0.8582, 0.8697, 0.8653, 0.8655, 0.8500, 0.8659,
        0.5646, 0.5757, 0.5638, 0.8596, 0.5234, 0.5415, 0.5649, 0.8701, 0.5896,
        0.5574, 0.5888, 0.5449, 0.5173, 0.8618, 0.5412, 0.5511, 0.5274, 0.8752,
        0.5288, 0.5551, 0.5400, 0.5490, 0.5773, 0.6051, 0.5551, 0.8644, 0.5436,
        0.5463, 0.5228, 0.5755, 0.5713, 0.8541, 0.5699, 0.8718, 0.5306, 0.8667,
        0.8674, 0.8598, 0.5250, 0.5516, 0.6061, 0.6050, 0.8640, 0.5761, 0.8562,
        0.5472, 0.5353, 0.8500, 0.5115, 0.5431, 0.8661, 0.6013, 0.5659, 0.8652],
       grad_fn=<IndexBackward0>)
pred_loss tensor([0.0741], grad_fn=<MulBackward0>)
size_loss tensor(-1502.8510, grad_fn=<MulBackward0>)
size_num_loss 2.9000000000000004
loss: tensor([-1498.7537], grad_fn=<AddBackward0>)
7
epoch:  7 ; loss:  -1498.753662109375 ; pred:  tensor([2.0065e-04, 2.8769e-03, 9.9262e-01, 4.3066e-03],
       grad_fn=<SoftmaxBackward0>)
num_high 29 len(mask) 82
mask_without_small tensor([0.8781, 0.8732, 0.8663, 0.8600, 0.5156, 0.5592, 0.5024, 0.5326, 0.8750,
        0.5456, 0.5096, 0.5335, 0.5395, 0.5321, 0.8632, 0.8749, 0.5545, 0.5418,
        0.5850, 0.5324, 0.8688, 0.8642, 0.8753, 0.8711, 0.8712, 0.8562, 0.8716,
        0.5517, 0.5629, 0.5509, 0.8656, 0.5102, 0.5284, 0.5520, 0.8757, 0.5769,
        0.5445, 0.5762, 0.5319, 0.5042, 0.8677, 0.5282, 0.5381, 0.5143, 0.8806,
        0.5157, 0.5422, 0.5270, 0.5361, 0.5646, 0.5926, 0.5422, 0.8702, 0.5305,
        0.5333, 0.5097, 0.5627, 0.5584, 0.8602, 0.5570, 0.8774, 0.5175, 0.8724,
        0.8731, 0.8657, 0.5119, 0.5387, 0.5936, 0.5925, 0.8698, 0.5634, 0.8623,
        0.5342, 0.5222, 0.8562, 0.5300, 0.8718, 0.5888, 0.5531, 0.8710],
       grad_fn=<IndexBackward0>)
pred_loss tensor([0.0860], grad_fn=<MulBackward0>)
size_loss tensor(-1592.4216, grad_fn=<MulBackward0>)
size_num_loss 2.9000000000000004
loss: tensor([-1588.3247], grad_fn=<AddBackward0>)
8
epoch:  8 ; loss:  -1588.32470703125 ; pred:  tensor([2.4056e-04, 3.3615e-03, 9.9144e-01, 4.9569e-03],
       grad_fn=<SoftmaxBackward0>)
num_high 29 len(mask) 82
mask_without_small tensor([0.8850, 0.8804, 0.8738, 0.8678, 0.5426, 0.5158, 0.8821, 0.5289, 0.5167,
        0.5228, 0.5152, 0.8708, 0.8820, 0.5378, 0.5250, 0.5686, 0.5156, 0.8762,
        0.8718, 0.8824, 0.8783, 0.8785, 0.8642, 0.8789, 0.5350, 0.5463, 0.5342,
        0.8731, 0.5116, 0.5353, 0.8828, 0.5604, 0.5277, 0.5596, 0.5151, 0.8751,
        0.5113, 0.5213, 0.8874, 0.5254, 0.5101, 0.5193, 0.5479, 0.5763, 0.5254,
        0.8775, 0.5137, 0.5165, 0.5460, 0.5418, 0.8680, 0.5404, 0.8843, 0.5006,
        0.8796, 0.8803, 0.8732, 0.5219, 0.5773, 0.5762, 0.8771, 0.5468, 0.8700,
        0.5174, 0.5054, 0.8642, 0.5132, 0.8791, 0.5724, 0.5364, 0.8782],
       grad_fn=<IndexBackward0>)
pred_loss tensor([0.1038], grad_fn=<MulBackward0>)
size_loss tensor(-1709.9760, grad_fn=<MulBackward0>)
size_num_loss 2.9000000000000004
loss: tensor([-1705.9156], grad_fn=<AddBackward0>)
9
epoch:  9 ; loss:  -1705.9156494140625 ; pred:  tensor([3.0300e-04, 4.0953e-03, 9.8968e-01, 5.9258e-03],
       grad_fn=<SoftmaxBackward0>)
num_high 29 len(mask) 82
mask_without_small tensor([0.8926, 0.8882, 0.8819, 0.8763, 0.5235, 0.8897, 0.5097, 0.5035, 0.8791,
        0.8897, 0.5187, 0.5058, 0.5498, 0.8842, 0.8800, 0.8901, 0.8862, 0.8864,
        0.8728, 0.8867, 0.5158, 0.5272, 0.5150, 0.8812, 0.5161, 0.8904, 0.5415,
        0.5085, 0.5407, 0.8832, 0.5021, 0.8948, 0.5062, 0.5000, 0.5289, 0.5576,
        0.5062, 0.8854, 0.5270, 0.5227, 0.8764, 0.5212, 0.8919, 0.8874, 0.8880,
        0.8814, 0.5026, 0.5586, 0.5575, 0.8850, 0.5277, 0.8783, 0.8728, 0.8869,
        0.5536, 0.5172, 0.8861], grad_fn=<IndexBackward0>)
pred_loss tensor([0.1272], grad_fn=<MulBackward0>)
size_loss tensor(-1825.0416, grad_fn=<MulBackward0>)
size_num_loss 2.9000000000000004
loss: tensor([-1821.0376], grad_fn=<AddBackward0>)
10
epoch:  10 ; loss:  -1821.03759765625 ; pred:  tensor([3.8920e-04, 5.0719e-03, 9.8736e-01, 7.1822e-03],
       grad_fn=<SoftmaxBackward0>)
num_high 29 len(mask) 82
mask_without_small tensor([0.9003, 0.8961, 0.8903, 0.8850, 0.5031, 0.8976, 0.8876, 0.8976, 0.5299,
        0.8924, 0.8885, 0.8979, 0.8943, 0.8944, 0.8818, 0.8948, 0.5069, 0.8896,
        0.8983, 0.5215, 0.5206, 0.8914, 0.9024, 0.5086, 0.5379, 0.8936, 0.5067,
        0.5023, 0.8851, 0.5009, 0.8996, 0.8954, 0.8960, 0.8898, 0.5390, 0.5378,
        0.8932, 0.5074, 0.8869, 0.8818, 0.8950, 0.5339, 0.8942],
       grad_fn=<IndexBackward0>)
pred_loss tensor([0.1548], grad_fn=<MulBackward0>)
size_loss tensor(-1777.9519, grad_fn=<MulBackward0>)
size_num_loss 2.9000000000000004
loss: tensor([-1773.9982], grad_fn=<AddBackward0>)
11
epoch:  11 ; loss:  -1773.9981689453125 ; pred:  tensor([4.9564e-04, 6.2314e-03, 9.8464e-01, 8.6363e-03],
       grad_fn=<SoftmaxBackward0>)
num_high 29 len(mask) 82
mask_without_small tensor([0.9078, 0.9040, 0.8985, 0.8936, 0.9054, 0.8960, 0.9053, 0.5104, 0.9005,
        0.8969, 0.9057, 0.9023, 0.9024, 0.8905, 0.9027, 0.8979, 0.9060, 0.5017,
        0.5009, 0.8996, 0.9098, 0.5186, 0.9016, 0.8937, 0.9073, 0.9033, 0.9039,
        0.8981, 0.5198, 0.5185, 0.9013, 0.8954, 0.8905, 0.9029, 0.5145, 0.9022],
       grad_fn=<IndexBackward0>)
pred_loss tensor([0.1850], grad_fn=<MulBackward0>)
size_loss tensor(-1561.6223, grad_fn=<MulBackward0>)
size_num_loss 2.9000000000000004
loss: tensor([-1557.6794], grad_fn=<AddBackward0>)
12
epoch:  12 ; loss:  -1557.679443359375 ; pred:  tensor([6.1738e-04, 7.5105e-03, 9.8167e-01, 1.0206e-02],
       grad_fn=<SoftmaxBackward0>)
num_high 29 len(mask) 82
mask_without_small tensor([0.9151, 0.9115, 0.9064, 0.9017, 0.9128, 0.9040, 0.9127, 0.9082, 0.9048,
        0.9130, 0.9099, 0.9100, 0.8989, 0.9103, 0.9058, 0.9133, 0.9074, 0.9169,
        0.9093, 0.9019, 0.9145, 0.9109, 0.9114, 0.9059, 0.5002, 0.9089, 0.9034,
        0.8989, 0.9105, 0.9098], grad_fn=<IndexBackward0>)
pred_loss tensor([0.2172], grad_fn=<MulBackward0>)
size_loss tensor(-746.9224, grad_fn=<MulBackward0>)
size_num_loss 2.9000000000000004
loss: tensor([-742.9836], grad_fn=<AddBackward0>)
13
epoch:  13 ; loss:  -742.9835815429688 ; pred:  tensor([7.5207e-04, 8.8782e-03, 9.7852e-01, 1.1853e-02],
       grad_fn=<SoftmaxBackward0>)
num_high 29 len(mask) 82
mask_without_small tensor([0.9217, 0.9183, 0.9134, 0.9089, 0.9195, 0.9111, 0.9194, 0.9152, 0.9119,
        0.9197, 0.9167, 0.9168, 0.9061, 0.9171, 0.9128, 0.9200, 0.9143, 0.9234,
        0.9161, 0.9090, 0.9212, 0.9177, 0.9182, 0.9129, 0.9158, 0.9105, 0.9061,
        0.9173, 0.9167], grad_fn=<IndexBackward0>)
pred_loss tensor([0.2504], grad_fn=<MulBackward0>)
size_loss tensor(-45.2652, grad_fn=<MulBackward0>)
size_num_loss 2.9000000000000004
loss: tensor([-41.3035], grad_fn=<AddBackward0>)
14
epoch:  14 ; loss:  -41.30350875854492 ; pred:  tensor([8.9640e-04, 1.0298e-02, 9.7527e-01, 1.3535e-02],
       grad_fn=<SoftmaxBackward0>)
num_high 29 len(mask) 82
mask_without_small tensor([0.9281, 0.9248, 0.9179, 0.9103, 0.9261, 0.9140, 0.9260, 0.9208, 0.9153,
        0.9263, 0.9230, 0.9232, 0.9061, 0.9235, 0.9170, 0.9266, 0.9195, 0.9296,
        0.9222, 0.9105, 0.9276, 0.9242, 0.9247, 0.9172, 0.9218, 0.9130, 0.9061,
        0.9237, 0.9229], grad_fn=<IndexBackward0>)
pred_loss tensor([0.2855], grad_fn=<MulBackward0>)
size_loss tensor(-65.1990, grad_fn=<MulBackward0>)
size_num_loss 2.9000000000000004
loss: tensor([-61.2071], grad_fn=<AddBackward0>)
15
epoch:  15 ; loss:  -61.20705032348633 ; pred:  tensor([0.0011, 0.0118, 0.9718, 0.0153], grad_fn=<SoftmaxBackward0>)
num_high 29 len(mask) 82
mask_without_small tensor([0.9343, 0.9311, 0.9209, 0.9089, 0.9324, 0.9144, 0.9324, 0.9258, 0.9165,
        0.9327, 0.9289, 0.9292, 0.9034, 0.9296, 0.9193, 0.9329, 0.9236, 0.9355,
        0.9279, 0.9092, 0.9339, 0.9304, 0.9310, 0.9196, 0.9272, 0.9128, 0.9034,
        0.9298, 0.9288], grad_fn=<IndexBackward0>)
pred_loss tensor([0.3216], grad_fn=<MulBackward0>)
size_loss tensor(-95.5931, grad_fn=<MulBackward0>)
size_num_loss 2.9000000000000004
loss: tensor([-91.5697], grad_fn=<AddBackward0>)
16
epoch:  16 ; loss:  -91.56967163085938 ; pred:  tensor([0.0012, 0.0134, 0.9684, 0.0171], grad_fn=<SoftmaxBackward0>)
num_high 29 len(mask) 82
mask_without_small tensor([0.9401, 0.9371, 0.9227, 0.9056, 0.9384, 0.9130, 0.9384, 0.9302, 0.9160,
        0.9386, 0.9346, 0.9348, 0.8988, 0.9354, 0.9202, 0.9389, 0.9270, 0.9412,
        0.9332, 0.9060, 0.9398, 0.9363, 0.9370, 0.9208, 0.9323, 0.9107, 0.8988,
        0.9356, 0.9344], grad_fn=<IndexBackward0>)
pred_loss tensor([0.3580], grad_fn=<MulBackward0>)
size_loss tensor(-132.6289, grad_fn=<MulBackward0>)
size_num_loss 2.9000000000000004
loss: tensor([-128.5732], grad_fn=<AddBackward0>)
17
epoch:  17 ; loss:  -128.5732421875 ; pred:  tensor([0.0014, 0.0149, 0.9648, 0.0189], grad_fn=<SoftmaxBackward0>)
num_high 29 len(mask) 82
mask_without_small tensor([0.9455, 0.9426, 0.9237, 0.9007, 0.9440, 0.9101, 0.9439, 0.9344, 0.9141,
        0.9442, 0.9398, 0.9402, 0.8926, 0.9408, 0.9200, 0.9444, 0.9299, 0.9464,
        0.9382, 0.9012, 0.9452, 0.9418, 0.9425, 0.9208, 0.9371, 0.9071, 0.8927,
        0.9411, 0.9397], grad_fn=<IndexBackward0>)
pred_loss tensor([0.3946], grad_fn=<MulBackward0>)
size_loss tensor(-174.6218, grad_fn=<MulBackward0>)
size_num_loss 2.9000000000000004
loss: tensor([-170.5336], grad_fn=<AddBackward0>)
18
epoch:  18 ; loss:  -170.53358459472656 ; pred:  tensor([0.0016, 0.0165, 0.9613, 0.0206], grad_fn=<SoftmaxBackward0>)
num_high 29 len(mask) 82
mask_without_small tensor([0.9505, 0.9478, 0.9238, 0.8945, 0.9491, 0.9058, 0.9490, 0.9383, 0.9110,
        0.9493, 0.9448, 0.9451, 0.8851, 0.9458, 0.9188, 0.9495, 0.9324, 0.9512,
        0.9429, 0.8951, 0.9502, 0.9469, 0.9477, 0.9199, 0.9417, 0.9022, 0.8852,
        0.9462, 0.9446], grad_fn=<IndexBackward0>)
pred_loss tensor([0.4310], grad_fn=<MulBackward0>)
size_loss tensor(-220.8118, grad_fn=<MulBackward0>)
size_num_loss 2.9000000000000004
loss: tensor([-216.6909], grad_fn=<AddBackward0>)
19
epoch:  19 ; loss:  -216.69093322753906 ; pred:  tensor([0.0018, 0.0181, 0.9578, 0.0224], grad_fn=<SoftmaxBackward0>)
Finished Training
sel masked ver tensor(indices=tensor([[7146, 7188, 7268, 7275, 7336, 7525, 7526, 7562, 7570,
                        7577, 7579, 7580, 7590, 7597, 7606, 7627, 5677, 5677,
                        5677, 5677, 5677, 5677, 5677, 5677, 5677, 5677, 5677,
                        5677, 5677],
                       [5677, 5677, 5677, 5677, 5677, 5677, 5677, 5677, 5677,
                        5677, 5677, 5677, 5677, 5677, 5677, 5677,  116, 7271,
                        7411, 7526, 7538, 7561, 7562, 7570, 7597, 7603, 7608,
                        7627, 5230]]),
       values=tensor([0.9505, 0.9478, 0.9238, 0.8945, 0.9491, 0.9058, 0.9490,
                      0.9383, 0.9110, 0.9493, 0.9448, 0.9451, 0.8851, 0.9458,
                      0.9188, 0.9495, 0.9324, 0.9512, 0.9429, 0.8951, 0.9502,
                      0.9469, 0.9477, 0.9199, 0.9417, 0.9022, 0.8852, 0.9462,
                      0.9446]),
       size=(7628, 7628), nnz=29, layout=torch.sparse_coo,
       grad_fn=<SparseCooTensorWithDimsAndTensorsBackward0>)
Counter({'author': 16, 'publication': 11, 'phone': 1, 'type': 1})
dict index: {}
node_idx 5677
 node original label [2]
 node predicted label explain 2
 node prediction probability explain tensor([0.0018, 0.0181, 0.9578, 0.0224], grad_fn=<SoftmaxBackward0>)
 node predicted label full 2 most important relations  {'phone': 1, 'type': 1, 'publication': 11, 'author': 16, 'label': 2, 'node_idx': '5677'}
 final masks and lenght tensor(indices=tensor([[ 23716,  23758,  23838,  23841,  23845,  23855,  23856,
                         23898,  23905,  23906,  23981,  24006,  24064,  24090,
                         24091,  24095,  24096,  24097,  24108,  24111,  24131,
                         24132,  24140,  24147,  24149,  24150,  24160,  24167,
                         24171,  24173,  24174,  24176,  24178,  24179,  24195,
                         24197,  24210,  24211,  81915,  88527, 196232, 229372,
                        254227, 254227, 254227, 254227, 254227, 254227, 254227,
                        254227, 254227, 254227, 254227, 254227, 254227, 254227,
                        254227, 254227, 254227, 254227, 254227, 254227, 254227,
                        254227, 254227, 254227, 254227, 254227, 254227, 254227,
                        254227, 254227, 254227, 254227, 254227, 254227, 254227,
                        254227, 254227, 254227, 254227, 328792],
                       [  5677,   5677,   5677,   5677,   5677,   5677,   5677,
                          5677,   5677,   5677,   5677,   5677,   5677,   5677,
                          5677,   5677,   5677,   5677,   5677,   5677,   5677,
                          5677,   5677,   5677,   5677,   5677,   5677,   5677,
                          5677,   5677,   5677,   5677,   5677,   5677,   5677,
                          5677,   5677,   5677,   5677,      0,   4253,    116,
                          7146,   7188,   7268,   7271,   7275,   7285,   7286,
                          7328,   7335,   7336,   7350,   7411,   7436,   7494,
                          7520,   7521,   7525,   7526,   7527,   7538,   7541,
                          7561,   7562,   7570,   7577,   7579,   7580,   7590,
                          7597,   7601,   7603,   7604,   7606,   7608,   7609,
                          7625,   7627,   7640,   7641,   5230]]),
       values=tensor([0.9505, 0.9478, 0.9238, 0.3669, 0.8945, 0.4015, 0.3888,
                      0.3888, 0.3915, 0.9491, 0.3849, 0.3957, 0.3923, 0.3788,
                      0.3909, 0.9058, 0.9490, 0.3940, 0.3810, 0.4070, 0.3913,
                      0.9383, 0.9110, 0.9493, 0.9448, 0.9451, 0.8851, 0.9458,
                      0.3911, 0.3927, 0.3903, 0.9188, 0.3963, 0.3874, 0.3914,
                      0.9495, 0.3978, 0.3837, 0.3970, 0.3908, 0.3907, 0.9324,
                      0.3872, 0.3775, 0.4005, 0.9512, 0.4019, 0.3816, 0.3862,
                      0.3754, 0.3946, 0.4159, 0.3815, 0.9429, 0.3897, 0.3923,
                      0.3960, 0.3926, 0.3880, 0.8951, 0.3865, 0.9502, 0.3772,
                      0.9469, 0.9477, 0.9199, 0.3983, 0.3780, 0.4183, 0.4158,
                      0.9417, 0.3934, 0.9022, 0.3933, 0.3817, 0.8852, 0.4198,
                      0.3892, 0.9462, 0.4115, 0.3927, 0.9446]),
       size=(753935, 8285), nnz=82, layout=torch.sparse_coo,
       grad_fn=<SparseCooTensorWithDimsAndTensorsBackward0>) 29
 ---------------------------------------------------------------
r_exp.py:102: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  hor_graph, ver_graph = torch.tensor(sub_hor_graph, dtype=torch.float), torch.tensor(sub_ver_graph, dtype=torch.float)
node label: 2
202
num_high 1676 len(mask) 1676
mask_without_small tensor([0.7439, 0.7410, 0.7371,  ..., 0.7245, 0.7416, 0.7436],
       grad_fn=<IndexBackward0>)
pred_loss tensor([-0.], grad_fn=<MulBackward0>)
size_loss tensor(-0.6741, grad_fn=<MulBackward0>)
size_num_loss 167.60000000000002
loss: tensor([780.1035], grad_fn=<AddBackward0>)
wandb: WARNING Calling wandb.login() after wandb.init() has no effect.
0
epoch:  0 ; loss:  780.103515625 ; pred:  tensor([0., 0., 1., 0.], grad_fn=<SoftmaxBackward0>)
num_high 1676 len(mask) 1676
mask_without_small tensor([0.7244, 0.7214, 0.7173,  ..., 0.7041, 0.7220, 0.7240],
       grad_fn=<IndexBackward0>)
pred_loss tensor([-0.], grad_fn=<MulBackward0>)
size_loss tensor(-0.7046, grad_fn=<MulBackward0>)
size_num_loss 167.60000000000002
loss: tensor([763.2437], grad_fn=<AddBackward0>)
1
epoch:  1 ; loss:  763.2437133789062 ; pred:  tensor([0., 0., 1., 0.], grad_fn=<SoftmaxBackward0>)
num_high 1676 len(mask) 1676
mask_without_small tensor([0.7041, 0.7009, 0.6966,  ..., 0.6829, 0.7015, 0.7036],
       grad_fn=<IndexBackward0>)
pred_loss tensor([-0.], grad_fn=<MulBackward0>)
size_loss tensor(-0.7336, grad_fn=<MulBackward0>)
size_num_loss 167.60000000000002
loss: tensor([745.6738], grad_fn=<AddBackward0>)
2
epoch:  2 ; loss:  745.6737670898438 ; pred:  tensor([0.0000e+00, 8.4078e-45, 1.0000e+00, 0.0000e+00],
       grad_fn=<SoftmaxBackward0>)
num_high 1676 len(mask) 1676
mask_without_small tensor([0.6829, 0.6796, 0.6751,  ..., 0.6609, 0.6802, 0.6824],
       grad_fn=<IndexBackward0>)
pred_loss tensor([-0.], grad_fn=<MulBackward0>)
size_loss tensor(-0.7611, grad_fn=<MulBackward0>)
size_num_loss 167.60000000000002
loss: tensor([727.4532], grad_fn=<AddBackward0>)
3
epoch:  3 ; loss:  727.4532470703125 ; pred:  tensor([0.0000e+00, 3.5607e-42, 1.0000e+00, 0.0000e+00],
       grad_fn=<SoftmaxBackward0>)
num_high 1673 len(mask) 1676
mask_without_small tensor([0.6610, 0.6575, 0.6529,  ..., 0.6382, 0.6582, 0.6605],
       grad_fn=<IndexBackward0>)
pred_loss tensor([-0.], grad_fn=<MulBackward0>)
size_loss tensor(-0.7868, grad_fn=<MulBackward0>)
size_num_loss 167.3
loss: tensor([708.3558], grad_fn=<AddBackward0>)
4
epoch:  4 ; loss:  708.3557739257812 ; pred:  tensor([0.0000e+00, 1.6242e-39, 1.0000e+00, 0.0000e+00],
       grad_fn=<SoftmaxBackward0>)
num_high 860 len(mask) 1676
mask_without_small tensor([0.6384, 0.6349, 0.6302,  ..., 0.6150, 0.6356, 0.6380],
       grad_fn=<IndexBackward0>)
pred_loss tensor([-0.], grad_fn=<MulBackward0>)
size_loss tensor(-0.8105, grad_fn=<MulBackward0>)
size_num_loss 86.0
loss: tensor([607.7664], grad_fn=<AddBackward0>)
5
epoch:  5 ; loss:  607.7664184570312 ; pred:  tensor([0.0000e+00, 7.0760e-37, 1.0000e+00, 6.3899e-43],
       grad_fn=<SoftmaxBackward0>)
num_high 6 len(mask) 1676
mask_without_small tensor([0.6154, 0.6118, 0.6069,  ..., 0.5913, 0.6125, 0.6149],
       grad_fn=<IndexBackward0>)
pred_loss tensor([-0.], grad_fn=<MulBackward0>)
size_loss tensor(-0.8322, grad_fn=<MulBackward0>)
size_num_loss 0.6000000000000001
loss: tensor([100.8898], grad_fn=<AddBackward0>)
6
epoch:  6 ; loss:  100.88984680175781 ; pred:  tensor([0.0000e+00, 2.8146e-34, 1.0000e+00, 6.7033e-40],
       grad_fn=<SoftmaxBackward0>)
num_high 0 len(mask) 1676
mask_without_small tensor([0.5937, 0.5898, 0.5847,  ..., 0.5686, 0.5906, 0.5932],
       grad_fn=<IndexBackward0>)
pred_loss tensor([-0.], grad_fn=<MulBackward0>)
size_loss tensor(-0.8640, grad_fn=<MulBackward0>)
size_num_loss 0.0
loss: tensor([96.5103], grad_fn=<AddBackward0>)
7
epoch:  7 ; loss:  96.51031494140625 ; pred:  tensor([0.0000e+00, 6.9797e-32, 1.0000e+00, 4.0976e-37],
       grad_fn=<SoftmaxBackward0>)
num_high 0 len(mask) 1676
mask_without_small tensor([0.5732, 0.5692, 0.5638,  ..., 0.5469, 0.5700, 0.5727],
       grad_fn=<IndexBackward0>)
pred_loss tensor([-0.], grad_fn=<MulBackward0>)
size_loss tensor(-0.9067, grad_fn=<MulBackward0>)
size_num_loss 0.0
loss: tensor([92.8958], grad_fn=<AddBackward0>)
8
epoch:  8 ; loss:  92.89580535888672 ; pred:  tensor([0.0000e+00, 1.1163e-29, 1.0000e+00, 1.4941e-34],
       grad_fn=<SoftmaxBackward0>)
num_high 0 len(mask) 1676
mask_without_small tensor([0.5542, 0.5498, 0.5440,  ..., 0.5261, 0.5506, 0.5536],
       grad_fn=<IndexBackward0>)
pred_loss tensor([-0.], grad_fn=<MulBackward0>)
size_loss tensor(-0.9610, grad_fn=<MulBackward0>)
size_num_loss 0.0
loss: tensor([89.4510], grad_fn=<AddBackward0>)
9
epoch:  9 ; loss:  89.4510498046875 ; pred:  tensor([0.0000e+00, 1.1704e-27, 1.0000e+00, 3.3047e-32],
       grad_fn=<SoftmaxBackward0>)
num_high 0 len(mask) 1676
mask_without_small tensor([0.5365, 0.5317, 0.5255,  ..., 0.5063, 0.5327, 0.5359],
       grad_fn=<IndexBackward0>)
pred_loss tensor([-0.], grad_fn=<MulBackward0>)
size_loss tensor(-93.4937, grad_fn=<MulBackward0>)
size_num_loss 0.0
loss: tensor([-84.5409], grad_fn=<AddBackward0>)
10
epoch:  10 ; loss:  -84.5408935546875 ; pred:  tensor([4.4842e-43, 8.1380e-26, 1.0000e+00, 4.5338e-30],
       grad_fn=<SoftmaxBackward0>)
num_high 0 len(mask) 1676
mask_without_small tensor([0.5464, 0.5408, 0.5319, 0.5271, 0.5429, 0.5291, 0.5428, 0.5193, 0.5349,
        0.5299, 0.5433, 0.5379, 0.5382, 0.5253, 0.5387, 0.5311, 0.5438, 0.5131,
        0.5123, 0.5335, 0.5488, 0.5227, 0.5366, 0.5271, 0.5454, 0.5394, 0.5402,
        0.5310, 0.5499, 0.5226, 0.5147, 0.5379, 0.5226, 0.5187, 0.5243, 0.5230,
        0.5226, 0.5359, 0.5287, 0.5251, 0.5387, 0.5211, 0.5213, 0.5296, 0.5262,
        0.5245, 0.5347, 0.5289, 0.5177, 0.5038, 0.5097, 0.5378, 0.5209, 0.5283,
        0.5227, 0.5080, 0.5008, 0.5291, 0.5352, 0.5143, 0.5281, 0.5181, 0.5464,
        0.5338, 0.5427, 0.5265, 0.5243, 0.5359, 0.5323, 0.5342, 0.5261, 0.5329,
        0.5425, 0.5403, 0.5100, 0.5148, 0.5331, 0.5423, 0.5302, 0.5072, 0.5111,
        0.5035, 0.5389, 0.5380, 0.5290, 0.5294, 0.5476, 0.5001, 0.5005, 0.5340,
        0.5278, 0.5279, 0.5445, 0.5330, 0.5154, 0.5209, 0.5390, 0.5227, 0.5487,
        0.5042, 0.5347, 0.5383, 0.5202, 0.5022, 0.5194, 0.5349, 0.5411, 0.5125,
        0.5342, 0.5434, 0.5431, 0.5021, 0.5175, 0.5472, 0.5048, 0.5180, 0.5104,
        0.5203, 0.5405, 0.5376, 0.5310, 0.5222, 0.5231, 0.5240, 0.5220, 0.5068,
        0.5239, 0.5190, 0.5295, 0.5280, 0.5383, 0.5084, 0.5258, 0.5437, 0.5367,
        0.5465, 0.5283, 0.5333, 0.5182, 0.5361, 0.5100, 0.5332, 0.5475, 0.5089,
        0.5325, 0.5280, 0.5388, 0.5280, 0.5177, 0.5306, 0.5199, 0.5374, 0.5039,
        0.5368, 0.5191, 0.5345, 0.5204, 0.5161, 0.5164, 0.5278, 0.5191, 0.5097,
        0.5316, 0.5040, 0.5296, 0.5240, 0.5253, 0.5427, 0.5350, 0.5327, 0.5163,
        0.5471, 0.5461, 0.5435, 0.5459, 0.5029, 0.5490, 0.5436, 0.5147, 0.5262,
        0.5273, 0.5197, 0.5310, 0.5268, 0.5029, 0.5405, 0.5308, 0.5334, 0.5304,
        0.5330, 0.5021, 0.5327, 0.5334, 0.5359, 0.5294, 0.5273, 0.5250, 0.5128,
        0.5014, 0.5342, 0.5274, 0.5358, 0.5399, 0.5309, 0.5269, 0.5343, 0.5274,
        0.5366, 0.5291, 0.5213, 0.5304, 0.5421, 0.5350, 0.5352, 0.5027, 0.5367,
        0.5123, 0.5182, 0.5573, 0.5467, 0.5479, 0.5425, 0.5309, 0.5167, 0.5365,
        0.5264, 0.5469, 0.5279, 0.5303, 0.5256, 0.5186, 0.5224, 0.5047, 0.5027,
        0.5411, 0.5061, 0.5225, 0.5040, 0.5388, 0.5292, 0.5263, 0.5151, 0.5089,
        0.5367, 0.5338, 0.5253, 0.5371, 0.5354, 0.5428, 0.5474, 0.5355, 0.5445,
        0.5455, 0.5548, 0.5159, 0.5165, 0.5052, 0.5122, 0.5208, 0.5264, 0.5363,
        0.5229, 0.5253, 0.5049, 0.5495, 0.5286, 0.5288, 0.5128, 0.5300, 0.5217,
        0.5005, 0.5194, 0.5261, 0.5025, 0.5444, 0.5258, 0.5512, 0.5268, 0.5381,
        0.5068, 0.5429, 0.5423, 0.5190, 0.5159, 0.5291, 0.5365, 0.5224, 0.5014,
        0.5239, 0.5302, 0.5409, 0.5402, 0.5074, 0.5220, 0.5462, 0.5142, 0.5004,
        0.5294, 0.5200, 0.5087, 0.5448, 0.5351, 0.5307, 0.5489, 0.5012, 0.5330,
        0.5106, 0.5226, 0.5307, 0.5003, 0.5314, 0.5020, 0.5286, 0.5102, 0.5561,
        0.5190, 0.5415, 0.5401, 0.5595, 0.5389, 0.5310, 0.5133, 0.5196, 0.5412,
        0.5379, 0.5392, 0.5165, 0.5336, 0.5379, 0.5329, 0.5305, 0.5484, 0.5298,
        0.5106, 0.5043, 0.5152, 0.5441, 0.5170, 0.5145, 0.5070, 0.5334, 0.5379,
        0.5112, 0.5313, 0.5202, 0.5228, 0.5467, 0.5444, 0.5123, 0.5365, 0.5135,
        0.5575, 0.5304, 0.5299, 0.5459, 0.5248, 0.5243, 0.5146, 0.5226, 0.5251,
        0.5293, 0.5287, 0.5388, 0.5005, 0.5183, 0.5461, 0.5148, 0.5286, 0.5398,
        0.5273, 0.5322, 0.5228, 0.5052, 0.5404, 0.5139, 0.5349, 0.5118, 0.5191,
        0.5069, 0.5392, 0.5463, 0.5491, 0.5280, 0.5144, 0.5421, 0.5187, 0.5211,
        0.5213, 0.5338, 0.5171, 0.5304, 0.5407, 0.5307, 0.5335, 0.5282, 0.5358,
        0.5432, 0.5158, 0.5216, 0.5378, 0.5381, 0.5406, 0.5397, 0.5392, 0.5286,
        0.5388, 0.5322, 0.5329, 0.5133, 0.5457, 0.5353, 0.5027, 0.5221, 0.5432,
        0.5422, 0.5456, 0.5316, 0.5341, 0.5134, 0.5357, 0.5545, 0.5376, 0.5313,
        0.5186, 0.5313, 0.5278, 0.5153, 0.5247, 0.5322, 0.5268, 0.5084, 0.5008,
        0.5002, 0.5209, 0.5104, 0.5241, 0.5143, 0.5404, 0.5383, 0.5324, 0.5495,
        0.5199, 0.5309, 0.5365, 0.5276, 0.5452, 0.5232, 0.5191, 0.5004, 0.5177,
        0.5490, 0.5350, 0.5375, 0.5333, 0.5071, 0.5215, 0.5120, 0.5264, 0.5364,
        0.5277, 0.5269, 0.5014, 0.5354, 0.5236, 0.5420, 0.5330, 0.5270, 0.5372,
        0.5187, 0.5267, 0.5103, 0.5366, 0.5114, 0.5385, 0.5486, 0.5253, 0.5297,
        0.5295, 0.5273, 0.5149, 0.5002, 0.5119, 0.5388, 0.5404, 0.5345, 0.5302,
        0.5317, 0.5161, 0.5411, 0.5226, 0.5150, 0.5086, 0.5366, 0.5397, 0.5299,
        0.5215, 0.5076, 0.5309, 0.5053, 0.5167, 0.5372, 0.5372, 0.5278, 0.5135,
        0.5460, 0.5396, 0.5353, 0.5089, 0.5293, 0.5336, 0.5330, 0.5368, 0.5271,
        0.5048, 0.5423, 0.5015, 0.5356, 0.5645, 0.5232, 0.5004, 0.5371, 0.5136,
        0.5530, 0.5243, 0.5017, 0.5263, 0.5224, 0.5363, 0.5494, 0.5445, 0.5092,
        0.5449, 0.5180, 0.5312, 0.5582, 0.5403, 0.5023, 0.5373, 0.5339, 0.5338,
        0.5260, 0.5314, 0.5302, 0.5050, 0.5241, 0.5291, 0.5237, 0.5382, 0.5443,
        0.5060, 0.5408, 0.5238, 0.5149, 0.5309, 0.5477, 0.5164, 0.5212, 0.5351,
        0.5506, 0.5418, 0.5347, 0.5408, 0.5315, 0.5234, 0.5226, 0.5381, 0.5230,
        0.5373, 0.5398, 0.5031, 0.5252, 0.5119, 0.5282, 0.5233, 0.5326, 0.5191,
        0.5356, 0.5220, 0.5304, 0.5505, 0.5260, 0.5340, 0.5329, 0.5526, 0.5126,
        0.5350, 0.5034, 0.5222, 0.5356, 0.5090, 0.5426, 0.5270, 0.5334, 0.5392,
        0.5077, 0.5398, 0.5292, 0.5304, 0.5371, 0.5360, 0.5371, 0.5195, 0.5048,
        0.5228, 0.5363, 0.5439, 0.5480, 0.5208, 0.5014, 0.5260, 0.5180, 0.5136,
        0.5266, 0.5230, 0.5373, 0.5394, 0.5152, 0.5401, 0.5578, 0.5466, 0.5021,
        0.5283, 0.5366, 0.5473, 0.5319, 0.5243, 0.5442, 0.5049, 0.5445, 0.5346,
        0.5382, 0.5137, 0.5018, 0.5100, 0.5230, 0.5298, 0.5266, 0.5075, 0.5310,
        0.5245, 0.5339, 0.5159, 0.5189, 0.5476, 0.5125, 0.5411, 0.5261, 0.5164,
        0.5246, 0.5355, 0.5105, 0.5346, 0.5033, 0.5363, 0.5443, 0.5059, 0.5410,
        0.5388, 0.5411, 0.5163, 0.5081, 0.5040, 0.5257, 0.5198, 0.5161, 0.5094,
        0.5398, 0.5270, 0.5405, 0.5339, 0.5107, 0.5275, 0.5281, 0.5165, 0.5131,
        0.5226, 0.5289, 0.5245, 0.5374, 0.5396, 0.5322, 0.5222, 0.5236, 0.5344,
        0.5379, 0.5066, 0.5135, 0.5385, 0.5274, 0.5296, 0.5235, 0.5552, 0.5346,
        0.5048, 0.5469, 0.5358, 0.5066, 0.5181, 0.5373, 0.5360, 0.5270, 0.5323,
        0.5100, 0.5258, 0.5319, 0.5167, 0.5185, 0.5397, 0.5417, 0.5365, 0.5366,
        0.5480, 0.5267, 0.5200, 0.5172, 0.5357, 0.5304, 0.5327, 0.5362, 0.5424,
        0.5350, 0.5316, 0.5333, 0.5286, 0.5151, 0.5263, 0.5041, 0.5410, 0.5066,
        0.5430, 0.5347, 0.5223, 0.5011, 0.5371, 0.5322, 0.5178, 0.5087, 0.5006,
        0.5306, 0.5086, 0.5295, 0.5465, 0.5313, 0.5373, 0.5185, 0.5138, 0.5419,
        0.5457], grad_fn=<IndexBackward0>)
pred_loss tensor([-0.], grad_fn=<MulBackward0>)
size_loss tensor(-130.8165, grad_fn=<MulBackward0>)
size_num_loss 0.0
loss: tensor([-126.1772], grad_fn=<AddBackward0>)
11
epoch:  11 ; loss:  -126.17721557617188 ; pred:  tensor([9.4938e-42, 5.6268e-25, 1.0000e+00, 4.2309e-29],
       grad_fn=<SoftmaxBackward0>)
num_high 0 len(mask) 1676
mask_without_small tensor([0.5612, 0.5552, 0.5436, 0.5290, 0.5575, 0.5368, 0.5574, 0.5075, 0.5482,
        0.5393, 0.5579, 0.5519, 0.5522, 0.5211, 0.5528, 0.5421, 0.5584, 0.5004,
        0.5463, 0.5637, 0.5131, 0.5504, 0.5287, 0.5601, 0.5536, 0.5546, 0.5419,
        0.5649, 0.5129, 0.5021, 0.5519, 0.5130, 0.5068, 0.5173, 0.5139, 0.5130,
        0.5495, 0.5354, 0.5203, 0.5528, 0.5102, 0.5105, 0.5383, 0.5246, 0.5181,
        0.5480, 0.5361, 0.5055, 0.5518, 0.5098, 0.5339, 0.5133, 0.5370, 0.5486,
        0.5017, 0.5333, 0.5059, 0.5613, 0.5467, 0.5573, 0.5263, 0.5174, 0.5495,
        0.5444, 0.5472, 0.5241, 0.5453, 0.5571, 0.5546, 0.5022, 0.5456, 0.5569,
        0.5399, 0.5531, 0.5521, 0.5364, 0.5377, 0.5625, 0.5470, 0.5318, 0.5324,
        0.5592, 0.5455, 0.5028, 0.5099, 0.5531, 0.5131, 0.5636, 0.5480, 0.5524,
        0.5087, 0.5076, 0.5483, 0.5555, 0.5473, 0.5580, 0.5577, 0.5052, 0.5620,
        0.5058, 0.5089, 0.5548, 0.5515, 0.5418, 0.5122, 0.5142, 0.5164, 0.5117,
        0.5163, 0.5071, 0.5382, 0.5329, 0.5524, 0.5228, 0.5583, 0.5504, 0.5613,
        0.5341, 0.5459, 0.5061, 0.5498, 0.5457, 0.5624, 0.5447, 0.5326, 0.5530,
        0.5327, 0.5055, 0.5409, 0.5084, 0.5514, 0.5507, 0.5072, 0.5477, 0.5091,
        0.5036, 0.5040, 0.5319, 0.5073, 0.5430, 0.5384, 0.5165, 0.5208, 0.5572,
        0.5483, 0.5450, 0.5038, 0.5620, 0.5608, 0.5581, 0.5607, 0.5640, 0.5582,
        0.5021, 0.5248, 0.5298, 0.5081, 0.5418, 0.5276, 0.5549, 0.5414, 0.5461,
        0.5405, 0.5455, 0.5450, 0.5461, 0.5496, 0.5378, 0.5296, 0.5199, 0.5001,
        0.5473, 0.5303, 0.5494, 0.5542, 0.5416, 0.5279, 0.5474, 0.5303, 0.5504,
        0.5367, 0.5104, 0.5406, 0.5567, 0.5484, 0.5486, 0.5505, 0.5061, 0.5724,
        0.5615, 0.5628, 0.5571, 0.5417, 0.5043, 0.5502, 0.5256, 0.5617, 0.5323,
        0.5403, 0.5223, 0.5066, 0.5125, 0.5555, 0.5128, 0.5530, 0.5371, 0.5251,
        0.5025, 0.5506, 0.5467, 0.5211, 0.5509, 0.5489, 0.5574, 0.5622, 0.5490,
        0.5592, 0.5603, 0.5699, 0.5034, 0.5041, 0.5096, 0.5257, 0.5500, 0.5136,
        0.5209, 0.5645, 0.5350, 0.5358, 0.5395, 0.5112, 0.5076, 0.5242, 0.5591,
        0.5229, 0.5662, 0.5276, 0.5522, 0.5575, 0.5568, 0.5070, 0.5034, 0.5369,
        0.5502, 0.5126, 0.5162, 0.5399, 0.5553, 0.5545, 0.5117, 0.5610, 0.5016,
        0.5377, 0.5085, 0.5596, 0.5485, 0.5412, 0.5638, 0.5455, 0.5130, 0.5412,
        0.5427, 0.5350, 0.5712, 0.5071, 0.5560, 0.5545, 0.5747, 0.5530, 0.5419,
        0.5006, 0.5079, 0.5557, 0.5520, 0.5534, 0.5041, 0.5464, 0.5519, 0.5454,
        0.5407, 0.5633, 0.5389, 0.5026, 0.5588, 0.5046, 0.5019, 0.5461, 0.5519,
        0.5424, 0.5087, 0.5135, 0.5616, 0.5591, 0.5502, 0.5008, 0.5727, 0.5405,
        0.5391, 0.5607, 0.5192, 0.5174, 0.5020, 0.5129, 0.5201, 0.5373, 0.5355,
        0.5530, 0.5062, 0.5609, 0.5022, 0.5351, 0.5541, 0.5296, 0.5443, 0.5133,
        0.5547, 0.5012, 0.5482, 0.5072, 0.5534, 0.5611, 0.5640, 0.5326, 0.5018,
        0.5566, 0.5067, 0.5102, 0.5105, 0.5466, 0.5048, 0.5404, 0.5551, 0.5412,
        0.5463, 0.5334, 0.5494, 0.5578, 0.5033, 0.5111, 0.5518, 0.5521, 0.5550,
        0.5540, 0.5534, 0.5349, 0.5529, 0.5442, 0.5453, 0.5006, 0.5605, 0.5488,
        0.5120, 0.5578, 0.5567, 0.5603, 0.5430, 0.5472, 0.5007, 0.5492, 0.5696,
        0.5516, 0.5424, 0.5066, 0.5425, 0.5320, 0.5027, 0.5188, 0.5442, 0.5276,
        0.5099, 0.5168, 0.5017, 0.5547, 0.5524, 0.5446, 0.5645, 0.5083, 0.5418,
        0.5502, 0.5308, 0.5600, 0.5143, 0.5073, 0.5055, 0.5639, 0.5483, 0.5514,
        0.5460, 0.5108, 0.5254, 0.5501, 0.5315, 0.5278, 0.5488, 0.5154, 0.5565,
        0.5455, 0.5282, 0.5511, 0.5067, 0.5269, 0.5503, 0.5526, 0.5635, 0.5208,
        0.5386, 0.5382, 0.5295, 0.5023, 0.5530, 0.5548, 0.5477, 0.5401, 0.5433,
        0.5036, 0.5555, 0.5130, 0.5024, 0.5504, 0.5540, 0.5391, 0.5109, 0.5416,
        0.5043, 0.5511, 0.5511, 0.5318, 0.5008, 0.5608, 0.5538, 0.5488, 0.5374,
        0.5463, 0.5455, 0.5506, 0.5287, 0.5568, 0.5492, 0.5797, 0.5145, 0.5510,
        0.5009, 0.5681, 0.5175, 0.5251, 0.5127, 0.5500, 0.5644, 0.5593, 0.5596,
        0.5058, 0.5422, 0.5733, 0.5546, 0.5512, 0.5468, 0.5468, 0.5239, 0.5427,
        0.5400, 0.5167, 0.5368, 0.5157, 0.5523, 0.5589, 0.5552, 0.5158, 0.5022,
        0.5416, 0.5626, 0.5039, 0.5103, 0.5485, 0.5656, 0.5564, 0.5479, 0.5552,
        0.5429, 0.5148, 0.5130, 0.5522, 0.5138, 0.5512, 0.5541, 0.5207, 0.5335,
        0.5147, 0.5449, 0.5072, 0.5491, 0.5117, 0.5405, 0.5655, 0.5239, 0.5470,
        0.5453, 0.5676, 0.5483, 0.5121, 0.5492, 0.5572, 0.5283, 0.5461, 0.5534,
        0.5541, 0.5372, 0.5405, 0.5509, 0.5496, 0.5510, 0.5077, 0.5134, 0.5500,
        0.5586, 0.5629, 0.5097, 0.5237, 0.5058, 0.5009, 0.5267, 0.5139, 0.5512,
        0.5537, 0.5026, 0.5545, 0.5730, 0.5614, 0.5338, 0.5503, 0.5622, 0.5436,
        0.5175, 0.5589, 0.5592, 0.5478, 0.5522, 0.5010, 0.5138, 0.5390, 0.5267,
        0.5418, 0.5182, 0.5468, 0.5033, 0.5069, 0.5625, 0.5556, 0.5243, 0.5040,
        0.5184, 0.5490, 0.5478, 0.5500, 0.5590, 0.5554, 0.5529, 0.5555, 0.5039,
        0.5227, 0.5081, 0.5037, 0.5541, 0.5285, 0.5549, 0.5469, 0.5304, 0.5332,
        0.5041, 0.5004, 0.5130, 0.5361, 0.5180, 0.5513, 0.5538, 0.5441, 0.5121,
        0.5155, 0.5475, 0.5519, 0.5008, 0.5526, 0.5300, 0.5382, 0.5152, 0.5703,
        0.5478, 0.5617, 0.5494, 0.5060, 0.5512, 0.5496, 0.5283, 0.5443, 0.5231,
        0.5437, 0.5043, 0.5064, 0.5540, 0.5562, 0.5503, 0.5504, 0.5628, 0.5269,
        0.5085, 0.5049, 0.5493, 0.5406, 0.5450, 0.5499, 0.5569, 0.5483, 0.5431,
        0.5459, 0.5352, 0.5025, 0.5251, 0.5555, 0.5576, 0.5480, 0.5123, 0.5510,
        0.5442, 0.5056, 0.5410, 0.5382, 0.5613, 0.5425, 0.5513, 0.5064, 0.5011,
        0.5564, 0.5604], grad_fn=<IndexBackward0>)
pred_loss tensor([-0.], grad_fn=<MulBackward0>)
size_loss tensor(-201.0664, grad_fn=<MulBackward0>)
size_num_loss 0.0
loss: tensor([-196.9826], grad_fn=<AddBackward0>)
12
epoch:  12 ; loss:  -196.98260498046875 ; pred:  tensor([1.3102e-40, 3.0073e-24, 1.0000e+00, 2.9331e-28],
       grad_fn=<SoftmaxBackward0>)
num_high 0 len(mask) 1676
mask_without_small tensor([0.5789, 0.5727, 0.5591, 0.5201, 0.5751, 0.5436, 0.5750, 0.5650, 0.5510,
        0.5755, 0.5692, 0.5695, 0.5084, 0.5702, 0.5568, 0.5761, 0.5628, 0.5815,
        0.5675, 0.5195, 0.5778, 0.5710, 0.5720, 0.5565, 0.5827, 0.5692, 0.5039,
        0.5665, 0.5385, 0.5075, 0.5702, 0.5484, 0.5130, 0.5049, 0.5648, 0.5410,
        0.5691, 0.5329, 0.5441, 0.5654, 0.5310, 0.5790, 0.5633, 0.5749, 0.5154,
        0.5040, 0.5665, 0.5602, 0.5639, 0.5124, 0.5615, 0.5747, 0.5721, 0.5618,
        0.5745, 0.5525, 0.5705, 0.5694, 0.5421, 0.5466, 0.5802, 0.5636, 0.5264,
        0.5280, 0.5769, 0.5617, 0.5705, 0.5813, 0.5648, 0.5697, 0.5651, 0.5730,
        0.5639, 0.5757, 0.5754, 0.5797, 0.5723, 0.5688, 0.5563, 0.5003, 0.5029,
        0.5027, 0.5480, 0.5296, 0.5697, 0.5107, 0.5760, 0.5676, 0.5791, 0.5337,
        0.5623, 0.5668, 0.5620, 0.5802, 0.5607, 0.5287, 0.5703, 0.5290, 0.5547,
        0.5686, 0.5678, 0.5645, 0.5267, 0.5583, 0.5487, 0.5030, 0.5081, 0.5748,
        0.5652, 0.5611, 0.5797, 0.5786, 0.5758, 0.5784, 0.5818, 0.5759, 0.5133,
        0.5217, 0.5564, 0.5176, 0.5724, 0.5556, 0.5625, 0.5538, 0.5617, 0.5610,
        0.5625, 0.5666, 0.5469, 0.5213, 0.5070, 0.5639, 0.5228, 0.5664, 0.5716,
        0.5559, 0.5181, 0.5641, 0.5227, 0.5675, 0.5432, 0.5540, 0.5743, 0.5652,
        0.5655, 0.5676, 0.5903, 0.5793, 0.5805, 0.5747, 0.5562, 0.5673, 0.5145,
        0.5794, 0.5279, 0.5534, 0.5100, 0.5730, 0.5704, 0.5446, 0.5138, 0.5677,
        0.5632, 0.5085, 0.5681, 0.5658, 0.5750, 0.5800, 0.5660, 0.5768, 0.5780,
        0.5877, 0.5147, 0.5670, 0.5083, 0.5823, 0.5369, 0.5397, 0.5516, 0.5125,
        0.5768, 0.5108, 0.5841, 0.5176, 0.5695, 0.5751, 0.5744, 0.5440, 0.5673,
        0.5026, 0.5525, 0.5729, 0.5720, 0.5788, 0.5467, 0.5772, 0.5654, 0.5552,
        0.5816, 0.5617, 0.5551, 0.5578, 0.5371, 0.5890, 0.5735, 0.5719, 0.5926,
        0.5704, 0.5565, 0.5732, 0.5692, 0.5708, 0.5629, 0.5691, 0.5616, 0.5543,
        0.5811, 0.5502, 0.5764, 0.5624, 0.5692, 0.5574, 0.5793, 0.5768, 0.5674,
        0.5905, 0.5538, 0.5507, 0.5784, 0.5062, 0.5041, 0.5072, 0.5453, 0.5387,
        0.5704, 0.5786, 0.5373, 0.5716, 0.5212, 0.5601, 0.5722, 0.5650, 0.5708,
        0.5788, 0.5818, 0.5286, 0.5742, 0.5631, 0.5537, 0.5726, 0.5552, 0.5627,
        0.5311, 0.5664, 0.5754, 0.5690, 0.5694, 0.5725, 0.5715, 0.5708, 0.5366,
        0.5703, 0.5600, 0.5615, 0.5782, 0.5657, 0.5754, 0.5743, 0.5781, 0.5583,
        0.5638, 0.5662, 0.5875, 0.5688, 0.5574, 0.5574, 0.5269, 0.5056, 0.5600,
        0.5176, 0.5033, 0.5722, 0.5697, 0.5605, 0.5823, 0.5562, 0.5673, 0.5240,
        0.5777, 0.5003, 0.5817, 0.5651, 0.5687, 0.5623, 0.5142, 0.5672, 0.5256,
        0.5178, 0.5658, 0.5017, 0.5741, 0.5617, 0.5187, 0.5683, 0.5164, 0.5674,
        0.5699, 0.5813, 0.5081, 0.5493, 0.5480, 0.5212, 0.5704, 0.5723, 0.5645,
        0.5529, 0.5587, 0.5731, 0.5675, 0.5715, 0.5506, 0.5560, 0.5683, 0.5683,
        0.5264, 0.5786, 0.5713, 0.5657, 0.5456, 0.5628, 0.5617, 0.5677, 0.5195,
        0.5744, 0.5662, 0.5976, 0.5005, 0.5681, 0.5859, 0.5042, 0.5137, 0.5671,
        0.5822, 0.5769, 0.5773, 0.5571, 0.5912, 0.5721, 0.5684, 0.5633, 0.5633,
        0.5120, 0.5578, 0.5527, 0.5032, 0.5436, 0.5020, 0.5696, 0.5766, 0.5727,
        0.5021, 0.5559, 0.5804, 0.5654, 0.5834, 0.5739, 0.5647, 0.5727, 0.5582,
        0.5009, 0.5695, 0.5684, 0.5716, 0.5079, 0.5315, 0.5008, 0.5609, 0.5661,
        0.5539, 0.5833, 0.5121, 0.5636, 0.5615, 0.5855, 0.5652, 0.5662, 0.5748,
        0.5187, 0.5624, 0.5708, 0.5715, 0.5447, 0.5539, 0.5681, 0.5666, 0.5681,
        0.5670, 0.5762, 0.5806, 0.5118, 0.5160, 0.5684, 0.5711, 0.5719, 0.5909,
        0.5791, 0.5326, 0.5675, 0.5799, 0.5591, 0.5041, 0.5765, 0.5769, 0.5646,
        0.5696, 0.5504, 0.5162, 0.5563, 0.5050, 0.5634, 0.5803, 0.5731, 0.5127,
        0.5052, 0.5659, 0.5645, 0.5670, 0.5766, 0.5730, 0.5703, 0.5730, 0.5105,
        0.5716, 0.5191, 0.5724, 0.5635, 0.5230, 0.5305, 0.5411, 0.5047, 0.5685,
        0.5712, 0.5599, 0.5017, 0.5642, 0.5692, 0.5699, 0.5221, 0.5482, 0.5014,
        0.5881, 0.5645, 0.5795, 0.5664, 0.5684, 0.5666, 0.5187, 0.5602, 0.5111,
        0.5593, 0.5714, 0.5738, 0.5674, 0.5675, 0.5806, 0.5165, 0.5663, 0.5540,
        0.5611, 0.5670, 0.5745, 0.5651, 0.5584, 0.5623, 0.5377, 0.5138, 0.5730,
        0.5752, 0.5647, 0.5682, 0.5599, 0.5549, 0.5480, 0.5791, 0.5575, 0.5685,
        0.5740, 0.5782], grad_fn=<IndexBackward0>)
pred_loss tensor([-0.], grad_fn=<MulBackward0>)
size_loss tensor(-236.6019, grad_fn=<MulBackward0>)
size_num_loss 0.0
loss: tensor([-233.1979], grad_fn=<AddBackward0>)
13
epoch:  13 ; loss:  -233.19789123535156 ; pred:  tensor([1.2483e-39, 1.2835e-23, 1.0000e+00, 1.5655e-27],
       grad_fn=<SoftmaxBackward0>)
num_high 0 len(mask) 1676
mask_without_small tensor([0.5986, 0.5922, 0.5756, 0.5065, 0.5947, 0.5352, 0.5946, 0.5839, 0.5518,
        0.5951, 0.5885, 0.5889, 0.5896, 0.5704, 0.5957, 0.5811, 0.6011, 0.5867,
        0.5058, 0.5974, 0.5905, 0.5915, 0.5697, 0.6024, 0.5885, 0.5856, 0.5278,
        0.5896, 0.5446, 0.5836, 0.5313, 0.5884, 0.5210, 0.5360, 0.5844, 0.5188,
        0.5986, 0.5818, 0.5945, 0.5013, 0.5856, 0.5775, 0.5825, 0.5795, 0.5942,
        0.5916, 0.5799, 0.5940, 0.5567, 0.5899, 0.5887, 0.5329, 0.5407, 0.5999,
        0.5822, 0.5136, 0.5154, 0.5965, 0.5797, 0.5899, 0.6010, 0.5836, 0.5891,
        0.5840, 0.5925, 0.5826, 0.5953, 0.5949, 0.5994, 0.5918, 0.5881, 0.5693,
        0.5435, 0.5172, 0.5891, 0.5955, 0.5868, 0.5987, 0.5220, 0.5805, 0.5859,
        0.5802, 0.5998, 0.5782, 0.5162, 0.5897, 0.5165, 0.5643, 0.5879, 0.5870,
        0.5832, 0.5139, 0.5740, 0.5454, 0.5944, 0.5841, 0.5789, 0.5994, 0.5982,
        0.5953, 0.5980, 0.6014, 0.5955, 0.5082, 0.5693, 0.5037, 0.5919, 0.5670,
        0.5808, 0.5613, 0.5797, 0.5787, 0.5808, 0.5857, 0.5412, 0.5078, 0.5826,
        0.5095, 0.5855, 0.5911, 0.5681, 0.5042, 0.5828, 0.5094, 0.5867, 0.5345,
        0.5620, 0.5938, 0.5841, 0.5844, 0.5868, 0.6099, 0.5989, 0.6002, 0.5943,
        0.5687, 0.5865, 0.5002, 0.5991, 0.5153, 0.5597, 0.5926, 0.5898, 0.5368,
        0.5869, 0.5817, 0.5874, 0.5848, 0.5946, 0.5996, 0.5850, 0.5965, 0.5977,
        0.6074, 0.5004, 0.5862, 0.6019, 0.5259, 0.5295, 0.5537, 0.5964, 0.6037,
        0.5037, 0.5888, 0.5947, 0.5940, 0.5358, 0.5865, 0.5568, 0.5924, 0.5915,
        0.5984, 0.5407, 0.5969, 0.5843, 0.5659, 0.6012, 0.5797, 0.5656, 0.5728,
        0.5261, 0.6087, 0.5931, 0.5914, 0.6122, 0.5898, 0.5696, 0.5928, 0.5886,
        0.5902, 0.5813, 0.5885, 0.5795, 0.5629, 0.6007, 0.5494, 0.5960, 0.5807,
        0.5885, 0.5719, 0.5990, 0.5964, 0.5865, 0.6102, 0.5613, 0.5507, 0.5981,
        0.5380, 0.5282, 0.5898, 0.5982, 0.5264, 0.5910, 0.5077, 0.5772, 0.5917,
        0.5839, 0.5902, 0.5985, 0.6015, 0.5160, 0.5938, 0.5816, 0.5608, 0.5921,
        0.5658, 0.5811, 0.5189, 0.5855, 0.5950, 0.5883, 0.5888, 0.5919, 0.5909,
        0.5903, 0.5255, 0.5897, 0.5771, 0.5794, 0.5979, 0.5846, 0.5950, 0.5938,
        0.5977, 0.5740, 0.5824, 0.5853, 0.6071, 0.5881, 0.5719, 0.5721, 0.5142,
        0.5771, 0.5037, 0.5917, 0.5891, 0.5779, 0.6020, 0.5689, 0.5865, 0.5108,
        0.5973, 0.6014, 0.5840, 0.5880, 0.5806, 0.5863, 0.5126, 0.5039, 0.5847,
        0.5937, 0.5797, 0.5049, 0.5876, 0.5023, 0.5866, 0.5893, 0.6009, 0.5467,
        0.5437, 0.5076, 0.5898, 0.5918, 0.5833, 0.5580, 0.5749, 0.5926, 0.5867,
        0.5909, 0.5506, 0.5682, 0.5876, 0.5875, 0.5136, 0.5982, 0.5907, 0.5847,
        0.5386, 0.5812, 0.5797, 0.5869, 0.5058, 0.5940, 0.5852, 0.6172, 0.5874,
        0.6056, 0.5863, 0.6019, 0.5965, 0.5969, 0.5711, 0.6108, 0.5916, 0.5877,
        0.5818, 0.5818, 0.5730, 0.5575, 0.5351, 0.5889, 0.5962, 0.5922, 0.5681,
        0.6000, 0.5843, 0.6031, 0.5935, 0.5835, 0.5922, 0.5737, 0.5888, 0.5877,
        0.5910, 0.5193, 0.5786, 0.5851, 0.5616, 0.6030, 0.5822, 0.5794, 0.6051,
        0.5841, 0.5852, 0.5944, 0.5049, 0.5807, 0.5902, 0.5910, 0.5371, 0.5615,
        0.5874, 0.5857, 0.5874, 0.5862, 0.5958, 0.6003, 0.5019, 0.5877, 0.5905,
        0.5914, 0.6105, 0.5988, 0.5207, 0.5866, 0.5996, 0.5756, 0.5961, 0.5965,
        0.5834, 0.5889, 0.5498, 0.5021, 0.5693, 0.5819, 0.5999, 0.5926, 0.5850,
        0.5833, 0.5862, 0.5962, 0.5925, 0.5897, 0.5926, 0.5910, 0.5054, 0.5919,
        0.5820, 0.5097, 0.5183, 0.5314, 0.5878, 0.5907, 0.5770, 0.5829, 0.5886,
        0.5893, 0.5087, 0.5442, 0.6078, 0.5833, 0.5991, 0.5855, 0.5877, 0.5857,
        0.5049, 0.5774, 0.5759, 0.5908, 0.5933, 0.5866, 0.5867, 0.6003, 0.5024,
        0.5853, 0.5618, 0.5788, 0.5861, 0.5941, 0.5840, 0.5742, 0.5805, 0.5268,
        0.5925, 0.5948, 0.5836, 0.5875, 0.5770, 0.5649, 0.5435, 0.5987, 0.5722,
        0.5878, 0.5935, 0.5978], grad_fn=<IndexBackward0>)
pred_loss tensor([-0.], grad_fn=<MulBackward0>)
size_loss tensor(-280.9297, grad_fn=<MulBackward0>)
size_num_loss 0.0
loss: tensor([-277.7411], grad_fn=<AddBackward0>)
14
epoch:  14 ; loss:  -277.7410583496094 ; pred:  tensor([8.1487e-39, 4.3278e-23, 1.0000e+00, 6.3339e-27],
       grad_fn=<SoftmaxBackward0>)
num_high 19 len(mask) 1676
mask_without_small tensor([0.6194, 0.6130, 0.5905, 0.6155, 0.5212, 0.6154, 0.6041, 0.5408, 0.6160,
        0.6092, 0.6095, 0.6103, 0.5756, 0.6165, 0.6006, 0.6220, 0.6073, 0.6183,
        0.6112, 0.6123, 0.5738, 0.6232, 0.6092, 0.6060, 0.5129, 0.6103, 0.5320,
        0.6037, 0.5168, 0.6091, 0.5055, 0.5220, 0.6046, 0.5030, 0.6195, 0.6014,
        0.6153, 0.6060, 0.5948, 0.6024, 0.5981, 0.6151, 0.6124, 0.5988, 0.6148,
        0.5474, 0.6106, 0.6094, 0.5186, 0.5275, 0.6207, 0.6019, 0.6173, 0.5985,
        0.6106, 0.6218, 0.6038, 0.6097, 0.6042, 0.6133, 0.6025, 0.6161, 0.6158,
        0.6202, 0.6126, 0.6087, 0.5724, 0.5308, 0.5013, 0.6097, 0.6164, 0.6073,
        0.6196, 0.5065, 0.5997, 0.6064, 0.5992, 0.6207, 0.5961, 0.5002, 0.6105,
        0.5005, 0.5602, 0.6085, 0.6076, 0.6033, 0.5865, 0.5330, 0.6152, 0.6043,
        0.5972, 0.6202, 0.6191, 0.6162, 0.6189, 0.6223, 0.6163, 0.5724, 0.6127,
        0.5662, 0.6001, 0.5545, 0.5984, 0.5969, 0.6001, 0.6061, 0.5281, 0.6025,
        0.6059, 0.6118, 0.5692, 0.6028, 0.6072, 0.5204, 0.5559, 0.6146, 0.6043,
        0.6047, 0.6074, 0.6307, 0.6198, 0.6210, 0.6151, 0.5708, 0.6070, 0.6199,
        0.5518, 0.6133, 0.6105, 0.5230, 0.6074, 0.6014, 0.6079, 0.6051, 0.6154,
        0.6205, 0.6054, 0.6173, 0.6185, 0.6282, 0.6066, 0.6228, 0.5107, 0.5147,
        0.5432, 0.6173, 0.6245, 0.6095, 0.6155, 0.6148, 0.5218, 0.6070, 0.5476,
        0.6132, 0.6123, 0.6192, 0.5275, 0.6177, 0.6046, 0.5637, 0.6221, 0.5985,
        0.5631, 0.5831, 0.5110, 0.6295, 0.6139, 0.6122, 0.6329, 0.6105, 0.5733,
        0.6136, 0.6092, 0.6110, 0.6008, 0.6091, 0.5982, 0.5573, 0.6216, 0.5378,
        0.6169, 0.6000, 0.6092, 0.5802, 0.6198, 0.6172, 0.6070, 0.6309, 0.5545,
        0.5395, 0.6189, 0.5244, 0.5133, 0.6105, 0.6191, 0.5113, 0.6118, 0.5942,
        0.6125, 0.6040, 0.6109, 0.6193, 0.6223, 0.5000, 0.6146, 0.6013, 0.5537,
        0.6129, 0.5635, 0.6006, 0.5031, 0.6059, 0.6158, 0.6090, 0.6094, 0.6127,
        0.6117, 0.6110, 0.5103, 0.6104, 0.5940, 0.5981, 0.6187, 0.6049, 0.6158,
        0.6146, 0.6185, 0.5863, 0.6023, 0.6056, 0.6279, 0.6088, 0.5802, 0.5808,
        0.5938, 0.6124, 0.6097, 0.5955, 0.6228, 0.5713, 0.6070, 0.6182, 0.6222,
        0.6042, 0.6086, 0.5998, 0.6068, 0.6050, 0.6145, 0.5985, 0.6082, 0.6071,
        0.6100, 0.6218, 0.5346, 0.5310, 0.6105, 0.6125, 0.6033, 0.5493, 0.5888,
        0.6134, 0.6072, 0.6117, 0.5393, 0.5695, 0.6082, 0.6081, 0.6190, 0.6114,
        0.6050, 0.5250, 0.6006, 0.5985, 0.6075, 0.6148, 0.6056, 0.6378, 0.6080,
        0.6264, 0.6068, 0.6227, 0.6174, 0.6178, 0.5779, 0.6315, 0.6123, 0.6083,
        0.6015, 0.6015, 0.5835, 0.5486, 0.5211, 0.6096, 0.6171, 0.6130, 0.5691,
        0.6209, 0.6046, 0.6239, 0.6143, 0.6036, 0.6130, 0.5856, 0.6095, 0.6083,
        0.6118, 0.5036, 0.5967, 0.6055, 0.5551, 0.6238, 0.6020, 0.5981, 0.6259,
        0.6043, 0.6056, 0.6152, 0.6000, 0.6110, 0.6117, 0.5234, 0.5549, 0.6079,
        0.6061, 0.6080, 0.6067, 0.6167, 0.6211, 0.6083, 0.6113, 0.6122, 0.6312,
        0.6196, 0.5050, 0.6072, 0.6204, 0.5906, 0.6170, 0.6173, 0.6034, 0.6096,
        0.5384, 0.5724, 0.6017, 0.6208, 0.6134, 0.6053, 0.6034, 0.6067, 0.6171,
        0.6133, 0.6104, 0.6134, 0.6118, 0.6126, 0.6018, 0.5024, 0.5169, 0.6084,
        0.6114, 0.5937, 0.6029, 0.6092, 0.6100, 0.5315, 0.6285, 0.6034, 0.6200,
        0.6059, 0.6083, 0.6061, 0.5946, 0.5913, 0.6116, 0.6142, 0.6071, 0.6072,
        0.6211, 0.6057, 0.5554, 0.5970, 0.6066, 0.6149, 0.6042, 0.5869, 0.5997,
        0.5118, 0.6133, 0.6156, 0.6037, 0.6080, 0.5938, 0.5614, 0.5308, 0.6195,
        0.5812, 0.6084, 0.6144, 0.6186], grad_fn=<IndexBackward0>)
pred_loss tensor([-0.], grad_fn=<MulBackward0>)
size_loss tensor(-319.5472, grad_fn=<MulBackward0>)
size_num_loss 1.9000000000000001
loss: tensor([-314.5821], grad_fn=<AddBackward0>)
15
epoch:  15 ; loss:  -314.5821228027344 ; pred:  tensor([3.9758e-38, 1.2148e-22, 1.0000e+00, 2.0608e-26],
       grad_fn=<SoftmaxBackward0>)
num_high 264 len(mask) 1676
mask_without_small tensor([0.6409, 0.6345, 0.5983, 0.6370, 0.5042, 0.6369, 0.6250, 0.5255, 0.6375,
        0.6306, 0.6310, 0.6318, 0.5675, 0.6380, 0.6204, 0.6435, 0.6286, 0.6398,
        0.6327, 0.6338, 0.5648, 0.6447, 0.6306, 0.6272, 0.6318, 0.5159, 0.6245,
        0.6305, 0.5051, 0.6256, 0.6410, 0.6216, 0.6368, 0.6272, 0.6094, 0.6229,
        0.6165, 0.6366, 0.6339, 0.6177, 0.6364, 0.5329, 0.6321, 0.6308, 0.5014,
        0.5109, 0.6422, 0.6223, 0.6389, 0.6170, 0.6321, 0.6433, 0.6246, 0.6312,
        0.6251, 0.6348, 0.6230, 0.6376, 0.6373, 0.6418, 0.6341, 0.6301, 0.5629,
        0.5145, 0.6312, 0.6379, 0.6286, 0.6411, 0.6191, 0.6276, 0.6183, 0.6422,
        0.6124, 0.6319, 0.5476, 0.6299, 0.6289, 0.6240, 0.5877, 0.5169, 0.6368,
        0.6252, 0.6147, 0.6418, 0.6406, 0.6377, 0.6404, 0.6437, 0.6379, 0.5630,
        0.6342, 0.5549, 0.6197, 0.5410, 0.6170, 0.6141, 0.6197, 0.6273, 0.5116,
        0.6230, 0.6270, 0.6333, 0.5587, 0.6234, 0.6285, 0.5033, 0.5425, 0.6362,
        0.6253, 0.6257, 0.6287, 0.6520, 0.6413, 0.6425, 0.6366, 0.5608, 0.6283,
        0.6414, 0.5379, 0.6349, 0.6319, 0.5061, 0.6288, 0.6215, 0.6293, 0.6262,
        0.6370, 0.6420, 0.6265, 0.6388, 0.6400, 0.6495, 0.6279, 0.6442, 0.5282,
        0.6388, 0.6460, 0.6309, 0.6370, 0.6363, 0.5049, 0.6282, 0.5331, 0.6347,
        0.6338, 0.6408, 0.5110, 0.6392, 0.6256, 0.5518, 0.6436, 0.6170, 0.5511,
        0.5802, 0.6508, 0.6354, 0.6337, 0.6542, 0.6320, 0.5642, 0.6351, 0.6306,
        0.6324, 0.6207, 0.6305, 0.6166, 0.5442, 0.6431, 0.5222, 0.6384, 0.6196,
        0.6306, 0.5748, 0.6413, 0.6388, 0.6283, 0.6522, 0.5410, 0.5240, 0.6404,
        0.5077, 0.6320, 0.6406, 0.6333, 0.6080, 0.6340, 0.6249, 0.6324, 0.6408,
        0.6438, 0.6361, 0.6214, 0.5400, 0.6344, 0.5516, 0.6204, 0.6271, 0.6374,
        0.6304, 0.6309, 0.6343, 0.6332, 0.6325, 0.6319, 0.6075, 0.6164, 0.6402,
        0.6260, 0.6374, 0.6362, 0.6401, 0.5873, 0.6227, 0.6268, 0.6493, 0.6302,
        0.5749, 0.5758, 0.6071, 0.6340, 0.6312, 0.6111, 0.6443, 0.5614, 0.6283,
        0.6397, 0.6437, 0.6252, 0.6300, 0.6192, 0.6281, 0.6261, 0.6360, 0.6171,
        0.6295, 0.6284, 0.6315, 0.6433, 0.5187, 0.5147, 0.6320, 0.6341, 0.6241,
        0.5350, 0.5935, 0.6349, 0.6285, 0.6332, 0.5238, 0.5591, 0.6295, 0.6295,
        0.6406, 0.6329, 0.6260, 0.5083, 0.6205, 0.6170, 0.6288, 0.6363, 0.6267,
        0.6589, 0.6293, 0.6478, 0.6280, 0.6442, 0.6389, 0.6393, 0.5710, 0.6528,
        0.6338, 0.6296, 0.6218, 0.6217, 0.5811, 0.5342, 0.5041, 0.6310, 0.6386,
        0.6345, 0.5585, 0.6424, 0.6255, 0.6454, 0.6358, 0.6244, 0.6345, 0.5856,
        0.6309, 0.6296, 0.6333, 0.6136, 0.6266, 0.5416, 0.6453, 0.6224, 0.6163,
        0.6474, 0.6252, 0.6267, 0.6368, 0.6195, 0.6325, 0.6332, 0.5065, 0.5414,
        0.6293, 0.6273, 0.6293, 0.6279, 0.6382, 0.6426, 0.6296, 0.6328, 0.6337,
        0.6525, 0.6411, 0.6285, 0.6419, 0.5985, 0.6385, 0.6389, 0.6242, 0.6310,
        0.5228, 0.5629, 0.6219, 0.6423, 0.6350, 0.6264, 0.6241, 0.6279, 0.6386,
        0.6348, 0.6319, 0.6349, 0.6333, 0.6342, 0.6221, 0.6298, 0.6329, 0.6067,
        0.6235, 0.6306, 0.6314, 0.5153, 0.6499, 0.6241, 0.6415, 0.6270, 0.6297,
        0.6273, 0.6090, 0.6002, 0.6331, 0.6357, 0.6284, 0.6285, 0.6426, 0.6268,
        0.5420, 0.6144, 0.6278, 0.6364, 0.6251, 0.5887, 0.6191, 0.6348, 0.6371,
        0.6245, 0.6294, 0.6069, 0.5490, 0.5145, 0.6411, 0.5767, 0.6297, 0.6359,
        0.6402], grad_fn=<IndexBackward0>)
pred_loss tensor([-0.], grad_fn=<MulBackward0>)
size_loss tensor(-373.5281, grad_fn=<MulBackward0>)
size_num_loss 26.400000000000002
loss: tensor([-344.1167], grad_fn=<AddBackward0>)
16
epoch:  16 ; loss:  -344.11669921875 ; pred:  tensor([1.6219e-37, 3.0369e-22, 1.0000e+00, 5.8319e-26],
       grad_fn=<SoftmaxBackward0>)
num_high 300 len(mask) 1676
mask_without_small tensor([0.6627, 0.6564, 0.5937, 0.6589, 0.6588, 0.6463, 0.5074, 0.6593, 0.6524,
        0.6528, 0.6536, 0.5533, 0.6599, 0.6402, 0.6652, 0.6503, 0.6616, 0.6545,
        0.6556, 0.5502, 0.6663, 0.6524, 0.6487, 0.6536, 0.6457, 0.6523, 0.6470,
        0.6627, 0.6419, 0.6587, 0.6487, 0.6171, 0.6436, 0.6334, 0.6584, 0.6557,
        0.6356, 0.6582, 0.5152, 0.6539, 0.6526, 0.6640, 0.6429, 0.6607, 0.6344,
        0.6540, 0.6650, 0.6458, 0.6530, 0.6464, 0.6567, 0.6438, 0.6595, 0.6591,
        0.6635, 0.6560, 0.6519, 0.5480, 0.6530, 0.6597, 0.6503, 0.6628, 0.6381,
        0.6492, 0.6368, 0.6639, 0.6245, 0.6538, 0.5311, 0.6517, 0.6506, 0.6451,
        0.5779, 0.6586, 0.6465, 0.6298, 0.6635, 0.6623, 0.6595, 0.6622, 0.6654,
        0.6597, 0.5482, 0.6561, 0.5391, 0.6391, 0.5239, 0.6344, 0.6285, 0.6391,
        0.6489, 0.6438, 0.6486, 0.6552, 0.5433, 0.6443, 0.6502, 0.5256, 0.6580,
        0.6466, 0.6471, 0.6504, 0.6734, 0.6630, 0.6642, 0.6585, 0.5457, 0.6499,
        0.6632, 0.5206, 0.6567, 0.6538, 0.6505, 0.6418, 0.6510, 0.6477, 0.6588,
        0.6637, 0.6479, 0.6606, 0.6618, 0.6710, 0.6495, 0.6659, 0.5102, 0.6606,
        0.6676, 0.6528, 0.6589, 0.6582, 0.6499, 0.5154, 0.6565, 0.6556, 0.6625,
        0.6610, 0.6469, 0.5358, 0.6652, 0.6344, 0.5349, 0.5683, 0.6722, 0.6572,
        0.6555, 0.6755, 0.6538, 0.5495, 0.6569, 0.6524, 0.6543, 0.6406, 0.6523,
        0.6336, 0.5275, 0.6648, 0.5038, 0.6602, 0.6389, 0.6524, 0.5619, 0.6631,
        0.6606, 0.6500, 0.6736, 0.5239, 0.5057, 0.6622, 0.6538, 0.6624, 0.6551,
        0.6138, 0.6558, 0.6462, 0.6542, 0.6626, 0.6655, 0.6580, 0.6416, 0.5229,
        0.6562, 0.5355, 0.6401, 0.6486, 0.6592, 0.6522, 0.6527, 0.6561, 0.6550,
        0.6543, 0.6537, 0.6127, 0.6332, 0.6620, 0.6474, 0.6592, 0.6580, 0.6618,
        0.5774, 0.6434, 0.6483, 0.6708, 0.6520, 0.5619, 0.5630, 0.6116, 0.6558,
        0.6530, 0.6215, 0.6659, 0.5464, 0.6500, 0.6615, 0.6654, 0.6465, 0.6517,
        0.6383, 0.6498, 0.6476, 0.6579, 0.6345, 0.6513, 0.6501, 0.6533, 0.6650,
        0.5001, 0.6538, 0.6559, 0.6451, 0.5175, 0.5860, 0.6568, 0.6502, 0.6550,
        0.5055, 0.5438, 0.6513, 0.6512, 0.6623, 0.6548, 0.6475, 0.6403, 0.6345,
        0.6505, 0.6582, 0.6482, 0.6801, 0.6511, 0.6694, 0.6497, 0.6658, 0.6607,
        0.6611, 0.5573, 0.6742, 0.6557, 0.6514, 0.6421, 0.6420, 0.5694, 0.5166,
        0.6528, 0.6604, 0.6563, 0.5432, 0.6641, 0.6469, 0.6670, 0.6577, 0.6456,
        0.6564, 0.5751, 0.6527, 0.6514, 0.6551, 0.6273, 0.6481, 0.5246, 0.6669,
        0.6429, 0.6331, 0.6689, 0.6465, 0.6482, 0.6586, 0.6388, 0.6543, 0.6551,
        0.5244, 0.6510, 0.6489, 0.6511, 0.6496, 0.6600, 0.6643, 0.6514, 0.6546,
        0.6555, 0.6739, 0.6629, 0.6501, 0.6636, 0.5940, 0.6603, 0.6607, 0.6453,
        0.6528, 0.5045, 0.5480, 0.6423, 0.6640, 0.6568, 0.6479, 0.6452, 0.6496,
        0.6604, 0.6566, 0.6537, 0.6567, 0.6552, 0.6560, 0.6425, 0.6515, 0.6548,
        0.6107, 0.6445, 0.6524, 0.6533, 0.6714, 0.6452, 0.6632, 0.6486, 0.6514,
        0.6489, 0.6162, 0.5972, 0.6549, 0.6575, 0.6501, 0.6502, 0.6643, 0.6484,
        0.5250, 0.6290, 0.6494, 0.6583, 0.6464, 0.5792, 0.6381, 0.6567, 0.6590,
        0.6456, 0.6511, 0.6112, 0.5327, 0.6628, 0.5640, 0.6515, 0.6577, 0.6619],
       grad_fn=<IndexBackward0>)
pred_loss tensor([-0.], grad_fn=<MulBackward0>)
size_loss tensor(-414.5932, grad_fn=<MulBackward0>)
size_num_loss 30.0
loss: tensor([-381.6259], grad_fn=<AddBackward0>)
17
epoch:  17 ; loss:  -381.6258544921875 ; pred:  tensor([5.3636e-37, 6.6440e-22, 1.0000e+00, 1.4096e-25],
       grad_fn=<SoftmaxBackward0>)
num_high 300 len(mask) 1676
mask_without_small tensor([0.6843, 0.6782, 0.5812, 0.6806, 0.6805, 0.6675, 0.6811, 0.6742, 0.6746,
        0.6754, 0.5359, 0.6816, 0.6591, 0.6867, 0.6720, 0.6833, 0.6764, 0.6775,
        0.5325, 0.6878, 0.6742, 0.6703, 0.6754, 0.6668, 0.6741, 0.6684, 0.6844,
        0.6618, 0.6804, 0.6703, 0.6129, 0.6642, 0.6461, 0.6802, 0.6775, 0.6509,
        0.6800, 0.6758, 0.6744, 0.6855, 0.6631, 0.6823, 0.6484, 0.6758, 0.6865,
        0.6669, 0.6748, 0.6676, 0.6785, 0.6644, 0.6812, 0.6809, 0.6851, 0.6778,
        0.6737, 0.5301, 0.6748, 0.6815, 0.6720, 0.6844, 0.6557, 0.6709, 0.6532,
        0.6854, 0.6265, 0.6756, 0.5121, 0.6734, 0.6724, 0.6660, 0.5629, 0.6804,
        0.6678, 0.6381, 0.6851, 0.6840, 0.6813, 0.6838, 0.6869, 0.6814, 0.5303,
        0.6779, 0.5206, 0.6574, 0.5045, 0.6483, 0.6352, 0.6573, 0.6705, 0.6644,
        0.6702, 0.6770, 0.5251, 0.6651, 0.6719, 0.5062, 0.6798, 0.6679, 0.6684,
        0.6721, 0.6945, 0.6846, 0.6858, 0.6802, 0.5276, 0.6716, 0.6848, 0.5009,
        0.6785, 0.6756, 0.6722, 0.6616, 0.6728, 0.6691, 0.6805, 0.6853, 0.6695,
        0.6823, 0.6834, 0.6923, 0.6712, 0.6874, 0.6823, 0.6890, 0.6746, 0.6806,
        0.6799, 0.6716, 0.6783, 0.6774, 0.6841, 0.6827, 0.6683, 0.5170, 0.6867,
        0.6484, 0.5161, 0.5522, 0.6934, 0.6790, 0.6773, 0.6966, 0.6756, 0.5317,
        0.6787, 0.6742, 0.6761, 0.6598, 0.6741, 0.6467, 0.5082, 0.6863, 0.6819,
        0.6570, 0.6742, 0.5451, 0.6847, 0.6823, 0.6717, 0.6948, 0.5045, 0.6838,
        0.6756, 0.6840, 0.6769, 0.6076, 0.6776, 0.6674, 0.6761, 0.6842, 0.6870,
        0.6797, 0.6613, 0.5034, 0.6780, 0.5167, 0.6590, 0.6702, 0.6809, 0.6740,
        0.6745, 0.6779, 0.6768, 0.6761, 0.6755, 0.6059, 0.6457, 0.6836, 0.6688,
        0.6809, 0.6798, 0.6835, 0.5623, 0.6639, 0.6698, 0.6921, 0.6737, 0.5452,
        0.5464, 0.6043, 0.6776, 0.6748, 0.6206, 0.6874, 0.5284, 0.6716, 0.6831,
        0.6869, 0.6677, 0.6735, 0.6560, 0.6714, 0.6690, 0.6796, 0.6486, 0.6730,
        0.6718, 0.6751, 0.6865, 0.6756, 0.6777, 0.6661, 0.5721, 0.6786, 0.6719,
        0.6769, 0.5256, 0.6731, 0.6730, 0.6839, 0.6766, 0.6689, 0.6593, 0.6484,
        0.6722, 0.6799, 0.6697, 0.7009, 0.6728, 0.6907, 0.6713, 0.6873, 0.6824,
        0.6828, 0.5402, 0.6953, 0.6775, 0.6732, 0.6621, 0.6620, 0.5534, 0.6746,
        0.6821, 0.6781, 0.5249, 0.6856, 0.6682, 0.6884, 0.6794, 0.6666, 0.6782,
        0.5597, 0.6745, 0.6732, 0.6770, 0.6326, 0.6696, 0.5052, 0.6883, 0.6632,
        0.6455, 0.6903, 0.6678, 0.6697, 0.6803, 0.6568, 0.6761, 0.6769, 0.5050,
        0.6728, 0.6705, 0.6728, 0.6712, 0.6817, 0.6859, 0.6732, 0.6764, 0.6773,
        0.6951, 0.6845, 0.6719, 0.6852, 0.5815, 0.6820, 0.6824, 0.6663, 0.6746,
        0.5301, 0.6624, 0.6855, 0.6786, 0.6694, 0.6662, 0.6712, 0.6821, 0.6784,
        0.6755, 0.6785, 0.6770, 0.6778, 0.6627, 0.6733, 0.6766, 0.6031, 0.6653,
        0.6742, 0.6751, 0.6926, 0.6662, 0.6848, 0.6702, 0.6732, 0.6705, 0.6114,
        0.5854, 0.6768, 0.6793, 0.6718, 0.6719, 0.6859, 0.6699, 0.5056, 0.6364,
        0.6711, 0.6800, 0.6677, 0.5643, 0.6555, 0.6785, 0.6807, 0.6668, 0.6729,
        0.6038, 0.5138, 0.6844, 0.5475, 0.6733, 0.6795, 0.6836],
       grad_fn=<IndexBackward0>)
pred_loss tensor([-0.], grad_fn=<MulBackward0>)
size_loss tensor(-478.7915, grad_fn=<MulBackward0>)
size_num_loss 30.0
loss: tensor([-445.8293], grad_fn=<AddBackward0>)
18
epoch:  18 ; loss:  -445.829345703125 ; pred:  tensor([1.4332e-36, 1.2736e-21, 1.0000e+00, 2.9147e-25],
       grad_fn=<SoftmaxBackward0>)
num_high 299 len(mask) 1676
mask_without_small tensor([0.7055, 0.6996, 0.5645, 0.7020, 0.7019, 0.6885, 0.7024, 0.6957, 0.6961,
        0.6969, 0.5158, 0.7029, 0.6768, 0.7077, 0.6934, 0.7045, 0.6978, 0.6989,
        0.5123, 0.7087, 0.6957, 0.6917, 0.6969, 0.6877, 0.6956, 0.6895, 0.7055,
        0.6809, 0.7018, 0.6917, 0.6008, 0.6843, 0.6513, 0.7015, 0.6990, 0.6612,
        0.7013, 0.6973, 0.6960, 0.7066, 0.6829, 0.7036, 0.6560, 0.6973, 0.7076,
        0.6878, 0.6963, 0.6886, 0.6999, 0.6846, 0.7025, 0.7022, 0.7062, 0.6992,
        0.6952, 0.5098, 0.6963, 0.7028, 0.6935, 0.7056, 0.6706, 0.6923, 0.6658,
        0.7065, 0.6184, 0.6971, 0.6949, 0.6938, 0.6867, 0.5446, 0.7017, 0.6889,
        0.6363, 0.7062, 0.7051, 0.7026, 0.7050, 0.7079, 0.7027, 0.5099, 0.6993,
        0.6738, 0.6559, 0.6314, 0.6737, 0.6919, 0.6845, 0.6915, 0.6985, 0.5045,
        0.6855, 0.6934, 0.7012, 0.6890, 0.6896, 0.6936, 0.7151, 0.7058, 0.7069,
        0.7016, 0.5071, 0.6931, 0.7059, 0.7000, 0.6971, 0.6937, 0.6806, 0.6943,
        0.6904, 0.7019, 0.7064, 0.6907, 0.7036, 0.7046, 0.7130, 0.6926, 0.7084,
        0.7035, 0.7099, 0.6961, 0.7020, 0.7013, 0.6931, 0.6998, 0.6989, 0.7053,
        0.7039, 0.6894, 0.7078, 0.6561, 0.5331, 0.7141, 0.7004, 0.6988, 0.7170,
        0.6971, 0.5115, 0.7002, 0.6958, 0.6976, 0.6779, 0.6956, 0.6526, 0.7073,
        0.7032, 0.6730, 0.6957, 0.5256, 0.7058, 0.7035, 0.6931, 0.7153, 0.7050,
        0.6971, 0.7052, 0.6984, 0.5944, 0.6991, 0.6883, 0.6976, 0.7054, 0.7080,
        0.7011, 0.6802, 0.6995, 0.6766, 0.6916, 0.7022, 0.6955, 0.6960, 0.6994,
        0.6983, 0.6976, 0.6970, 0.5924, 0.6505, 0.7048, 0.6900, 0.7022, 0.7012,
        0.7047, 0.5440, 0.6839, 0.6912, 0.7128, 0.6953, 0.5257, 0.5270, 0.5906,
        0.6991, 0.6963, 0.6104, 0.7084, 0.5079, 0.6931, 0.7043, 0.7079, 0.6888,
        0.6950, 0.6713, 0.6929, 0.6902, 0.7010, 0.6565, 0.6946, 0.6933, 0.6966,
        0.7075, 0.6971, 0.6992, 0.6868, 0.5546, 0.7000, 0.6934, 0.6983, 0.5050,
        0.6946, 0.6945, 0.7051, 0.6981, 0.6901, 0.6771, 0.6562, 0.6937, 0.7013,
        0.6911, 0.7211, 0.6943, 0.7115, 0.6928, 0.7083, 0.7037, 0.7040, 0.5204,
        0.7158, 0.6990, 0.6947, 0.6814, 0.6812, 0.5344, 0.6962, 0.7034, 0.6996,
        0.5043, 0.7067, 0.6894, 0.7093, 0.7008, 0.6875, 0.6996, 0.5412, 0.6960,
        0.6947, 0.6984, 0.6273, 0.6909, 0.7093, 0.6830, 0.6501, 0.7111, 0.6888,
        0.6910, 0.7017, 0.6728, 0.6976, 0.6984, 0.6943, 0.6919, 0.6943, 0.6927,
        0.7030, 0.7069, 0.6947, 0.6979, 0.6988, 0.7156, 0.7056, 0.6933, 0.7063,
        0.5649, 0.7033, 0.7036, 0.6871, 0.6962, 0.5098, 0.6818, 0.7066, 0.7000,
        0.6906, 0.6869, 0.6926, 0.7034, 0.6999, 0.6970, 0.7000, 0.6985, 0.6993,
        0.6822, 0.6948, 0.6981, 0.5891, 0.6858, 0.6957, 0.6966, 0.7133, 0.6869,
        0.7059, 0.6915, 0.6947, 0.6919, 0.5990, 0.5691, 0.6982, 0.7007, 0.6933,
        0.6933, 0.7069, 0.6912, 0.6333, 0.6925, 0.7014, 0.6887, 0.5461, 0.6704,
        0.6999, 0.7021, 0.6876, 0.6944, 0.5900, 0.7056, 0.5282, 0.6948, 0.7009,
        0.7048], grad_fn=<IndexBackward0>)
pred_loss tensor([-0.], grad_fn=<MulBackward0>)
size_loss tensor(-488.7655, grad_fn=<MulBackward0>)
size_num_loss 29.900000000000002
loss: tensor([-455.9300], grad_fn=<AddBackward0>)
19
epoch:  19 ; loss:  -455.9300231933594 ; pred:  tensor([3.1537e-36, 2.1600e-21, 1.0000e+00, 5.2371e-25],
       grad_fn=<SoftmaxBackward0>)
Finished Training
sel masked ver tensor(indices=tensor([[6813, 6817, 6822, 6849, 6867, 6925, 6940, 6941, 6944,
                        6946, 6985, 7037, 7212, 7217, 7264, 7312, 7324, 7331,
                        7342, 7457, 7486, 7740, 7752, 7760, 7780, 7907, 7928,
                        7931, 7935, 7964, 7973, 8009, 8027, 8032, 8035, 5916,
                        5942, 5504, 5504, 5504, 5352, 5353, 5353, 5443, 5443,
                        5444, 5444, 5459, 5460, 5460, 5460, 5462, 5463, 5473,
                        5473, 5473, 6817, 6828, 6837, 6838, 6840, 6849, 6857,
                        6860, 6864, 6889, 6925, 6945, 6961, 7034, 7198, 7213,
                        7214, 7311, 7312, 7323, 7463, 7486, 7486, 7728, 7741,
                        7744, 7753, 7778, 7779, 7779, 7904, 7921, 7922, 7922,
                        7923, 7934, 7972, 8016, 8017, 8032, 8038, 8038, 8044,
                        5915, 5915, 5948, 5954, 5960, 5971, 5971, 5975, 5978,
                        5988, 6813, 6813, 6813, 6817, 6817, 6817, 6825, 6838,
                        6889, 6920, 6924, 6925, 6928, 6928, 6952, 6961, 6985,
                        7032, 7037, 7207, 7209, 7212, 7213, 7214, 7214, 7216,
                        7216, 7216, 7217, 7217, 7217, 7238, 7240, 7247, 7247,
                        7273, 7294, 7294, 7303, 7311, 7312, 7323, 7323, 7324,
                        7463, 7481, 7486, 7486, 7633, 7727, 7738, 7740, 7741,
                        7743, 7745, 7745, 7746, 7747, 7747, 7749, 7749, 7751,
                        7751, 7752, 7753, 7753, 7754, 7755, 7755, 7758, 7758,
                        7759, 7759, 7759, 7760, 7760, 7761, 7761, 7763, 7777,
                        7780, 7784, 7788, 7793, 7835, 7837, 7883, 7907, 7921,
                        7921, 7922, 7931, 7933, 7933, 7934, 7951, 7969, 7973,
                        7973, 8016, 8016, 8017, 8025, 8036, 8038, 8044, 5418,
                        5446, 5459, 5915, 5942, 5960, 5979, 5791, 5915, 5915,
                        5915, 5915, 5915, 5915, 5947, 5947, 5947, 5947, 5947,
                        5947, 5947, 5947, 5947, 5947, 5947, 5947, 5947, 5947,
                        5947, 5947, 5947, 5947, 5947, 5947, 5947, 5947, 5948,
                        5948, 5948, 5948, 5948, 5954, 5954, 5954, 5954, 5954,
                        5954, 5954, 5954, 5954, 5954, 5954, 5954, 5954, 5791,
                        5791, 5791, 5791, 5791, 5791, 5791, 5791, 5791, 5791,
                        5791, 5791, 5791, 5791, 5791, 5791, 5791, 5791, 5791,
                        5791, 5791, 5791, 5791, 5791, 5791, 5791, 5791, 5791,
                        5791, 5504, 5504, 5504, 5504, 5504, 5504, 5504, 5504,
                        5504, 5504, 5504, 5504, 5504, 5504, 5504, 5504, 5504,
                        5504, 5504, 5504, 5504, 5504, 5504, 5504, 5504, 5504,
                        5504, 5504, 5504, 5504, 5504, 5504, 5791, 5791, 5791,
                        5791],
                       [5791, 5791, 5791, 5791, 5791, 5791, 5791, 5791, 5791,
                        5791, 5791, 5791, 5791, 5791, 5791, 5791, 5791, 5791,
                        5791, 5791, 5791, 5791, 5791, 5791, 5791, 5791, 5791,
                        5791, 5791, 5791, 5791, 5791, 5791, 5791, 5791, 5504,
                        5504, 5942, 5947, 5971, 5915, 5942, 5960, 5942, 5960,
                        5954, 5960, 5964, 5947, 5971, 5988, 5947, 5947, 5964,
                        5971, 5988, 5960, 5954, 5948, 5948, 5948, 5948, 5947,
                        5954, 5954, 5947, 5947, 5954, 5947, 5979, 5964, 5915,
                        5915, 5915, 5915, 5915, 5954, 5947, 5954, 5948, 5947,
                        5954, 5947, 5947, 5947, 5954, 5947, 5948, 5948, 5954,
                        5954, 5947, 5947, 5947, 5947, 5954, 5948, 5954, 5915,
                        5351, 5352, 5351, 5444, 5462, 5351, 5471, 5471, 5471,
                        5460, 5353, 5443, 5473, 5443, 5471, 5473, 5459, 5351,
                        5351, 5460, 5471, 5473, 5349, 5443, 5460, 5463, 5462,
                        5460, 5471, 5462, 5351, 5462, 5459, 5352, 5460, 5352,
                        5353, 5459, 5351, 5352, 5459, 5473, 5471, 5351, 5471,
                        5460, 5460, 5462, 5351, 5462, 5351, 5352, 5460, 5443,
                        5471, 5459, 5418, 5462, 5443, 5460, 5471, 5443, 5460,
                        5459, 5352, 5471, 5459, 5460, 5471, 5460, 5462, 5462,
                        5471, 5460, 5462, 5463, 5471, 5462, 5471, 5351, 5471,
                        5352, 5459, 5462, 5443, 5460, 5353, 5443, 5351, 5353,
                        5351, 5471, 5462, 5459, 5460, 5459, 5462, 5471, 5463,
                        5471, 5459, 5471, 5459, 5462, 5459, 5351, 5471, 5351,
                        5459, 5459, 5463, 5460, 5471, 5459, 5471, 5351, 5791,
                        5791, 5791, 5791, 5791, 5791, 5791, 4921, 7216, 7251,
                        7312, 7457, 7749, 8044, 6864, 6867, 6889, 6941, 7032,
                        7051, 7061, 7728, 7729, 7738, 7743, 7857, 7858, 7904,
                        7922, 7923, 7934, 7969, 7975, 8017, 8025, 8027, 6849,
                        6945, 6986, 7922, 7931, 6817, 6840, 6856, 6860, 6998,
                        7463, 7741, 7779, 7835, 7904, 7975, 8032, 8036, 6834,
                        6837, 6838, 6946, 6961, 7051, 7207, 7209, 7238, 7247,
                        7264, 7311, 7323, 7331, 7342, 7615, 7727, 7740, 7749,
                        7759, 7761, 7780, 7784, 7793, 7857, 7858, 7969, 7973,
                        8016, 6924, 6940, 6941, 6962, 6998, 7037, 7216, 7217,
                        7240, 7294, 7465, 7476, 7502, 7541, 7727, 7729, 7740,
                        7741, 7743, 7752, 7754, 7759, 7760, 7762, 7763, 7779,
                        7870, 7923, 7928, 7951, 7964, 8044, 5947, 5954, 5203,
                        5230]]),
       values=tensor([0.7055, 0.6996, 0.5645, 0.7020, 0.7019, 0.6885, 0.7024,
                      0.6957, 0.6961, 0.6969, 0.5158, 0.7029, 0.6768, 0.7077,
                      0.6934, 0.7045, 0.6978, 0.6989, 0.5123, 0.7087, 0.6957,
                      0.6917, 0.6969, 0.6877, 0.6956, 0.6895, 0.7055, 0.6809,
                      0.7018, 0.6917, 0.6008, 0.6843, 0.6513, 0.7015, 0.6990,
                      0.6612, 0.7013, 0.6973, 0.6960, 0.7066, 0.6829, 0.7036,
                      0.6560, 0.6973, 0.7076, 0.6878, 0.6963, 0.6886, 0.6999,
                      0.6846, 0.7025, 0.7022, 0.7062, 0.6992, 0.6952, 0.5098,
                      0.6963, 0.7028, 0.6935, 0.7056, 0.6706, 0.6923, 0.6658,
                      0.7065, 0.6184, 0.6971, 0.6949, 0.6938, 0.6867, 0.5446,
                      0.7017, 0.6889, 0.6363, 0.7062, 0.7051, 0.7026, 0.7050,
                      0.7079, 0.7027, 0.5099, 0.6993, 0.6738, 0.6559, 0.6314,
                      0.6737, 0.6919, 0.6845, 0.6915, 0.6985, 0.5045, 0.6855,
                      0.6934, 0.7012, 0.6890, 0.6896, 0.6936, 0.7151, 0.7058,
                      0.7069, 0.7016, 0.5071, 0.6931, 0.7059, 0.7000, 0.6971,
                      0.6937, 0.6806, 0.6943, 0.6904, 0.7019, 0.7064, 0.6907,
                      0.7036, 0.7046, 0.7130, 0.6926, 0.7084, 0.7035, 0.7099,
                      0.6961, 0.7020, 0.7013, 0.6931, 0.6998, 0.6989, 0.7053,
                      0.7039, 0.6894, 0.7078, 0.6561, 0.5331, 0.7141, 0.7004,
                      0.6988, 0.7170, 0.6971, 0.5115, 0.7002, 0.6958, 0.6976,
                      0.6779, 0.6956, 0.6526, 0.7073, 0.7032, 0.6730, 0.6957,
                      0.5256, 0.7058, 0.7035, 0.6931, 0.7153, 0.7050, 0.6971,
                      0.7052, 0.6984, 0.5944, 0.6991, 0.6883, 0.6976, 0.7054,
                      0.7080, 0.7011, 0.6802, 0.6995, 0.6766, 0.6916, 0.7022,
                      0.6955, 0.6960, 0.6994, 0.6983, 0.6976, 0.6970, 0.5924,
                      0.6505, 0.7048, 0.6900, 0.7022, 0.7012, 0.7047, 0.5440,
                      0.6839, 0.6912, 0.7128, 0.6953, 0.5257, 0.5270, 0.5906,
                      0.6991, 0.6963, 0.6104, 0.7084, 0.5079, 0.6931, 0.7043,
                      0.7079, 0.6888, 0.6950, 0.6713, 0.6929, 0.6902, 0.7010,
                      0.6565, 0.6946, 0.6933, 0.6966, 0.7075, 0.6971, 0.6992,
                      0.6868, 0.5546, 0.7000, 0.6934, 0.6983, 0.5050, 0.6946,
                      0.6945, 0.7051, 0.6981, 0.6901, 0.6771, 0.6562, 0.6937,
                      0.7013, 0.6911, 0.7211, 0.6943, 0.7115, 0.6928, 0.7083,
                      0.7037, 0.7040, 0.5204, 0.7158, 0.6990, 0.6947, 0.6814,
                      0.6812, 0.5344, 0.6962, 0.7034, 0.6996, 0.5043, 0.7067,
                      0.6894, 0.7093, 0.7008, 0.6875, 0.6996, 0.5412, 0.6960,
                      0.6947, 0.6984, 0.6273, 0.6909, 0.7093, 0.6830, 0.6501,
                      0.7111, 0.6888, 0.6910, 0.7017, 0.6728, 0.6976, 0.6984,
                      0.6943, 0.6919, 0.6943, 0.6927, 0.7030, 0.7069, 0.6947,
                      0.6979, 0.6988, 0.7156, 0.7056, 0.6933, 0.7063, 0.5649,
                      0.7033, 0.7036, 0.6871, 0.6962, 0.5098, 0.6818, 0.7066,
                      0.7000, 0.6906, 0.6869, 0.6926, 0.7034, 0.6999, 0.6970,
                      0.7000, 0.6985, 0.6993, 0.6822, 0.6948, 0.6981, 0.5891,
                      0.6858, 0.6957, 0.6966, 0.7133, 0.6869, 0.7059, 0.6915,
                      0.6947, 0.6919, 0.5990, 0.5691, 0.6982, 0.7007, 0.6933,
                      0.6933, 0.7069, 0.6912, 0.6333, 0.6925, 0.7014, 0.6887,
                      0.5461, 0.6704, 0.6999, 0.7021, 0.6876, 0.6944, 0.5900,
                      0.7056, 0.5282, 0.6948, 0.7009, 0.7048]),
       size=(8045, 8045), nnz=334, layout=torch.sparse_coo,
       grad_fn=<SparseCooTensorWithDimsAndTensorsBackward0>)
Counter({'isAbout': 116, 'projectInfo': 46, 'hasProject': 43, 'author': 35, 'publishes': 32, 'publication': 29, 'dealtWithIn': 16, 'member': 4, 'isWorkedOnBy': 3, 'carriesOut': 3, 'type': 2, 'worksAtProject': 2, 'carriedOutBy': 2, 'name': 1})
dict index: {}
node_idx 5791
 node original label [2]
 node predicted label explain 2
 node prediction probability explain tensor([3.1537e-36, 2.1600e-21, 1.0000e+00, 5.2371e-25],
       grad_fn=<SoftmaxBackward0>)
 node predicted label full 2 most important relations  {'dealtWithIn': 16, 'isWorkedOnBy': 3, 'carriesOut': 3, 'publishes': 32, 'name': 1, 'type': 2, 'worksAtProject': 2, 'publication': 29, 'isAbout': 116, 'member': 4, 'projectInfo': 46, 'carriedOutBy': 2, 'author': 35, 'hasProject': 43, 'label': 2, 'node_idx': '5791'}
 final masks and lenght tensor(indices=tensor([[ 23383,  23387,  23392,  ..., 304051, 328906, 328906],
                       [  5791,   5791,   5791,  ...,   5988,   5203,   5230]]),
       values=tensor([0.7055, 0.6996, 0.5645,  ..., 0.4099, 0.7009, 0.7048]),
       size=(753935, 8285), nnz=1676, layout=torch.sparse_coo,
       grad_fn=<SparseCooTensorWithDimsAndTensorsBackward0>) 334
 ---------------------------------------------------------------
r_exp.py:102: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  hor_graph, ver_graph = torch.tensor(sub_hor_graph, dtype=torch.float), torch.tensor(sub_ver_graph, dtype=torch.float)
wandb: WARNING Calling wandb.login() after wandb.init() has no effect.
node label: 2
27
num_high 98 len(mask) 98
mask_without_small tensor([0.7816, 0.7707, 0.7556, 0.6680, 0.7497, 0.6950, 0.7298, 0.6837, 0.7094,
        0.7748, 0.7199, 0.6899, 0.7101, 0.7151, 0.7089, 0.7519, 0.7746, 0.7266,
        0.7169, 0.7432, 0.7092, 0.7602, 0.7529, 0.7756, 0.7654, 0.7659, 0.7479,
        0.7669, 0.7245, 0.7322, 0.7239, 0.7545, 0.6904, 0.7059, 0.7247, 0.7765,
        0.7399, 0.7190, 0.7396, 0.7088, 0.6851, 0.7581, 0.7056, 0.7138, 0.6938,
        0.7864, 0.6950, 0.7171, 0.7046, 0.7122, 0.7332, 0.7456, 0.7171, 0.7632,
        0.7076, 0.7099, 0.6899, 0.7321, 0.7293, 0.7496, 0.7283, 0.7796, 0.6965,
        0.7681, 0.7697, 0.7544, 0.7887, 0.7455, 0.7407, 0.7255, 0.7004, 0.7654,
        0.7262, 0.7455, 0.7326, 0.7429, 0.7469, 0.7127, 0.6648, 0.7095, 0.7314,
        0.7214, 0.7525, 0.7319, 0.7487, 0.7471, 0.7600, 0.7182, 0.7116, 0.7469,
        0.7423, 0.7360, 0.7384, 0.7653, 0.7310, 0.7224, 0.7025, 0.7345],
       grad_fn=<IndexBackward0>)
pred_loss tensor([0.0966], grad_fn=<MulBackward0>)
size_loss tensor(-2.7411, grad_fn=<MulBackward0>)
size_num_loss 9.8
loss: tensor([14.9137], grad_fn=<AddBackward0>)
0
epoch:  0 ; loss:  14.913705825805664 ; pred:  tensor([2.5567e-04, 6.9350e-03, 9.9038e-01, 2.4257e-03],
       grad_fn=<SoftmaxBackward0>)
num_high 98 len(mask) 98
mask_without_small tensor([0.7982, 0.7879, 0.7736, 0.6455, 0.7305, 0.6734, 0.7097, 0.6617, 0.6884,
        0.7918, 0.6993, 0.6681, 0.6891, 0.6943, 0.6879, 0.7701, 0.7916, 0.7062,
        0.6961, 0.7237, 0.6882, 0.7780, 0.7711, 0.7925, 0.7829, 0.7833, 0.7285,
        0.7843, 0.7041, 0.7122, 0.7035, 0.7726, 0.6687, 0.6847, 0.7043, 0.7934,
        0.7202, 0.6983, 0.7198, 0.6877, 0.6632, 0.7760, 0.6845, 0.6930, 0.6722,
        0.8027, 0.6734, 0.6964, 0.6834, 0.6913, 0.7132, 0.7261, 0.6964, 0.7808,
        0.6865, 0.6889, 0.6681, 0.7120, 0.7091, 0.7304, 0.7081, 0.7963, 0.6750,
        0.7854, 0.7869, 0.7725, 0.8048, 0.7261, 0.7210, 0.7051, 0.6790, 0.7829,
        0.7059, 0.7261, 0.7126, 0.7233, 0.7275, 0.6918, 0.6422, 0.6884, 0.7113,
        0.7009, 0.7707, 0.7118, 0.7294, 0.7278, 0.7777, 0.6976, 0.6906, 0.7275,
        0.7227, 0.7162, 0.7187, 0.7828, 0.7109, 0.7020, 0.6812, 0.7146],
       grad_fn=<IndexBackward0>)
pred_loss tensor([0.1167], grad_fn=<MulBackward0>)
size_loss tensor(-4.1877, grad_fn=<MulBackward0>)
size_num_loss 9.8
loss: tensor([13.3923], grad_fn=<AddBackward0>)
1
epoch:  1 ; loss:  13.392284393310547 ; pred:  tensor([3.3823e-04, 8.2813e-03, 9.8839e-01, 2.9855e-03],
       grad_fn=<SoftmaxBackward0>)
num_high 97 len(mask) 98
mask_without_small tensor([0.8138, 0.8042, 0.7894, 0.6225, 0.7146, 0.6512, 0.6887, 0.6391, 0.6666,
        0.8078, 0.6778, 0.6457, 0.6673, 0.6726, 0.6660, 0.7842, 0.8076, 0.6851,
        0.6746, 0.7043, 0.6664, 0.7945, 0.7858, 0.8084, 0.7994, 0.7998, 0.7111,
        0.8007, 0.6828, 0.6913, 0.6822, 0.7881, 0.6463, 0.6628, 0.6831, 0.8093,
        0.7001, 0.6768, 0.6997, 0.6659, 0.6407, 0.7923, 0.6625, 0.6713, 0.6499,
        0.8179, 0.6512, 0.6749, 0.6614, 0.6695, 0.6924, 0.7076, 0.6749, 0.7973,
        0.6646, 0.6671, 0.6457, 0.6911, 0.6880, 0.7141, 0.6870, 0.8120, 0.6528,
        0.8018, 0.8032, 0.7879, 0.8199, 0.7076, 0.7011, 0.6839, 0.6569, 0.7994,
        0.6847, 0.7083, 0.6918, 0.7042, 0.7120, 0.6700, 0.6192, 0.6666, 0.6903,
        0.6795, 0.7855, 0.6909, 0.7131, 0.7102, 0.7942, 0.6760, 0.6689, 0.7096,
        0.7031, 0.6956, 0.6984, 0.7993, 0.6900, 0.6806, 0.6591, 0.6938],
       grad_fn=<IndexBackward0>)
pred_loss tensor([0.1408], grad_fn=<MulBackward0>)
size_loss tensor(-5.7346, grad_fn=<MulBackward0>)
size_num_loss 9.700000000000001
loss: tensor([11.6672], grad_fn=<AddBackward0>)
2
epoch:  2 ; loss:  11.667243003845215 ; pred:  tensor([4.4732e-04, 9.8670e-03, 9.8601e-01, 3.6708e-03],
       grad_fn=<SoftmaxBackward0>)
num_high 94 len(mask) 98
mask_without_small tensor([0.8281, 0.8194, 0.8049, 0.5993, 0.6963, 0.6284, 0.6668, 0.6161, 0.6440,
        0.8227, 0.6556, 0.6228, 0.6448, 0.6502, 0.6435, 0.7990, 0.8225, 0.6630,
        0.6522, 0.6838, 0.6439, 0.8103, 0.8009, 0.8233, 0.8150, 0.8154, 0.6920,
        0.8162, 0.6607, 0.6695, 0.6601, 0.8034, 0.6234, 0.6402, 0.6610, 0.8240,
        0.6791, 0.6545, 0.6786, 0.6433, 0.6177, 0.8080, 0.6399, 0.6489, 0.6271,
        0.8319, 0.6284, 0.6525, 0.6388, 0.6471, 0.6707, 0.6877, 0.6525, 0.8130,
        0.6421, 0.6446, 0.6228, 0.6693, 0.6661, 0.6956, 0.6650, 0.8265, 0.6300,
        0.8172, 0.8185, 0.8032, 0.8337, 0.6878, 0.6802, 0.6618, 0.6342, 0.8150,
        0.6626, 0.6888, 0.6700, 0.6839, 0.6938, 0.6476, 0.5959, 0.6441, 0.6685,
        0.6573, 0.8006, 0.6691, 0.6946, 0.6909, 0.8099, 0.6537, 0.6464, 0.6902,
        0.6824, 0.6741, 0.6772, 0.8149, 0.6681, 0.6584, 0.6365, 0.6722],
       grad_fn=<IndexBackward0>)
pred_loss tensor([0.1716], grad_fn=<MulBackward0>)
size_loss tensor(-7.3171, grad_fn=<MulBackward0>)
size_num_loss 9.4
loss: tensor([9.7037], grad_fn=<AddBackward0>)
3
epoch:  3 ; loss:  9.703704833984375 ; pred:  tensor([5.9977e-04, 1.1863e-02, 9.8298e-01, 4.5538e-03],
       grad_fn=<SoftmaxBackward0>)
num_high 74 len(mask) 98
mask_without_small tensor([0.8412, 0.8334, 0.8198, 0.5759, 0.6763, 0.6051, 0.6441, 0.5928, 0.6209,
        0.8363, 0.6326, 0.5995, 0.6217, 0.6272, 0.6204, 0.8136, 0.8362, 0.6402,
        0.6292, 0.6622, 0.6207, 0.8251, 0.8156, 0.8369, 0.8295, 0.8298, 0.6714,
        0.8306, 0.6378, 0.6469, 0.6372, 0.8182, 0.6001, 0.6171, 0.6381, 0.8375,
        0.6570, 0.6316, 0.6565, 0.6202, 0.5944, 0.8228, 0.6168, 0.6259, 0.6038,
        0.8445, 0.6052, 0.6295, 0.6157, 0.6240, 0.6482, 0.6666, 0.6295, 0.8277,
        0.6190, 0.6215, 0.5996, 0.6467, 0.6434, 0.6755, 0.6422, 0.8398, 0.6068,
        0.8315, 0.8327, 0.8180, 0.8462, 0.6667, 0.6582, 0.6390, 0.6111, 0.8295,
        0.6398, 0.6679, 0.6475, 0.6625, 0.6739, 0.6246, 0.5726, 0.6211, 0.6459,
        0.6344, 0.8154, 0.6465, 0.6745, 0.6703, 0.8247, 0.6308, 0.6233, 0.6694,
        0.6607, 0.6517, 0.6551, 0.8295, 0.6455, 0.6355, 0.6133, 0.6498],
       grad_fn=<IndexBackward0>)
pred_loss tensor([0.2106], grad_fn=<MulBackward0>)
size_loss tensor(-8.9050, grad_fn=<MulBackward0>)
size_num_loss 7.4
loss: tensor([6.0335], grad_fn=<AddBackward0>)
4
epoch:  4 ; loss:  6.033540725708008 ; pred:  tensor([8.1270e-04, 1.4347e-02, 9.7916e-01, 5.6825e-03],
       grad_fn=<SoftmaxBackward0>)
num_high 51 len(mask) 98
mask_without_small tensor([0.8529, 0.8462, 0.8337, 0.5524, 0.6551, 0.5815, 0.6206, 0.5693, 0.5973,
        0.8487, 0.6090, 0.5760, 0.5981, 0.6036, 0.5968, 0.8275, 0.8486, 0.6167,
        0.6056, 0.6397, 0.5971, 0.8388, 0.8296, 0.8491, 0.8427, 0.8430, 0.6497,
        0.8437, 0.6143, 0.6236, 0.6136, 0.8322, 0.5766, 0.5935, 0.6146, 0.8497,
        0.6341, 0.6080, 0.6336, 0.5966, 0.5708, 0.8367, 0.5932, 0.6023, 0.5802,
        0.8556, 0.5816, 0.6059, 0.5922, 0.6004, 0.6249, 0.6444, 0.6059, 0.8412,
        0.5954, 0.5979, 0.5761, 0.6234, 0.6200, 0.6542, 0.6188, 0.8518, 0.5833,
        0.8445, 0.8456, 0.8320, 0.8574, 0.6446, 0.6354, 0.6155, 0.5875, 0.8429,
        0.6164, 0.6460, 0.6241, 0.6401, 0.6528, 0.6012, 0.5494, 0.5977, 0.6225,
        0.6108, 0.8294, 0.6231, 0.6531, 0.6485, 0.8384, 0.6072, 0.5998, 0.6475,
        0.6380, 0.6286, 0.6321, 0.8430, 0.6221, 0.6119, 0.5897, 0.6265],
       grad_fn=<IndexBackward0>)
pred_loss tensor([0.2598], grad_fn=<MulBackward0>)
size_loss tensor(-10.4789, grad_fn=<MulBackward0>)
size_num_loss 5.1000000000000005
loss: tensor([2.0788], grad_fn=<AddBackward0>)
5
epoch:  5 ; loss:  2.078803062438965 ; pred:  tensor([0.0011, 0.0174, 0.9744, 0.0071], grad_fn=<SoftmaxBackward0>)
num_high 34 len(mask) 98
mask_without_small tensor([0.8631, 0.8575, 0.8465, 0.5290, 0.6328, 0.5578, 0.5966, 0.5457, 0.5734,
        0.8595, 0.5850, 0.5523, 0.5741, 0.5796, 0.5728, 0.8406, 0.8595, 0.5926,
        0.5816, 0.6163, 0.5732, 0.8512, 0.8425, 0.8599, 0.8545, 0.8548, 0.6270,
        0.8554, 0.5902, 0.5996, 0.5896, 0.8451, 0.5529, 0.5695, 0.5905, 0.8604,
        0.6105, 0.5839, 0.6099, 0.5726, 0.5472, 0.8492, 0.5693, 0.5783, 0.5565,
        0.8652, 0.5580, 0.5819, 0.5683, 0.5764, 0.6009, 0.6213, 0.5818, 0.8534,
        0.5716, 0.5740, 0.5525, 0.5994, 0.5960, 0.6318, 0.5947, 0.8623, 0.5596,
        0.8560, 0.8573, 0.8448, 0.8671, 0.6216, 0.6118, 0.5914, 0.5637, 0.8552,
        0.5925, 0.6232, 0.6002, 0.6168, 0.6306, 0.5774, 0.5265, 0.5741, 0.5986,
        0.5867, 0.8425, 0.5991, 0.6307, 0.6257, 0.8508, 0.5832, 0.5759, 0.6247,
        0.6146, 0.6047, 0.6083, 0.8553, 0.5982, 0.5879, 0.5659, 0.6025],
       grad_fn=<IndexBackward0>)
pred_loss tensor([0.3214], grad_fn=<MulBackward0>)
size_loss tensor(-12.0228, grad_fn=<MulBackward0>)
size_num_loss 3.4000000000000004
loss: tensor([-1.2418], grad_fn=<AddBackward0>)
6
epoch:  6 ; loss:  -1.241847038269043 ; pred:  tensor([0.0015, 0.0212, 0.9684, 0.0089], grad_fn=<SoftmaxBackward0>)
num_high 26 len(mask) 98
mask_without_small tensor([0.8717, 0.8672, 0.8579, 0.5059, 0.6096, 0.5340, 0.5720, 0.5221, 0.5492,
        0.8689, 0.5605, 0.5286, 0.5499, 0.5552, 0.5486, 0.8524, 0.8688, 0.5681,
        0.5572, 0.5922, 0.5490, 0.8622, 0.8542, 0.8692, 0.8648, 0.8651, 0.6034,
        0.8655, 0.5657, 0.5750, 0.5651, 0.8565, 0.5292, 0.5454, 0.5660, 0.8696,
        0.5861, 0.5595, 0.5855, 0.5485, 0.5236, 0.8603, 0.5452, 0.5541, 0.5327,
        0.8733, 0.5342, 0.5575, 0.5444, 0.5521, 0.5765, 0.5975, 0.5574, 0.8641,
        0.5476, 0.5500, 0.5290, 0.5750, 0.5717, 0.6085, 0.5702, 0.8714, 0.5358,
        0.8660, 0.8675, 0.8562, 0.8754, 0.5978, 0.5876, 0.5670, 0.5398, 0.8662,
        0.5685, 0.5996, 0.5759, 0.5930, 0.6080, 0.5535, 0.5039, 0.5504, 0.5741,
        0.5623, 0.8544, 0.5747, 0.6073, 0.6021, 0.8616, 0.5589, 0.5517, 0.6010,
        0.5904, 0.5802, 0.5840, 0.8666, 0.5738, 0.5634, 0.5419, 0.5780],
       grad_fn=<IndexBackward0>)
pred_loss tensor([0.3974], grad_fn=<MulBackward0>)
size_loss tensor(-13.5225, grad_fn=<MulBackward0>)
size_num_loss 2.6
loss: tensor([-3.6115], grad_fn=<AddBackward0>)
7
epoch:  7 ; loss:  -3.6114983558654785 ; pred:  tensor([0.0021, 0.0257, 0.9610, 0.0112], grad_fn=<SoftmaxBackward0>)
num_high 26 len(mask) 98
mask_without_small tensor([0.8789, 0.8753, 0.8676, 0.5855, 0.5102, 0.5472, 0.5249, 0.8766, 0.5359,
        0.5050, 0.5256, 0.5307, 0.5244, 0.8627, 0.8765, 0.5433, 0.5326, 0.5675,
        0.5247, 0.8717, 0.8643, 0.8769, 0.8735, 0.8736, 0.5791, 0.8740, 0.5409,
        0.5501, 0.5403, 0.8664, 0.5056, 0.5213, 0.5412, 0.8772, 0.5613, 0.5349,
        0.5607, 0.5242, 0.5002, 0.8697, 0.5210, 0.5298, 0.5090, 0.8798, 0.5107,
        0.5329, 0.5205, 0.5277, 0.5517, 0.5729, 0.5328, 0.8734, 0.5236, 0.5259,
        0.5057, 0.5502, 0.5472, 0.5843, 0.5453, 0.8791, 0.5122, 0.8744, 0.8762,
        0.8660, 0.8823, 0.5733, 0.5628, 0.5423, 0.5159, 0.8759, 0.5444, 0.5758,
        0.5514, 0.5688, 0.5855, 0.5297, 0.5270, 0.5495, 0.5377, 0.8649, 0.5499,
        0.5833, 0.5778, 0.8708, 0.5345, 0.5275, 0.5766, 0.5657, 0.5552, 0.5593,
        0.8766, 0.5493, 0.5387, 0.5179, 0.5531], grad_fn=<IndexBackward0>)
pred_loss tensor([0.4894], grad_fn=<MulBackward0>)
size_loss tensor(-1498.7291, grad_fn=<MulBackward0>)
size_num_loss 2.6
loss: tensor([-1494.4320], grad_fn=<AddBackward0>)
8
epoch:  8 ; loss:  -1494.4320068359375 ; pred:  tensor([0.0028, 0.0310, 0.9522, 0.0139], grad_fn=<SoftmaxBackward0>)
num_high 26 len(mask) 98
mask_without_small tensor([0.8841, 0.8806, 0.8732, 0.5728, 0.5343, 0.5119, 0.8819, 0.5229, 0.5126,
        0.5178, 0.5114, 0.8684, 0.8818, 0.5304, 0.5197, 0.5547, 0.5117, 0.8771,
        0.8700, 0.8821, 0.8788, 0.8790, 0.5664, 0.8793, 0.5280, 0.5373, 0.5274,
        0.8721, 0.5082, 0.5283, 0.8824, 0.5485, 0.5219, 0.5479, 0.5112, 0.8752,
        0.5080, 0.5169, 0.8850, 0.5199, 0.5075, 0.5147, 0.5389, 0.5602, 0.5198,
        0.8788, 0.5106, 0.5129, 0.5374, 0.5344, 0.5716, 0.5324, 0.8843, 0.8797,
        0.8815, 0.8717, 0.8874, 0.5607, 0.5501, 0.5294, 0.5028, 0.8812, 0.5316,
        0.5633, 0.5386, 0.5562, 0.5732, 0.5168, 0.5141, 0.5367, 0.5247, 0.8706,
        0.5371, 0.5707, 0.5651, 0.8762, 0.5216, 0.5145, 0.5639, 0.5529, 0.5424,
        0.5466, 0.8820, 0.5365, 0.5258, 0.5049, 0.5402],
       grad_fn=<IndexBackward0>)
pred_loss tensor([0.5467], grad_fn=<MulBackward0>)
size_loss tensor(-1598.9456, grad_fn=<MulBackward0>)
size_num_loss 2.6
loss: tensor([-1594.6398], grad_fn=<AddBackward0>)
9
epoch:  9 ; loss:  -1594.6397705078125 ; pred:  tensor([0.0033, 0.0342, 0.9468, 0.0156], grad_fn=<SoftmaxBackward0>)
num_high 26 len(mask) 98
mask_without_small tensor([0.8906, 0.8873, 0.8803, 0.5565, 0.5176, 0.8885, 0.5062, 0.5010, 0.8757,
        0.8885, 0.5137, 0.5029, 0.5382, 0.8840, 0.8773, 0.8888, 0.8857, 0.8858,
        0.5500, 0.8861, 0.5113, 0.5206, 0.5107, 0.8792, 0.5116, 0.8891, 0.5319,
        0.5052, 0.5313, 0.8822, 0.5002, 0.8915, 0.5032, 0.5223, 0.5437, 0.5031,
        0.8856, 0.5208, 0.5178, 0.5552, 0.5158, 0.8908, 0.8865, 0.8882, 0.8788,
        0.8938, 0.5442, 0.5336, 0.5127, 0.8879, 0.5151, 0.5469, 0.5220, 0.5398,
        0.5570, 0.5001, 0.5201, 0.5080, 0.8778, 0.5204, 0.5543, 0.5487, 0.8832,
        0.5049, 0.5475, 0.5364, 0.5258, 0.5301, 0.8886, 0.5199, 0.5091, 0.5236],
       grad_fn=<IndexBackward0>)
pred_loss tensor([0.6246], grad_fn=<MulBackward0>)
size_loss tensor(-1753.2795, grad_fn=<MulBackward0>)
size_num_loss 2.6
loss: tensor([-1748.9810], grad_fn=<AddBackward0>)
10
epoch:  10 ; loss:  -1748.98095703125 ; pred:  tensor([0.0041, 0.0385, 0.9394, 0.0180], grad_fn=<SoftmaxBackward0>)
num_high 26 len(mask) 98
mask_without_small tensor([0.8978, 0.8947, 0.8880, 0.5382, 0.8958, 0.8837, 0.8958, 0.5195, 0.8916,
        0.8852, 0.8961, 0.8931, 0.8933, 0.5316, 0.8936, 0.5017, 0.8870, 0.8963,
        0.5132, 0.5126, 0.8899, 0.8986, 0.5034, 0.5252, 0.8930, 0.5019, 0.5369,
        0.8980, 0.8939, 0.8955, 0.8867, 0.9008, 0.5257, 0.5148, 0.8953, 0.5285,
        0.5032, 0.5212, 0.5388, 0.5012, 0.8857, 0.5016, 0.5360, 0.5302, 0.8908,
        0.5290, 0.5177, 0.5070, 0.5113, 0.8960, 0.5010, 0.5047],
       grad_fn=<IndexBackward0>)
pred_loss tensor([0.7167], grad_fn=<MulBackward0>)
size_loss tensor(-1897.7550, grad_fn=<MulBackward0>)
size_num_loss 2.6
loss: tensor([-1893.4730], grad_fn=<AddBackward0>)
11
epoch:  11 ; loss:  -1893.4730224609375 ; pred:  tensor([0.0050, 0.0434, 0.9308, 0.0208], grad_fn=<SoftmaxBackward0>)
num_high 26 len(mask) 98
mask_without_small tensor([0.9052, 0.9023, 0.8960, 0.5197, 0.9033, 0.8920, 0.9033, 0.5004, 0.8994,
        0.8934, 0.9035, 0.9008, 0.9009, 0.5128, 0.9012, 0.8951, 0.9038, 0.8977,
        0.9059, 0.5062, 0.9007, 0.5183, 0.9054, 0.9015, 0.9030, 0.8947, 0.9080,
        0.5067, 0.9028, 0.5096, 0.5022, 0.5203, 0.8938, 0.5174, 0.5114, 0.8986,
        0.5101, 0.9035], grad_fn=<IndexBackward0>)
pred_loss tensor([0.8134], grad_fn=<MulBackward0>)
size_loss tensor(-1834.7372, grad_fn=<MulBackward0>)
size_num_loss 2.6
loss: tensor([-1830.4346], grad_fn=<AddBackward0>)
12
epoch:  12 ; loss:  -1830.4345703125 ; pred:  tensor([0.0060, 0.0484, 0.9219, 0.0237], grad_fn=<SoftmaxBackward0>)
num_high 26 len(mask) 98
mask_without_small tensor([0.9125, 0.9097, 0.9039, 0.5013, 0.9107, 0.9002, 0.9107, 0.9070, 0.9014,
        0.9109, 0.9084, 0.9085, 0.9088, 0.9030, 0.9112, 0.9055, 0.9132, 0.9083,
        0.9126, 0.9091, 0.9105, 0.9027, 0.9151, 0.9103, 0.5020, 0.9019, 0.9063,
        0.9109], grad_fn=<IndexBackward0>)
pred_loss tensor([0.9102], grad_fn=<MulBackward0>)
size_loss tensor(-1066.9900, grad_fn=<MulBackward0>)
size_num_loss 2.6
loss: tensor([-1062.6456], grad_fn=<AddBackward0>)
13
epoch:  13 ; loss:  -1062.6456298828125 ; pred:  tensor([0.0071, 0.0533, 0.9130, 0.0266], grad_fn=<SoftmaxBackward0>)
num_high 26 len(mask) 98
mask_without_small tensor([0.9193, 0.9167, 0.9112, 0.9176, 0.9076, 0.9176, 0.9141, 0.9088, 0.9178,
        0.9154, 0.9155, 0.9158, 0.9103, 0.9180, 0.9127, 0.9199, 0.9153, 0.9194,
        0.9160, 0.9174, 0.9100, 0.9217, 0.9172, 0.9092, 0.9134, 0.9178],
       grad_fn=<IndexBackward0>)
pred_loss tensor([1.0029], grad_fn=<MulBackward0>)
size_loss tensor(-37.7172, grad_fn=<MulBackward0>)
size_num_loss 2.6
loss: tensor([-33.2942], grad_fn=<AddBackward0>)
14
epoch:  14 ; loss:  -33.29423141479492 ; pred:  tensor([0.0083, 0.0578, 0.9046, 0.0294], grad_fn=<SoftmaxBackward0>)
num_high 26 len(mask) 98
mask_without_small tensor([0.9259, 0.9233, 0.9134, 0.9244, 0.9075, 0.9243, 0.9192, 0.9094, 0.9246,
        0.9214, 0.9216, 0.9220, 0.9119, 0.9248, 0.9164, 0.9264, 0.9213, 0.9260,
        0.9224, 0.9241, 0.9114, 0.9279, 0.9239, 0.9101, 0.9178, 0.9245],
       grad_fn=<IndexBackward0>)
pred_loss tensor([1.0909], grad_fn=<MulBackward0>)
size_loss tensor(-60.0295, grad_fn=<MulBackward0>)
size_num_loss 2.6
loss: tensor([-55.5224], grad_fn=<AddBackward0>)
15
epoch:  15 ; loss:  -55.52238464355469 ; pred:  tensor([0.0094, 0.0620, 0.8966, 0.0320], grad_fn=<SoftmaxBackward0>)
num_high 26 len(mask) 98
mask_without_small tensor([0.9323, 0.9296, 0.9130, 0.9308, 0.9047, 0.9308, 0.9230, 0.9071, 0.9310,
        0.9269, 0.9272, 0.9278, 0.9107, 0.9312, 0.9179, 0.9327, 0.9268, 0.9324,
        0.9284, 0.9305, 0.9099, 0.9339, 0.9303, 0.9081, 0.9206, 0.9310],
       grad_fn=<IndexBackward0>)
pred_loss tensor([1.1729], grad_fn=<MulBackward0>)
size_loss tensor(-93.5059, grad_fn=<MulBackward0>)
size_num_loss 2.6
loss: tensor([-88.9204], grad_fn=<AddBackward0>)
16
epoch:  16 ; loss:  -88.92040252685547 ; pred:  tensor([0.0105, 0.0657, 0.8893, 0.0345], grad_fn=<SoftmaxBackward0>)
num_high 26 len(mask) 98
mask_without_small tensor([0.9383, 0.9356, 0.9107, 0.9369, 0.9000, 0.9369, 0.9261, 0.9030, 0.9371,
        0.9321, 0.9325, 0.9333, 0.9075, 0.9374, 0.9178, 0.9387, 0.9319, 0.9384,
        0.9341, 0.9366, 0.9066, 0.9396, 0.9364, 0.9042, 0.9221, 0.9371],
       grad_fn=<IndexBackward0>)
pred_loss tensor([1.2488], grad_fn=<MulBackward0>)
size_loss tensor(-133.8153, grad_fn=<MulBackward0>)
size_num_loss 2.6
loss: tensor([-129.1576], grad_fn=<AddBackward0>)
17
epoch:  17 ; loss:  -129.15760803222656 ; pred:  tensor([0.0116, 0.0691, 0.8826, 0.0368], grad_fn=<SoftmaxBackward0>)
num_high 26 len(mask) 98
mask_without_small tensor([0.9439, 0.9412, 0.9068, 0.9426, 0.8939, 0.9426, 0.9286, 0.8974, 0.9428,
        0.9370, 0.9375, 0.9386, 0.9029, 0.9431, 0.9164, 0.9442, 0.9367, 0.9440,
        0.9395, 0.9423, 0.9017, 0.9449, 0.9421, 0.8989, 0.9225, 0.9428],
       grad_fn=<IndexBackward0>)
pred_loss tensor([1.3184], grad_fn=<MulBackward0>)
size_loss tensor(-179.1534, grad_fn=<MulBackward0>)
size_num_loss 2.6
loss: tensor([-174.4295], grad_fn=<AddBackward0>)
18
epoch:  18 ; loss:  -174.42953491210938 ; pred:  tensor([0.0126, 0.0721, 0.8765, 0.0389], grad_fn=<SoftmaxBackward0>)
num_high 26 len(mask) 98
mask_without_small tensor([0.9491, 0.9464, 0.9015, 0.9479, 0.8865, 0.9479, 0.9306, 0.8905, 0.9481,
        0.9417, 0.9423, 0.9435, 0.8968, 0.9483, 0.9137, 0.9493, 0.9413, 0.9491,
        0.9446, 0.9476, 0.8954, 0.9498, 0.9473, 0.8921, 0.9220, 0.9481],
       grad_fn=<IndexBackward0>)
pred_loss tensor([1.3819], grad_fn=<MulBackward0>)
size_loss tensor(-228.7410, grad_fn=<MulBackward0>)
size_num_loss 2.6
loss: tensor([-223.9569], grad_fn=<AddBackward0>)
19
epoch:  19 ; loss:  -223.95689392089844 ; pred:  tensor([0.0136, 0.0747, 0.8709, 0.0408], grad_fn=<SoftmaxBackward0>)
Finished Training
sel masked ver tensor(indices=tensor([[6991, 7006, 7014, 7117, 5504, 5504, 5811, 6991, 7006,
                        7014, 7018, 7035, 7089, 7006, 7035, 7089, 5943, 5943,
                        5944, 5956, 5956, 5956, 5811, 5504, 5504, 5811],
                       [5811, 5811, 5811, 5811, 5943, 5944,    0, 5943, 5956,
                        5956, 5956, 5944, 5956, 5471, 5385, 5471, 5811, 6991,
                        7082, 7006, 7014, 7018, 6991, 7006, 7035, 5956]]),
       values=tensor([0.9491, 0.9464, 0.9015, 0.9479, 0.8865, 0.9479, 0.9306,
                      0.8905, 0.9481, 0.9417, 0.9423, 0.9435, 0.8968, 0.9483,
                      0.9137, 0.9493, 0.9413, 0.9491, 0.9446, 0.9476, 0.8954,
                      0.9498, 0.9473, 0.8921, 0.9220, 0.9481]),
       size=(7118, 7083), nnz=26, layout=torch.sparse_coo,
       grad_fn=<SparseCooTensorWithDimsAndTensorsBackward0>)
Counter({'hasProject': 6, 'projectInfo': 5, 'author': 4, 'isAbout': 3, 'carriesOut': 2, 'publishes': 2, 'fax': 1, 'worksAtProject': 1, 'publication': 1, 'member': 1})
dict index: {}
node_idx 5811
 node original label [2]
 node predicted label explain 2
 node prediction probability explain tensor([0.0136, 0.0747, 0.8709, 0.0408], grad_fn=<SoftmaxBackward0>)
 node predicted label full 2 most important relations  {'carriesOut': 2, 'publishes': 2, 'fax': 1, 'worksAtProject': 1, 'publication': 1, 'member': 1, 'projectInfo': 5, 'author': 4, 'hasProject': 6, 'isAbout': 3, 'label': 2, 'node_idx': '5811'}
 final masks and lenght tensor(indices=tensor([[ 23561,  23576,  23584,  23588,  23604,  23605,  23623,
                         23652,  23659,  23687,  39083,  39084,  39096,  39097,
                         39102,  46929,  46929,  46929,  46929,  46929,  63466,
                         88661, 114696, 114711, 114719, 114723, 114739, 114740,
                        114740, 114758, 114787, 114794, 146801, 147836, 147851,
                        147851, 147859, 147859, 147863, 147879, 147879, 147880,
                        147880, 147898, 147898, 147934, 147962, 154481, 154515,
                        154544, 154582, 154601, 179489, 179928, 179929, 179941,
                        179942, 179947, 196366, 229506, 237791, 246208, 246209,
                        246209, 246221, 246221, 246221, 246221, 246221, 246221,
                        246222, 254361, 254361, 254361, 254361, 254361, 254361,
                        254361, 254361, 254361, 254361, 262339, 262339, 262339,
                        262339, 262339, 262339, 262339, 262339, 262339, 262339,
                        304071, 304071, 304071, 304071, 304071, 328926, 328926],
                       [  5811,   5811,   5811,   5811,   5811,   5811,   5811,
                          5811,   5811,   5811,   5504,   5504,   5504,   5504,
                          5504,   5943,   5944,   5956,   5957,   5962,   5956,
                             0,   5943,   5956,   5956,   5956,   5956,   5944,
                          5957,   5956,   5944,   5956,   5471,   5385,   5385,
                          5471,   5452,   5471,   5471,   5351,   5471,   5385,
                          5414,   5452,   5471,   5471,   5452,   5811,   5811,
                          5811,   5811,   5811,   5811,   5811,   5811,   5811,
                          5811,   5811,   1322,     97,   5602,   6991,   7035,
                          7082,   7006,   7014,   7018,   7034,   7053,   7089,
                          7035,   6991,   7006,   7014,   7018,   7034,   7035,
                          7053,   7082,   7089,   7117,   6991,   7006,   7014,
                          7018,   7034,   7035,   7053,   7082,   7089,   7117,
                          5943,   5944,   5956,   5957,   5962,   5230,   5231]]),
       values=tensor([0.9491, 0.9464, 0.9015, 0.3624, 0.4227, 0.4217, 0.4019,
                      0.3735, 0.4036, 0.9479, 0.3906, 0.4166, 0.4043, 0.3855,
                      0.4031, 0.8865, 0.9479, 0.3980, 0.3874, 0.4017, 0.4034,
                      0.9306, 0.8905, 0.9481, 0.9417, 0.9423, 0.4152, 0.9435,
                      0.3956, 0.3911, 0.3950, 0.8968, 0.4172, 0.4000, 0.3959,
                      0.9483, 0.4033, 0.3896, 0.4026, 0.4029, 0.4119, 0.9137,
                      0.3998, 0.3849, 0.4205, 0.9493, 0.4229, 0.3876, 0.3999,
                      0.4064, 0.3932, 0.4080, 0.3875, 0.9413, 0.4031, 0.4054,
                      0.4185, 0.3915, 0.4032, 0.4211, 0.4001, 0.9491, 0.4244,
                      0.9446, 0.9476, 0.8954, 0.9498, 0.4087, 0.4052, 0.3973,
                      0.3950, 0.9473, 0.4009, 0.4121, 0.3932, 0.4040, 0.4236,
                      0.3856, 0.4126, 0.4081, 0.3909, 0.3925, 0.8921, 0.3911,
                      0.4203, 0.4138, 0.9220, 0.3897, 0.4066, 0.4123, 0.4082,
                      0.3965, 0.4017, 0.9481, 0.3909, 0.3935, 0.3969, 0.3941]),
       size=(753935, 8285), nnz=98, layout=torch.sparse_coo,
       grad_fn=<SparseCooTensorWithDimsAndTensorsBackward0>) 26
 ---------------------------------------------------------------
node label: 2
17
num_high 32 len(mask) 33
mask_without_small tensor([0.8137, 0.7968, 0.7724, 0.6181, 0.7626, 0.6673, 0.7290, 0.6468, 0.6931,
        0.8031, 0.7116, 0.6580, 0.6944, 0.7031, 0.6923, 0.7663, 0.8029, 0.6869,
        0.7201, 0.8058, 0.7462, 0.7100, 0.7456, 0.6920, 0.7327, 0.7765, 0.6864,
        0.7010, 0.6651, 0.8209, 0.6673, 0.7068, 0.6572],
       grad_fn=<IndexBackward0>)
pred_loss tensor([4.6335], grad_fn=<MulBackward0>)
size_loss tensor(-5.4612, grad_fn=<MulBackward0>)
size_num_loss 3.2
loss: tensor([5.3404], grad_fn=<AddBackward0>)
r_exp.py:102: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  hor_graph, ver_graph = torch.tensor(sub_hor_graph, dtype=torch.float), torch.tensor(sub_ver_graph, dtype=torch.float)
wandb: WARNING Calling wandb.login() after wandb.init() has no effect.
0
epoch:  0 ; loss:  5.340426921844482 ; pred:  tensor([0.1053, 0.1503, 0.6292, 0.1152], grad_fn=<SoftmaxBackward0>)
num_high 32 len(mask) 33
mask_without_small tensor([0.8284, 0.8125, 0.7895, 0.5943, 0.7802, 0.6448, 0.7088, 0.6236, 0.6715,
        0.8185, 0.6907, 0.6352, 0.6728, 0.6818, 0.6706, 0.7838, 0.8182, 0.6650,
        0.7398, 0.8209, 0.7647, 0.6890, 0.7641, 0.6702, 0.7519, 0.7933, 0.7075,
        0.6796, 0.6425, 0.8352, 0.6448, 0.6857, 0.6343],
       grad_fn=<IndexBackward0>)
pred_loss tensor([4.6102], grad_fn=<MulBackward0>)
size_loss tensor(-7.1179, grad_fn=<MulBackward0>)
size_num_loss 3.2
loss: tensor([3.6455], grad_fn=<AddBackward0>)
1
epoch:  1 ; loss:  3.6454882621765137 ; pred:  tensor([0.1049, 0.1501, 0.6306, 0.1143], grad_fn=<SoftmaxBackward0>)
num_high 26 len(mask) 33
mask_without_small tensor([0.8420, 0.8272, 0.8056, 0.5700, 0.7969, 0.6215, 0.6904, 0.5999, 0.6490,
        0.8327, 0.6692, 0.6117, 0.6504, 0.6598, 0.6481, 0.8002, 0.8325, 0.6423,
        0.7567, 0.8350, 0.7818, 0.6680, 0.7815, 0.6478, 0.7700, 0.8092, 0.7254,
        0.6577, 0.6192, 0.8483, 0.6215, 0.6639, 0.6109],
       grad_fn=<IndexBackward0>)
pred_loss tensor([4.6115], grad_fn=<MulBackward0>)
size_loss tensor(-8.8817, grad_fn=<MulBackward0>)
size_num_loss 2.6
loss: tensor([1.2642], grad_fn=<AddBackward0>)
2
epoch:  2 ; loss:  1.2641913890838623 ; pred:  tensor([0.1051, 0.1504, 0.6306, 0.1139], grad_fn=<SoftmaxBackward0>)
num_high 25 len(mask) 33
mask_without_small tensor([0.8546, 0.8408, 0.8208, 0.5455, 0.8127, 0.5977, 0.6706, 0.5757, 0.6258,
        0.8460, 0.6468, 0.5877, 0.6273, 0.6370, 0.6249, 0.8158, 0.8457, 0.6189,
        0.7734, 0.8480, 0.7981, 0.6461, 0.7980, 0.6246, 0.7873, 0.8241, 0.7433,
        0.6351, 0.5954, 0.8605, 0.5977, 0.6413, 0.5869],
       grad_fn=<IndexBackward0>)
pred_loss tensor([4.6248], grad_fn=<MulBackward0>)
size_loss tensor(-10.6898, grad_fn=<MulBackward0>)
size_num_loss 2.5
loss: tensor([-0.6532], grad_fn=<AddBackward0>)
3
epoch:  3 ; loss:  -0.653240442276001 ; pred:  tensor([0.1056, 0.1511, 0.6297, 0.1137], grad_fn=<SoftmaxBackward0>)
num_high 18 len(mask) 33
mask_without_small tensor([0.8660, 0.8533, 0.8349, 0.5209, 0.8274, 0.5734, 0.6495, 0.5512, 0.6020,
        0.8581, 0.6237, 0.5633, 0.6034, 0.6135, 0.6010, 0.8303, 0.8579, 0.5949,
        0.7897, 0.8597, 0.8136, 0.6233, 0.8137, 0.6007, 0.8037, 0.8380, 0.7610,
        0.6117, 0.5711, 0.8716, 0.5735, 0.6180, 0.5625],
       grad_fn=<IndexBackward0>)
pred_loss tensor([4.6531], grad_fn=<MulBackward0>)
size_loss tensor(-12.5133, grad_fn=<MulBackward0>)
size_num_loss 1.8
loss: tensor([-3.1744], grad_fn=<AddBackward0>)
4
epoch:  4 ; loss:  -3.1743903160095215 ; pred:  tensor([0.1064, 0.1519, 0.6279, 0.1137], grad_fn=<SoftmaxBackward0>)
num_high 16 len(mask) 33
mask_without_small tensor([0.8764, 0.8648, 0.8481, 0.8412, 0.5487, 0.6273, 0.5265, 0.5776, 0.8691,
        0.5999, 0.5386, 0.5790, 0.5893, 0.5766, 0.8438, 0.8690, 0.5704, 0.8054,
        0.8703, 0.8283, 0.5998, 0.8285, 0.5763, 0.8190, 0.8508, 0.7781, 0.5875,
        0.5465, 0.8817, 0.5491, 0.5940, 0.5379], grad_fn=<IndexBackward0>)
pred_loss tensor([4.6940], grad_fn=<MulBackward0>)
size_loss tensor(-1411.0153, grad_fn=<MulBackward0>)
size_num_loss 1.6
loss: tensor([-1403.9307], grad_fn=<AddBackward0>)
5
epoch:  5 ; loss:  -1403.9306640625 ; pred:  tensor([0.1075, 0.1531, 0.6254, 0.1140], grad_fn=<SoftmaxBackward0>)
num_high 15 len(mask) 33
mask_without_small tensor([0.8821, 0.8710, 0.8548, 0.8482, 0.5354, 0.6147, 0.5130, 0.5644, 0.8752,
        0.5869, 0.5251, 0.5659, 0.5762, 0.5634, 0.8508, 0.8750, 0.5572, 0.8137,
        0.8762, 0.8358, 0.5869, 0.8359, 0.5631, 0.8271, 0.8577, 0.7875, 0.5746,
        0.5332, 0.8873, 0.5361, 0.5810, 0.5244], grad_fn=<IndexBackward0>)
pred_loss tensor([4.7178], grad_fn=<MulBackward0>)
size_loss tensor(-1510.1779, grad_fn=<MulBackward0>)
size_num_loss 1.5
loss: tensor([-1503.1742], grad_fn=<AddBackward0>)
6
epoch:  6 ; loss:  -1503.1741943359375 ; pred:  tensor([0.1082, 0.1538, 0.6239, 0.1142], grad_fn=<SoftmaxBackward0>)
num_high 15 len(mask) 33
mask_without_small tensor([0.8891, 0.8785, 0.8632, 0.8569, 0.5182, 0.5982, 0.5473, 0.8825, 0.5701,
        0.5078, 0.5488, 0.5593, 0.5463, 0.8593, 0.8824, 0.5401, 0.8240, 0.8835,
        0.8450, 0.5702, 0.8452, 0.5461, 0.8368, 0.8661, 0.7990, 0.5578, 0.5160,
        0.8940, 0.5191, 0.5642, 0.5072], grad_fn=<IndexBackward0>)
pred_loss tensor([4.7568], grad_fn=<MulBackward0>)
size_loss tensor(-1623.6453, grad_fn=<MulBackward0>)
size_num_loss 1.5
loss: tensor([-1616.6143], grad_fn=<AddBackward0>)
7
epoch:  7 ; loss:  -1616.6142578125 ; pred:  tensor([0.1092, 0.1548, 0.6215, 0.1145], grad_fn=<SoftmaxBackward0>)
num_high 15 len(mask) 33
mask_without_small tensor([0.8966, 0.8866, 0.8721, 0.8662, 0.5793, 0.5278, 0.8904, 0.5508, 0.5293,
        0.5399, 0.5268, 0.8685, 0.8902, 0.5205, 0.8350, 0.8912, 0.8549, 0.5509,
        0.8551, 0.5266, 0.8473, 0.8750, 0.8114, 0.5384, 0.9012, 0.5448],
       grad_fn=<IndexBackward0>)
pred_loss tensor([4.8105], grad_fn=<MulBackward0>)
size_loss tensor(-1676.1267, grad_fn=<MulBackward0>)
size_num_loss 1.5
loss: tensor([-1669.0750], grad_fn=<AddBackward0>)
8
epoch:  8 ; loss:  -1669.074951171875 ; pred:  tensor([0.1105, 0.1562, 0.6181, 0.1151], grad_fn=<SoftmaxBackward0>)
num_high 15 len(mask) 33
mask_without_small tensor([0.9041, 0.8948, 0.8812, 0.8756, 0.5591, 0.5068, 0.8983, 0.5300, 0.5083,
        0.5190, 0.5058, 0.8778, 0.8982, 0.8463, 0.8991, 0.8650, 0.5302, 0.8652,
        0.5057, 0.8579, 0.8839, 0.8240, 0.5176, 0.9085, 0.5240],
       grad_fn=<IndexBackward0>)
pred_loss tensor([4.8568], grad_fn=<MulBackward0>)
size_loss tensor(-1802.0342, grad_fn=<MulBackward0>)
size_num_loss 1.5
loss: tensor([-1794.9495], grad_fn=<AddBackward0>)
9
epoch:  9 ; loss:  -1794.949462890625 ; pred:  tensor([0.1117, 0.1575, 0.6153, 0.1155], grad_fn=<SoftmaxBackward0>)
num_high 15 len(mask) 33
mask_without_small tensor([0.9115, 0.9028, 0.8901, 0.8849, 0.5375, 0.9061, 0.5080, 0.8869, 0.9059,
        0.8575, 0.9068, 0.8750, 0.5081, 0.8752, 0.8684, 0.8927, 0.8366, 0.9155,
        0.5019], grad_fn=<IndexBackward0>)
pred_loss tensor([4.8996], grad_fn=<MulBackward0>)
size_loss tensor(-1579.3333, grad_fn=<MulBackward0>)
size_num_loss 1.5
loss: tensor([-1572.2445], grad_fn=<AddBackward0>)
10
epoch:  10 ; loss:  -1572.2445068359375 ; pred:  tensor([0.1127, 0.1588, 0.6127, 0.1159], grad_fn=<SoftmaxBackward0>)
num_high 15 len(mask) 33
mask_without_small tensor([0.9185, 0.9104, 0.8986, 0.8937, 0.5168, 0.9134, 0.8956, 0.9133, 0.8680,
        0.9141, 0.8844, 0.8846, 0.8783, 0.9010, 0.8482, 0.9222],
       grad_fn=<IndexBackward0>)
pred_loss tensor([4.9368], grad_fn=<MulBackward0>)
size_loss tensor(-969.5438, grad_fn=<MulBackward0>)
size_num_loss 1.5
loss: tensor([-962.4410], grad_fn=<AddBackward0>)
11
epoch:  11 ; loss:  -962.4409790039062 ; pred:  tensor([0.1136, 0.1599, 0.6104, 0.1161], grad_fn=<SoftmaxBackward0>)
num_high 15 len(mask) 33
mask_without_small tensor([0.9250, 0.9174, 0.9063, 0.9017, 0.9203, 0.9035, 0.9202, 0.8766, 0.9209,
        0.8928, 0.8930, 0.8869, 0.9087, 0.8560, 0.9285],
       grad_fn=<IndexBackward0>)
pred_loss tensor([4.9796], grad_fn=<MulBackward0>)
size_loss tensor(-201.0861, grad_fn=<MulBackward0>)
size_num_loss 1.5
loss: tensor([-193.9534], grad_fn=<AddBackward0>)
12
epoch:  12 ; loss:  -193.9534454345703 ; pred:  tensor([0.1145, 0.1612, 0.6078, 0.1165], grad_fn=<SoftmaxBackward0>)
num_high 15 len(mask) 33
mask_without_small tensor([0.9313, 0.9242, 0.9131, 0.9081, 0.9269, 0.9100, 0.9268, 0.8773, 0.9274,
        0.8976, 0.8978, 0.8903, 0.9156, 0.8527, 0.9346],
       grad_fn=<IndexBackward0>)
pred_loss tensor([5.0641], grad_fn=<MulBackward0>)
size_loss tensor(-226.7929, grad_fn=<MulBackward0>)
size_num_loss 1.5
loss: tensor([-219.5821], grad_fn=<AddBackward0>)
13
epoch:  13 ; loss:  -219.58213806152344 ; pred:  tensor([0.1163, 0.1634, 0.6027, 0.1177], grad_fn=<SoftmaxBackward0>)
num_high 15 len(mask) 33
mask_without_small tensor([0.9373, 0.9305, 0.9192, 0.9134, 0.9331, 0.9157, 0.9330, 0.8745, 0.9336,
        0.9002, 0.9005, 0.8908, 0.9219, 0.8461, 0.9403],
       grad_fn=<IndexBackward0>)
pred_loss tensor([5.1597], grad_fn=<MulBackward0>)
size_loss tensor(-264.1706, grad_fn=<MulBackward0>)
size_num_loss 1.5
loss: tensor([-256.8701], grad_fn=<AddBackward0>)
14
epoch:  14 ; loss:  -256.87005615234375 ; pred:  tensor([0.1182, 0.1657, 0.5969, 0.1191], grad_fn=<SoftmaxBackward0>)
num_high 15 len(mask) 33
mask_without_small tensor([0.9428, 0.9365, 0.9248, 0.9181, 0.9389, 0.9208, 0.9389, 0.8693, 0.9394,
        0.9012, 0.9016, 0.8891, 0.9277, 0.8373, 0.9457],
       grad_fn=<IndexBackward0>)
pred_loss tensor([5.2614], grad_fn=<MulBackward0>)
size_loss tensor(-309.1737, grad_fn=<MulBackward0>)
size_num_loss 1.5
loss: tensor([-301.7767], grad_fn=<AddBackward0>)
15
epoch:  15 ; loss:  -301.7767333984375 ; pred:  tensor([0.1203, 0.1681, 0.5909, 0.1207], grad_fn=<SoftmaxBackward0>)
num_high 15 len(mask) 33
mask_without_small tensor([0.9480, 0.9420, 0.9299, 0.9222, 0.9443, 0.9255, 0.9442, 0.8622, 0.9448,
        0.9009, 0.9015, 0.8857, 0.9332, 0.8266, 0.9506],
       grad_fn=<IndexBackward0>)
pred_loss tensor([5.3676], grad_fn=<MulBackward0>)
size_loss tensor(-360.1700, grad_fn=<MulBackward0>)
size_num_loss 1.5
loss: tensor([-352.6716], grad_fn=<AddBackward0>)
16
epoch:  16 ; loss:  -352.671630859375 ; pred:  tensor([0.1224, 0.1706, 0.5846, 0.1223], grad_fn=<SoftmaxBackward0>)
num_high 15 len(mask) 33
mask_without_small tensor([0.9526, 0.9470, 0.9348, 0.9261, 0.9492, 0.9299, 0.9492, 0.8534, 0.9497,
        0.8995, 0.9003, 0.8808, 0.9382, 0.8140, 0.9551],
       grad_fn=<IndexBackward0>)
pred_loss tensor([5.4789], grad_fn=<MulBackward0>)
size_loss tensor(-416.4603, grad_fn=<MulBackward0>)
size_num_loss 1.5
loss: tensor([-408.8549], grad_fn=<AddBackward0>)
17
epoch:  17 ; loss:  -408.8549499511719 ; pred:  tensor([0.1247, 0.1731, 0.5782, 0.1241], grad_fn=<SoftmaxBackward0>)
num_high 15 len(mask) 33
mask_without_small tensor([0.9569, 0.9516, 0.9393, 0.9297, 0.9537, 0.9340, 0.9536, 0.8429, 0.9541,
        0.8971, 0.8981, 0.8744, 0.9429, 0.7998, 0.9591],
       grad_fn=<IndexBackward0>)
pred_loss tensor([5.5955], grad_fn=<MulBackward0>)
size_loss tensor(-477.7978, grad_fn=<MulBackward0>)
size_num_loss 1.5
loss: tensor([-470.0797], grad_fn=<AddBackward0>)
18
epoch:  18 ; loss:  -470.0797119140625 ; pred:  tensor([0.1271, 0.1755, 0.5715, 0.1259], grad_fn=<SoftmaxBackward0>)
num_high 15 len(mask) 33
mask_without_small tensor([0.9607, 0.9557, 0.9436, 0.9332, 0.9577, 0.9379, 0.9577, 0.8309, 0.9581,
        0.8938, 0.8950, 0.8666, 0.9473, 0.7839, 0.9627],
       grad_fn=<IndexBackward0>)
pred_loss tensor([5.7171], grad_fn=<MulBackward0>)
size_loss tensor(-544.1768, grad_fn=<MulBackward0>)
size_num_loss 1.5
loss: tensor([-536.3404], grad_fn=<AddBackward0>)
19
epoch:  19 ; loss:  -536.3403930664062 ; pred:  tensor([0.1296, 0.1780, 0.5646, 0.1279], grad_fn=<SoftmaxBackward0>)
Finished Training
sel masked ver tensor(indices=tensor([[7102, 7116, 7117, 5504, 7116, 5478, 5504, 5831, 5831,
                        5831, 5982, 5831, 5831, 5831, 5504],
                       [5831, 5831, 5831, 5982, 5444, 5831, 5831, 1871,   19,
                        5619, 7116, 7102, 7116, 7117, 7117]]),
       values=tensor([0.9607, 0.9557, 0.9436, 0.9332, 0.9577, 0.9379, 0.9577,
                      0.8309, 0.9581, 0.8938, 0.8950, 0.8666, 0.9473, 0.7839,
                      0.9627]),
       size=(7118, 7118), nnz=15, layout=torch.sparse_coo,
       grad_fn=<SparseCooTensorWithDimsAndTensorsBackward0>)
Counter({'publication': 3, 'author': 3, 'isWorkedOnBy': 1, 'member': 1, 'carriesOut': 1, 'publishes': 1, 'phone': 1, 'name': 1, 'photo': 1, 'projectInfo': 1, 'isAbout': 1})
dict index: {}
node_idx 5831
 node original label [2]
 node predicted label explain 2
 node prediction probability explain tensor([0.1296, 0.1780, 0.5646, 0.1279], grad_fn=<SoftmaxBackward0>)
 node predicted label full 2 most important relations  {'isWorkedOnBy': 1, 'member': 1, 'carriesOut': 1, 'publishes': 1, 'phone': 1, 'name': 1, 'photo': 1, 'publication': 3, 'projectInfo': 1, 'author': 3, 'isAbout': 1, 'label': 2, 'node_idx': '5831'}
 final masks and lenght tensor(indices=tensor([[ 23672,  23686,  23687,  39122,  46929,  88681, 114807,
                        114821, 114822, 147961, 154528, 154574, 154589, 154592,
                        154603, 154608, 179489, 179967, 196386, 229526, 237811,
                        246247, 246247, 246247, 254381, 254381, 254381, 262339,
                        262339, 262339, 304091, 328946, 328946],
                       [  5831,   5831,   5831,   5504,   5982,     91,   5982,
                          5982,   5982,   5444,   5831,   5831,   5831,   5831,
                          5831,   5831,   5831,   5831,   1871,     19,   5619,
                          7102,   7116,   7117,   7102,   7116,   7117,   7102,
                          7116,   7117,   5982,   5230,   5231]]),
       values=tensor([0.9607, 0.9557, 0.9436, 0.3540, 0.9332, 0.3822, 0.4096,
                      0.3896, 0.3646, 0.9577, 0.3785, 0.3718, 0.3660, 0.3763,
                      0.3636, 0.9379, 0.9577, 0.3789, 0.8309, 0.9581, 0.8938,
                      0.3788, 0.8950, 0.3637, 0.8666, 0.9473, 0.7839, 0.3757,
                      0.3810, 0.9627, 0.3862, 0.3725, 0.3718]),
       size=(753935, 8285), nnz=33, layout=torch.sparse_coo,
       grad_fn=<SparseCooTensorWithDimsAndTensorsBackward0>) 15
 ---------------------------------------------------------------
node label: 2
9
num_high 13 len(mask) 14
mask_without_small tensor([0.7553, 0.7405, 0.7481, 0.7478, 0.6401, 0.7170, 0.8623, 0.6811, 0.7640,
        0.7505, 0.7689, 0.7868, 0.8053, 0.5894], grad_fn=<IndexBackward0>)
pred_loss tensor([11.2098], grad_fn=<MulBackward0>)
size_loss tensor(-6.7923, grad_fn=<MulBackward0>)
size_num_loss 1.3
loss: tensor([7.3155], grad_fn=<AddBackward0>)
r_exp.py:102: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  hor_graph, ver_graph = torch.tensor(sub_hor_graph, dtype=torch.float), torch.tensor(sub_ver_graph, dtype=torch.float)
wandb: WARNING Calling wandb.login() after wandb.init() has no effect.
0
epoch:  0 ; loss:  7.315454959869385 ; pred:  tensor([0.2335, 0.2605, 0.3260, 0.1800], grad_fn=<SoftmaxBackward0>)
num_high 12 len(mask) 14
mask_without_small tensor([0.7733, 0.7209, 0.7665, 0.7662, 0.6167, 0.6963, 0.8738, 0.6590, 0.7815,
        0.7687, 0.7862, 0.8031, 0.8205, 0.5650], grad_fn=<IndexBackward0>)
pred_loss tensor([11.0555], grad_fn=<MulBackward0>)
size_loss tensor(-8.3258, grad_fn=<MulBackward0>)
size_num_loss 1.2000000000000002
loss: tensor([5.5231], grad_fn=<AddBackward0>)
1
epoch:  1 ; loss:  5.523094654083252 ; pred:  tensor([0.2311, 0.2589, 0.3310, 0.1789], grad_fn=<SoftmaxBackward0>)
num_high 12 len(mask) 14
mask_without_small tensor([0.7900, 0.7034, 0.7819, 0.7831, 0.5928, 0.6750, 0.8843, 0.6362, 0.7981,
        0.7859, 0.8025, 0.8184, 0.8347, 0.5403], grad_fn=<IndexBackward0>)
pred_loss tensor([10.9097], grad_fn=<MulBackward0>)
size_loss tensor(-9.9627, grad_fn=<MulBackward0>)
size_num_loss 1.2000000000000002
loss: tensor([3.7340], grad_fn=<AddBackward0>)
2
epoch:  2 ; loss:  3.7340049743652344 ; pred:  tensor([0.2288, 0.2574, 0.3359, 0.1779], grad_fn=<SoftmaxBackward0>)
num_high 11 len(mask) 14
mask_without_small tensor([0.8059, 0.6843, 0.7972, 0.7992, 0.5684, 0.6530, 0.8938, 0.6126, 0.8137,
        0.8022, 0.8180, 0.8328, 0.8479, 0.5154], grad_fn=<IndexBackward0>)
pred_loss tensor([10.7717], grad_fn=<MulBackward0>)
size_loss tensor(-11.6581, grad_fn=<MulBackward0>)
size_num_loss 1.1
loss: tensor([1.7923], grad_fn=<AddBackward0>)
3
epoch:  3 ; loss:  1.7923405170440674 ; pred:  tensor([0.2266, 0.2558, 0.3406, 0.1770], grad_fn=<SoftmaxBackward0>)
num_high 11 len(mask) 14
mask_without_small tensor([0.8209, 0.6639, 0.8121, 0.8146, 0.5436, 0.6302, 0.9025, 0.5885, 0.8285,
        0.8177, 0.8324, 0.8461, 0.8600], grad_fn=<IndexBackward0>)
pred_loss tensor([10.6415], grad_fn=<MulBackward0>)
size_loss tensor(-1162.6278, grad_fn=<MulBackward0>)
size_num_loss 1.1
loss: tensor([-1150.2633], grad_fn=<AddBackward0>)
4
epoch:  4 ; loss:  -1150.2633056640625 ; pred:  tensor([0.2245, 0.2543, 0.3450, 0.1762], grad_fn=<SoftmaxBackward0>)
num_high 10 len(mask) 14
mask_without_small tensor([0.8290, 0.6514, 0.8204, 0.8229, 0.5298, 0.6172, 0.9074, 0.5749, 0.8363,
        0.8259, 0.8401, 0.8535, 0.8667], grad_fn=<IndexBackward0>)
pred_loss tensor([10.5753], grad_fn=<MulBackward0>)
size_loss tensor(-1256.6775, grad_fn=<MulBackward0>)
size_num_loss 1.0
loss: tensor([-1244.4855], grad_fn=<AddBackward0>)
5
epoch:  5 ; loss:  -1244.4854736328125 ; pred:  tensor([0.2234, 0.2537, 0.3473, 0.1756], grad_fn=<SoftmaxBackward0>)
num_high 10 len(mask) 14
mask_without_small tensor([0.8388, 0.6352, 0.8306, 0.8330, 0.5121, 0.6004, 0.9132, 0.5575, 0.8457,
        0.8359, 0.8494, 0.8622, 0.8747], grad_fn=<IndexBackward0>)
pred_loss tensor([10.4890], grad_fn=<MulBackward0>)
size_loss tensor(-1376.3368, grad_fn=<MulBackward0>)
size_num_loss 1.0
loss: tensor([-1364.2395], grad_fn=<AddBackward0>)
6
epoch:  6 ; loss:  -1364.239501953125 ; pred:  tensor([0.2220, 0.2526, 0.3503, 0.1751], grad_fn=<SoftmaxBackward0>)
num_high 9 len(mask) 14
mask_without_small tensor([0.8493, 0.6166, 0.8415, 0.8438, 0.5811, 0.9193, 0.5377, 0.8559, 0.8466,
        0.8593, 0.8715, 0.8832], grad_fn=<IndexBackward0>)
pred_loss tensor([10.3930], grad_fn=<MulBackward0>)
size_loss tensor(-1316.9404, grad_fn=<MulBackward0>)
size_num_loss 0.9
loss: tensor([-1305.0537], grad_fn=<AddBackward0>)
7
epoch:  7 ; loss:  -1305.0537109375 ; pred:  tensor([0.2205, 0.2514, 0.3537, 0.1745], grad_fn=<SoftmaxBackward0>)
num_high 9 len(mask) 14
mask_without_small tensor([0.8598, 0.5962, 0.8526, 0.8548, 0.5601, 0.9254, 0.5163, 0.8660, 0.8574,
        0.8693, 0.8807, 0.8916], grad_fn=<IndexBackward0>)
pred_loss tensor([10.2940], grad_fn=<MulBackward0>)
size_loss tensor(-1450.9543, grad_fn=<MulBackward0>)
size_num_loss 0.9
loss: tensor([-1439.1771], grad_fn=<AddBackward0>)
8
epoch:  8 ; loss:  -1439.1771240234375 ; pred:  tensor([0.2190, 0.2500, 0.3572, 0.1738], grad_fn=<SoftmaxBackward0>)
num_high 9 len(mask) 14
mask_without_small tensor([0.8702, 0.5744, 0.8634, 0.8655, 0.5377, 0.9312, 0.8760, 0.8679, 0.8790,
        0.8898, 0.8999], grad_fn=<IndexBackward0>)
pred_loss tensor([10.1938], grad_fn=<MulBackward0>)
size_loss tensor(-1337.4174, grad_fn=<MulBackward0>)
size_num_loss 0.9
loss: tensor([-1325.7566], grad_fn=<AddBackward0>)
9
epoch:  9 ; loss:  -1325.756591796875 ; pred:  tensor([0.2174, 0.2485, 0.3608, 0.1732], grad_fn=<SoftmaxBackward0>)
num_high 9 len(mask) 14
mask_without_small tensor([0.8801, 0.5518, 0.8738, 0.8757, 0.5145, 0.9367, 0.8855, 0.8780, 0.8883,
        0.8983, 0.9077], grad_fn=<IndexBackward0>)
pred_loss tensor([10.1005], grad_fn=<MulBackward0>)
size_loss tensor(-1463.4547, grad_fn=<MulBackward0>)
size_num_loss 0.9
loss: tensor([-1451.8984], grad_fn=<AddBackward0>)
10
epoch:  10 ; loss:  -1451.8984375 ; pred:  tensor([0.2160, 0.2471, 0.3642, 0.1727], grad_fn=<SoftmaxBackward0>)
num_high 9 len(mask) 14
mask_without_small tensor([0.8894, 0.5280, 0.8835, 0.8853, 0.9418, 0.8944, 0.8875, 0.8970, 0.9063,
        0.9150], grad_fn=<IndexBackward0>)
pred_loss tensor([10.0144], grad_fn=<MulBackward0>)
size_loss tensor(-1189.5122, grad_fn=<MulBackward0>)
size_num_loss 0.9
loss: tensor([-1178.0587], grad_fn=<AddBackward0>)
11
epoch:  11 ; loss:  -1178.0587158203125 ; pred:  tensor([0.2147, 0.2459, 0.3674, 0.1721], grad_fn=<SoftmaxBackward0>)
num_high 9 len(mask) 14
mask_without_small tensor([0.8979, 0.5042, 0.8924, 0.8941, 0.9465, 0.9026, 0.8961, 0.9050, 0.9137,
        0.9217], grad_fn=<IndexBackward0>)
pred_loss tensor([9.9359], grad_fn=<MulBackward0>)
size_loss tensor(-1286.7172, grad_fn=<MulBackward0>)
size_num_loss 0.9
loss: tensor([-1275.3535], grad_fn=<AddBackward0>)
12
epoch:  12 ; loss:  -1275.353515625 ; pred:  tensor([0.2134, 0.2447, 0.3702, 0.1716], grad_fn=<SoftmaxBackward0>)
num_high 9 len(mask) 14
mask_without_small tensor([0.9057, 0.9005, 0.9021, 0.9508, 0.9101, 0.9040, 0.9123, 0.9205, 0.9279],
       grad_fn=<IndexBackward0>)
pred_loss tensor([9.8649], grad_fn=<MulBackward0>)
size_loss tensor(-161.8018, grad_fn=<MulBackward0>)
size_num_loss 0.9
loss: tensor([-150.5252], grad_fn=<AddBackward0>)
13
epoch:  13 ; loss:  -150.5251922607422 ; pred:  tensor([0.2123, 0.2437, 0.3729, 0.1712], grad_fn=<SoftmaxBackward0>)
num_high 9 len(mask) 14
mask_without_small tensor([0.9101, 0.9032, 0.9054, 0.9550, 0.9155, 0.9080, 0.9181, 0.9267, 0.9338],
       grad_fn=<IndexBackward0>)
pred_loss tensor([9.8065], grad_fn=<MulBackward0>)
size_loss tensor(-166.4424, grad_fn=<MulBackward0>)
size_num_loss 0.9
loss: tensor([-155.2322], grad_fn=<AddBackward0>)
14
epoch:  14 ; loss:  -155.232177734375 ; pred:  tensor([0.2113, 0.2428, 0.3751, 0.1708], grad_fn=<SoftmaxBackward0>)
num_high 9 len(mask) 14
mask_without_small tensor([0.9123, 0.9029, 0.9059, 0.9590, 0.9195, 0.9094, 0.9228, 0.9324, 0.9394],
       grad_fn=<IndexBackward0>)
pred_loss tensor([9.7566], grad_fn=<MulBackward0>)
size_loss tensor(-182.3049, grad_fn=<MulBackward0>)
size_num_loss 0.9
loss: tensor([-171.1509], grad_fn=<AddBackward0>)
15
epoch:  15 ; loss:  -171.15089416503906 ; pred:  tensor([0.2104, 0.2421, 0.3769, 0.1706], grad_fn=<SoftmaxBackward0>)
num_high 9 len(mask) 14
mask_without_small tensor([0.9129, 0.9004, 0.9042, 0.9627, 0.9227, 0.9089, 0.9269, 0.9378, 0.9447],
       grad_fn=<IndexBackward0>)
pred_loss tensor([9.7133], grad_fn=<MulBackward0>)
size_loss tensor(-206.9111, grad_fn=<MulBackward0>)
size_num_loss 0.9
loss: tensor([-195.8052], grad_fn=<AddBackward0>)
16
epoch:  16 ; loss:  -195.80516052246094 ; pred:  tensor([0.2096, 0.2414, 0.3786, 0.1705], grad_fn=<SoftmaxBackward0>)
num_high 9 len(mask) 14
mask_without_small tensor([0.9122, 0.8960, 0.9009, 0.9662, 0.9252, 0.9069, 0.9306, 0.9428, 0.9496],
       grad_fn=<IndexBackward0>)
pred_loss tensor([9.6758], grad_fn=<MulBackward0>)
size_loss tensor(-238.7785, grad_fn=<MulBackward0>)
size_num_loss 0.9
loss: tensor([-227.7137], grad_fn=<AddBackward0>)
17
epoch:  17 ; loss:  -227.71365356445312 ; pred:  tensor([0.2088, 0.2408, 0.3800, 0.1704], grad_fn=<SoftmaxBackward0>)
num_high 9 len(mask) 14
mask_without_small tensor([0.9103, 0.8901, 0.8960, 0.9693, 0.9274, 0.9035, 0.9342, 0.9476, 0.9541],
       grad_fn=<IndexBackward0>)
pred_loss tensor([9.6436], grad_fn=<MulBackward0>)
size_loss tensor(-277.0192, grad_fn=<MulBackward0>)
size_num_loss 0.9
loss: tensor([-265.9892], grad_fn=<AddBackward0>)
18
epoch:  18 ; loss:  -265.98919677734375 ; pred:  tensor([0.2081, 0.2402, 0.3812, 0.1704], grad_fn=<SoftmaxBackward0>)
num_high 9 len(mask) 14
mask_without_small tensor([0.9075, 0.8828, 0.8897, 0.9722, 0.9295, 0.8989, 0.9376, 0.9520, 0.9582],
       grad_fn=<IndexBackward0>)
pred_loss tensor([9.6165], grad_fn=<MulBackward0>)
size_loss tensor(-321.1964, grad_fn=<MulBackward0>)
size_num_loss 0.9
loss: tensor([-310.1952], grad_fn=<AddBackward0>)
19
epoch:  19 ; loss:  -310.1951599121094 ; pred:  tensor([0.2075, 0.2397, 0.3823, 0.1705], grad_fn=<SoftmaxBackward0>)
Finished Training
sel masked ver tensor(indices=tensor([[7169, 5504, 5839, 5839, 5839, 5839, 5504, 5839, 5839],
                       [5839, 5985,   67, 3896, 5625, 7169, 7169, 5985, 5230]]),
       values=tensor([0.9075, 0.8828, 0.8897, 0.9722, 0.9295, 0.8989, 0.9376,
                      0.9520, 0.9582]),
       size=(7170, 7170), nnz=9, layout=torch.sparse_coo,
       grad_fn=<SparseCooTensorWithDimsAndTensorsBackward0>)
Counter({'carriesOut': 1, 'publishes': 1, 'fax': 1, 'name': 1, 'type': 1, 'photo': 1, 'worksAtProject': 1, 'publication': 1, 'author': 1})
dict index: {}
node_idx 5839
 node original label [2]
 node predicted label explain 2
 node prediction probability explain tensor([0.2075, 0.2397, 0.3823, 0.1705], grad_fn=<SoftmaxBackward0>)
 node predicted label full 2 most important relations  {'carriesOut': 1, 'publishes': 1, 'fax': 1, 'name': 1, 'type': 1, 'photo': 1, 'worksAtProject': 1, 'publication': 1, 'author': 1, 'label': 2, 'node_idx': '5839'}
 final masks and lenght tensor(indices=tensor([[ 23739,  39125,  46929,  88689, 179489, 179970, 196394,
                        229534, 237819, 254389, 262339, 304099, 328954, 328954],
                       [  5839,   5504,   5985,     67,   5839,   5839,   3896,
                            66,   5625,   7169,   7169,   5985,   5230,   5231]]),
       values=tensor([0.9075, 0.3767, 0.8828, 0.8897, 0.3708, 0.3686, 0.9722,
                      0.3645, 0.9295, 0.8989, 0.9376, 0.9520, 0.9582, 0.3687]),
       size=(753935, 8285), nnz=14, layout=torch.sparse_coo,
       grad_fn=<SparseCooTensorWithDimsAndTensorsBackward0>) 9
 ---------------------------------------------------------------
node label: 2
36
num_high 207 len(mask) 207
mask_without_small tensor([0.7666, 0.7588, 0.7481, 0.6885, 0.7440, 0.7065, 0.7302, 0.6989, 0.7163,
        0.7617, 0.7234, 0.7031, 0.7168, 0.7201, 0.7159, 0.7455, 0.7616, 0.7280,
        0.7213, 0.7395, 0.7162, 0.7514, 0.7463, 0.7623, 0.7551, 0.7554, 0.7427,
        0.7561, 0.7266, 0.7319, 0.7262, 0.7473, 0.7035, 0.7139, 0.7267, 0.7629,
        0.7372, 0.7228, 0.7369, 0.7158, 0.6999, 0.7499, 0.7137, 0.7193, 0.7057,
        0.7701, 0.7065, 0.7215, 0.7130, 0.7182, 0.7326, 0.7411, 0.7215, 0.7535,
        0.7150, 0.7166, 0.7031, 0.7318, 0.7298, 0.7439, 0.7292, 0.7652, 0.7076,
        0.7569, 0.7581, 0.7473, 0.7717, 0.7410, 0.7377, 0.7272, 0.7102, 0.7550,
        0.7277, 0.7411, 0.7322, 0.7392, 0.7420, 0.7185, 0.6864, 0.7163, 0.7313,
        0.7245, 0.7044, 0.7196, 0.7413, 0.7411, 0.7525, 0.7321, 0.7452, 0.7217,
        0.7103, 0.7426, 0.6965, 0.7148, 0.7561, 0.7403, 0.6799, 0.7404, 0.7459,
        0.7316, 0.7433, 0.7422, 0.7512, 0.7223, 0.7275, 0.7454, 0.7388, 0.7345,
        0.7361, 0.7549, 0.7310, 0.7252, 0.7020, 0.7291, 0.7193, 0.7402, 0.7449,
        0.7328, 0.7235, 0.7411, 0.7308, 0.7357, 0.7336, 0.7456, 0.7517, 0.7376,
        0.7447, 0.7389, 0.7667, 0.7502, 0.7024, 0.7087, 0.7284, 0.7615, 0.7435,
        0.7420, 0.7525, 0.7314, 0.6948, 0.7486, 0.7237, 0.7505, 0.7176, 0.7432,
        0.7119, 0.7492, 0.7612, 0.7582, 0.7362, 0.7270, 0.7167, 0.7331, 0.7377,
        0.7493, 0.7220, 0.7609, 0.6805, 0.7229, 0.7073, 0.7465, 0.6928, 0.7355,
        0.7315, 0.7243, 0.7366, 0.7167, 0.7344, 0.7094, 0.6990, 0.7564, 0.7552,
        0.7321, 0.7001, 0.7454, 0.7458, 0.7684, 0.7318, 0.7334, 0.7152, 0.7270,
        0.7127, 0.6992, 0.7085, 0.7208, 0.7209, 0.7011, 0.6922, 0.7335, 0.7504,
        0.7202, 0.7444, 0.7373, 0.7150, 0.7109, 0.7214, 0.7195, 0.7340, 0.7395,
        0.7282, 0.7304, 0.7233, 0.7514, 0.6953, 0.7592, 0.7370, 0.7213, 0.7506],
       grad_fn=<IndexBackward0>)
pred_loss tensor([-0.], grad_fn=<MulBackward0>)
size_loss tensor(-1.9098, grad_fn=<MulBackward0>)
size_num_loss 20.700000000000003
loss: tensor([34.5092], grad_fn=<AddBackward0>)
r_exp.py:102: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  hor_graph, ver_graph = torch.tensor(sub_hor_graph, dtype=torch.float), torch.tensor(sub_ver_graph, dtype=torch.float)
wandb: WARNING Calling wandb.login() after wandb.init() has no effect.
0
epoch:  0 ; loss:  34.50920486450195 ; pred:  tensor([2.6632e-11, 5.8234e-08, 1.0000e+00, 5.5633e-09],
       grad_fn=<SoftmaxBackward0>)
num_high 207 len(mask) 207
mask_without_small tensor([0.7840, 0.7400, 0.7288, 0.6666, 0.7245, 0.6854, 0.7101, 0.6775, 0.6955,
        0.7794, 0.7030, 0.6818, 0.6960, 0.6995, 0.6952, 0.7261, 0.7793, 0.7077,
        0.7008, 0.7197, 0.6954, 0.7322, 0.7269, 0.7799, 0.7361, 0.7364, 0.7231,
        0.7371, 0.7062, 0.7118, 0.7058, 0.7280, 0.6822, 0.6930, 0.7064, 0.7805,
        0.7173, 0.7023, 0.7171, 0.6951, 0.6785, 0.7306, 0.6929, 0.6987, 0.6845,
        0.7873, 0.6854, 0.7010, 0.6921, 0.6975, 0.7125, 0.7215, 0.7010, 0.7344,
        0.6942, 0.6959, 0.6818, 0.7117, 0.7097, 0.7244, 0.7090, 0.7827, 0.6864,
        0.7381, 0.7392, 0.7279, 0.7889, 0.7214, 0.7179, 0.7069, 0.6892, 0.7361,
        0.7075, 0.7214, 0.7121, 0.7195, 0.7224, 0.6978, 0.6644, 0.6956, 0.7112,
        0.7041, 0.6831, 0.6990, 0.7217, 0.7214, 0.7334, 0.7120, 0.7257, 0.7011,
        0.6893, 0.7230, 0.6750, 0.6939, 0.7371, 0.7206, 0.6578, 0.7207, 0.7265,
        0.7115, 0.7237, 0.7226, 0.7320, 0.7018, 0.7072, 0.7259, 0.7191, 0.7145,
        0.7163, 0.7360, 0.7109, 0.7048, 0.6807, 0.7089, 0.6987, 0.7205, 0.7254,
        0.7128, 0.7030, 0.7215, 0.7107, 0.7158, 0.7136, 0.7261, 0.7326, 0.7178,
        0.7253, 0.7192, 0.7841, 0.7310, 0.6811, 0.6876, 0.7082, 0.7792, 0.7240,
        0.7224, 0.7335, 0.7113, 0.6732, 0.7293, 0.7033, 0.7314, 0.6969, 0.7236,
        0.6909, 0.7299, 0.7789, 0.7393, 0.7164, 0.7067, 0.6959, 0.7131, 0.7179,
        0.7301, 0.7015, 0.7786, 0.6584, 0.7024, 0.6862, 0.7271, 0.6711, 0.7155,
        0.7115, 0.7039, 0.7167, 0.6960, 0.7145, 0.6884, 0.6775, 0.7375, 0.7363,
        0.7120, 0.6787, 0.7260, 0.7264, 0.7857, 0.7117, 0.7134, 0.6944, 0.7067,
        0.6918, 0.6778, 0.6875, 0.7003, 0.7004, 0.6797, 0.6705, 0.7135, 0.7312,
        0.6996, 0.7250, 0.7175, 0.6942, 0.6899, 0.7009, 0.6988, 0.7141, 0.7198,
        0.7079, 0.7103, 0.7028, 0.7323, 0.6737, 0.7404, 0.7172, 0.7007, 0.7314],
       grad_fn=<IndexBackward0>)
pred_loss tensor([1.1921e-06], grad_fn=<MulBackward0>)
size_loss tensor(-2.5166, grad_fn=<MulBackward0>)
size_num_loss 20.700000000000003
loss: tensor([33.5504], grad_fn=<AddBackward0>)
1
epoch:  1 ; loss:  33.55038833618164 ; pred:  tensor([8.4668e-11, 1.2996e-07, 1.0000e+00, 1.3835e-08],
       grad_fn=<SoftmaxBackward0>)
num_high 207 len(mask) 207
mask_without_small tensor([0.7989, 0.7235, 0.7086, 0.6442, 0.7040, 0.6635, 0.6891, 0.6554, 0.6740,
        0.7929, 0.6817, 0.6598, 0.6745, 0.6781, 0.6736, 0.7058, 0.7927, 0.6866,
        0.6795, 0.6991, 0.6739, 0.7123, 0.7065, 0.7936, 0.7169, 0.7173, 0.7026,
        0.7183, 0.6851, 0.6908, 0.6847, 0.7078, 0.6602, 0.6714, 0.6853, 0.7945,
        0.6966, 0.6810, 0.6963, 0.6735, 0.6564, 0.7106, 0.6712, 0.6772, 0.6627,
        0.8026, 0.6635, 0.6797, 0.6705, 0.6760, 0.6916, 0.7009, 0.6797, 0.7148,
        0.6727, 0.6743, 0.6598, 0.6907, 0.6886, 0.7040, 0.6879, 0.7972, 0.6646,
        0.7197, 0.7218, 0.7077, 0.8043, 0.7008, 0.6972, 0.6858, 0.6675, 0.7169,
        0.6864, 0.7009, 0.6912, 0.6988, 0.7019, 0.6764, 0.6419, 0.6740, 0.6902,
        0.6828, 0.6612, 0.6776, 0.7011, 0.7009, 0.7137, 0.6911, 0.7054, 0.6798,
        0.6676, 0.7025, 0.6528, 0.6724, 0.7183, 0.7000, 0.6351, 0.7001, 0.7062,
        0.6906, 0.7033, 0.7021, 0.7121, 0.6805, 0.6861, 0.7056, 0.6984, 0.6937,
        0.6955, 0.7167, 0.6899, 0.6836, 0.6586, 0.6878, 0.6773, 0.6999, 0.7050,
        0.6919, 0.6818, 0.7009, 0.6897, 0.6950, 0.6927, 0.7058, 0.7127, 0.6971,
        0.7049, 0.6985, 0.7990, 0.7109, 0.6591, 0.6658, 0.6871, 0.7925, 0.7035,
        0.7019, 0.7137, 0.6904, 0.6509, 0.7091, 0.6821, 0.7114, 0.6754, 0.7032,
        0.6692, 0.7098, 0.7921, 0.7220, 0.6956, 0.6856, 0.6744, 0.6922, 0.6972,
        0.7100, 0.6801, 0.7918, 0.6357, 0.6812, 0.6644, 0.7068, 0.6488, 0.6947,
        0.6905, 0.6827, 0.6960, 0.6745, 0.6936, 0.6666, 0.6554, 0.7188, 0.7171,
        0.6911, 0.6567, 0.7056, 0.7060, 0.8008, 0.6907, 0.6925, 0.6728, 0.6856,
        0.6701, 0.6557, 0.6657, 0.6789, 0.6790, 0.6577, 0.6482, 0.6926, 0.7112,
        0.6782, 0.7046, 0.6968, 0.6727, 0.6682, 0.6795, 0.6774, 0.6932, 0.6991,
        0.6869, 0.6893, 0.6816, 0.7124, 0.6515, 0.7245, 0.6964, 0.6794, 0.7114],
       grad_fn=<IndexBackward0>)
pred_loss tensor([2.3842e-06], grad_fn=<MulBackward0>)
size_loss tensor(-3.2270, grad_fn=<MulBackward0>)
size_num_loss 20.700000000000003
loss: tensor([32.4684], grad_fn=<AddBackward0>)
2
epoch:  2 ; loss:  32.46843719482422 ; pred:  tensor([2.7094e-10, 2.9238e-07, 1.0000e+00, 3.4695e-08],
       grad_fn=<SoftmaxBackward0>)
num_high 203 len(mask) 207
mask_without_small tensor([0.8136, 0.7050, 0.6875, 0.6213, 0.6827, 0.6411, 0.6673, 0.6327, 0.6518,
        0.8070, 0.6597, 0.6373, 0.6524, 0.6561, 0.6515, 0.6845, 0.8069, 0.6648,
        0.6574, 0.6776, 0.6517, 0.6915, 0.6853, 0.8079, 0.6967, 0.6972, 0.6813,
        0.6984, 0.6632, 0.6691, 0.6628, 0.6866, 0.6377, 0.6492, 0.6634, 0.8088,
        0.6750, 0.6590, 0.6748, 0.6513, 0.6338, 0.6896, 0.6490, 0.6551, 0.6402,
        0.8174, 0.6411, 0.6576, 0.6482, 0.6539, 0.6699, 0.6795, 0.6576, 0.6943,
        0.6505, 0.6522, 0.6373, 0.6690, 0.6668, 0.6826, 0.6661, 0.8118, 0.6422,
        0.7001, 0.7028, 0.6865, 0.8192, 0.6794, 0.6756, 0.6640, 0.6451, 0.6967,
        0.6645, 0.6794, 0.6694, 0.6773, 0.6805, 0.6543, 0.6190, 0.6518, 0.6684,
        0.6609, 0.6387, 0.6555, 0.6797, 0.6794, 0.6930, 0.6693, 0.6841, 0.6578,
        0.6452, 0.6811, 0.6301, 0.6501, 0.6984, 0.6786, 0.6120, 0.6786, 0.6850,
        0.6688, 0.6819, 0.6807, 0.6912, 0.6585, 0.6642, 0.6843, 0.6769, 0.6720,
        0.6739, 0.6965, 0.6682, 0.6616, 0.6361, 0.6660, 0.6552, 0.6784, 0.6837,
        0.6702, 0.6598, 0.6795, 0.6679, 0.6734, 0.6711, 0.6845, 0.6919, 0.6755,
        0.6836, 0.6770, 0.8137, 0.6900, 0.6365, 0.6434, 0.6653, 0.8067, 0.6822,
        0.6805, 0.6930, 0.6686, 0.6282, 0.6881, 0.6601, 0.6905, 0.6533, 0.6818,
        0.6469, 0.6888, 0.8062, 0.7030, 0.6740, 0.6637, 0.6522, 0.6705, 0.6757,
        0.6890, 0.6581, 0.8058, 0.6127, 0.6592, 0.6420, 0.6856, 0.6260, 0.6731,
        0.6688, 0.6607, 0.6744, 0.6523, 0.6720, 0.6442, 0.6328, 0.6990, 0.6970,
        0.6693, 0.6340, 0.6844, 0.6848, 0.8156, 0.6690, 0.6708, 0.6506, 0.6637,
        0.6478, 0.6330, 0.6433, 0.6569, 0.6570, 0.6351, 0.6254, 0.6710, 0.6902,
        0.6561, 0.6833, 0.6752, 0.6504, 0.6459, 0.6575, 0.6553, 0.6715, 0.6776,
        0.6650, 0.6675, 0.6596, 0.6916, 0.6287, 0.7063, 0.6748, 0.6574, 0.6905],
       grad_fn=<IndexBackward0>)
pred_loss tensor([8.3447e-06], grad_fn=<MulBackward0>)
size_loss tensor(-4.0090, grad_fn=<MulBackward0>)
size_num_loss 20.3
loss: tensor([30.8985], grad_fn=<AddBackward0>)
3
epoch:  3 ; loss:  30.89847183227539 ; pred:  tensor([8.8523e-10, 6.6668e-07, 1.0000e+00, 8.8313e-08],
       grad_fn=<SoftmaxBackward0>)
num_high 176 len(mask) 207
mask_without_small tensor([0.8278, 0.6850, 0.6655, 0.5981, 0.6605, 0.6182, 0.6448, 0.6097, 0.6291,
        0.8211, 0.6371, 0.6144, 0.6297, 0.6334, 0.6287, 0.6624, 0.8209, 0.6423,
        0.6348, 0.6553, 0.6290, 0.6698, 0.6633, 0.8220, 0.6754, 0.6760, 0.6591,
        0.6774, 0.6407, 0.6467, 0.6403, 0.6646, 0.6148, 0.6264, 0.6409, 0.8230,
        0.6527, 0.6364, 0.6524, 0.6286, 0.6108, 0.6677, 0.6263, 0.6325, 0.6173,
        0.8316, 0.6182, 0.6350, 0.6255, 0.6312, 0.6475, 0.6572, 0.6350, 0.6728,
        0.6277, 0.6295, 0.6144, 0.6466, 0.6444, 0.6605, 0.6436, 0.8260, 0.6194,
        0.6794, 0.6824, 0.6645, 0.8333, 0.6572, 0.6533, 0.6415, 0.6223, 0.6754,
        0.6420, 0.6572, 0.6470, 0.6551, 0.6583, 0.6316, 0.5958, 0.6292, 0.6460,
        0.6383, 0.6158, 0.6329, 0.6574, 0.6572, 0.6714, 0.6469, 0.6620, 0.6352,
        0.6224, 0.6589, 0.6070, 0.6274, 0.6774, 0.6563, 0.5887, 0.6564, 0.6629,
        0.6464, 0.6597, 0.6585, 0.6695, 0.6359, 0.6417, 0.6622, 0.6546, 0.6497,
        0.6515, 0.6752, 0.6457, 0.6391, 0.6131, 0.6435, 0.6325, 0.6562, 0.6616,
        0.6478, 0.6372, 0.6573, 0.6455, 0.6510, 0.6487, 0.6624, 0.6702, 0.6532,
        0.6615, 0.6547, 0.8279, 0.6681, 0.6136, 0.6206, 0.6428, 0.8207, 0.6600,
        0.6583, 0.6714, 0.6462, 0.6051, 0.6661, 0.6375, 0.6686, 0.6306, 0.6596,
        0.6242, 0.6669, 0.8203, 0.6827, 0.6516, 0.6412, 0.6296, 0.6481, 0.6534,
        0.6671, 0.6355, 0.8199, 0.5894, 0.6366, 0.6191, 0.6635, 0.6029, 0.6508,
        0.6463, 0.6382, 0.6520, 0.6296, 0.6496, 0.6214, 0.6098, 0.6781, 0.6758,
        0.6469, 0.6111, 0.6623, 0.6627, 0.8298, 0.6466, 0.6484, 0.6279, 0.6412,
        0.6251, 0.6101, 0.6205, 0.6343, 0.6343, 0.6121, 0.6023, 0.6486, 0.6684,
        0.6335, 0.6611, 0.6529, 0.6277, 0.6231, 0.6349, 0.6327, 0.6491, 0.6554,
        0.6425, 0.6450, 0.6370, 0.6698, 0.6057, 0.6866, 0.6525, 0.6347, 0.6687],
       grad_fn=<IndexBackward0>)
pred_loss tensor([1.7881e-05], grad_fn=<MulBackward0>)
size_loss tensor(-4.8245, grad_fn=<MulBackward0>)
size_num_loss 17.6
loss: tensor([26.9797], grad_fn=<AddBackward0>)
4
epoch:  4 ; loss:  26.97969627380371 ; pred:  tensor([2.9219e-09, 1.5313e-06, 1.0000e+00, 2.2623e-07],
       grad_fn=<SoftmaxBackward0>)
num_high 113 len(mask) 207
mask_without_small tensor([0.8412, 0.6638, 0.6427, 0.5749, 0.6376, 0.5951, 0.6218, 0.5866, 0.6061,
        0.8346, 0.6141, 0.5912, 0.6066, 0.6104, 0.6057, 0.6395, 0.8345, 0.6193,
        0.6118, 0.6324, 0.6059, 0.6471, 0.6404, 0.8355, 0.6532, 0.6539, 0.6361,
        0.6554, 0.6177, 0.6237, 0.6172, 0.6418, 0.5916, 0.6034, 0.6179, 0.8365,
        0.6298, 0.6134, 0.6295, 0.6056, 0.5877, 0.6450, 0.6032, 0.6095, 0.5942,
        0.8449, 0.5951, 0.6120, 0.6024, 0.6082, 0.6245, 0.6343, 0.6120, 0.6503,
        0.6047, 0.6064, 0.5912, 0.6236, 0.6214, 0.6376, 0.6206, 0.8395, 0.5962,
        0.6575, 0.6609, 0.6417, 0.8465, 0.6342, 0.6304, 0.6184, 0.5992, 0.6532,
        0.6190, 0.6342, 0.6240, 0.6321, 0.6354, 0.6085, 0.5725, 0.6061, 0.6230,
        0.6153, 0.5927, 0.6098, 0.6345, 0.6343, 0.6489, 0.6239, 0.6391, 0.6121,
        0.5993, 0.6360, 0.5838, 0.6044, 0.6554, 0.6334, 0.5654, 0.6335, 0.6400,
        0.6234, 0.6368, 0.6355, 0.6468, 0.6128, 0.6187, 0.6393, 0.6316, 0.6267,
        0.6286, 0.6530, 0.6228, 0.6161, 0.5900, 0.6205, 0.6095, 0.6332, 0.6387,
        0.6248, 0.6142, 0.6343, 0.6225, 0.6281, 0.6257, 0.6396, 0.6476, 0.6302,
        0.6386, 0.6318, 0.8413, 0.6454, 0.5904, 0.5975, 0.6198, 0.8343, 0.6371,
        0.6354, 0.6489, 0.6232, 0.5819, 0.6433, 0.6145, 0.6460, 0.6075, 0.6367,
        0.6011, 0.6441, 0.8338, 0.6612, 0.6287, 0.6182, 0.6065, 0.6251, 0.6304,
        0.6443, 0.6125, 0.8334, 0.5661, 0.6136, 0.5960, 0.6407, 0.5797, 0.6278,
        0.6233, 0.6152, 0.6291, 0.6065, 0.6266, 0.5983, 0.5866, 0.6561, 0.6536,
        0.6239, 0.5879, 0.6394, 0.6398, 0.8431, 0.6236, 0.6254, 0.6049, 0.6182,
        0.6020, 0.5869, 0.5973, 0.6112, 0.6113, 0.5890, 0.5791, 0.6256, 0.6457,
        0.6105, 0.6382, 0.6299, 0.6046, 0.6000, 0.6118, 0.6097, 0.6262, 0.6324,
        0.6195, 0.6221, 0.6140, 0.6472, 0.5825, 0.6656, 0.6296, 0.6117, 0.6460],
       grad_fn=<IndexBackward0>)
pred_loss tensor([4.1723e-05], grad_fn=<MulBackward0>)
size_loss tensor(-5.6510, grad_fn=<MulBackward0>)
size_num_loss 11.3
loss: tensor([19.4366], grad_fn=<AddBackward0>)
5
epoch:  5 ; loss:  19.436603546142578 ; pred:  tensor([9.6473e-09, 3.5138e-06, 1.0000e+00, 5.7911e-07],
       grad_fn=<SoftmaxBackward0>)
num_high 35 len(mask) 207
mask_without_small tensor([0.8536, 0.6416, 0.6192, 0.5517, 0.6141, 0.5719, 0.5985, 0.5634, 0.5828,
        0.8473, 0.5908, 0.5680, 0.5833, 0.5871, 0.5824, 0.6160, 0.8472, 0.5959,
        0.5885, 0.6089, 0.5827, 0.6238, 0.6169, 0.8482, 0.6302, 0.6308, 0.6126,
        0.6325, 0.5943, 0.6003, 0.5939, 0.6183, 0.5684, 0.5801, 0.5945, 0.8491,
        0.6063, 0.5901, 0.6060, 0.5823, 0.5644, 0.6216, 0.5799, 0.5862, 0.5710,
        0.8570, 0.5719, 0.5887, 0.5791, 0.5849, 0.6011, 0.6108, 0.5887, 0.6271,
        0.5814, 0.5831, 0.5680, 0.6002, 0.5980, 0.6141, 0.5973, 0.8519, 0.5730,
        0.6348, 0.6384, 0.6182, 0.8585, 0.6107, 0.6069, 0.5951, 0.5760, 0.6301,
        0.5957, 0.6108, 0.6006, 0.6086, 0.6119, 0.5853, 0.5494, 0.5828, 0.5996,
        0.5920, 0.5694, 0.5865, 0.6110, 0.6108, 0.6256, 0.6005, 0.6156, 0.5888,
        0.5761, 0.6125, 0.5606, 0.5811, 0.6324, 0.6099, 0.5422, 0.6100, 0.6165,
        0.6000, 0.6133, 0.6121, 0.6235, 0.5895, 0.5954, 0.6158, 0.6082, 0.6033,
        0.6052, 0.6299, 0.5994, 0.5928, 0.5668, 0.5972, 0.5862, 0.6097, 0.6152,
        0.6014, 0.5909, 0.6108, 0.5991, 0.6046, 0.6023, 0.6161, 0.6242, 0.6068,
        0.6151, 0.6083, 0.8537, 0.6220, 0.5672, 0.5742, 0.5964, 0.8470, 0.6136,
        0.6119, 0.6256, 0.5998, 0.5587, 0.6198, 0.5912, 0.6225, 0.5843, 0.6132,
        0.5778, 0.6206, 0.8465, 0.6388, 0.6052, 0.5948, 0.5832, 0.6017, 0.6070,
        0.6209, 0.5892, 0.8461, 0.5429, 0.5902, 0.5728, 0.6172, 0.5565, 0.6044,
        0.6000, 0.5918, 0.6056, 0.5833, 0.6032, 0.5751, 0.5634, 0.6333, 0.6305,
        0.6005, 0.5647, 0.6159, 0.6163, 0.8554, 0.6002, 0.6020, 0.5816, 0.5949,
        0.5787, 0.5637, 0.5741, 0.5879, 0.5880, 0.5657, 0.5559, 0.6022, 0.6223,
        0.5872, 0.6147, 0.6065, 0.5814, 0.5767, 0.5885, 0.5864, 0.6028, 0.6090,
        0.5962, 0.5987, 0.5907, 0.6238, 0.5593, 0.6435, 0.6061, 0.5884, 0.6226],
       grad_fn=<IndexBackward0>)
pred_loss tensor([9.4176e-05], grad_fn=<MulBackward0>)
size_loss tensor(-6.4725, grad_fn=<MulBackward0>)
size_num_loss 3.5
loss: tensor([10.3875], grad_fn=<AddBackward0>)
6
epoch:  6 ; loss:  10.387470245361328 ; pred:  tensor([3.1486e-08, 7.9867e-06, 9.9999e-01, 1.4721e-06],
       grad_fn=<SoftmaxBackward0>)
num_high 13 len(mask) 207
mask_without_small tensor([0.8647, 0.6185, 0.5952, 0.5288, 0.5902, 0.5487, 0.5749, 0.5403, 0.5595,
        0.8589, 0.5674, 0.5449, 0.5600, 0.5637, 0.5591, 0.5921, 0.8587, 0.5724,
        0.5651, 0.5851, 0.5593, 0.5998, 0.5929, 0.8597, 0.6064, 0.6071, 0.5887,
        0.6088, 0.5708, 0.5767, 0.5704, 0.5943, 0.5453, 0.5568, 0.5710, 0.8605,
        0.5826, 0.5667, 0.5823, 0.5590, 0.5414, 0.5976, 0.5566, 0.5628, 0.5478,
        0.8678, 0.5487, 0.5653, 0.5559, 0.5616, 0.5775, 0.5869, 0.5653, 0.6032,
        0.5581, 0.5598, 0.5449, 0.5766, 0.5744, 0.5902, 0.5737, 0.8632, 0.5498,
        0.6113, 0.6151, 0.5942, 0.8691, 0.5869, 0.5831, 0.5716, 0.5527, 0.6064,
        0.5721, 0.5869, 0.5770, 0.5848, 0.5880, 0.5619, 0.5265, 0.5595, 0.5760,
        0.5685, 0.5463, 0.5632, 0.5872, 0.5869, 0.6016, 0.5769, 0.5917, 0.5654,
        0.5529, 0.5886, 0.5376, 0.5578, 0.6088, 0.5860, 0.5194, 0.5861, 0.5926,
        0.5764, 0.5894, 0.5882, 0.5995, 0.5661, 0.5718, 0.5919, 0.5844, 0.5796,
        0.5814, 0.6062, 0.5758, 0.5693, 0.5437, 0.5736, 0.5629, 0.5859, 0.5913,
        0.5777, 0.5674, 0.5870, 0.5755, 0.5809, 0.5786, 0.5921, 0.6003, 0.5830,
        0.5911, 0.5845, 0.8648, 0.5980, 0.5441, 0.5511, 0.5729, 0.8586, 0.5897,
        0.5880, 0.6016, 0.5762, 0.5357, 0.5958, 0.5677, 0.5986, 0.5609, 0.5893,
        0.5546, 0.5967, 0.8582, 0.6155, 0.5815, 0.5713, 0.5599, 0.5780, 0.5832,
        0.5969, 0.5658, 0.8578, 0.5201, 0.5668, 0.5496, 0.5932, 0.5335, 0.5806,
        0.5763, 0.5684, 0.5819, 0.5600, 0.5795, 0.5519, 0.5404, 0.6097, 0.6068,
        0.5769, 0.5416, 0.5919, 0.5924, 0.8663, 0.5766, 0.5784, 0.5583, 0.5714,
        0.5555, 0.5406, 0.5509, 0.5645, 0.5646, 0.5427, 0.5329, 0.5785, 0.5983,
        0.5638, 0.5908, 0.5827, 0.5581, 0.5535, 0.5651, 0.5630, 0.5791, 0.5851,
        0.5726, 0.5751, 0.5672, 0.5999, 0.5363, 0.6205, 0.5824, 0.5650, 0.5986],
       grad_fn=<IndexBackward0>)
pred_loss tensor([0.0002], grad_fn=<MulBackward0>)
size_loss tensor(-7.2762, grad_fn=<MulBackward0>)
size_num_loss 1.3
loss: tensor([6.9481], grad_fn=<AddBackward0>)
7
epoch:  7 ; loss:  6.948064804077148 ; pred:  tensor([1.0037e-07, 1.7857e-05, 9.9998e-01, 3.6829e-06],
       grad_fn=<SoftmaxBackward0>)
num_high 13 len(mask) 207
mask_without_small tensor([0.8743, 0.5948, 0.5709, 0.5063, 0.5660, 0.5258, 0.5513, 0.5176, 0.5363,
        0.8691, 0.5440, 0.5220, 0.5368, 0.5405, 0.5360, 0.5678, 0.8690, 0.5489,
        0.5418, 0.5611, 0.5362, 0.5754, 0.5687, 0.8698, 0.5822, 0.5829, 0.5646,
        0.5846, 0.5474, 0.5530, 0.5469, 0.5700, 0.5225, 0.5337, 0.5475, 0.8706,
        0.5587, 0.5433, 0.5584, 0.5358, 0.5186, 0.5732, 0.5336, 0.5396, 0.5249,
        0.8770, 0.5258, 0.5420, 0.5328, 0.5383, 0.5538, 0.5629, 0.5420, 0.5789,
        0.5350, 0.5367, 0.5221, 0.5529, 0.5509, 0.5660, 0.5501, 0.8730, 0.5269,
        0.5872, 0.5912, 0.5699, 0.8782, 0.5628, 0.5592, 0.5481, 0.5297, 0.5821,
        0.5486, 0.5628, 0.5533, 0.5608, 0.5639, 0.5387, 0.5040, 0.5364, 0.5524,
        0.5451, 0.5234, 0.5399, 0.5631, 0.5628, 0.5773, 0.5532, 0.5674, 0.5421,
        0.5299, 0.5645, 0.5149, 0.5347, 0.5846, 0.5620, 0.5621, 0.5683, 0.5527,
        0.5652, 0.5641, 0.5751, 0.5428, 0.5483, 0.5676, 0.5604, 0.5558, 0.5576,
        0.5819, 0.5521, 0.5459, 0.5209, 0.5500, 0.5396, 0.5619, 0.5670, 0.5540,
        0.5441, 0.5629, 0.5519, 0.5571, 0.5549, 0.5679, 0.5759, 0.5591, 0.5669,
        0.5605, 0.8744, 0.5736, 0.5213, 0.5281, 0.5494, 0.8688, 0.5655, 0.5639,
        0.5773, 0.5525, 0.5130, 0.5715, 0.5444, 0.5742, 0.5377, 0.5652, 0.5315,
        0.5723, 0.8685, 0.5916, 0.5577, 0.5478, 0.5367, 0.5543, 0.5593, 0.5725,
        0.5424, 0.8681, 0.5435, 0.5267, 0.5689, 0.5109, 0.5568, 0.5527, 0.5450,
        0.5580, 0.5368, 0.5557, 0.5289, 0.5176, 0.5855, 0.5826, 0.5532, 0.5189,
        0.5677, 0.5681, 0.8758, 0.5529, 0.5546, 0.5352, 0.5479, 0.5324, 0.5179,
        0.5280, 0.5412, 0.5413, 0.5199, 0.5103, 0.5548, 0.5739, 0.5405, 0.5666,
        0.5588, 0.5350, 0.5305, 0.5418, 0.5398, 0.5553, 0.5611, 0.5491, 0.5515,
        0.5439, 0.5755, 0.5136, 0.5969, 0.5585, 0.5417, 0.5743],
       grad_fn=<IndexBackward0>)
pred_loss tensor([0.0005], grad_fn=<MulBackward0>)
size_loss tensor(-805.7303, grad_fn=<MulBackward0>)
size_num_loss 1.3
loss: tensor([-802.5901], grad_fn=<AddBackward0>)
8
epoch:  8 ; loss:  -802.5901489257812 ; pred:  tensor([3.1046e-07, 3.9189e-05, 9.9995e-01, 8.9781e-06],
       grad_fn=<SoftmaxBackward0>)
num_high 13 len(mask) 207
mask_without_small tensor([0.8797, 0.6060, 0.5460, 0.5492, 0.5122, 0.5371, 0.5040, 0.5226, 0.8747,
        0.5301, 0.5084, 0.5231, 0.5266, 0.5222, 0.5495, 0.8745, 0.5348, 0.5279,
        0.5460, 0.5224, 0.5812, 0.5492, 0.8753, 0.5922, 0.5931, 0.5485, 0.5951,
        0.5334, 0.5388, 0.5330, 0.5476, 0.5089, 0.5200, 0.5335, 0.8761, 0.5439,
        0.5294, 0.5437, 0.5221, 0.5050, 0.5702, 0.5199, 0.5258, 0.5113, 0.8823,
        0.5122, 0.5281, 0.5191, 0.5246, 0.5395, 0.5474, 0.5281, 0.5879, 0.5213,
        0.5229, 0.5085, 0.5387, 0.5367, 0.5492, 0.5360, 0.8784, 0.5133, 0.5980,
        0.6023, 0.5477, 0.8834, 0.5473, 0.5444, 0.5341, 0.5161, 0.5922, 0.5346,
        0.5473, 0.5391, 0.5458, 0.5481, 0.5249, 0.5226, 0.5382, 0.5312, 0.5098,
        0.5261, 0.5475, 0.5473, 0.5853, 0.5390, 0.5495, 0.5283, 0.5162, 0.5484,
        0.5014, 0.5210, 0.5951, 0.5467, 0.5468, 0.5494, 0.5385, 0.5489, 0.5482,
        0.5802, 0.5289, 0.5343, 0.5495, 0.5454, 0.5414, 0.5430, 0.5919, 0.5379,
        0.5319, 0.5073, 0.5360, 0.5258, 0.5466, 0.5495, 0.5397, 0.5302, 0.5474,
        0.5377, 0.5425, 0.5405, 0.5495, 0.5824, 0.5443, 0.5495, 0.5455, 0.8797,
        0.5734, 0.5077, 0.5144, 0.5353, 0.8744, 0.5490, 0.5481, 0.5853, 0.5383,
        0.5478, 0.5305, 0.5766, 0.5240, 0.5488, 0.5179, 0.5585, 0.8740, 0.6027,
        0.5430, 0.5338, 0.5230, 0.5400, 0.5445, 0.5619, 0.5286, 0.8737, 0.5296,
        0.5130, 0.5490, 0.5423, 0.5384, 0.5311, 0.5434, 0.5230, 0.5413, 0.5152,
        0.5040, 0.5962, 0.5927, 0.5390, 0.5053, 0.5495, 0.5495, 0.8811, 0.5387,
        0.5403, 0.5214, 0.5339, 0.5187, 0.5043, 0.5143, 0.5274, 0.5275, 0.5063,
        0.5404, 0.5752, 0.5267, 0.5494, 0.5440, 0.5212, 0.5168, 0.5280, 0.5260,
        0.5409, 0.5460, 0.5350, 0.5373, 0.5300, 0.5813, 0.5001, 0.6082, 0.5438,
        0.5279, 0.5769], grad_fn=<IndexBackward0>)
pred_loss tensor([0.0007], grad_fn=<MulBackward0>)
size_loss tensor(-862.6672, grad_fn=<MulBackward0>)
size_num_loss 1.3
loss: tensor([-859.5728], grad_fn=<AddBackward0>)
9
epoch:  9 ; loss:  -859.57275390625 ; pred:  tensor([5.0951e-07, 5.6028e-05, 9.9993e-01, 1.3434e-05],
       grad_fn=<SoftmaxBackward0>)
num_high 14 len(mask) 207
mask_without_small tensor([0.8864, 0.6208, 0.5321, 0.5321, 0.5197, 0.5053, 0.8817, 0.5128, 0.5059,
        0.5094, 0.5050, 0.5330, 0.8815, 0.5175, 0.5107, 0.5284, 0.5052, 0.5937,
        0.5332, 0.8823, 0.6062, 0.6072, 0.5311, 0.6094, 0.5160, 0.5213, 0.5156,
        0.5328, 0.5028, 0.5162, 0.8830, 0.5264, 0.5121, 0.5262, 0.5049, 0.5809,
        0.5026, 0.5085, 0.8889, 0.5108, 0.5019, 0.5073, 0.5220, 0.5298, 0.5108,
        0.6014, 0.5040, 0.5057, 0.5212, 0.5193, 0.5320, 0.5187, 0.8852, 0.6125,
        0.6170, 0.5329, 0.8900, 0.5298, 0.5269, 0.5167, 0.6062, 0.5172, 0.5298,
        0.5216, 0.5282, 0.5306, 0.5077, 0.5054, 0.5208, 0.5139, 0.5088, 0.5300,
        0.5298, 0.5984, 0.5215, 0.5328, 0.5110, 0.5310, 0.5037, 0.6094, 0.5292,
        0.5292, 0.5332, 0.5211, 0.5315, 0.5307, 0.5926, 0.5116, 0.5169, 0.5329,
        0.5279, 0.5239, 0.5254, 0.6059, 0.5205, 0.5146, 0.5186, 0.5085, 0.5291,
        0.5326, 0.5223, 0.5129, 0.5299, 0.5203, 0.5250, 0.5230, 0.5330, 0.5951,
        0.5268, 0.5326, 0.5280, 0.8865, 0.5850, 0.5179, 0.8814, 0.5317, 0.5306,
        0.5984, 0.5209, 0.5341, 0.5131, 0.5886, 0.5067, 0.5315, 0.5007, 0.5433,
        0.8810, 0.6174, 0.5255, 0.5165, 0.5058, 0.5225, 0.5269, 0.5481, 0.5113,
        0.8807, 0.5123, 0.5332, 0.5248, 0.5210, 0.5137, 0.5258, 0.5058, 0.5238,
        0.6105, 0.6068, 0.5215, 0.5330, 0.5331, 0.8878, 0.5212, 0.5228, 0.5042,
        0.5165, 0.5015, 0.5101, 0.5102, 0.5230, 0.5870, 0.5094, 0.5324, 0.5265,
        0.5040, 0.5107, 0.5087, 0.5234, 0.5285, 0.5177, 0.5199, 0.5127, 0.5939,
        0.6231, 0.5262, 0.5106, 0.5889], grad_fn=<IndexBackward0>)
pred_loss tensor([0.0010], grad_fn=<MulBackward0>)
size_loss tensor(-972.7987, grad_fn=<MulBackward0>)
size_num_loss 1.4000000000000001
loss: tensor([-969.7507], grad_fn=<AddBackward0>)
10
epoch:  10 ; loss:  -969.7506713867188 ; pred:  tensor([8.7004e-07, 8.2552e-05, 9.9990e-01, 2.0802e-05],
       grad_fn=<SoftmaxBackward0>)
num_high 25 len(mask) 207
mask_without_small tensor([0.8938, 0.6378, 0.5153, 0.5142, 0.5005, 0.8893, 0.5155, 0.8892, 0.5098,
        0.6092, 0.5159, 0.8899, 0.6226, 0.6237, 0.5130, 0.6260, 0.5021, 0.5158,
        0.8906, 0.5075, 0.5073, 0.5952, 0.8962, 0.5029, 0.5114, 0.6174, 0.5020,
        0.5001, 0.5141, 0.8927, 0.6292, 0.6339, 0.5158, 0.8972, 0.5114, 0.5080,
        0.6226, 0.5114, 0.5024, 0.5096, 0.5123, 0.5015, 0.5116, 0.5114, 0.6142,
        0.5023, 0.5152, 0.5129, 0.6260, 0.5106, 0.5107, 0.5158, 0.5019, 0.5135,
        0.5125, 0.6080, 0.5154, 0.5091, 0.5048, 0.5065, 0.6223, 0.5013, 0.5105,
        0.5150, 0.5031, 0.5115, 0.5011, 0.5060, 0.5039, 0.5155, 0.6107, 0.5079,
        0.5149, 0.5093, 0.8939, 0.5997, 0.8891, 0.5138, 0.5123, 0.6142, 0.5017,
        0.5175, 0.6037, 0.5134, 0.5282, 0.8888, 0.6343, 0.5066, 0.5034, 0.5081,
        0.5345, 0.8885, 0.5160, 0.5058, 0.5018, 0.5069, 0.5047, 0.6272, 0.6232,
        0.5024, 0.5154, 0.5157, 0.8951, 0.5020, 0.5037, 0.5038, 0.6019, 0.5146,
        0.5076, 0.5043, 0.5098, 0.5007, 0.6094, 0.6401, 0.5073, 0.6040],
       grad_fn=<IndexBackward0>)
pred_loss tensor([0.0016], grad_fn=<MulBackward0>)
size_loss tensor(-1213.7134, grad_fn=<MulBackward0>)
size_num_loss 2.5
loss: tensor([-1209.8767], grad_fn=<AddBackward0>)
11
epoch:  11 ; loss:  -1209.876708984375 ; pred:  tensor([1.5020e-06, 1.2246e-04, 9.9984e-01, 3.2514e-05],
       grad_fn=<SoftmaxBackward0>)
num_high 33 len(mask) 207
mask_without_small tensor([0.9015, 0.6562, 0.8973, 0.8972, 0.6269, 0.8978, 0.6408, 0.6418, 0.6442,
        0.8985, 0.6122, 0.9037, 0.6354, 0.9004, 0.6475, 0.6523, 0.9046, 0.6407,
        0.6321, 0.6442, 0.6257, 0.6404, 0.6285, 0.9015, 0.6170, 0.8970, 0.6321,
        0.5007, 0.6212, 0.5129, 0.8967, 0.6527, 0.5202, 0.8965, 0.6454, 0.6414,
        0.9026, 0.6194, 0.6271, 0.6585, 0.6215], grad_fn=<IndexBackward0>)
pred_loss tensor([0.0022], grad_fn=<MulBackward0>)
size_loss tensor(-1347.4703, grad_fn=<MulBackward0>)
size_num_loss 3.3000000000000003
loss: tensor([-1343.2122], grad_fn=<AddBackward0>)
12
epoch:  12 ; loss:  -1343.212158203125 ; pred:  tensor([2.4363e-06, 1.7354e-04, 9.9978e-01, 4.8391e-05],
       grad_fn=<SoftmaxBackward0>)
num_high 29 len(mask) 207
mask_without_small tensor([0.9091, 0.6561, 0.9052, 0.9051, 0.6192, 0.9057, 0.6361, 0.6374, 0.6404,
        0.9063, 0.6024, 0.9111, 0.6294, 0.9081, 0.6446, 0.6509, 0.9120, 0.6360,
        0.6254, 0.6404, 0.6178, 0.6356, 0.6211, 0.9091, 0.6078, 0.9050, 0.6254,
        0.6126, 0.9047, 0.6515, 0.5071, 0.9044, 0.6419, 0.6368, 0.9102, 0.6105,
        0.6195, 0.6593, 0.6129], grad_fn=<IndexBackward0>)
pred_loss tensor([0.0034], grad_fn=<MulBackward0>)
size_loss tensor(-1363.4850, grad_fn=<MulBackward0>)
size_num_loss 2.9000000000000004
loss: tensor([-1359.6384], grad_fn=<AddBackward0>)
13
epoch:  13 ; loss:  -1359.638427734375 ; pred:  tensor([4.2722e-06, 2.5720e-04, 9.9966e-01, 7.5785e-05],
       grad_fn=<SoftmaxBackward0>)
num_high 26 len(mask) 207
mask_without_small tensor([0.9165, 0.6482, 0.9129, 0.9128, 0.6059, 0.9134, 0.6247, 0.6262, 0.6297,
        0.9139, 0.5878, 0.9184, 0.6172, 0.9156, 0.6346, 0.6419, 0.9192, 0.6246,
        0.6127, 0.6297, 0.6044, 0.6242, 0.6079, 0.9166, 0.5935, 0.9127, 0.6127,
        0.5987, 0.9125, 0.6426, 0.9122, 0.6314, 0.6255, 0.9176, 0.5964, 0.6062,
        0.6521, 0.5991], grad_fn=<IndexBackward0>)
pred_loss tensor([0.0050], grad_fn=<MulBackward0>)
size_loss tensor(-1429.5468, grad_fn=<MulBackward0>)
size_num_loss 2.6
loss: tensor([-1426.0079], grad_fn=<AddBackward0>)
14
epoch:  14 ; loss:  -1426.0079345703125 ; pred:  tensor([7.3786e-06, 3.7560e-04, 9.9950e-01, 1.1666e-04],
       grad_fn=<SoftmaxBackward0>)
num_high 17 len(mask) 207
mask_without_small tensor([0.9237, 0.6356, 0.9204, 0.9203, 0.5890, 0.9208, 0.6093, 0.6109, 0.6148,
        0.9213, 0.5699, 0.9254, 0.6012, 0.9228, 0.6202, 0.6284, 0.9262, 0.6092,
        0.5963, 0.6148, 0.5874, 0.6087, 0.5912, 0.9237, 0.5759, 0.9202, 0.5963,
        0.5813, 0.9199, 0.6292, 0.9197, 0.6167, 0.6102, 0.9246, 0.5790, 0.5893,
        0.6400, 0.5817], grad_fn=<IndexBackward0>)
pred_loss tensor([0.0072], grad_fn=<MulBackward0>)
size_loss tensor(-1540.5461, grad_fn=<MulBackward0>)
size_num_loss 1.7000000000000002
loss: tensor([-1537.9105], grad_fn=<AddBackward0>)
15
epoch:  15 ; loss:  -1537.9105224609375 ; pred:  tensor([1.2379e-05, 5.3617e-04, 9.9928e-01, 1.7496e-04],
       grad_fn=<SoftmaxBackward0>)
num_high 14 len(mask) 207
mask_without_small tensor([0.9305, 0.6197, 0.9274, 0.9273, 0.5695, 0.9278, 0.5910, 0.5927, 0.5969,
        0.9283, 0.5495, 0.9321, 0.5823, 0.9297, 0.6027, 0.6117, 0.9327, 0.5909,
        0.5772, 0.5969, 0.5678, 0.5904, 0.5718, 0.9305, 0.5558, 0.9272, 0.5772,
        0.5615, 0.9270, 0.6126, 0.9268, 0.5990, 0.5920, 0.9313, 0.5590, 0.5698,
        0.6246, 0.5619], grad_fn=<IndexBackward0>)
pred_loss tensor([0.0102], grad_fn=<MulBackward0>)
size_loss tensor(-1662.9962, grad_fn=<MulBackward0>)
size_num_loss 1.4000000000000001
loss: tensor([-1660.6639], grad_fn=<AddBackward0>)
16
epoch:  16 ; loss:  -1660.6639404296875 ; pred:  tensor([2.0102e-05, 7.4692e-04, 9.9898e-01, 2.5528e-04],
       grad_fn=<SoftmaxBackward0>)
num_high 13 len(mask) 207
mask_without_small tensor([0.9368, 0.6012, 0.9340, 0.9339, 0.5479, 0.9344, 0.5704, 0.5723, 0.5766,
        0.9348, 0.5273, 0.9382, 0.5613, 0.9360, 0.5829, 0.5925, 0.9388, 0.5703,
        0.5559, 0.5766, 0.5461, 0.5698, 0.5503, 0.9368, 0.5337, 0.9338, 0.5560,
        0.5396, 0.9336, 0.5935, 0.9334, 0.5789, 0.5714, 0.9375, 0.5370, 0.5482,
        0.6066, 0.5400], grad_fn=<IndexBackward0>)
pred_loss tensor([0.0141], grad_fn=<MulBackward0>)
size_loss tensor(-1793.6647, grad_fn=<MulBackward0>)
size_num_loss 1.3
loss: tensor([-1791.4355], grad_fn=<AddBackward0>)
17
epoch:  17 ; loss:  -1791.435546875 ; pred:  tensor([3.1640e-05, 1.0170e-03, 9.9859e-01, 3.6283e-04],
       grad_fn=<SoftmaxBackward0>)
num_high 13 len(mask) 207
mask_without_small tensor([0.9426, 0.5806, 0.9400, 0.9400, 0.5247, 0.9404, 0.5480, 0.5500, 0.5545,
        0.9408, 0.5036, 0.9439, 0.5385, 0.9419, 0.5611, 0.5713, 0.9445, 0.5479,
        0.5330, 0.5545, 0.5229, 0.5473, 0.5272, 0.9426, 0.5102, 0.9399, 0.5330,
        0.5162, 0.9397, 0.5723, 0.9395, 0.5569, 0.5491, 0.9433, 0.5136, 0.5251,
        0.5864, 0.5166], grad_fn=<IndexBackward0>)
pred_loss tensor([0.0191], grad_fn=<MulBackward0>)
size_loss tensor(-1930.2928, grad_fn=<MulBackward0>)
size_num_loss 1.3
loss: tensor([-1928.0667], grad_fn=<AddBackward0>)
18
epoch:  18 ; loss:  -1928.066650390625 ; pred:  tensor([4.8405e-05, 1.3565e-03, 9.9809e-01, 5.0351e-04],
       grad_fn=<SoftmaxBackward0>)
num_high 13 len(mask) 207
mask_without_small tensor([0.9479, 0.5582, 0.9456, 0.9455, 0.5003, 0.9459, 0.5241, 0.5262, 0.5309,
        0.9462, 0.9491, 0.5144, 0.9473, 0.5377, 0.5484, 0.9496, 0.5240, 0.5087,
        0.5309, 0.5235, 0.5028, 0.9479, 0.9455, 0.5087, 0.9453, 0.5495, 0.9451,
        0.5333, 0.5253, 0.9486, 0.5006, 0.5644], grad_fn=<IndexBackward0>)
pred_loss tensor([0.0253], grad_fn=<MulBackward0>)
size_loss tensor(-2100.4304, grad_fn=<MulBackward0>)
size_num_loss 1.3
loss: tensor([-2098.2356], grad_fn=<AddBackward0>)
19
epoch:  19 ; loss:  -2098.235595703125 ; pred:  tensor([7.2105e-05, 1.7748e-03, 9.9747e-01, 6.8346e-04],
       grad_fn=<SoftmaxBackward0>)
Finished Training
sel masked ver tensor(indices=tensor([[6821, 6961, 7121, 7962, 8026, 5954, 5504, 5504, 5439,
                        6961, 7121, 7779, 7969, 8009, 8016, 8017, 5954, 7037,
                        7753, 8017, 5947, 5947, 5947, 5947, 5954, 5954, 5954,
                        5755, 5755, 5755, 5504, 5755],
                       [5755, 5755, 5755, 5755, 5755, 5504, 5947, 5954, 5954,
                        5954, 5947, 5947, 5947, 5947, 5947, 5954, 5439, 5463,
                        5463, 5471, 6961, 7051, 7777, 7904, 6964, 7037, 7962,
                        7753, 7777, 7962, 8017, 5947]]),
       values=tensor([0.9479, 0.5582, 0.9456, 0.9455, 0.5003, 0.9459, 0.5241,
                      0.5262, 0.5309, 0.9462, 0.9491, 0.5144, 0.9473, 0.5377,
                      0.5484, 0.9496, 0.5240, 0.5087, 0.5309, 0.5235, 0.5028,
                      0.9479, 0.9455, 0.5087, 0.9453, 0.5495, 0.9451, 0.5333,
                      0.5253, 0.9486, 0.5006, 0.5644]),
       size=(8027, 8018), nnz=32, layout=torch.sparse_coo,
       grad_fn=<SparseCooTensorWithDimsAndTensorsBackward0>)
Counter({'projectInfo': 7, 'hasProject': 7, 'author': 5, 'isAbout': 4, 'publication': 3, 'carriesOut': 2, 'dealtWithIn': 1, 'publishes': 1, 'worksAtProject': 1, 'carriedOutBy': 1})
dict index: {}
node_idx 5755
 node original label [2]
 node predicted label explain 2
 node prediction probability explain tensor([7.2105e-05, 1.7748e-03, 9.9747e-01, 6.8346e-04],
       grad_fn=<SoftmaxBackward0>)
 node predicted label full 2 most important relations  {'dealtWithIn': 1, 'carriesOut': 2, 'publishes': 1, 'worksAtProject': 1, 'publication': 3, 'projectInfo': 7, 'isAbout': 4, 'carriedOutBy': 1, 'author': 5, 'hasProject': 7, 'label': 2, 'node_idx': '5755'}
 final masks and lenght tensor(indices=tensor([[ 23391,  23531,  23532,  23534,  23607,  23621,  23625,
                         23658,  23672,  23691,  24323,  24347,  24348,  24349,
                         24474,  24520,  24532,  24539,  24579,  24586,  24587,
                         24596,  39087,  39094,  46929,  46929,  63434,  63434,
                         63439,  63458,  63458,  63466,  88605, 114526, 114666,
                        114666, 114667, 114667, 114669, 114669, 114742, 114742,
                        114756, 114760, 114807, 114826, 114826, 115458, 115458,
                        115482, 115482, 115483, 115483, 115484, 115484, 115609,
                        115609, 115655, 115655, 115667, 115667, 115674, 115674,
                        115714, 115721, 115722, 115722, 115731, 115731, 146792,
                        146792, 146799, 146799, 146799, 146799, 147666, 147666,
                        147666, 147806, 147806, 147807, 147807, 147809, 147809,
                        147809, 147882, 147882, 147882, 147900, 147900, 147933,
                        147966, 147966, 147966, 148598, 148598, 148622, 148623,
                        148623, 148749, 148749, 148749, 148795, 148795, 148807,
                        148814, 148814, 148861, 148861, 148862, 148871, 148871,
                        154490, 154569, 154574, 154593, 154601, 179489, 179932,
                        179939, 196310, 229450, 237735, 246212, 246212, 246212,
                        246212, 246212, 246212, 246212, 246212, 246212, 246212,
                        246212, 246212, 246212, 246212, 246212, 246212, 246212,
                        246212, 246212, 246212, 246212, 246219, 246219, 246219,
                        246219, 246219, 246219, 246219, 246219, 246219, 246219,
                        246219, 246219, 246219, 246219, 246219, 254305, 254305,
                        254305, 254305, 254305, 254305, 254305, 254305, 254305,
                        254305, 254305, 254305, 254305, 254305, 254305, 254305,
                        254305, 254305, 254305, 254305, 254305, 254305, 262339,
                        262339, 262339, 262339, 262339, 262339, 262339, 262339,
                        262339, 262339, 262339, 262339, 262339, 262339, 262339,
                        262339, 262339, 262339, 262339, 262339, 262339, 262339,
                        304015, 304015, 328870, 328870],
                       [  5755,   5755,   5755,   5755,   5755,   5755,   5755,
                          5755,   5755,   5755,   5755,   5755,   5755,   5755,
                          5755,   5755,   5755,   5755,   5755,   5755,   5755,
                          5755,   5504,   5504,   5947,   5954,   5947,   5954,
                          5954,   5947,   5954,   5954,     79,   5947,   5947,
                          5954,   5947,   5954,   5947,   5954,   5947,   5954,
                          5947,   5947,   5947,   5947,   5954,   5947,   5954,
                          5947,   5954,   5947,   5954,   5947,   5954,   5947,
                          5954,   5947,   5954,   5947,   5954,   5947,   5954,
                          5947,   5947,   5947,   5954,   5947,   5954,   5439,
                          5463,   5439,   5444,   5463,   5471,   5444,   5463,
                          5471,   5463,   5471,   5463,   5471,   5360,   5463,
                          5471,   5444,   5463,   5471,   5463,   5471,   5463,
                          5360,   5463,   5471,   5463,   5471,   5471,   5463,
                          5471,   5439,   5463,   5471,   5360,   5463,   5463,
                          5463,   5471,   5360,   5463,   5471,   5360,   5463,
                          5755,   5755,   5755,   5755,   5755,   5755,   5755,
                          5755,   2619,     91,   5562,   6821,   6961,   6962,
                          6964,   7037,   7051,   7055,   7102,   7121,   7753,
                          7777,   7778,   7779,   7904,   7950,   7962,   7969,
                          8009,   8016,   8017,   8026,   6961,   6962,   6964,
                          7037,   7121,   7753,   7777,   7778,   7779,   7904,
                          7950,   7962,   7969,   8017,   8026,   6821,   6961,
                          6962,   6964,   7037,   7051,   7055,   7088,   7102,
                          7121,   7753,   7777,   7778,   7779,   7904,   7950,
                          7962,   7969,   8009,   8016,   8017,   8026,   6821,
                          6961,   6962,   6964,   7037,   7051,   7055,   7088,
                          7102,   7121,   7753,   7777,   7778,   7779,   7904,
                          7950,   7962,   7969,   8009,   8016,   8017,   8026,
                          5947,   5954,   5230,   5231]]),
       values=tensor([0.9479, 0.5582, 0.4180, 0.4154, 0.4158, 0.4017, 0.3930,
                      0.3941, 0.3875, 0.9456, 0.3949, 0.3983, 0.3880, 0.3915,
                      0.3871, 0.4182, 0.9455, 0.3998, 0.3927, 0.4079, 0.3874,
                      0.5003, 0.4189, 0.9459, 0.5241, 0.5262, 0.4136, 0.5309,
                      0.3983, 0.3955, 0.3979, 0.4187, 0.3986, 0.3850, 0.3985,
                      0.9462, 0.4040, 0.3943, 0.4035, 0.3870, 0.3951, 0.4789,
                      0.3849, 0.3906, 0.4009, 0.9491, 0.4017, 0.3929, 0.3842,
                      0.3894, 0.3966, 0.4107, 0.3929, 0.5144, 0.3862, 0.3878,
                      0.3983, 0.3953, 0.3925, 0.4157, 0.4012, 0.9473, 0.4027,
                      0.5377, 0.5484, 0.4188, 0.9496, 0.4106, 0.4048, 0.3990,
                      0.4053, 0.5240, 0.3996, 0.4107, 0.3959, 0.4074, 0.4124,
                      0.3898, 0.4132, 0.3875, 0.3946, 0.3961, 0.3995, 0.3909,
                      0.4111, 0.4107, 0.5087, 0.3958, 0.4177, 0.3931, 0.4054,
                      0.4133, 0.3917, 0.3859, 0.5309, 0.4093, 0.3824, 0.4095,
                      0.4186, 0.3951, 0.4146, 0.4126, 0.4984, 0.3937, 0.3993,
                      0.4179, 0.4067, 0.3996, 0.4023, 0.5235, 0.3943, 0.3968,
                      0.3972, 0.4011, 0.3906, 0.4091, 0.4172, 0.3970, 0.3950,
                      0.4108, 0.3939, 0.4015, 0.3982, 0.4182, 0.5028, 0.4046,
                      0.4170, 0.4069, 0.9479, 0.4855, 0.3976, 0.4038, 0.4004,
                      0.9455, 0.4150, 0.4124, 0.5087, 0.3948, 0.4217, 0.4286,
                      0.3953, 0.4916, 0.3888, 0.4144, 0.3829, 0.4426, 0.9453,
                      0.5495, 0.4024, 0.3988, 0.3879, 0.3974, 0.4049, 0.4272,
                      0.3934, 0.9451, 0.3829, 0.3944, 0.4025, 0.4190, 0.4197,
                      0.4012, 0.3950, 0.3959, 0.4030, 0.3879, 0.3995, 0.4045,
                      0.3942, 0.5333, 0.5253, 0.3958, 0.3953, 0.4181, 0.4185,
                      0.9486, 0.3954, 0.3978, 0.3864, 0.3989, 0.3838, 0.3944,
                      0.4037, 0.3922, 0.3923, 0.3963, 0.4192, 0.3981, 0.4889,
                      0.3915, 0.4166, 0.4042, 0.3862, 0.4060, 0.3928, 0.3908,
                      0.3989, 0.4079, 0.4001, 0.3933, 0.3948, 0.5006, 0.3905,
                      0.5644, 0.4037, 0.3927, 0.4921]),
       size=(753935, 8285), nnz=207, layout=torch.sparse_coo,
       grad_fn=<SparseCooTensorWithDimsAndTensorsBackward0>) 32
 ---------------------------------------------------------------
node label: 2
6
num_high 8 len(mask) 9
mask_without_small tensor([0.7611, 0.7428, 0.7522, 0.7519, 0.6155, 0.7134, 0.8850, 0.6680, 0.7716],
       grad_fn=<IndexBackward0>)
pred_loss tensor([10.5926], grad_fn=<MulBackward0>)
size_loss tensor(-7.4292, grad_fn=<MulBackward0>)
size_num_loss 0.8
loss: tensor([5.1890], grad_fn=<AddBackward0>)
r_exp.py:102: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  hor_graph, ver_graph = torch.tensor(sub_hor_graph, dtype=torch.float), torch.tensor(sub_ver_graph, dtype=torch.float)
wandb: WARNING Calling wandb.login() after wandb.init() has no effect.
0
epoch:  0 ; loss:  5.189043998718262 ; pred:  tensor([0.3554, 0.1763, 0.3467, 0.1216], grad_fn=<SoftmaxBackward0>)
num_high 8 len(mask) 9
mask_without_small tensor([0.7788, 0.7233, 0.7704, 0.7700, 0.5916, 0.6926, 0.8948, 0.6455, 0.7888],
       grad_fn=<IndexBackward0>)
pred_loss tensor([10.5540], grad_fn=<MulBackward0>)
size_loss tensor(-8.8853, grad_fn=<MulBackward0>)
size_num_loss 0.8
loss: tensor([3.6889], grad_fn=<AddBackward0>)
1
epoch:  1 ; loss:  3.688878297805786 ; pred:  tensor([0.3587, 0.1725, 0.3481, 0.1207], grad_fn=<SoftmaxBackward0>)
num_high 7 len(mask) 9
mask_without_small tensor([0.7953, 0.7077, 0.7873, 0.7871, 0.5673, 0.6710, 0.9038, 0.6223, 0.8049],
       grad_fn=<IndexBackward0>)
pred_loss tensor([10.5197], grad_fn=<MulBackward0>)
size_loss tensor(-10.4911, grad_fn=<MulBackward0>)
size_num_loss 0.7000000000000001
loss: tensor([1.9418], grad_fn=<AddBackward0>)
2
epoch:  2 ; loss:  1.9418118000030518 ; pred:  tensor([0.3618, 0.1689, 0.3492, 0.1200], grad_fn=<SoftmaxBackward0>)
num_high 7 len(mask) 9
mask_without_small tensor([0.8110, 0.6896, 0.8033, 0.8033, 0.5426, 0.6486, 0.9119, 0.5984, 0.8201],
       grad_fn=<IndexBackward0>)
pred_loss tensor([10.4884], grad_fn=<MulBackward0>)
size_loss tensor(-12.1890, grad_fn=<MulBackward0>)
size_num_loss 0.7000000000000001
loss: tensor([0.2041], grad_fn=<AddBackward0>)
3
epoch:  3 ; loss:  0.2041085958480835 ; pred:  tensor([0.3649, 0.1655, 0.3503, 0.1193], grad_fn=<SoftmaxBackward0>)
num_high 7 len(mask) 9
mask_without_small tensor([0.8259, 0.6700, 0.8185, 0.8187, 0.5177, 0.6254, 0.9192, 0.5740, 0.8345],
       grad_fn=<IndexBackward0>)
pred_loss tensor([10.4597], grad_fn=<MulBackward0>)
size_loss tensor(-13.9394, grad_fn=<MulBackward0>)
size_num_loss 0.7000000000000001
loss: tensor([-1.5850], grad_fn=<AddBackward0>)
4
epoch:  4 ; loss:  -1.5849967002868652 ; pred:  tensor([0.3678, 0.1622, 0.3514, 0.1186], grad_fn=<SoftmaxBackward0>)
num_high 6 len(mask) 9
mask_without_small tensor([0.8398, 0.6490, 0.8329, 0.8331, 0.6015, 0.9258, 0.5492, 0.8479],
       grad_fn=<IndexBackward0>)
pred_loss tensor([10.4331], grad_fn=<MulBackward0>)
size_loss tensor(-1384.2772, grad_fn=<MulBackward0>)
size_num_loss 0.6000000000000001
loss: tensor([-1372.6573], grad_fn=<AddBackward0>)
5
epoch:  5 ; loss:  -1372.6573486328125 ; pred:  tensor([0.3707, 0.1590, 0.3523, 0.1180], grad_fn=<SoftmaxBackward0>)
num_high 6 len(mask) 9
mask_without_small tensor([0.8469, 0.6369, 0.8403, 0.8406, 0.5887, 0.9295, 0.5359, 0.8547],
       grad_fn=<IndexBackward0>)
pred_loss tensor([10.4338], grad_fn=<MulBackward0>)
size_loss tensor(-1477.8767, grad_fn=<MulBackward0>)
size_num_loss 0.6000000000000001
loss: tensor([-1466.2610], grad_fn=<AddBackward0>)
6
epoch:  6 ; loss:  -1466.260986328125 ; pred:  tensor([0.3724, 0.1573, 0.3523, 0.1180], grad_fn=<SoftmaxBackward0>)
num_high 5 len(mask) 9
mask_without_small tensor([0.8557, 0.6209, 0.8493, 0.8496, 0.5720, 0.9339, 0.5188, 0.8631],
       grad_fn=<IndexBackward0>)
pred_loss tensor([10.4186], grad_fn=<MulBackward0>)
size_loss tensor(-1598.5793, grad_fn=<MulBackward0>)
size_num_loss 0.5
loss: tensor([-1587.0856], grad_fn=<AddBackward0>)
7
epoch:  7 ; loss:  -1587.0855712890625 ; pred:  tensor([0.3742, 0.1553, 0.3528, 0.1177], grad_fn=<SoftmaxBackward0>)
num_high 5 len(mask) 9
mask_without_small tensor([0.8651, 0.6024, 0.8591, 0.8594, 0.5527, 0.9386, 0.8720],
       grad_fn=<IndexBackward0>)
pred_loss tensor([10.3932], grad_fn=<MulBackward0>)
size_loss tensor(-1502.6458, grad_fn=<MulBackward0>)
size_num_loss 0.5
loss: tensor([-1491.1903], grad_fn=<AddBackward0>)
8
epoch:  8 ; loss:  -1491.1903076171875 ; pred:  tensor([0.3760, 0.1531, 0.3537, 0.1172], grad_fn=<SoftmaxBackward0>)
num_high 5 len(mask) 9
mask_without_small tensor([0.8746, 0.5823, 0.8690, 0.8693, 0.5319, 0.9432, 0.8811],
       grad_fn=<IndexBackward0>)
pred_loss tensor([10.3601], grad_fn=<MulBackward0>)
size_loss tensor(-1638.7683, grad_fn=<MulBackward0>)
size_num_loss 0.5
loss: tensor([-1627.3546], grad_fn=<AddBackward0>)
9
epoch:  9 ; loss:  -1627.3546142578125 ; pred:  tensor([0.3776, 0.1509, 0.3549, 0.1166], grad_fn=<SoftmaxBackward0>)
num_high 5 len(mask) 9
mask_without_small tensor([0.8839, 0.5607, 0.8787, 0.8790, 0.5096, 0.9477, 0.8900],
       grad_fn=<IndexBackward0>)
pred_loss tensor([10.3211], grad_fn=<MulBackward0>)
size_loss tensor(-1782.3440, grad_fn=<MulBackward0>)
size_num_loss 0.5
loss: tensor([-1770.9786], grad_fn=<AddBackward0>)
10
epoch:  10 ; loss:  -1770.9786376953125 ; pred:  tensor([0.3790, 0.1488, 0.3563, 0.1160], grad_fn=<SoftmaxBackward0>)
num_high 5 len(mask) 9
mask_without_small tensor([0.8929, 0.5377, 0.8880, 0.8883, 0.9520, 0.8986],
       grad_fn=<IndexBackward0>)
pred_loss tensor([10.2778], grad_fn=<MulBackward0>)
size_loss tensor(-1514.6923, grad_fn=<MulBackward0>)
size_num_loss 0.5
loss: tensor([-1503.3850], grad_fn=<AddBackward0>)
11
epoch:  11 ; loss:  -1503.385009765625 ; pred:  tensor([0.3801, 0.1467, 0.3578, 0.1154], grad_fn=<SoftmaxBackward0>)
num_high 5 len(mask) 9
mask_without_small tensor([0.9013, 0.5145, 0.8968, 0.8970, 0.9559, 0.9065],
       grad_fn=<IndexBackward0>)
pred_loss tensor([10.2374], grad_fn=<MulBackward0>)
size_loss tensor(-1636.3011, grad_fn=<MulBackward0>)
size_num_loss 0.5
loss: tensor([-1625.0441], grad_fn=<AddBackward0>)
12
epoch:  12 ; loss:  -1625.0440673828125 ; pred:  tensor([0.3812, 0.1448, 0.3592, 0.1147], grad_fn=<SoftmaxBackward0>)
num_high 5 len(mask) 9
mask_without_small tensor([0.9091, 0.9049, 0.9052, 0.9596, 0.9140], grad_fn=<IndexBackward0>)
pred_loss tensor([10.1997], grad_fn=<MulBackward0>)
size_loss tensor(-232.3090, grad_fn=<MulBackward0>)
size_num_loss 0.5
loss: tensor([-221.1046], grad_fn=<AddBackward0>)
13
epoch:  13 ; loss:  -221.1046142578125 ; pred:  tensor([0.3821, 0.1431, 0.3606, 0.1141], grad_fn=<SoftmaxBackward0>)
num_high 5 len(mask) 9
mask_without_small tensor([0.9142, 0.9093, 0.9096, 0.9631, 0.9197], grad_fn=<IndexBackward0>)
pred_loss tensor([10.1734], grad_fn=<MulBackward0>)
size_loss tensor(-227.1878, grad_fn=<MulBackward0>)
size_num_loss 0.5
loss: tensor([-216.0169], grad_fn=<AddBackward0>)
14
epoch:  14 ; loss:  -216.01693725585938 ; pred:  tensor([0.3826, 0.1420, 0.3616, 0.1139], grad_fn=<SoftmaxBackward0>)
num_high 5 len(mask) 9
mask_without_small tensor([0.9174, 0.9111, 0.9115, 0.9664, 0.9241], grad_fn=<IndexBackward0>)
pred_loss tensor([10.1548], grad_fn=<MulBackward0>)
size_loss tensor(-231.4396, grad_fn=<MulBackward0>)
size_num_loss 0.5
loss: tensor([-220.2929], grad_fn=<AddBackward0>)
15
epoch:  15 ; loss:  -220.29293823242188 ; pred:  tensor([0.3827, 0.1412, 0.3622, 0.1138], grad_fn=<SoftmaxBackward0>)
num_high 5 len(mask) 9
mask_without_small tensor([0.9193, 0.9111, 0.9116, 0.9696, 0.9278], grad_fn=<IndexBackward0>)
pred_loss tensor([10.1422], grad_fn=<MulBackward0>)
size_loss tensor(-242.7466, grad_fn=<MulBackward0>)
size_num_loss 0.5
loss: tensor([-231.6167], grad_fn=<AddBackward0>)
16
epoch:  16 ; loss:  -231.61671447753906 ; pred:  tensor([0.3826, 0.1408, 0.3627, 0.1139], grad_fn=<SoftmaxBackward0>)
num_high 5 len(mask) 9
mask_without_small tensor([0.9201, 0.9095, 0.9102, 0.9724, 0.9309], grad_fn=<IndexBackward0>)
pred_loss tensor([10.1347], grad_fn=<MulBackward0>)
size_loss tensor(-259.9587, grad_fn=<MulBackward0>)
size_num_loss 0.5
loss: tensor([-248.8394], grad_fn=<AddBackward0>)
17
epoch:  17 ; loss:  -248.83937072753906 ; pred:  tensor([0.3824, 0.1406, 0.3630, 0.1141], grad_fn=<SoftmaxBackward0>)
num_high 5 len(mask) 9
mask_without_small tensor([0.9201, 0.9065, 0.9074, 0.9751, 0.9338], grad_fn=<IndexBackward0>)
pred_loss tensor([10.1319], grad_fn=<MulBackward0>)
size_loss tensor(-282.6231, grad_fn=<MulBackward0>)
size_num_loss 0.5
loss: tensor([-271.5085], grad_fn=<AddBackward0>)
18
epoch:  18 ; loss:  -271.50848388671875 ; pred:  tensor([0.3819, 0.1405, 0.3631, 0.1145], grad_fn=<SoftmaxBackward0>)
num_high 5 len(mask) 9
mask_without_small tensor([0.9195, 0.9023, 0.9034, 0.9775, 0.9367], grad_fn=<IndexBackward0>)
pred_loss tensor([10.1337], grad_fn=<MulBackward0>)
size_loss tensor(-310.7350, grad_fn=<MulBackward0>)
size_num_loss 0.5
loss: tensor([-299.6198], grad_fn=<AddBackward0>)
19
epoch:  19 ; loss:  -299.6197814941406 ; pred:  tensor([0.3813, 0.1407, 0.3630, 0.1149], grad_fn=<SoftmaxBackward0>)
Finished Training
sel masked ver tensor(indices=tensor([[7650, 5844, 5844, 5844, 5844],
                       [5844,    0,    0, 7650, 5230]]),
       values=tensor([0.9195, 3.6092, 3.6135, 0.9775, 0.9367]),
       size=(7651, 7651), nnz=5, layout=torch.sparse_coo,
       grad_fn=<SparseCooTensorWithDimsAndTensorsBackward0>)
Counter({'fax': 1, 'homepage': 1, 'type': 1, 'publication': 1, 'author': 1})
dict index: {}
node_idx 5844
 node original label [2]
 node predicted label explain 0
 node prediction probability explain tensor([0.4532, 0.0575, 0.4435, 0.0458], grad_fn=<SoftmaxBackward0>)
 node predicted label full 2 most important relations  {'fax': 1, 'homepage': 1, 'type': 1, 'publication': 1, 'author': 1, 'label': 2, 'node_idx': '5844'}
 final masks and lenght tensor(indices=tensor([[ 24220,  24244,  88694, 130119, 196399, 229539, 254394,
                        254394, 328959],
                       [  5844,   5844,      0,      0,   1415,    116,   7650,
                          7674,   5230]]),
       values=tensor([0.9195, 0.3887, 3.6092, 3.6135, 0.3792, 0.3682, 0.9775,
                      0.3829, 0.9367]),
       size=(753935, 8285), nnz=9, layout=torch.sparse_coo,
       grad_fn=<SparseCooTensorWithDimsAndTensorsBackward0>) 5
 ---------------------------------------------------------------
node label: 2
78
num_high 253 len(mask) 253
mask_without_small tensor([0.7634, 0.7563, 0.7465, 0.6927, 0.7428, 0.7089, 0.7303, 0.7021, 0.7177,
        0.7589, 0.7241, 0.7058, 0.7181, 0.7212, 0.7174, 0.7442, 0.7588, 0.7283,
        0.7223, 0.7387, 0.7176, 0.7495, 0.7448, 0.7594, 0.7528, 0.7531, 0.7416,
        0.7537, 0.7270, 0.7318, 0.7266, 0.7458, 0.7062, 0.7156, 0.7271, 0.7600,
        0.7366, 0.7236, 0.7364, 0.7173, 0.7030, 0.7481, 0.7154, 0.7204, 0.7082,
        0.7665, 0.7089, 0.7224, 0.7148, 0.7194, 0.7324, 0.7402, 0.7224, 0.7514,
        0.7166, 0.7180, 0.7058, 0.7317, 0.7299, 0.7427, 0.7293, 0.7621, 0.7099,
        0.7545, 0.7556, 0.7458, 0.7680, 0.7401, 0.7371, 0.7276, 0.7122, 0.7528,
        0.7280, 0.7401, 0.7320, 0.7384, 0.7410, 0.7197, 0.6908, 0.7177, 0.7312,
        0.7251, 0.7070, 0.7207, 0.7403, 0.7401, 0.7505, 0.7320, 0.7439, 0.7226,
        0.7123, 0.7415, 0.6999, 0.7163, 0.7537, 0.7394, 0.6850, 0.7395, 0.7446,
        0.7316, 0.7421, 0.7411, 0.7493, 0.7231, 0.7278, 0.7440, 0.7381, 0.7342,
        0.7357, 0.7527, 0.7310, 0.7257, 0.7048, 0.7293, 0.7205, 0.7393, 0.7436,
        0.7326, 0.7242, 0.7402, 0.7308, 0.7352, 0.7334, 0.7442, 0.7498, 0.7370,
        0.7435, 0.7382, 0.7635, 0.7484, 0.7052, 0.7109, 0.7287, 0.7587, 0.7423,
        0.7410, 0.7505, 0.7314, 0.6983, 0.7469, 0.7244, 0.7487, 0.7189, 0.7420,
        0.7137, 0.7475, 0.7584, 0.7556, 0.7357, 0.7274, 0.7181, 0.7329, 0.7371,
        0.7476, 0.7228, 0.7582, 0.6856, 0.7237, 0.7097, 0.7450, 0.6966, 0.7350,
        0.7315, 0.7250, 0.7360, 0.7181, 0.7341, 0.7115, 0.7021, 0.7540, 0.7530,
        0.7320, 0.7032, 0.7441, 0.7444, 0.7650, 0.7317, 0.7332, 0.7167, 0.7274,
        0.7145, 0.7024, 0.7107, 0.7218, 0.7219, 0.7040, 0.6961, 0.7333, 0.7486,
        0.7212, 0.7432, 0.7433, 0.7609, 0.7146, 0.7475, 0.7251, 0.7100, 0.7373,
        0.7393, 0.7541, 0.7402, 0.7663, 0.7219, 0.7145, 0.7343, 0.7493, 0.7533,
        0.7390, 0.7166, 0.7128, 0.7223, 0.7206, 0.7337, 0.7387, 0.7285, 0.6887,
        0.7240, 0.7495, 0.6988, 0.7566, 0.7364, 0.7222, 0.7488, 0.7596, 0.7310,
        0.7592, 0.7337, 0.7121, 0.7209, 0.7325, 0.7380, 0.7644, 0.7298, 0.7149,
        0.6938, 0.7117, 0.7314, 0.7410, 0.7412, 0.7307, 0.7189, 0.7408, 0.7039,
        0.7020, 0.7044, 0.7400, 0.7184, 0.7349, 0.7022, 0.7291, 0.7434, 0.7317,
        0.7533], grad_fn=<IndexBackward0>)
pred_loss tensor([-0.], grad_fn=<MulBackward0>)
size_loss tensor(-1.7620, grad_fn=<MulBackward0>)
size_num_loss 25.3
loss: tensor([42.6216], grad_fn=<AddBackward0>)
r_exp.py:102: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  hor_graph, ver_graph = torch.tensor(sub_hor_graph, dtype=torch.float), torch.tensor(sub_ver_graph, dtype=torch.float)
wandb: WARNING Calling wandb.login() after wandb.init() has no effect.
0
epoch:  0 ; loss:  42.62157440185547 ; pred:  tensor([2.5298e-13, 6.0589e-10, 1.0000e+00, 2.4002e-10],
       grad_fn=<SoftmaxBackward0>)
num_high 253 len(mask) 253
mask_without_small tensor([0.7448, 0.7373, 0.7271, 0.6710, 0.7232, 0.6879, 0.7102, 0.6808, 0.6970,
        0.7401, 0.7037, 0.6846, 0.6975, 0.7006, 0.6967, 0.7247, 0.7400, 0.7080,
        0.7018, 0.7189, 0.6969, 0.7302, 0.7254, 0.7407, 0.7337, 0.7340, 0.7220,
        0.7347, 0.7067, 0.7117, 0.7063, 0.7264, 0.6850, 0.6948, 0.7069, 0.7413,
        0.7167, 0.7031, 0.7165, 0.6966, 0.6817, 0.7288, 0.6946, 0.6998, 0.6871,
        0.7839, 0.6879, 0.7020, 0.6940, 0.6988, 0.7124, 0.7205, 0.7020, 0.7322,
        0.6959, 0.6973, 0.6847, 0.7116, 0.7098, 0.7231, 0.7092, 0.7435, 0.6888,
        0.7356, 0.7366, 0.7263, 0.7854, 0.7204, 0.7172, 0.7073, 0.6913, 0.7337,
        0.7078, 0.7204, 0.7120, 0.7187, 0.7213, 0.6991, 0.6690, 0.6970, 0.7111,
        0.7047, 0.6859, 0.7001, 0.7206, 0.7204, 0.7313, 0.7119, 0.7244, 0.7021,
        0.6914, 0.7219, 0.6785, 0.6956, 0.7347, 0.7197, 0.6630, 0.7198, 0.7251,
        0.7115, 0.7225, 0.7215, 0.7300, 0.7027, 0.7076, 0.7245, 0.7183, 0.7142,
        0.7158, 0.7336, 0.7109, 0.7054, 0.6836, 0.7091, 0.6999, 0.7196, 0.7240,
        0.7126, 0.7038, 0.7205, 0.7107, 0.7153, 0.7134, 0.7247, 0.7305, 0.7171,
        0.7239, 0.7184, 0.7449, 0.7291, 0.6840, 0.6899, 0.7085, 0.7399, 0.7228,
        0.7214, 0.7314, 0.7113, 0.6769, 0.7276, 0.7040, 0.7295, 0.6982, 0.7224,
        0.6929, 0.7281, 0.7396, 0.7367, 0.7158, 0.7071, 0.6974, 0.7129, 0.7173,
        0.7283, 0.7024, 0.7394, 0.6636, 0.7033, 0.6886, 0.7256, 0.6750, 0.7151,
        0.7114, 0.7046, 0.7162, 0.6974, 0.7141, 0.6906, 0.6808, 0.7350, 0.7339,
        0.7119, 0.6819, 0.7246, 0.7249, 0.7825, 0.7116, 0.7131, 0.6960, 0.7071,
        0.6936, 0.6810, 0.6898, 0.7013, 0.7014, 0.6828, 0.6745, 0.7133, 0.7293,
        0.7007, 0.7236, 0.7237, 0.7423, 0.6938, 0.7282, 0.7048, 0.6890, 0.7174,
        0.7196, 0.7350, 0.7205, 0.7838, 0.7013, 0.6936, 0.7143, 0.7301, 0.7342,
        0.7193, 0.6958, 0.6919, 0.7018, 0.7000, 0.7138, 0.7189, 0.7082, 0.6668,
        0.7036, 0.7303, 0.6774, 0.7377, 0.7166, 0.7017, 0.7295, 0.7408, 0.7109,
        0.7404, 0.7138, 0.6912, 0.7004, 0.7125, 0.7182, 0.7459, 0.7096, 0.6941,
        0.6721, 0.6908, 0.7113, 0.7213, 0.7216, 0.7106, 0.6983, 0.7211, 0.6827,
        0.6807, 0.6832, 0.7203, 0.6978, 0.7149, 0.6809, 0.7090, 0.7238, 0.7117,
        0.7342], grad_fn=<IndexBackward0>)
pred_loss tensor([-0.], grad_fn=<MulBackward0>)
size_loss tensor(-2.0020, grad_fn=<MulBackward0>)
size_num_loss 25.3
loss: tensor([41.9066], grad_fn=<AddBackward0>)
1
epoch:  1 ; loss:  41.906639099121094 ; pred:  tensor([9.8374e-13, 1.8450e-09, 1.0000e+00, 7.4196e-10],
       grad_fn=<SoftmaxBackward0>)
num_high 253 len(mask) 253
mask_without_small tensor([0.7289, 0.7176, 0.7069, 0.6486, 0.7028, 0.6661, 0.6892, 0.6587, 0.6755,
        0.7206, 0.6825, 0.6627, 0.6760, 0.6793, 0.6752, 0.7043, 0.7205, 0.6870,
        0.6805, 0.6983, 0.6754, 0.7102, 0.7050, 0.7212, 0.7138, 0.7142, 0.7015,
        0.7149, 0.6856, 0.6908, 0.6852, 0.7061, 0.6631, 0.6732, 0.6857, 0.7219,
        0.6960, 0.6819, 0.6958, 0.6751, 0.6596, 0.7086, 0.6730, 0.6785, 0.6653,
        0.7969, 0.6661, 0.6807, 0.6724, 0.6774, 0.6915, 0.6999, 0.6807, 0.7122,
        0.6743, 0.6758, 0.6627, 0.6907, 0.6888, 0.7027, 0.6882, 0.7247, 0.6670,
        0.7158, 0.7169, 0.7061, 0.7987, 0.6999, 0.6966, 0.6862, 0.6696, 0.7138,
        0.6867, 0.6999, 0.6911, 0.6981, 0.7008, 0.6777, 0.6466, 0.6756, 0.6902,
        0.6835, 0.6640, 0.6788, 0.7001, 0.6999, 0.7113, 0.6910, 0.7040, 0.6808,
        0.6697, 0.7014, 0.6563, 0.6741, 0.7149, 0.6991, 0.6404, 0.6992, 0.7047,
        0.6906, 0.7021, 0.7010, 0.7100, 0.6814, 0.6865, 0.7042, 0.6977, 0.6934,
        0.6950, 0.7137, 0.6900, 0.6842, 0.6616, 0.6881, 0.6785, 0.6990, 0.7037,
        0.6917, 0.6826, 0.7000, 0.6898, 0.6946, 0.6925, 0.7044, 0.7105, 0.6964,
        0.7035, 0.6978, 0.7295, 0.7089, 0.6621, 0.6681, 0.6874, 0.7204, 0.7023,
        0.7009, 0.7113, 0.6904, 0.6547, 0.7074, 0.6828, 0.7093, 0.6768, 0.7020,
        0.6712, 0.7080, 0.7201, 0.7170, 0.6951, 0.6860, 0.6759, 0.6920, 0.6966,
        0.7081, 0.6811, 0.7198, 0.6410, 0.6820, 0.6668, 0.7053, 0.6528, 0.6943,
        0.6905, 0.6834, 0.6954, 0.6759, 0.6933, 0.6688, 0.6588, 0.7152, 0.7140,
        0.6910, 0.6599, 0.7042, 0.7046, 0.7950, 0.6907, 0.6923, 0.6745, 0.6861,
        0.6720, 0.6590, 0.6680, 0.6800, 0.6801, 0.6608, 0.6522, 0.6924, 0.7091,
        0.6793, 0.7033, 0.7034, 0.7231, 0.6722, 0.7080, 0.6836, 0.6672, 0.6968,
        0.6991, 0.7152, 0.6999, 0.7967, 0.6800, 0.6720, 0.6935, 0.7100, 0.7143,
        0.6987, 0.6743, 0.6703, 0.6805, 0.6786, 0.6929, 0.6983, 0.6872, 0.6443,
        0.6824, 0.7102, 0.6552, 0.7180, 0.6959, 0.6804, 0.7094, 0.7214, 0.6899,
        0.7210, 0.6929, 0.6695, 0.6790, 0.6916, 0.6976, 0.7550, 0.6886, 0.6725,
        0.6498, 0.6691, 0.6904, 0.7008, 0.7011, 0.6897, 0.6768, 0.7006, 0.6606,
        0.6586, 0.6612, 0.6997, 0.6763, 0.6942, 0.6588, 0.6879, 0.7035, 0.6908,
        0.7143], grad_fn=<IndexBackward0>)
pred_loss tensor([-0.], grad_fn=<MulBackward0>)
size_loss tensor(-2.3160, grad_fn=<MulBackward0>)
size_num_loss 25.3
loss: tensor([41.0981], grad_fn=<AddBackward0>)
2
epoch:  2 ; loss:  41.09809112548828 ; pred:  tensor([3.8798e-12, 5.6257e-09, 1.0000e+00, 2.3090e-09],
       grad_fn=<SoftmaxBackward0>)
num_high 250 len(mask) 253
mask_without_small tensor([0.7348, 0.6972, 0.6859, 0.6256, 0.6816, 0.6436, 0.6675, 0.6360, 0.6534,
        0.7004, 0.6606, 0.6401, 0.6538, 0.6572, 0.6530, 0.6832, 0.7003, 0.6652,
        0.6585, 0.6770, 0.6532, 0.6893, 0.6840, 0.7011, 0.6932, 0.6935, 0.6803,
        0.6942, 0.6638, 0.6692, 0.6634, 0.6851, 0.6405, 0.6510, 0.6639, 0.7019,
        0.6746, 0.6599, 0.6743, 0.6529, 0.6370, 0.6877, 0.6508, 0.6564, 0.6428,
        0.8106, 0.6436, 0.6587, 0.6501, 0.6553, 0.6699, 0.6786, 0.6587, 0.6915,
        0.6521, 0.6537, 0.6401, 0.6691, 0.6671, 0.6815, 0.6664, 0.7064, 0.6446,
        0.6952, 0.6964, 0.6850, 0.8125, 0.6786, 0.6751, 0.6644, 0.6472, 0.6931,
        0.6649, 0.6786, 0.6695, 0.6767, 0.6796, 0.6556, 0.6235, 0.6534, 0.6686,
        0.6616, 0.6414, 0.6567, 0.6788, 0.6786, 0.6905, 0.6694, 0.6829, 0.6588,
        0.6474, 0.6802, 0.6336, 0.6518, 0.6942, 0.6778, 0.6172, 0.6779, 0.6836,
        0.6689, 0.6809, 0.6798, 0.6891, 0.6594, 0.6647, 0.6830, 0.6763, 0.6719,
        0.6736, 0.6930, 0.6683, 0.6623, 0.6390, 0.6663, 0.6564, 0.6777, 0.6825,
        0.6701, 0.6606, 0.6787, 0.6681, 0.6731, 0.6710, 0.6833, 0.6896, 0.6750,
        0.6824, 0.6764, 0.7368, 0.6880, 0.6395, 0.6457, 0.6657, 0.7001, 0.6811,
        0.6796, 0.6905, 0.6687, 0.6319, 0.6864, 0.6609, 0.6884, 0.6547, 0.6808,
        0.6489, 0.6870, 0.6998, 0.6965, 0.6736, 0.6642, 0.6538, 0.6704, 0.6752,
        0.6872, 0.6591, 0.6995, 0.6178, 0.6601, 0.6444, 0.6842, 0.6299, 0.6728,
        0.6688, 0.6615, 0.6740, 0.6538, 0.6718, 0.6464, 0.6361, 0.6946, 0.6933,
        0.6694, 0.6372, 0.6831, 0.6835, 0.8086, 0.6691, 0.6707, 0.6523, 0.6642,
        0.6497, 0.6363, 0.6456, 0.6580, 0.6580, 0.6381, 0.6294, 0.6709, 0.6882,
        0.6573, 0.6821, 0.6822, 0.7034, 0.6499, 0.6871, 0.6617, 0.6448, 0.6754,
        0.6777, 0.6946, 0.6786, 0.8104, 0.6580, 0.6497, 0.6720, 0.6891, 0.6937,
        0.6774, 0.6521, 0.6479, 0.6585, 0.6566, 0.6714, 0.6770, 0.6654, 0.6212,
        0.6605, 0.6893, 0.6324, 0.6976, 0.6744, 0.6584, 0.6885, 0.7013, 0.6683,
        0.7008, 0.6714, 0.6471, 0.6570, 0.6700, 0.6762, 0.7668, 0.6669, 0.6503,
        0.6268, 0.6467, 0.6687, 0.6796, 0.6798, 0.6680, 0.6547, 0.6794, 0.6380,
        0.6359, 0.6386, 0.6784, 0.6542, 0.6727, 0.6361, 0.6662, 0.6823, 0.6691,
        0.6937], grad_fn=<IndexBackward0>)
pred_loss tensor([-0.], grad_fn=<MulBackward0>)
size_loss tensor(-2.7265, grad_fn=<MulBackward0>)
size_num_loss 25.0
loss: tensor([39.8789], grad_fn=<AddBackward0>)
3
epoch:  3 ; loss:  39.87891387939453 ; pred:  tensor([1.5434e-11, 1.7091e-08, 1.0000e+00, 7.1802e-09],
       grad_fn=<SoftmaxBackward0>)
num_high 220 len(mask) 253
mask_without_small tensor([0.7466, 0.6759, 0.6642, 0.6023, 0.6597, 0.6206, 0.6452, 0.6129, 0.6306,
        0.6793, 0.6380, 0.6171, 0.6311, 0.6346, 0.6303, 0.6614, 0.6792, 0.6428,
        0.6359, 0.6549, 0.6305, 0.6677, 0.6622, 0.6801, 0.6717, 0.6720, 0.6584,
        0.6728, 0.6413, 0.6469, 0.6409, 0.6633, 0.6175, 0.6282, 0.6415, 0.6810,
        0.6525, 0.6374, 0.6522, 0.6302, 0.6138, 0.6660, 0.6280, 0.6338, 0.6198,
        0.8243, 0.6206, 0.6361, 0.6273, 0.6326, 0.6476, 0.6567, 0.6361, 0.6699,
        0.6294, 0.6310, 0.6171, 0.6468, 0.6448, 0.6597, 0.6441, 0.6904, 0.6217,
        0.6738, 0.6750, 0.6633, 0.8263, 0.6566, 0.6531, 0.6420, 0.6244, 0.6717,
        0.6426, 0.6566, 0.6472, 0.6547, 0.6577, 0.6329, 0.6001, 0.6307, 0.6463,
        0.6391, 0.6184, 0.6341, 0.6569, 0.6566, 0.6689, 0.6471, 0.6610, 0.6362,
        0.6245, 0.6582, 0.6104, 0.6291, 0.6728, 0.6558, 0.5937, 0.6559, 0.6618,
        0.6466, 0.6590, 0.6578, 0.6675, 0.6369, 0.6423, 0.6612, 0.6542, 0.6497,
        0.6514, 0.6716, 0.6460, 0.6399, 0.6160, 0.6440, 0.6338, 0.6557, 0.6607,
        0.6479, 0.6381, 0.6567, 0.6458, 0.6509, 0.6487, 0.6614, 0.6680, 0.6529,
        0.6606, 0.6544, 0.7487, 0.6664, 0.6164, 0.6228, 0.6433, 0.6790, 0.6593,
        0.6577, 0.6690, 0.6464, 0.6086, 0.6647, 0.6384, 0.6668, 0.6320, 0.6589,
        0.6261, 0.6653, 0.6787, 0.6751, 0.6515, 0.6418, 0.6310, 0.6482, 0.6531,
        0.6655, 0.6365, 0.6784, 0.5943, 0.6375, 0.6214, 0.6624, 0.6066, 0.6507,
        0.6466, 0.6390, 0.6519, 0.6311, 0.6496, 0.6236, 0.6129, 0.6732, 0.6719,
        0.6471, 0.6141, 0.6613, 0.6617, 0.8223, 0.6468, 0.6485, 0.6295, 0.6418,
        0.6269, 0.6131, 0.6227, 0.6354, 0.6355, 0.6150, 0.6061, 0.6486, 0.6666,
        0.6347, 0.6602, 0.6604, 0.6829, 0.6271, 0.6654, 0.6392, 0.6219, 0.6533,
        0.6557, 0.6732, 0.6567, 0.8241, 0.6354, 0.6269, 0.6498, 0.6675, 0.6722,
        0.6553, 0.6294, 0.6251, 0.6359, 0.6339, 0.6492, 0.6550, 0.6430, 0.5977,
        0.6379, 0.6677, 0.6091, 0.6763, 0.6523, 0.6358, 0.6668, 0.6803, 0.6460,
        0.6798, 0.6492, 0.6243, 0.6343, 0.6478, 0.6541, 0.7804, 0.6446, 0.6275,
        0.6035, 0.6238, 0.6464, 0.6576, 0.6579, 0.6457, 0.6320, 0.6574, 0.6149,
        0.6127, 0.6155, 0.6565, 0.6315, 0.6505, 0.6130, 0.6438, 0.6605, 0.6469,
        0.6722], grad_fn=<IndexBackward0>)
pred_loss tensor([-0.], grad_fn=<MulBackward0>)
size_loss tensor(-3.1974, grad_fn=<MulBackward0>)
size_num_loss 22.0
loss: tensor([35.8835], grad_fn=<AddBackward0>)
4
epoch:  4 ; loss:  35.8835334777832 ; pred:  tensor([6.1516e-11, 5.1681e-08, 1.0000e+00, 2.2242e-08],
       grad_fn=<SoftmaxBackward0>)
num_high 141 len(mask) 253
mask_without_small tensor([0.7602, 0.6538, 0.6418, 0.5787, 0.6373, 0.5973, 0.6224, 0.5894, 0.6075,
        0.6574, 0.6151, 0.5937, 0.6080, 0.6116, 0.6072, 0.6390, 0.6573, 0.6200,
        0.6129, 0.6324, 0.6074, 0.6454, 0.6398, 0.6582, 0.6495, 0.6499, 0.6359,
        0.6507, 0.6185, 0.6241, 0.6180, 0.6410, 0.5941, 0.6050, 0.6186, 0.6593,
        0.6299, 0.6144, 0.6296, 0.6071, 0.5904, 0.6437, 0.6048, 0.6107, 0.5965,
        0.8376, 0.5973, 0.6131, 0.6041, 0.6095, 0.6249, 0.6341, 0.6131, 0.6477,
        0.6062, 0.6079, 0.5937, 0.6240, 0.6220, 0.6372, 0.6212, 0.6859, 0.5984,
        0.6517, 0.6529, 0.6409, 0.8395, 0.6341, 0.6305, 0.6192, 0.6011, 0.6495,
        0.6197, 0.6341, 0.6245, 0.6321, 0.6351, 0.6099, 0.5765, 0.6076, 0.6235,
        0.6162, 0.5951, 0.6110, 0.6344, 0.6341, 0.6467, 0.6244, 0.6386, 0.6132,
        0.6012, 0.6357, 0.5869, 0.6059, 0.6507, 0.6333, 0.5699, 0.6334, 0.6394,
        0.6239, 0.6365, 0.6353, 0.6452, 0.6139, 0.6194, 0.6388, 0.6316, 0.6270,
        0.6288, 0.6494, 0.6233, 0.6170, 0.5926, 0.6212, 0.6108, 0.6331, 0.6382,
        0.6252, 0.6152, 0.6342, 0.6230, 0.6283, 0.6260, 0.6390, 0.6458, 0.6303,
        0.6381, 0.6318, 0.7624, 0.6441, 0.5930, 0.5995, 0.6204, 0.6571, 0.6368,
        0.6352, 0.6467, 0.6237, 0.5851, 0.6423, 0.6155, 0.6445, 0.6089, 0.6364,
        0.6029, 0.6430, 0.6567, 0.6531, 0.6289, 0.6189, 0.6079, 0.6254, 0.6305,
        0.6432, 0.6136, 0.6564, 0.5706, 0.6146, 0.5982, 0.6400, 0.5831, 0.6280,
        0.6238, 0.6161, 0.6292, 0.6080, 0.6269, 0.6003, 0.5895, 0.6510, 0.6497,
        0.6244, 0.5907, 0.6389, 0.6393, 0.8356, 0.6240, 0.6258, 0.6064, 0.6190,
        0.6037, 0.5897, 0.5994, 0.6124, 0.6124, 0.5916, 0.5825, 0.6259, 0.6443,
        0.6117, 0.6378, 0.6379, 0.6616, 0.6040, 0.6431, 0.6163, 0.5986, 0.6307,
        0.6332, 0.6511, 0.6341, 0.8374, 0.6124, 0.6037, 0.6271, 0.6452, 0.6501,
        0.6328, 0.6062, 0.6018, 0.6129, 0.6109, 0.6265, 0.6324, 0.6202, 0.5741,
        0.6150, 0.6455, 0.5856, 0.6543, 0.6297, 0.6128, 0.6445, 0.6585, 0.6232,
        0.6579, 0.6265, 0.6010, 0.6113, 0.6250, 0.6315, 0.7946, 0.6218, 0.6043,
        0.5799, 0.6006, 0.6236, 0.6351, 0.6354, 0.6229, 0.6089, 0.6349, 0.5915,
        0.5893, 0.5921, 0.6339, 0.6084, 0.6278, 0.5896, 0.6210, 0.6380, 0.6241,
        0.6501], grad_fn=<IndexBackward0>)
pred_loss tensor([2.3842e-06], grad_fn=<MulBackward0>)
size_loss tensor(-3.7102, grad_fn=<MulBackward0>)
size_num_loss 14.100000000000001
loss: tensor([26.9337], grad_fn=<AddBackward0>)
5
epoch:  5 ; loss:  26.933696746826172 ; pred:  tensor([2.4375e-10, 1.5449e-07, 1.0000e+00, 6.8165e-08],
       grad_fn=<SoftmaxBackward0>)
num_high 40 len(mask) 253
mask_without_small tensor([0.7747, 0.6311, 0.6190, 0.5550, 0.6144, 0.5739, 0.5993, 0.5659, 0.5842,
        0.6347, 0.5919, 0.5702, 0.5847, 0.5883, 0.5839, 0.6161, 0.6346, 0.5968,
        0.5896, 0.6094, 0.5841, 0.6226, 0.6169, 0.6356, 0.6268, 0.6271, 0.6130,
        0.6279, 0.5953, 0.6011, 0.5949, 0.6181, 0.5706, 0.5817, 0.5955, 0.6367,
        0.6069, 0.5912, 0.6066, 0.5837, 0.5669, 0.6209, 0.5815, 0.5874, 0.5730,
        0.8503, 0.5739, 0.5898, 0.5808, 0.5862, 0.6018, 0.6112, 0.5898, 0.6250,
        0.5829, 0.5846, 0.5702, 0.6009, 0.5988, 0.6143, 0.5981, 0.6951, 0.5749,
        0.6289, 0.6302, 0.6181, 0.8521, 0.6111, 0.6075, 0.5960, 0.5777, 0.6267,
        0.5965, 0.6112, 0.6014, 0.6091, 0.6122, 0.5866, 0.5528, 0.5842, 0.6004,
        0.5930, 0.5716, 0.5878, 0.6114, 0.6112, 0.6239, 0.6013, 0.6157, 0.5900,
        0.5778, 0.6128, 0.5633, 0.5826, 0.6279, 0.6103, 0.5462, 0.6104, 0.6166,
        0.6008, 0.6136, 0.6124, 0.6224, 0.5906, 0.5963, 0.6159, 0.6087, 0.6039,
        0.6057, 0.6266, 0.6001, 0.5938, 0.5691, 0.5980, 0.5875, 0.6102, 0.6154,
        0.6021, 0.5919, 0.6112, 0.5999, 0.6052, 0.6030, 0.6161, 0.6230, 0.6073,
        0.6152, 0.6088, 0.7770, 0.6213, 0.5695, 0.5761, 0.5973, 0.6344, 0.6139,
        0.6122, 0.6239, 0.6006, 0.5615, 0.6195, 0.5922, 0.6217, 0.5856, 0.6135,
        0.5795, 0.6202, 0.6340, 0.6303, 0.6058, 0.5957, 0.5846, 0.6024, 0.6075,
        0.6204, 0.5903, 0.6337, 0.5469, 0.5913, 0.5747, 0.6171, 0.5595, 0.6050,
        0.6007, 0.5929, 0.6062, 0.5847, 0.6039, 0.5769, 0.5659, 0.6283, 0.6270,
        0.6013, 0.5671, 0.6160, 0.6164, 0.8484, 0.6009, 0.6027, 0.5831, 0.5958,
        0.5804, 0.5662, 0.5760, 0.5891, 0.5892, 0.5681, 0.5589, 0.6029, 0.6215,
        0.5884, 0.6149, 0.6150, 0.6396, 0.5806, 0.6202, 0.5930, 0.5751, 0.6077,
        0.6102, 0.6283, 0.6112, 0.8501, 0.5891, 0.5804, 0.6041, 0.6224, 0.6273,
        0.6098, 0.5829, 0.5785, 0.5897, 0.5876, 0.6034, 0.6094, 0.5970, 0.5504,
        0.5918, 0.6227, 0.5621, 0.6315, 0.6067, 0.5896, 0.6217, 0.6359, 0.6001,
        0.6352, 0.6034, 0.5776, 0.5880, 0.6019, 0.6086, 0.8089, 0.5987, 0.5809,
        0.5563, 0.5772, 0.6005, 0.6122, 0.6125, 0.5998, 0.5856, 0.6120, 0.5680,
        0.5658, 0.5686, 0.6110, 0.5850, 0.6048, 0.5660, 0.5979, 0.6151, 0.6010,
        0.6273], grad_fn=<IndexBackward0>)
pred_loss tensor([7.1526e-06], grad_fn=<MulBackward0>)
size_loss tensor(-4.2529, grad_fn=<MulBackward0>)
size_num_loss 4.0
loss: tensor([15.7446], grad_fn=<AddBackward0>)
6
epoch:  6 ; loss:  15.744629859924316 ; pred:  tensor([9.5444e-10, 4.5445e-07, 1.0000e+00, 2.0587e-07],
       grad_fn=<SoftmaxBackward0>)
num_high 8 len(mask) 253
mask_without_small tensor([0.7896, 0.6078, 0.5958, 0.5316, 0.5912, 0.5505, 0.5760, 0.5425, 0.5609,
        0.6114, 0.5686, 0.5468, 0.5614, 0.5650, 0.5605, 0.5929, 0.6113, 0.5735,
        0.5663, 0.5862, 0.5607, 0.5995, 0.5937, 0.6123, 0.6036, 0.6039, 0.5898,
        0.6047, 0.5720, 0.5778, 0.5716, 0.5950, 0.5472, 0.5583, 0.5722, 0.6135,
        0.5837, 0.5679, 0.5834, 0.5604, 0.5435, 0.5978, 0.5581, 0.5641, 0.5496,
        0.8621, 0.5505, 0.5665, 0.5574, 0.5629, 0.5786, 0.5880, 0.5665, 0.6018,
        0.5595, 0.5612, 0.5468, 0.5777, 0.5756, 0.5911, 0.5748, 0.7081, 0.5515,
        0.6057, 0.6069, 0.5949, 0.8638, 0.5880, 0.5842, 0.5727, 0.5543, 0.6035,
        0.5733, 0.5880, 0.5781, 0.5859, 0.5890, 0.5632, 0.5294, 0.5609, 0.5771,
        0.5697, 0.5482, 0.5644, 0.5882, 0.5880, 0.6007, 0.5780, 0.5926, 0.5667,
        0.5545, 0.5896, 0.5399, 0.5592, 0.6047, 0.5871, 0.5228, 0.5872, 0.5934,
        0.5775, 0.5904, 0.5892, 0.5992, 0.5673, 0.5730, 0.5927, 0.5855, 0.5807,
        0.5825, 0.6034, 0.5769, 0.5705, 0.5457, 0.5747, 0.5641, 0.5870, 0.5922,
        0.5789, 0.5686, 0.5881, 0.5767, 0.5820, 0.5797, 0.5930, 0.5998, 0.5841,
        0.5921, 0.5856, 0.7918, 0.5981, 0.5461, 0.5527, 0.5740, 0.6111, 0.5907,
        0.5891, 0.6008, 0.5773, 0.5381, 0.5963, 0.5689, 0.5985, 0.5623, 0.5903,
        0.5561, 0.5970, 0.6107, 0.6071, 0.5826, 0.5724, 0.5613, 0.5791, 0.5843,
        0.5972, 0.5670, 0.6104, 0.5234, 0.5680, 0.5513, 0.5940, 0.5360, 0.5818,
        0.5774, 0.5696, 0.5830, 0.5613, 0.5806, 0.5535, 0.5425, 0.6051, 0.6038,
        0.5780, 0.5437, 0.5928, 0.5932, 0.8603, 0.5777, 0.5795, 0.5597, 0.5725,
        0.5570, 0.5428, 0.5526, 0.5658, 0.5659, 0.5447, 0.5355, 0.5796, 0.5983,
        0.5651, 0.5917, 0.5918, 0.6170, 0.5572, 0.5971, 0.5697, 0.5517, 0.5845,
        0.5870, 0.6051, 0.5880, 0.8619, 0.5658, 0.5570, 0.5808, 0.5993, 0.6041,
        0.5866, 0.5595, 0.5551, 0.5664, 0.5643, 0.5802, 0.5862, 0.5738, 0.5269,
        0.5684, 0.5995, 0.5386, 0.6082, 0.5835, 0.5663, 0.5986, 0.6126, 0.5768,
        0.6119, 0.5802, 0.5542, 0.5647, 0.5787, 0.5854, 0.8229, 0.5754, 0.5575,
        0.5328, 0.5538, 0.5773, 0.5890, 0.5893, 0.5765, 0.5623, 0.5888, 0.5446,
        0.5423, 0.5452, 0.5878, 0.5617, 0.5816, 0.5426, 0.5746, 0.5920, 0.5777,
        0.6041], grad_fn=<IndexBackward0>)
pred_loss tensor([1.9074e-05], grad_fn=<MulBackward0>)
size_loss tensor(-4.8094, grad_fn=<MulBackward0>)
size_num_loss 0.8
loss: tensor([11.4357], grad_fn=<AddBackward0>)
7
epoch:  7 ; loss:  11.43570327758789 ; pred:  tensor([3.6393e-09, 1.2987e-06, 1.0000e+00, 6.0600e-07],
       grad_fn=<SoftmaxBackward0>)
num_high 8 len(mask) 253
mask_without_small tensor([0.8044, 0.5842, 0.5725, 0.5085, 0.5680, 0.5273, 0.5528, 0.5193, 0.5377,
        0.5877, 0.5454, 0.5237, 0.5382, 0.5418, 0.5373, 0.5697, 0.5875, 0.5503,
        0.5431, 0.5630, 0.5375, 0.5761, 0.5705, 0.5885, 0.5801, 0.5805, 0.5665,
        0.5812, 0.5488, 0.5546, 0.5484, 0.5717, 0.5241, 0.5351, 0.5490, 0.5898,
        0.5604, 0.5447, 0.5602, 0.5372, 0.5203, 0.5745, 0.5349, 0.5409, 0.5264,
        0.8729, 0.5273, 0.5433, 0.5342, 0.5397, 0.5554, 0.5648, 0.5433, 0.5784,
        0.5363, 0.5380, 0.5237, 0.5545, 0.5524, 0.5679, 0.5516, 0.7229, 0.5284,
        0.5822, 0.5834, 0.5716, 0.8745, 0.5647, 0.5610, 0.5495, 0.5312, 0.5801,
        0.5501, 0.5647, 0.5549, 0.5627, 0.5658, 0.5400, 0.5063, 0.5377, 0.5539,
        0.5465, 0.5250, 0.5412, 0.5650, 0.5648, 0.5774, 0.5548, 0.5693, 0.5435,
        0.5313, 0.5664, 0.5168, 0.5360, 0.5812, 0.5639, 0.5640, 0.5701, 0.5543,
        0.5672, 0.5660, 0.5759, 0.5441, 0.5498, 0.5695, 0.5622, 0.5575, 0.5593,
        0.5800, 0.5537, 0.5473, 0.5225, 0.5515, 0.5409, 0.5638, 0.5689, 0.5556,
        0.5454, 0.5648, 0.5534, 0.5588, 0.5565, 0.5697, 0.5765, 0.5609, 0.5688,
        0.5624, 0.8065, 0.5748, 0.5229, 0.5296, 0.5508, 0.5873, 0.5675, 0.5658,
        0.5774, 0.5541, 0.5150, 0.5730, 0.5457, 0.5752, 0.5391, 0.5671, 0.5329,
        0.5737, 0.5870, 0.5835, 0.5594, 0.5492, 0.5381, 0.5559, 0.5611, 0.5739,
        0.5438, 0.5866, 0.5004, 0.5448, 0.5281, 0.5707, 0.5129, 0.5585, 0.5542,
        0.5464, 0.5598, 0.5381, 0.5574, 0.5303, 0.5194, 0.5816, 0.5803, 0.5548,
        0.5206, 0.5696, 0.5700, 0.8712, 0.5545, 0.5563, 0.5365, 0.5493, 0.5338,
        0.5196, 0.5294, 0.5426, 0.5427, 0.5215, 0.5124, 0.5564, 0.5750, 0.5419,
        0.5685, 0.5686, 0.5943, 0.5340, 0.5738, 0.5465, 0.5286, 0.5613, 0.5638,
        0.5816, 0.5648, 0.8727, 0.5426, 0.5338, 0.5576, 0.5759, 0.5807, 0.5634,
        0.5363, 0.5319, 0.5432, 0.5411, 0.5570, 0.5630, 0.5506, 0.5039, 0.5452,
        0.5762, 0.5155, 0.5846, 0.5602, 0.5431, 0.5753, 0.5888, 0.5536, 0.5881,
        0.5570, 0.5310, 0.5415, 0.5555, 0.5621, 0.8363, 0.5522, 0.5344, 0.5097,
        0.5306, 0.5541, 0.5658, 0.5660, 0.5533, 0.5391, 0.5656, 0.5214, 0.5192,
        0.5220, 0.5646, 0.5385, 0.5584, 0.5194, 0.5514, 0.5687, 0.5545, 0.5807],
       grad_fn=<IndexBackward0>)
pred_loss tensor([5.2452e-05], grad_fn=<MulBackward0>)
size_loss tensor(-536.4559, grad_fn=<MulBackward0>)
size_num_loss 0.8
loss: tensor([-533.5606], grad_fn=<AddBackward0>)
8
epoch:  8 ; loss:  -533.5606079101562 ; pred:  tensor([1.3284e-08, 3.5703e-06, 9.9999e-01, 1.7146e-06],
       grad_fn=<SoftmaxBackward0>)
num_high 8 len(mask) 253
mask_without_small tensor([0.8121, 0.5957, 0.5828, 0.5764, 0.5136, 0.5375, 0.5057, 0.5237, 0.5993,
        0.5311, 0.5100, 0.5242, 0.5277, 0.5234, 0.5791, 0.5991, 0.5355, 0.5290,
        0.5534, 0.5236, 0.5870, 0.5802, 0.6001, 0.5914, 0.5917, 0.5736, 0.5925,
        0.5342, 0.5388, 0.5338, 0.5818, 0.5104, 0.5213, 0.5343, 0.6015, 0.5384,
        0.5304, 0.5388, 0.5233, 0.5067, 0.5851, 0.5211, 0.5269, 0.5128, 0.8784,
        0.5136, 0.5291, 0.5204, 0.5257, 0.5392, 0.5681, 0.5291, 0.5895, 0.5225,
        0.5241, 0.5100, 0.5387, 0.5372, 0.5763, 0.5366, 0.7327, 0.5147, 0.5936,
        0.5948, 0.5817, 0.8799, 0.5679, 0.5375, 0.5348, 0.5174, 0.5914, 0.5353,
        0.5679, 0.5390, 0.5494, 0.5717, 0.5260, 0.5238, 0.5383, 0.5321, 0.5114,
        0.5272, 0.5690, 0.5680, 0.5884, 0.5389, 0.5786, 0.5293, 0.5175, 0.5733,
        0.5032, 0.5222, 0.5925, 0.5630, 0.5637, 0.5798, 0.5386, 0.5749, 0.5722,
        0.5868, 0.5299, 0.5350, 0.5788, 0.5427, 0.5401, 0.5397, 0.5913, 0.5381,
        0.5328, 0.5089, 0.5365, 0.5269, 0.5620, 0.5780, 0.5394, 0.5311, 0.5683,
        0.5380, 0.5400, 0.5398, 0.5792, 0.5874, 0.5377, 0.5778, 0.5446, 0.8141,
        0.5855, 0.5093, 0.5158, 0.5359, 0.5989, 0.5755, 0.5718, 0.5884, 0.5384,
        0.5014, 0.5835, 0.5314, 0.5860, 0.5251, 0.5748, 0.5191, 0.5843, 0.5985,
        0.5949, 0.5396, 0.5346, 0.5241, 0.5395, 0.5374, 0.5845, 0.5296, 0.5982,
        0.5306, 0.5144, 0.5805, 0.5400, 0.5385, 0.5320, 0.5393, 0.5242, 0.5400,
        0.5166, 0.5058, 0.5929, 0.5916, 0.5389, 0.5070, 0.5789, 0.5795, 0.8767,
        0.5387, 0.5397, 0.5226, 0.5346, 0.5200, 0.5060, 0.5157, 0.5284, 0.5285,
        0.5079, 0.5397, 0.5858, 0.5278, 0.5773, 0.5775, 0.6060, 0.5202, 0.5843,
        0.5321, 0.5149, 0.5373, 0.5623, 0.5929, 0.5681, 0.8782, 0.5285, 0.5200,
        0.5401, 0.5868, 0.5920, 0.5586, 0.5224, 0.5181, 0.5290, 0.5270, 0.5399,
        0.5538, 0.5357, 0.5309, 0.5871, 0.5020, 0.5961, 0.5387, 0.5289, 0.5860,
        0.6004, 0.5381, 0.5997, 0.5399, 0.5173, 0.5274, 0.5393, 0.5415, 0.8430,
        0.5370, 0.5205, 0.5169, 0.5384, 0.5717, 0.5724, 0.5379, 0.5251, 0.5710,
        0.5078, 0.5056, 0.5084, 0.5671, 0.5246, 0.5401, 0.5059, 0.5364, 0.5777,
        0.5387, 0.5919], grad_fn=<IndexBackward0>)
pred_loss tensor([7.2718e-05], grad_fn=<MulBackward0>)
size_loss tensor(-596.8444, grad_fn=<MulBackward0>)
size_num_loss 0.8
loss: tensor([-593.9995], grad_fn=<AddBackward0>)
9
epoch:  9 ; loss:  -593.9994506835938 ; pred:  tensor([1.9883e-08, 4.9138e-06, 9.9999e-01, 2.3313e-06],
       grad_fn=<SoftmaxBackward0>)
num_high 8 len(mask) 253
mask_without_small tensor([0.8219, 0.6109, 0.5973, 0.5900, 0.5205, 0.5065, 0.6145, 0.5138, 0.5070,
        0.5105, 0.5062, 0.5931, 0.6144, 0.5184, 0.5117, 0.5392, 0.5064, 0.6018,
        0.5943, 0.6154, 0.6064, 0.6068, 0.5867, 0.6077, 0.5170, 0.5220, 0.5166,
        0.5961, 0.5041, 0.5172, 0.6168, 0.5241, 0.5132, 0.5243, 0.5061, 0.5998,
        0.5039, 0.5096, 0.8852, 0.5119, 0.5032, 0.5085, 0.5226, 0.5803, 0.5119,
        0.6045, 0.5053, 0.5069, 0.5219, 0.5202, 0.5898, 0.5195, 0.7452, 0.6087,
        0.6100, 0.5960, 0.8866, 0.5800, 0.5236, 0.5176, 0.5003, 0.6064, 0.5181,
        0.5801, 0.5223, 0.5355, 0.5845, 0.5088, 0.5066, 0.5215, 0.5149, 0.5100,
        0.5814, 0.5802, 0.6033, 0.5222, 0.5924, 0.5121, 0.5004, 0.5863, 0.5050,
        0.6077, 0.5737, 0.5747, 0.5938, 0.5218, 0.5883, 0.5851, 0.6016, 0.5127,
        0.5179, 0.5928, 0.5292, 0.5241, 0.5246, 0.6063, 0.5213, 0.5156, 0.5194,
        0.5097, 0.5720, 0.5918, 0.5228, 0.5139, 0.5805, 0.5211, 0.5246, 0.5235,
        0.5931, 0.6022, 0.5237, 0.5916, 0.5311, 0.8238, 0.6002, 0.5188, 0.6142,
        0.5889, 0.5846, 0.6033, 0.5216, 0.5979, 0.5142, 0.6007, 0.5079, 0.5881,
        0.5020, 0.5988, 0.6138, 0.6101, 0.5246, 0.5174, 0.5069, 0.5230, 0.5236,
        0.5991, 0.5124, 0.6135, 0.5133, 0.5947, 0.5245, 0.5217, 0.5148, 0.5245,
        0.5070, 0.5240, 0.6080, 0.6067, 0.5222, 0.5929, 0.5935, 0.8836, 0.5219,
        0.5233, 0.5054, 0.5175, 0.5028, 0.5112, 0.5113, 0.5234, 0.6005, 0.5106,
        0.5910, 0.5912, 0.6213, 0.5030, 0.5989, 0.5149, 0.5236, 0.5725, 0.6081,
        0.5803, 0.8850, 0.5113, 0.5028, 0.5241, 0.6016, 0.6070, 0.5605, 0.5053,
        0.5009, 0.5118, 0.5098, 0.5238, 0.5396, 0.5186, 0.5137, 0.6019, 0.6113,
        0.5243, 0.5117, 0.6008, 0.6157, 0.5212, 0.6150, 0.5238, 0.5001, 0.5102,
        0.5227, 0.5280, 0.8515, 0.5200, 0.5034, 0.5216, 0.5845, 0.5853, 0.5210,
        0.5079, 0.5837, 0.5791, 0.5074, 0.5245, 0.5193, 0.5914, 0.5220, 0.6070],
       grad_fn=<IndexBackward0>)
pred_loss tensor([9.4176e-05], grad_fn=<MulBackward0>)
size_loss tensor(-700.1249, grad_fn=<MulBackward0>)
size_num_loss 0.8
loss: tensor([-697.4368], grad_fn=<AddBackward0>)
10
epoch:  10 ; loss:  -697.436767578125 ; pred:  tensor([2.7972e-08, 6.4647e-06, 9.9999e-01, 3.0153e-06],
       grad_fn=<SoftmaxBackward0>)
num_high 34 len(mask) 253
mask_without_small tensor([0.8326, 0.6284, 0.6142, 0.6063, 0.5025, 0.6321, 0.6097, 0.6319, 0.5000,
        0.5248, 0.6190, 0.6111, 0.6330, 0.6238, 0.6242, 0.6028, 0.6251, 0.5042,
        0.6130, 0.6343, 0.5074, 0.5075, 0.6169, 0.8926, 0.5049, 0.5958, 0.6218,
        0.5041, 0.5020, 0.6062, 0.5013, 0.7593, 0.6261, 0.6275, 0.6129, 0.8940,
        0.5955, 0.5070, 0.6238, 0.5956, 0.5045, 0.5201, 0.6004, 0.5036, 0.5970,
        0.5957, 0.6206, 0.5044, 0.6090, 0.6024, 0.6251, 0.5884, 0.5895, 0.6105,
        0.5039, 0.6045, 0.6010, 0.6187, 0.6094, 0.5131, 0.5068, 0.5077, 0.6237,
        0.5033, 0.5012, 0.5863, 0.6084, 0.5052, 0.5961, 0.5031, 0.5076, 0.5060,
        0.6098, 0.6194, 0.5071, 0.6081, 0.5151, 0.8345, 0.6173, 0.5005, 0.6317,
        0.6052, 0.6005, 0.6206, 0.5037, 0.6149, 0.6179, 0.6043, 0.6159, 0.6313,
        0.6276, 0.5077, 0.5054, 0.5070, 0.6161, 0.6310, 0.6115, 0.5074, 0.5039,
        0.5077, 0.5067, 0.6255, 0.6240, 0.5044, 0.6095, 0.6102, 0.8912, 0.5041,
        0.5058, 0.5059, 0.6176, 0.6075, 0.6077, 0.6388, 0.6160, 0.5070, 0.5869,
        0.6255, 0.5958, 0.8925, 0.5069, 0.6188, 0.6244, 0.5619, 0.5064, 0.5253,
        0.5002, 0.6191, 0.6288, 0.5075, 0.6179, 0.6333, 0.5033, 0.6326, 0.5064,
        0.5050, 0.5118, 0.8608, 0.5018, 0.5037, 0.6004, 0.6013, 0.5030, 0.5995,
        0.5945, 0.5073, 0.5011, 0.6079, 0.5042, 0.6244],
       grad_fn=<IndexBackward0>)
pred_loss tensor([0.0001], grad_fn=<MulBackward0>)
size_loss tensor(-836.0425, grad_fn=<MulBackward0>)
size_num_loss 3.4000000000000004
loss: tensor([-831.0882], grad_fn=<AddBackward0>)
11
epoch:  11 ; loss:  -831.0881958007812 ; pred:  tensor([3.8474e-08, 8.3694e-06, 9.9999e-01, 3.8282e-06],
       grad_fn=<SoftmaxBackward0>)
num_high 73 len(mask) 253
mask_without_small tensor([0.8439, 0.6474, 0.6330, 0.6247, 0.6510, 0.6283, 0.6509, 0.5096, 0.6379,
        0.6297, 0.6519, 0.6428, 0.6432, 0.6210, 0.6440, 0.6317, 0.6533, 0.6357,
        0.9003, 0.6133, 0.6407, 0.6246, 0.7742, 0.6451, 0.6464, 0.6316, 0.9016,
        0.6129, 0.6427, 0.6130, 0.5044, 0.6184, 0.6146, 0.6131, 0.6395, 0.6276,
        0.6206, 0.6440, 0.6030, 0.6050, 0.6291, 0.6228, 0.6191, 0.6376, 0.6279,
        0.6426, 0.5989, 0.6268, 0.6135, 0.6284, 0.6383, 0.6266, 0.8457, 0.6362,
        0.6507, 0.6235, 0.6185, 0.6395, 0.6337, 0.6367, 0.6226, 0.6347, 0.6503,
        0.6466, 0.6349, 0.6500, 0.6301, 0.6444, 0.6430, 0.6281, 0.6288, 0.8990,
        0.6365, 0.6259, 0.6262, 0.6577, 0.6348, 0.6002, 0.6445, 0.6133, 0.9002,
        0.6377, 0.6434, 0.5501, 0.5101, 0.6380, 0.6478, 0.6368, 0.6522, 0.6515,
        0.8704, 0.6184, 0.6193, 0.6174, 0.6117, 0.6264, 0.6434],
       grad_fn=<IndexBackward0>)
pred_loss tensor([0.0002], grad_fn=<MulBackward0>)
size_loss tensor(-723.5560, grad_fn=<MulBackward0>)
size_num_loss 7.300000000000001
loss: tensor([-714.9580], grad_fn=<AddBackward0>)
12
epoch:  12 ; loss:  -714.9580078125 ; pred:  tensor([4.9778e-08, 1.0318e-05, 9.9998e-01, 4.6227e-06],
       grad_fn=<SoftmaxBackward0>)
num_high 73 len(mask) 253
mask_without_small tensor([0.8552, 0.6642, 0.6427, 0.6275, 0.6687, 0.6342, 0.6685, 0.6509, 0.6368,
        0.6697, 0.6581, 0.6587, 0.6207, 0.6599, 0.6404, 0.6712, 0.6474, 0.9080,
        0.6077, 0.6552, 0.6273, 0.7894, 0.6613, 0.6630, 0.6402, 0.9092, 0.6071,
        0.6581, 0.6073, 0.6162, 0.6098, 0.6074, 0.6534, 0.6328, 0.6199, 0.6599,
        0.5936, 0.5961, 0.6357, 0.6239, 0.6173, 0.6505, 0.6335, 0.6579, 0.5887,
        0.6315, 0.6081, 0.6343, 0.6516, 0.6310, 0.8569, 0.6481, 0.6683, 0.6253,
        0.6163, 0.6534, 0.6440, 0.6490, 0.6235, 0.6456, 0.6678, 0.6632, 0.6461,
        0.6674, 0.6375, 0.6604, 0.6584, 0.6338, 0.6351, 0.9067, 0.6486, 0.6297,
        0.6302, 0.6762, 0.6458, 0.5903, 0.6604, 0.6078, 0.9079, 0.6505, 0.6590,
        0.5368, 0.6510, 0.6647, 0.6491, 0.6701, 0.6692, 0.8801, 0.6161, 0.6178,
        0.6145, 0.6054, 0.6306, 0.6590], grad_fn=<IndexBackward0>)
pred_loss tensor([0.0002], grad_fn=<MulBackward0>)
size_loss tensor(-717.1617, grad_fn=<MulBackward0>)
size_num_loss 7.300000000000001
loss: tensor([-708.5750], grad_fn=<AddBackward0>)
13
epoch:  13 ; loss:  -708.5750122070312 ; pred:  tensor([7.3186e-08, 1.3981e-05, 9.9998e-01, 6.1629e-06],
       grad_fn=<SoftmaxBackward0>)
num_high 68 len(mask) 253
mask_without_small tensor([0.8665, 0.6802, 0.6449, 0.6209, 0.6860, 0.6308, 0.6858, 0.6595, 0.6350,
        0.6873, 0.6714, 0.6723, 0.6115, 0.6741, 0.6410, 0.6892, 0.6532, 0.9155,
        0.5952, 0.6667, 0.6205, 0.8046, 0.6762, 0.6787, 0.6407, 0.9166, 0.5945,
        0.6714, 0.5947, 0.6056, 0.5978, 0.5949, 0.6637, 0.6288, 0.6105, 0.6741,
        0.5791, 0.5818, 0.6333, 0.6159, 0.6071, 0.6587, 0.6298, 0.6711, 0.5738,
        0.6267, 0.5958, 0.6310, 0.6607, 0.6260, 0.8680, 0.6545, 0.6855, 0.6178,
        0.6058, 0.6637, 0.6472, 0.6562, 0.6153, 0.6501, 0.6849, 0.6789, 0.6509,
        0.6844, 0.6362, 0.6749, 0.6719, 0.6302, 0.6323, 0.9144, 0.6554, 0.6241,
        0.6248, 0.6948, 0.6505, 0.5755, 0.6749, 0.5953, 0.9154, 0.6588, 0.6728,
        0.5199, 0.6596, 0.6810, 0.6563, 0.6877, 0.6868, 0.8897, 0.6055, 0.6077,
        0.6035, 0.5925, 0.6254, 0.6727], grad_fn=<IndexBackward0>)
pred_loss tensor([0.0003], grad_fn=<MulBackward0>)
size_loss tensor(-774.5974, grad_fn=<MulBackward0>)
size_num_loss 6.800000000000001
loss: tensor([-766.5107], grad_fn=<AddBackward0>)
14
epoch:  14 ; loss:  -766.5107421875 ; pred:  tensor([1.1180e-07, 1.9505e-05, 9.9997e-01, 8.4828e-06],
       grad_fn=<SoftmaxBackward0>)
num_high 58 len(mask) 253
mask_without_small tensor([0.8773, 0.6973, 0.6421, 0.6087, 0.7043, 0.6215, 0.7040, 0.6664, 0.6273,
        0.7057, 0.6854, 0.6866, 0.5974, 0.6891, 0.6361, 0.7078, 0.6557, 0.9227,
        0.5789, 0.6783, 0.6083, 0.8194, 0.6920, 0.6953, 0.6355, 0.9237, 0.5781,
        0.6853, 0.5783, 0.5906, 0.5817, 0.5785, 0.6734, 0.6188, 0.5962, 0.6891,
        0.5613, 0.5642, 0.6249, 0.6026, 0.5922, 0.6650, 0.6201, 0.6849, 0.5557,
        0.6161, 0.5795, 0.6218, 0.6684, 0.6151, 0.8788, 0.6579, 0.7037, 0.6049,
        0.5908, 0.6734, 0.6457, 0.6607, 0.6019, 0.6505, 0.7030, 0.6955, 0.6518,
        0.7024, 0.6290, 0.6902, 0.6861, 0.6207, 0.6235, 0.9216, 0.6594, 0.6127,
        0.6136, 0.7139, 0.6511, 0.5574, 0.6903, 0.5789, 0.9226, 0.6652, 0.6873,
        0.5003, 0.6667, 0.6982, 0.6610, 0.7062, 0.7051, 0.8988, 0.5904, 0.5929,
        0.5882, 0.5758, 0.6144, 0.6872], grad_fn=<IndexBackward0>)
pred_loss tensor([0.0004], grad_fn=<MulBackward0>)
size_loss tensor(-858.0578, grad_fn=<MulBackward0>)
size_num_loss 5.800000000000001
loss: tensor([-850.9738], grad_fn=<AddBackward0>)
15
epoch:  15 ; loss:  -850.9738159179688 ; pred:  tensor([1.7040e-07, 2.7164e-05, 9.9996e-01, 1.1664e-05],
       grad_fn=<SoftmaxBackward0>)
num_high 54 len(mask) 253
mask_without_small tensor([0.8876, 0.7153, 0.6351, 0.5927, 0.7230, 0.6079, 0.7227, 0.6736, 0.6150,
        0.7246, 0.7010, 0.7026, 0.5798, 0.7057, 0.6266, 0.7268, 0.6562, 0.9295,
        0.5596, 0.6915, 0.5921, 0.8338, 0.7092, 0.7130, 0.6259, 0.9304, 0.5588,
        0.7009, 0.5590, 0.5724, 0.5627, 0.5593, 0.6845, 0.6045, 0.5785, 0.7057,
        0.5411, 0.5441, 0.6119, 0.5857, 0.5742, 0.6714, 0.6061, 0.7005, 0.5352,
        0.6013, 0.5603, 0.6082, 0.6767, 0.6002, 0.8890, 0.6598, 0.7224, 0.5883,
        0.5726, 0.6845, 0.6405, 0.6644, 0.5850, 0.6478, 0.7216, 0.7133, 0.6499,
        0.7210, 0.6172, 0.7071, 0.7019, 0.6068, 0.6103, 0.9285, 0.6622, 0.5973,
        0.5983, 0.7332, 0.6487, 0.5370, 0.7071, 0.5597, 0.9294, 0.6718, 0.7034,
        0.6740, 0.7164, 0.6648, 0.7251, 0.7239, 0.9075, 0.5722, 0.5749, 0.5697,
        0.5564, 0.5993, 0.7034], grad_fn=<IndexBackward0>)
pred_loss tensor([0.0005], grad_fn=<MulBackward0>)
size_loss tensor(-951.9801, grad_fn=<MulBackward0>)
size_num_loss 5.4
loss: tensor([-945.3050], grad_fn=<AddBackward0>)
16
epoch:  16 ; loss:  -945.3049926757812 ; pred:  tensor([2.5549e-07, 3.7353e-05, 9.9995e-01, 1.5836e-05],
       grad_fn=<SoftmaxBackward0>)
num_high 52 len(mask) 253
mask_without_small tensor([0.8972, 0.7338, 0.6241, 0.5737, 0.7420, 0.5907, 0.7417, 0.6823, 0.5990,
        0.7436, 0.7181, 0.7199, 0.5597, 0.7234, 0.6130, 0.7459, 0.6550, 0.9357,
        0.5382, 0.7068, 0.5731, 0.8475, 0.7273, 0.7314, 0.6122, 0.9366, 0.5373,
        0.7180, 0.5375, 0.5517, 0.5414, 0.5378, 0.6978, 0.5869, 0.5583, 0.7234,
        0.5188, 0.5220, 0.5954, 0.5661, 0.5536, 0.6790, 0.5887, 0.7175, 0.5128,
        0.5832, 0.5389, 0.5911, 0.6869, 0.5820, 0.8985, 0.6607, 0.7413, 0.5689,
        0.5519, 0.6978, 0.6314, 0.6680, 0.5653, 0.6421, 0.7405, 0.7317, 0.6452,
        0.7398, 0.6016, 0.7249, 0.7191, 0.5895, 0.5935, 0.9348, 0.6646, 0.5788,
        0.5800, 0.7523, 0.6434, 0.5147, 0.7250, 0.5383, 0.9356, 0.6795, 0.7209,
        0.6830, 0.7350, 0.6687, 0.7441, 0.7429, 0.9155, 0.5515, 0.5544, 0.5489,
        0.5348, 0.5810, 0.7208], grad_fn=<IndexBackward0>)
pred_loss tensor([0.0007], grad_fn=<MulBackward0>)
size_loss tensor(-1080.1243, grad_fn=<MulBackward0>)
size_num_loss 5.2
loss: tensor([-1073.6545], grad_fn=<AddBackward0>)
17
epoch:  17 ; loss:  -1073.654541015625 ; pred:  tensor([3.7472e-07, 5.0499e-05, 9.9993e-01, 2.1137e-05],
       grad_fn=<SoftmaxBackward0>)
num_high 50 len(mask) 253
mask_without_small tensor([0.9061, 0.7525, 0.6096, 0.5524, 0.7608, 0.5708, 0.7605, 0.6935, 0.5801,
        0.7625, 0.7360, 0.7379, 0.5375, 0.7416, 0.5962, 0.7647, 0.6525, 0.9414,
        0.5150, 0.7236, 0.5518, 0.8603, 0.7457, 0.7500, 0.5952, 0.9422, 0.5141,
        0.7359, 0.5143, 0.5291, 0.5184, 0.5146, 0.7131, 0.5666, 0.5360, 0.7416,
        0.5761, 0.5442, 0.5311, 0.6888, 0.5686, 0.7353, 0.5627, 0.5157, 0.5713,
        0.6996, 0.5613, 0.9073, 0.6614, 0.7601, 0.5473, 0.5293, 0.7132, 0.6189,
        0.6728, 0.5434, 0.6334, 0.7593, 0.7503, 0.6379, 0.7586, 0.5830, 0.7432,
        0.7371, 0.5695, 0.5739, 0.9405, 0.6674, 0.5579, 0.5591, 0.7712, 0.6354,
        0.7433, 0.5151, 0.9413, 0.6896, 0.7389, 0.6943, 0.7537, 0.6738, 0.7630,
        0.7617, 0.9229, 0.5289, 0.5320, 0.5262, 0.5115, 0.5603, 0.7389],
       grad_fn=<IndexBackward0>)
pred_loss tensor([0.0010], grad_fn=<MulBackward0>)
size_loss tensor(-1201.9939, grad_fn=<MulBackward0>)
size_num_loss 5.0
loss: tensor([-1195.7505], grad_fn=<AddBackward0>)
18
epoch:  18 ; loss:  -1195.75048828125 ; pred:  tensor([5.3671e-07, 6.7038e-05, 9.9990e-01, 2.7701e-05],
       grad_fn=<SoftmaxBackward0>)
num_high 49 len(mask) 253
mask_without_small tensor([0.9142, 0.7710, 0.5917, 0.5292, 0.7793, 0.5488, 0.7789, 0.7068, 0.5587,
        0.7809, 0.7542, 0.7562, 0.5137, 0.7600, 0.5765, 0.7831, 0.6478, 0.9465,
        0.7413, 0.5286, 0.8722, 0.7641, 0.7684, 0.5753, 0.9472, 0.7541, 0.5049,
        0.7299, 0.5443, 0.5121, 0.7600, 0.5544, 0.5207, 0.5071, 0.7009, 0.5464,
        0.7535, 0.5401, 0.5492, 0.7144, 0.5386, 0.9153, 0.6612, 0.7786, 0.5239,
        0.5052, 0.7300, 0.6028, 0.6784, 0.5198, 0.6210, 0.7778, 0.7688, 0.6269,
        0.7771, 0.5619, 0.7616, 0.7554, 0.5474, 0.5521, 0.9457, 0.6703, 0.5350,
        0.5363, 0.7894, 0.6235, 0.7617, 0.9464, 0.7019, 0.7573, 0.7079, 0.7722,
        0.6798, 0.7814, 0.7802, 0.9296, 0.5048, 0.5079, 0.5019, 0.5375, 0.7572],
       grad_fn=<IndexBackward0>)
pred_loss tensor([0.0012], grad_fn=<MulBackward0>)
size_loss tensor(-1308.5154, grad_fn=<MulBackward0>)
size_num_loss 4.9
loss: tensor([-1302.4180], grad_fn=<AddBackward0>)
19
epoch:  19 ; loss:  -1302.41796875 ; pred:  tensor([7.5223e-07, 8.7530e-05, 9.9988e-01, 3.5704e-05],
       grad_fn=<SoftmaxBackward0>)
Finished Training
sel masked ver tensor(indices=tensor([[6813, 7011, 7019, 7076, 7187, 7224, 7225, 7232, 7233,
                        7234, 7235, 7237, 7238, 7239, 7246, 7339, 7348, 7354,
                        7474, 7760, 7762, 7804, 5471, 7350, 5861, 7188, 7227,
                        7235, 7238, 7246, 7343, 7412, 7474, 7480, 7480, 5933,
                        7178, 7194, 7471, 7471, 7633, 7740, 7740, 7760, 7760,
                        7761, 7761, 5351, 5443, 5471, 5933, 5861, 5861, 5931,
                        5931, 5931, 5933, 5933, 5933, 5933, 5933, 5861, 5861,
                        5861, 5861, 5861, 5861, 5861, 5861, 5861, 5861, 5861,
                        5861, 5861, 5861, 5861, 5861, 5861, 5861, 5861, 5861],
                       [5861, 5861, 5861, 5861, 5861, 5861, 5861, 5861, 5861,
                        5861, 5861, 5861, 5861, 5861, 5861, 5861, 5861, 5861,
                        5861, 5861, 5861, 5861, 5933, 5861,    9, 5933, 5931,
                        5931, 5931, 5931, 5933, 5933, 5933, 5931, 5933, 5471,
                        5471, 5444, 5443, 5471, 5443, 5351, 5443, 5351, 5443,
                        5351, 5443, 5861, 5861, 5861, 5861, 3937,    8, 7233,
                        7235, 7343, 7225, 7227, 7233, 7234, 7235, 7019, 7076,
                        7105, 7178, 7185, 7222, 7224, 7229, 7231, 7246, 7302,
                        7343, 7344, 7346, 7353, 7460, 7471, 7480, 7804, 5230]]),
       values=tensor([0.9142, 0.7710, 0.5917, 0.5292, 0.7793, 0.5488, 0.7789,
                      0.7068, 0.5587, 0.7809, 0.7542, 0.7562, 0.5137, 0.7600,
                      0.5765, 0.7831, 0.6478, 0.9465, 0.7413, 0.5286, 0.8722,
                      0.7641, 0.7684, 0.5753, 0.9472, 0.7541, 0.5049, 0.7299,
                      0.5443, 0.5121, 0.7600, 0.5544, 0.5207, 0.5071, 0.7009,
                      0.5464, 0.7535, 0.5401, 0.5492, 0.7144, 0.5386, 0.9153,
                      0.6612, 0.7786, 0.5239, 0.5052, 0.7300, 0.6028, 0.6784,
                      0.5198, 0.6210, 0.7778, 0.7688, 0.6269, 0.7771, 0.5619,
                      0.7616, 0.7554, 0.5474, 0.5521, 0.9457, 0.6703, 0.5350,
                      0.5363, 0.7894, 0.6235, 0.7617, 0.9464, 0.7019, 0.7573,
                      0.7079, 0.7722, 0.6798, 0.7814, 0.7802, 0.9296, 0.5048,
                      0.5079, 0.5019, 0.5375, 0.7572]),
       size=(7805, 7805), nnz=81, layout=torch.sparse_coo,
       grad_fn=<SparseCooTensorWithDimsAndTensorsBackward0>)
Counter({'author': 22, 'publication': 19, 'isAbout': 12, 'hasProject': 10, 'projectInfo': 8, 'isWorkedOnBy': 3, 'dealtWithIn': 1, 'phone': 1, 'fax': 1, 'name': 1, 'type': 1, 'member': 1, 'editor': 1})
dict index: {}
node_idx 5861
 node original label [2]
 node predicted label explain 2
 node prediction probability explain tensor([7.5223e-07, 8.7530e-05, 9.9988e-01, 3.5704e-05],
       grad_fn=<SoftmaxBackward0>)
 node predicted label full 2 most important relations  {'isWorkedOnBy': 3, 'dealtWithIn': 1, 'phone': 1, 'fax': 1, 'name': 1, 'type': 1, 'publication': 19, 'projectInfo': 8, 'isAbout': 12, 'member': 1, 'author': 22, 'hasProject': 10, 'editor': 1, 'label': 2, 'node_idx': '5861'}
 final masks and lenght tensor(indices=tensor([[ 23383,  23581,  23589,  23626,  23646,  23675,  23748,
                         23753,  23755,  23757,  23758,  23759,  23764,  23792,
                         23793,  23794,  23795,  23797,  23798,  23799,  23801,
                         23802,  23803,  23804,  23805,  23807,  23808,  23809,
                         23811,  23812,  23815,  23816,  23834,  23872,  23894,
                         23909,  23913,  23914,  23915,  23916,  23917,  23918,
                         23919,  23922,  23923,  23924,  23925,  23950,  23971,
                         23982,  24030,  24041,  24043,  24044,  24050,  24118,
                         24203,  24310,  24329,  24330,  24331,  24332,  24365,
                         24374,  63466,  81915,  88711, 114781, 114883, 114888,
                        114890, 114893, 114894, 114929, 114930, 114930, 114932,
                        114932, 114934, 114934, 114937, 114937, 114938, 114938,
                        114939, 114939, 114940, 114940, 114943, 114947, 114947,
                        114951, 114951, 115048, 115048, 115049, 115085, 115117,
                        115117, 115178, 115179, 115185, 115185, 115509, 130136,
                        146778, 147658, 147658, 147950, 148023, 148028, 148028,
                        148033, 148033, 148034, 148039, 148039, 148039, 148083,
                        148169, 148225, 148246, 148257, 148316, 148316, 148393,
                        148478, 148478, 148585, 148585, 148604, 148604, 148604,
                        148605, 148605, 148606, 148606, 148606, 148607, 154481,
                        154536, 154573, 154574, 154601, 179916, 179918, 196416,
                        229556, 237841, 246196, 246196, 246196, 246196, 246196,
                        246196, 246196, 246196, 246196, 246196, 246196, 246196,
                        246196, 246198, 246198, 246198, 246198, 246198, 246198,
                        246198, 246198, 246198, 246198, 246198, 246198, 246198,
                        246198, 246198, 246198, 246198, 246198, 246198, 246198,
                        246198, 246198, 246198, 246198, 254411, 254411, 254411,
                        254411, 254411, 254411, 254411, 254411, 254411, 254411,
                        254411, 254411, 254411, 254411, 254411, 254411, 254411,
                        254411, 254411, 254411, 254411, 254411, 254411, 254411,
                        254411, 254411, 254411, 254411, 254411, 254411, 254411,
                        254411, 254411, 254411, 254411, 254411, 254411, 254411,
                        254411, 254411, 254411, 254411, 254411, 254411, 254411,
                        254411, 254411, 254411, 254411, 254411, 254411, 254411,
                        254411, 254411, 254411, 254411, 254411, 254411, 254411,
                        254411, 254411, 254411, 254411, 254411, 254411, 304121,
                        328976],
                       [  5861,   5861,   5861,   5861,   5861,   5861,   5861,
                          5861,   5861,   5861,   5861,   5861,   5861,   5861,
                          5861,   5861,   5861,   5861,   5861,   5861,   5861,
                          5861,   5861,   5861,   5861,   5861,   5861,   5861,
                          5861,   5861,   5861,   5861,   5861,   5861,   5861,
                          5861,   5861,   5861,   5861,   5861,   5861,   5861,
                          5861,   5861,   5861,   5861,   5861,   5861,   5861,
                          5861,   5861,   5861,   5861,   5861,   5861,   5861,
                          5861,   5861,   5861,   5861,   5861,   5861,   5861,
                          5861,   5933,   5861,      9,   5933,   5933,   5933,
                          5933,   5933,   5933,   5933,   5931,   5933,   5931,
                          5933,   5931,   5933,   5931,   5933,   5931,   5933,
                          5931,   5933,   5931,   5933,   5931,   5931,   5933,
                          5931,   5933,   5931,   5933,   5933,   5933,   5931,
                          5933,   5933,   5933,   5931,   5933,   5933,   8058,
                          5471,   5443,   5471,   5471,   5471,   5406,   5471,
                          5406,   5471,   5471,   5406,   5444,   5471,   5471,
                          5443,   5471,   5471,   5471,   5443,   5471,   5471,
                          5443,   5471,   5351,   5443,   5351,   5443,   5471,
                          5351,   5443,   5351,   5443,   5471,   5443,   5861,
                          5861,   5861,   5861,   5861,   5861,   5861,   3937,
                             8,   5640,   7225,   7227,   7229,   7232,   7233,
                          7234,   7235,   7238,   7242,   7246,   7343,   7412,
                          7480,   7076,   7178,   7183,   7185,   7188,   7189,
                          7224,   7225,   7227,   7229,   7232,   7233,   7234,
                          7235,   7242,   7246,   7343,   7344,   7380,   7412,
                          7473,   7474,   7480,   7804,   6813,   7011,   7019,
                          7056,   7076,   7105,   7178,   7183,   7185,   7187,
                          7188,   7189,   7194,   7222,   7223,   7224,   7225,
                          7227,   7228,   7229,   7231,   7232,   7233,   7234,
                          7235,   7237,   7238,   7239,   7241,   7242,   7245,
                          7246,   7264,   7302,   7324,   7339,   7343,   7344,
                          7345,   7346,   7347,   7348,   7349,   7350,   7352,
                          7353,   7354,   7355,   7380,   7401,   7412,   7460,
                          7471,   7473,   7474,   7480,   7548,   7633,   7740,
                          7759,   7760,   7761,   7762,   7795,   7804,   5931,
                          5230]]),
       values=tensor([0.9142, 0.7710, 0.5917, 0.4176, 0.5292, 0.4032, 0.4018,
                      0.3959, 0.3906, 0.7793, 0.3995, 0.3999, 0.3911, 0.3952,
                      0.3902, 0.5488, 0.7789, 0.3980, 0.3968, 0.4301, 0.3904,
                      0.7068, 0.5587, 0.7809, 0.7542, 0.7562, 0.5137, 0.7600,
                      0.4041, 0.4047, 0.4035, 0.5765, 0.4002, 0.3878, 0.4043,
                      0.7831, 0.4102, 0.3987, 0.4104, 0.3901, 0.3968, 0.6478,
                      0.3876, 0.3942, 0.4024, 0.9465, 0.4032, 0.3970, 0.3869,
                      0.3928, 0.4058, 0.4905, 0.3970, 0.7413, 0.3891, 0.3910,
                      0.3999, 0.4045, 0.4012, 0.5286, 0.4000, 0.8722, 0.4042,
                      0.7641, 0.7684, 0.5753, 0.9472, 0.4895, 0.4098, 0.4051,
                      0.3838, 0.7541, 0.4059, 0.4898, 0.4051, 0.4236, 0.5049,
                      0.3932, 0.4156, 0.3906, 0.4036, 0.4010, 0.4011, 0.3946,
                      0.4939, 0.4901, 0.7299, 0.4050, 0.5443, 0.3972, 0.3839,
                      0.5121, 0.3935, 0.3888, 0.7600, 0.4735, 0.3814, 0.4768,
                      0.5544, 0.4042, 0.5207, 0.5071, 0.7009, 0.3980, 0.4055,
                      0.5464, 0.4186, 0.4089, 0.4105, 0.7535, 0.4032, 0.4020,
                      0.3988, 0.3999, 0.3942, 0.4673, 0.5401, 0.4063, 0.3996,
                      0.4912, 0.4029, 0.4102, 0.4076, 0.5492, 0.7144, 0.4099,
                      0.5386, 0.4217, 0.9153, 0.6612, 0.3992, 0.4052, 0.3987,
                      0.7786, 0.5239, 0.5052, 0.7300, 0.4039, 0.3919, 0.6028,
                      0.4000, 0.6784, 0.3921, 0.5198, 0.3856, 0.6210, 0.7778,
                      0.7688, 0.4105, 0.4047, 0.3910, 0.4067, 0.4098, 0.6269,
                      0.3976, 0.7771, 0.4101, 0.3989, 0.4039, 0.5619, 0.4217,
                      0.4100, 0.4041, 0.4008, 0.4105, 0.3911, 0.4088, 0.4059,
                      0.3959, 0.7616, 0.7554, 0.4050, 0.3970, 0.5474, 0.5521,
                      0.9457, 0.4045, 0.4072, 0.3893, 0.4048, 0.3865, 0.3962,
                      0.4051, 0.3961, 0.3962, 0.3979, 0.4212, 0.4074, 0.6703,
                      0.3953, 0.5350, 0.5363, 0.7894, 0.3867, 0.6235, 0.4011,
                      0.4043, 0.4099, 0.4692, 0.7617, 0.4906, 0.9464, 0.3962,
                      0.3865, 0.4090, 0.7019, 0.7573, 0.4259, 0.3891, 0.3845,
                      0.3968, 0.3944, 0.4082, 0.4307, 0.3983, 0.4134, 0.3994,
                      0.7079, 0.3924, 0.7722, 0.4103, 0.3967, 0.6798, 0.7814,
                      0.4031, 0.7802, 0.4082, 0.3836, 0.3949, 0.4060, 0.4167,
                      0.9296, 0.4009, 0.3871, 0.4188, 0.4062, 0.4039, 0.5048,
                      0.5079, 0.4026, 0.3921, 0.5019, 0.3978, 0.3958, 0.3984,
                      0.4868, 0.3915, 0.4098, 0.3960, 0.3996, 0.5375, 0.4046,
                      0.7572]),
       size=(753935, 8285), nnz=253, layout=torch.sparse_coo,
       grad_fn=<SparseCooTensorWithDimsAndTensorsBackward0>) 81
 ---------------------------------------------------------------
node label: 3
16
num_high 43 len(mask) 43
mask_without_small tensor([0.8046, 0.7893, 0.7675, 0.6332, 0.7588, 0.6756, 0.7292, 0.6579, 0.6980,
        0.7950, 0.7141, 0.6676, 0.6991, 0.7067, 0.6972, 0.7621, 0.7948, 0.7242,
        0.7095, 0.7493, 0.6977, 0.7743, 0.7636, 0.7962, 0.7817, 0.7824, 0.7561,
        0.6937, 0.7830, 0.7592, 0.6825, 0.7157, 0.6906, 0.7023, 0.7344, 0.8159,
        0.7780, 0.7122, 0.6684, 0.6732, 0.6676, 0.7326, 0.7284],
       grad_fn=<IndexBackward0>)
pred_loss tensor([6.6765], grad_fn=<MulBackward0>)
size_loss tensor(-4.6849, grad_fn=<MulBackward0>)
size_num_loss 4.3
loss: tensor([10.0110], grad_fn=<AddBackward0>)
r_exp.py:102: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  hor_graph, ver_graph = torch.tensor(sub_hor_graph, dtype=torch.float), torch.tensor(sub_ver_graph, dtype=torch.float)
wandb: WARNING Calling wandb.login() after wandb.init() has no effect.
0
epoch:  0 ; loss:  10.010978698730469 ; pred:  tensor([0.2839, 0.1458, 0.0573, 0.5129], grad_fn=<SoftmaxBackward0>)
num_high 42 len(mask) 43
mask_without_small tensor([0.8199, 0.8055, 0.7849, 0.6097, 0.7767, 0.6533, 0.7090, 0.6350, 0.6765,
        0.8109, 0.6932, 0.6450, 0.6777, 0.6855, 0.6757, 0.7798, 0.8106, 0.7038,
        0.6884, 0.7676, 0.6762, 0.7913, 0.7451, 0.8119, 0.7983, 0.7989, 0.7741,
        0.7146, 0.7995, 0.7770, 0.6604, 0.6950, 0.6688, 0.6809, 0.7144, 0.8305,
        0.7948, 0.6913, 0.6459, 0.6508, 0.6894, 0.7125, 0.7081],
       grad_fn=<IndexBackward0>)
pred_loss tensor([6.5648], grad_fn=<MulBackward0>)
size_loss tensor(-6.2284, grad_fn=<MulBackward0>)
size_num_loss 4.2
loss: tensor([8.2373], grad_fn=<AddBackward0>)
1
epoch:  1 ; loss:  8.237340927124023 ; pred:  tensor([0.2786, 0.1461, 0.0565, 0.5187], grad_fn=<SoftmaxBackward0>)
num_high 40 len(mask) 43
mask_without_small tensor([0.8341, 0.8206, 0.8013, 0.5857, 0.7934, 0.6303, 0.6886, 0.6116, 0.6542,
        0.8256, 0.6716, 0.6218, 0.6554, 0.6636, 0.6534, 0.7964, 0.8254, 0.6829,
        0.6666, 0.7837, 0.6539, 0.8073, 0.7275, 0.8266, 0.8139, 0.8145, 0.7911,
        0.7304, 0.8150, 0.7938, 0.6376, 0.6789, 0.6463, 0.6588, 0.6948, 0.8439,
        0.8106, 0.6696, 0.6227, 0.6280, 0.7094, 0.6948, 0.6885],
       grad_fn=<IndexBackward0>)
pred_loss tensor([6.4812], grad_fn=<MulBackward0>)
size_loss tensor(-7.9165, grad_fn=<MulBackward0>)
size_num_loss 4.0
loss: tensor([6.2431], grad_fn=<AddBackward0>)
2
epoch:  2 ; loss:  6.243072986602783 ; pred:  tensor([0.2739, 0.1470, 0.0560, 0.5230], grad_fn=<SoftmaxBackward0>)
num_high 36 len(mask) 43
mask_without_small tensor([0.8471, 0.8347, 0.8168, 0.5614, 0.8093, 0.6068, 0.6673, 0.5877, 0.6312,
        0.8393, 0.6492, 0.5981, 0.6324, 0.6408, 0.6303, 0.8122, 0.8392, 0.6611,
        0.6439, 0.7994, 0.6309, 0.8224, 0.7087, 0.8401, 0.8285, 0.8290, 0.8072,
        0.7473, 0.8295, 0.8097, 0.6142, 0.6598, 0.6230, 0.6359, 0.6742, 0.8561,
        0.8254, 0.6471, 0.5991, 0.6053, 0.7289, 0.6752, 0.6678],
       grad_fn=<IndexBackward0>)
pred_loss tensor([6.4114], grad_fn=<MulBackward0>)
size_loss tensor(-9.6652, grad_fn=<MulBackward0>)
size_num_loss 3.6
loss: tensor([3.9973], grad_fn=<AddBackward0>)
3
epoch:  3 ; loss:  3.997344970703125 ; pred:  tensor([0.2693, 0.1483, 0.0557, 0.5267], grad_fn=<SoftmaxBackward0>)
num_high 28 len(mask) 43
mask_without_small tensor([0.8590, 0.8477, 0.8313, 0.5370, 0.8243, 0.5827, 0.6450, 0.5634, 0.6075,
        0.8519, 0.6260, 0.5739, 0.6087, 0.6174, 0.6066, 0.8270, 0.8517, 0.6384,
        0.6206, 0.8146, 0.6072, 0.8364, 0.6887, 0.8524, 0.8420, 0.8425, 0.8223,
        0.7643, 0.8430, 0.8247, 0.5903, 0.6390, 0.5992, 0.6123, 0.6526, 0.8672,
        0.8392, 0.6238, 0.5750, 0.5839, 0.7478, 0.6543, 0.6460],
       grad_fn=<IndexBackward0>)
pred_loss tensor([6.3522], grad_fn=<MulBackward0>)
size_loss tensor(-11.4343, grad_fn=<MulBackward0>)
size_num_loss 2.8000000000000003
loss: tensor([1.3376], grad_fn=<AddBackward0>)
4
epoch:  4 ; loss:  1.337611198425293 ; pred:  tensor([0.2647, 0.1498, 0.0557, 0.5298], grad_fn=<SoftmaxBackward0>)
num_high 23 len(mask) 43
mask_without_small tensor([0.8698, 0.8595, 0.8447, 0.5125, 0.8383, 0.5582, 0.6219, 0.5389, 0.5832,
        0.8634, 0.6021, 0.5494, 0.5845, 0.5933, 0.5823, 0.8408, 0.8632, 0.6149,
        0.5965, 0.8290, 0.5829, 0.8494, 0.6676, 0.8633, 0.8545, 0.8549, 0.8364,
        0.7810, 0.8555, 0.8387, 0.5660, 0.6169, 0.5749, 0.5881, 0.6300, 0.8771,
        0.8520, 0.5998, 0.5508, 0.5660, 0.7660, 0.6322, 0.6232],
       grad_fn=<IndexBackward0>)
pred_loss tensor([6.2977], grad_fn=<MulBackward0>)
size_loss tensor(-13.1967, grad_fn=<MulBackward0>)
size_num_loss 2.3000000000000003
loss: tensor([-1.0149], grad_fn=<AddBackward0>)
5
epoch:  5 ; loss:  -1.0149359703063965 ; pred:  tensor([0.2601, 0.1516, 0.0556, 0.5327], grad_fn=<SoftmaxBackward0>)
num_high 20 len(mask) 43
mask_without_small tensor([0.8795, 0.8703, 0.8571, 0.8513, 0.5335, 0.5980, 0.5143, 0.5585, 0.8737,
        0.5776, 0.5247, 0.5598, 0.5687, 0.5576, 0.8536, 0.8736, 0.5908, 0.5719,
        0.8426, 0.5582, 0.8612, 0.6455, 0.8730, 0.8658, 0.8661, 0.8494, 0.7971,
        0.8669, 0.8516, 0.5415, 0.5938, 0.5503, 0.5634, 0.6066, 0.8860, 0.8637,
        0.5753, 0.5264, 0.5552, 0.7834, 0.6092, 0.5996],
       grad_fn=<IndexBackward0>)
pred_loss tensor([6.2385], grad_fn=<MulBackward0>)
size_loss tensor(-1475.8961, grad_fn=<MulBackward0>)
size_num_loss 2.0
loss: tensor([-1466.8024], grad_fn=<AddBackward0>)
6
epoch:  6 ; loss:  -1466.8023681640625 ; pred:  tensor([0.2552, 0.1533, 0.0556, 0.5359], grad_fn=<SoftmaxBackward0>)
num_high 20 len(mask) 43
mask_without_small tensor([0.8850, 0.8760, 0.8633, 0.8577, 0.5203, 0.5854, 0.5010, 0.5455, 0.8794,
        0.5647, 0.5115, 0.5467, 0.5553, 0.5446, 0.8600, 0.8792, 0.5780, 0.5590,
        0.8493, 0.5452, 0.8674, 0.6329, 0.8786, 0.8717, 0.8721, 0.8561, 0.8057,
        0.8731, 0.8581, 0.5286, 0.5814, 0.5375, 0.5504, 0.5940, 0.8912, 0.8698,
        0.5624, 0.5135, 0.5426, 0.7929, 0.5968, 0.5871],
       grad_fn=<IndexBackward0>)
pred_loss tensor([6.2271], grad_fn=<MulBackward0>)
size_loss tensor(-1570.4067, grad_fn=<MulBackward0>)
size_num_loss 2.0
loss: tensor([-1561.3296], grad_fn=<AddBackward0>)
7
epoch:  7 ; loss:  -1561.32958984375 ; pred:  tensor([0.2531, 0.1546, 0.0558, 0.5365], grad_fn=<SoftmaxBackward0>)
num_high 19 len(mask) 43
mask_without_small tensor([0.8917, 0.8832, 0.8711, 0.8657, 0.5033, 0.5689, 0.5286, 0.8864, 0.5480,
        0.5299, 0.5382, 0.5277, 0.8679, 0.8862, 0.5614, 0.5422, 0.8577, 0.5283,
        0.8749, 0.6167, 0.8856, 0.8791, 0.8794, 0.8643, 0.8162, 0.8805, 0.8662,
        0.5118, 0.5651, 0.5208, 0.5336, 0.5776, 0.8976, 0.8774, 0.5456, 0.5260,
        0.8042, 0.5805, 0.5706], grad_fn=<IndexBackward0>)
pred_loss tensor([6.2248], grad_fn=<MulBackward0>)
size_loss tensor(-1660.2638, grad_fn=<MulBackward0>)
size_num_loss 1.9000000000000001
loss: tensor([-1651.3108], grad_fn=<AddBackward0>)
8
epoch:  8 ; loss:  -1651.310791015625 ; pred:  tensor([0.2506, 0.1566, 0.0562, 0.5366], grad_fn=<SoftmaxBackward0>)
num_high 19 len(mask) 43
mask_without_small tensor([0.8989, 0.8909, 0.8795, 0.8744, 0.5498, 0.5092, 0.8939, 0.5287, 0.5105,
        0.5187, 0.5083, 0.8765, 0.8938, 0.5423, 0.5229, 0.8669, 0.5089, 0.8831,
        0.5983, 0.8931, 0.8870, 0.8873, 0.8731, 0.8276, 0.8885, 0.8748, 0.5461,
        0.5015, 0.5142, 0.5586, 0.9044, 0.8855, 0.5264, 0.5068, 0.8164, 0.5616,
        0.5516], grad_fn=<IndexBackward0>)
pred_loss tensor([6.2279], grad_fn=<MulBackward0>)
size_loss tensor(-1775.8469, grad_fn=<MulBackward0>)
size_num_loss 1.9000000000000001
loss: tensor([-1766.9088], grad_fn=<AddBackward0>)
9
epoch:  9 ; loss:  -1766.9088134765625 ; pred:  tensor([0.2478, 0.1590, 0.0568, 0.5364], grad_fn=<SoftmaxBackward0>)
num_high 19 len(mask) 43
mask_without_small tensor([0.9062, 0.8987, 0.8880, 0.8833, 0.5289, 0.9015, 0.5077, 0.8852, 0.9014,
        0.5213, 0.5018, 0.8762, 0.8914, 0.5781, 0.9008, 0.8951, 0.8954, 0.8821,
        0.8393, 0.8966, 0.8837, 0.5253, 0.5378, 0.9114, 0.8937, 0.5053, 0.8289,
        0.5408, 0.5308], grad_fn=<IndexBackward0>)
pred_loss tensor([6.2334], grad_fn=<MulBackward0>)
size_loss tensor(-1751.6927, grad_fn=<MulBackward0>)
size_num_loss 1.9000000000000001
loss: tensor([-1742.7976], grad_fn=<AddBackward0>)
10
epoch:  10 ; loss:  -1742.797607421875 ; pred:  tensor([0.2448, 0.1616, 0.0575, 0.5362], grad_fn=<SoftmaxBackward0>)
num_high 19 len(mask) 43
mask_without_small tensor([0.9134, 0.9064, 0.8964, 0.8920, 0.5076, 0.9090, 0.8938, 0.9089, 0.8854,
        0.8996, 0.5580, 0.9083, 0.9030, 0.9033, 0.8909, 0.8508, 0.9045, 0.8924,
        0.5040, 0.5167, 0.9182, 0.9017, 0.8412, 0.5198, 0.5095],
       grad_fn=<IndexBackward0>)
pred_loss tensor([6.2381], grad_fn=<MulBackward0>)
size_loss tensor(-1652.2734, grad_fn=<MulBackward0>)
size_num_loss 1.9000000000000001
loss: tensor([-1643.4019], grad_fn=<AddBackward0>)
11
epoch:  11 ; loss:  -1643.40185546875 ; pred:  tensor([0.2419, 0.1641, 0.0581, 0.5359], grad_fn=<SoftmaxBackward0>)
num_high 19 len(mask) 43
mask_without_small tensor([0.9202, 0.9138, 0.9045, 0.9003, 0.9162, 0.9020, 0.9161, 0.8941, 0.9074,
        0.5377, 0.9155, 0.9106, 0.9109, 0.8993, 0.8617, 0.9120, 0.9007, 0.9247,
        0.9094, 0.8526], grad_fn=<IndexBackward0>)
pred_loss tensor([6.2439], grad_fn=<MulBackward0>)
size_loss tensor(-837.5809, grad_fn=<MulBackward0>)
size_num_loss 1.9000000000000001
loss: tensor([-828.7363], grad_fn=<AddBackward0>)
12
epoch:  12 ; loss:  -828.7362670898438 ; pred:  tensor([0.2393, 0.1665, 0.0586, 0.5356], grad_fn=<SoftmaxBackward0>)
num_high 19 len(mask) 43
mask_without_small tensor([0.9266, 0.9205, 0.9117, 0.9078, 0.9228, 0.9094, 0.9227, 0.9018, 0.9145,
        0.5203, 0.9222, 0.9175, 0.9178, 0.9068, 0.8690, 0.9189, 0.9081, 0.9308,
        0.9164, 0.8591], grad_fn=<IndexBackward0>)
pred_loss tensor([6.2723], grad_fn=<MulBackward0>)
size_loss tensor(-890.6769, grad_fn=<MulBackward0>)
size_num_loss 1.9000000000000001
loss: tensor([-881.8113], grad_fn=<AddBackward0>)
13
epoch:  13 ; loss:  -881.811279296875 ; pred:  tensor([0.2374, 0.1692, 0.0593, 0.5341], grad_fn=<SoftmaxBackward0>)
num_high 19 len(mask) 43
mask_without_small tensor([0.9324, 0.9267, 0.9183, 0.9145, 0.9289, 0.9160, 0.9288, 0.9086, 0.9210,
        0.5006, 0.9283, 0.9239, 0.9241, 0.9136, 0.8739, 0.9252, 0.9148, 0.9364,
        0.9228, 0.8624], grad_fn=<IndexBackward0>)
pred_loss tensor([6.3062], grad_fn=<MulBackward0>)
size_loss tensor(-948.4365, grad_fn=<MulBackward0>)
size_num_loss 1.9000000000000001
loss: tensor([-939.5441], grad_fn=<AddBackward0>)
14
epoch:  14 ; loss:  -939.5441284179688 ; pred:  tensor([0.2360, 0.1716, 0.0601, 0.5323], grad_fn=<SoftmaxBackward0>)
num_high 19 len(mask) 43
mask_without_small tensor([0.9378, 0.9323, 0.9242, 0.9205, 0.9344, 0.9220, 0.9343, 0.9147, 0.9269,
        0.9338, 0.9296, 0.9298, 0.9196, 0.8771, 0.9309, 0.9209, 0.9415, 0.9287,
        0.8633], grad_fn=<IndexBackward0>)
pred_loss tensor([6.3422], grad_fn=<MulBackward0>)
size_loss tensor(-196.8798, grad_fn=<MulBackward0>)
size_num_loss 1.9000000000000001
loss: tensor([-187.9633], grad_fn=<AddBackward0>)
15
epoch:  15 ; loss:  -187.96334838867188 ; pred:  tensor([0.2350, 0.1739, 0.0608, 0.5303], grad_fn=<SoftmaxBackward0>)
num_high 19 len(mask) 43
mask_without_small tensor([0.9429, 0.9377, 0.9295, 0.9254, 0.9397, 0.9271, 0.9396, 0.9189, 0.9322,
        0.9391, 0.9350, 0.9352, 0.9245, 0.8734, 0.9363, 0.9258, 0.9464, 0.9340,
        0.8581], grad_fn=<IndexBackward0>)
pred_loss tensor([6.4123], grad_fn=<MulBackward0>)
size_loss tensor(-226.0120, grad_fn=<MulBackward0>)
size_num_loss 1.9000000000000001
loss: tensor([-217.0317], grad_fn=<AddBackward0>)
16
epoch:  16 ; loss:  -217.03173828125 ; pred:  tensor([0.2348, 0.1767, 0.0618, 0.5266], grad_fn=<SoftmaxBackward0>)
num_high 19 len(mask) 43
mask_without_small tensor([0.9478, 0.9426, 0.9341, 0.9296, 0.9446, 0.9315, 0.9445, 0.9217, 0.9370,
        0.9440, 0.9399, 0.9401, 0.9285, 0.8672, 0.9413, 0.9300, 0.9511, 0.9389,
        0.8503], grad_fn=<IndexBackward0>)
pred_loss tensor([6.4937], grad_fn=<MulBackward0>)
size_loss tensor(-262.3886, grad_fn=<MulBackward0>)
size_num_loss 1.9000000000000001
loss: tensor([-253.3327], grad_fn=<AddBackward0>)
17
epoch:  17 ; loss:  -253.3326873779297 ; pred:  tensor([0.2350, 0.1796, 0.0630, 0.5224], grad_fn=<SoftmaxBackward0>)
num_high 19 len(mask) 43
mask_without_small tensor([0.9522, 0.9472, 0.9383, 0.9332, 0.9492, 0.9353, 0.9491, 0.9236, 0.9414,
        0.9486, 0.9444, 0.9447, 0.9319, 0.8589, 0.9458, 0.9337, 0.9554, 0.9434,
        0.8404], grad_fn=<IndexBackward0>)
pred_loss tensor([6.5835], grad_fn=<MulBackward0>)
size_loss tensor(-304.0197, grad_fn=<MulBackward0>)
size_num_loss 1.9000000000000001
loss: tensor([-294.8794], grad_fn=<AddBackward0>)
18
epoch:  18 ; loss:  -294.87939453125 ; pred:  tensor([0.2356, 0.1825, 0.0642, 0.5177], grad_fn=<SoftmaxBackward0>)
num_high 19 len(mask) 43
mask_without_small tensor([0.9563, 0.9514, 0.9420, 0.9363, 0.9533, 0.9387, 0.9533, 0.9247, 0.9454,
        0.9528, 0.9486, 0.9489, 0.9348, 0.8488, 0.9501, 0.9369, 0.9593, 0.9476,
        0.8287], grad_fn=<IndexBackward0>)
pred_loss tensor([6.6808], grad_fn=<MulBackward0>)
size_loss tensor(-350.2095, grad_fn=<MulBackward0>)
size_num_loss 1.9000000000000001
loss: tensor([-340.9770], grad_fn=<AddBackward0>)
19
epoch:  19 ; loss:  -340.9769592285156 ; pred:  tensor([0.2363, 0.1855, 0.0655, 0.5127], grad_fn=<SoftmaxBackward0>)
Finished Training
sel masked ver tensor(indices=tensor([[6894, 6949, 7140, 7258, 5505, 5857, 6949, 5935, 5917,
                        5967, 5857, 5857, 5935, 5935, 5857, 5857, 5857, 5505,
                        5505, 5857],
                       [5857, 5857, 5857, 5857, 5917,    0, 5935, 5388, 5857,
                        5857,    0, 5637, 6894, 6949, 6894, 6949, 7140, 7140,
                        7142, 5935]]),
       values=tensor([0.9563, 0.9514, 0.9420, 0.9363, 0.9533, 1.5205, 0.9387,
                      0.9533, 0.9247, 0.9454, 3.8111, 0.9486, 0.9489, 0.9348,
                      0.8488, 0.9501, 0.9369, 0.9593, 0.9476, 0.8287]),
       size=(7259, 7143), nnz=20, layout=torch.sparse_coo,
       grad_fn=<SparseCooTensorWithDimsAndTensorsBackward0>)
Counter({'author': 4, 'publication': 3, 'publishes': 2, 'member': 2, 'projectInfo': 2, 'carriesOut': 1, 'fax': 1, 'phone': 1, 'photo': 1, 'worksAtProject': 1, 'isAbout': 1, 'hasProject': 1})
dict index: {}
node_idx 5857
 node original label [3]
 node predicted label explain 3
 node prediction probability explain tensor([0.2704, 0.2570, 0.0475, 0.4251], grad_fn=<SoftmaxBackward0>)
 node predicted label full 3 most important relations  {'carriesOut': 1, 'publishes': 2, 'fax': 1, 'phone': 1, 'photo': 1, 'worksAtProject': 1, 'publication': 3, 'member': 2, 'isAbout': 1, 'projectInfo': 2, 'author': 4, 'hasProject': 1, 'label': 3, 'node_idx': '5857'}
 final masks and lenght tensor(indices=tensor([[ 23464,  23519,  23710,  23712,  23828,  24462,  39057,
                         39075,  39107,  46930,  46930,  46930,  63383,  88707,
                        114599, 114654, 146780, 154518, 179490, 179902, 179920,
                        179952, 196412, 229552, 237837, 246200, 246200, 254407,
                        254407, 254407, 254407, 254407, 254407, 262340, 262340,
                        262340, 262340, 262340, 262340, 304117, 304117, 328972,
                        328972],
                       [  5857,   5857,   5857,   5857,   5857,   5857,   5505,
                          5505,   5505,   5917,   5935,   5967,   5935,      0,
                          5935,   5935,   5388,   5857,   5857,   5857,   5857,
                          5857,   4559,      0,   5637,   6894,   6949,   6894,
                          6949,   7140,   7142,   7258,   7892,   6894,   6949,
                          7140,   7142,   7258,   7892,   5917,   5935,   5230,
                          5231]]),
       values=tensor([0.9563, 0.9514, 0.9420, 0.3485, 0.9363, 0.3733, 0.3855,
                      0.3829, 0.3738, 0.9533, 0.3766, 0.3929, 0.3750, 1.5205,
                      0.3729, 0.9387, 0.9533, 0.3904, 0.3709, 0.9247, 0.3735,
                      0.9454, 0.4116, 3.8111, 0.9486, 0.9489, 0.9348, 0.8488,
                      0.9501, 0.9369, 0.3835, 0.3824, 0.3681, 0.3785, 0.3950,
                      0.9593, 0.9476, 0.3743, 0.3977, 0.3747, 0.8287, 0.3984,
                      0.3876]),
       size=(753935, 8285), nnz=43, layout=torch.sparse_coo,
       grad_fn=<SparseCooTensorWithDimsAndTensorsBackward0>) 20
 ---------------------------------------------------------------
r_exp.py:102: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  hor_graph, ver_graph = torch.tensor(sub_hor_graph, dtype=torch.float), torch.tensor(sub_ver_graph, dtype=torch.float)
wandb: WARNING Calling wandb.login() after wandb.init() has no effect.
node label: 3
23
num_high 67 len(mask) 67
mask_without_small tensor([0.7913, 0.7785, 0.7605, 0.6539, 0.7535, 0.6871, 0.7296, 0.6732, 0.7048,
        0.7833, 0.7175, 0.6808, 0.7056, 0.7116, 0.7042, 0.7562, 0.7831, 0.7256,
        0.7138, 0.7457, 0.7045, 0.7661, 0.7574, 0.7842, 0.7722, 0.7728, 0.7513,
        0.7739, 0.7231, 0.7325, 0.7224, 0.7592, 0.6815, 0.7005, 0.7234, 0.7853,
        0.7418, 0.7164, 0.7413, 0.7039, 0.6750, 0.7635, 0.7001, 0.7102, 0.6856,
        0.7969, 0.6871, 0.7142, 0.6989, 0.7081, 0.7337, 0.7485, 0.7427, 0.7243,
        0.6938, 0.7722, 0.7360, 0.7388, 0.7314, 0.7453, 0.7501, 0.7087, 0.6499,
        0.7048, 0.8155, 0.7431, 0.7280], grad_fn=<IndexBackward0>)
pred_loss tensor([0.9925], grad_fn=<MulBackward0>)
size_loss tensor(-3.5865, grad_fn=<MulBackward0>)
size_num_loss 6.7
loss: tensor([9.5830], grad_fn=<AddBackward0>)
0
epoch:  0 ; loss:  9.582986831665039 ; pred:  tensor([0.0414, 0.0289, 0.0242, 0.9055], grad_fn=<SoftmaxBackward0>)
num_high 67 len(mask) 67
mask_without_small tensor([0.8074, 0.7953, 0.7783, 0.6309, 0.7716, 0.6652, 0.7094, 0.6508, 0.6835,
        0.7998, 0.6968, 0.6587, 0.6844, 0.6907, 0.6829, 0.7741, 0.7996, 0.7053,
        0.6930, 0.7263, 0.6833, 0.7835, 0.7753, 0.8007, 0.7893, 0.7898, 0.7695,
        0.7909, 0.7027, 0.7124, 0.7019, 0.7771, 0.6594, 0.6791, 0.7030, 0.8017,
        0.7221, 0.6956, 0.7600, 0.6827, 0.6527, 0.7811, 0.6787, 0.6891, 0.6637,
        0.8126, 0.6652, 0.6933, 0.6775, 0.6870, 0.7528, 0.7668, 0.7613, 0.7039,
        0.6721, 0.7893, 0.7161, 0.7191, 0.7113, 0.7638, 0.7684, 0.6876, 0.6269,
        0.6836, 0.8301, 0.7617, 0.7078], grad_fn=<IndexBackward0>)
pred_loss tensor([1.0106], grad_fn=<MulBackward0>)
size_loss tensor(-5.2491, grad_fn=<MulBackward0>)
size_num_loss 6.7
loss: tensor([7.9030], grad_fn=<AddBackward0>)
1
epoch:  1 ; loss:  7.903029441833496 ; pred:  tensor([0.0418, 0.0296, 0.0247, 0.9039], grad_fn=<SoftmaxBackward0>)
num_high 65 len(mask) 67
mask_without_small tensor([0.8222, 0.8110, 0.7950, 0.6075, 0.7881, 0.6426, 0.6887, 0.6279, 0.6615,
        0.8152, 0.6753, 0.6359, 0.6624, 0.6689, 0.6608, 0.7909, 0.8150, 0.6841,
        0.6713, 0.7105, 0.6613, 0.8000, 0.7921, 0.8160, 0.8055, 0.8059, 0.7856,
        0.8070, 0.6814, 0.6920, 0.6807, 0.7938, 0.6367, 0.6569, 0.6817, 0.8169,
        0.7039, 0.6741, 0.7744, 0.6606, 0.6298, 0.7977, 0.6565, 0.6673, 0.6411,
        0.8272, 0.6427, 0.6716, 0.6552, 0.6651, 0.7689, 0.7841, 0.7780, 0.6841,
        0.6497, 0.8054, 0.6961, 0.7006, 0.6912, 0.7783, 0.7853, 0.6657, 0.6034,
        0.6615, 0.8435, 0.7780, 0.6870], grad_fn=<IndexBackward0>)
pred_loss tensor([1.0400], grad_fn=<MulBackward0>)
size_loss tensor(-6.9986, grad_fn=<MulBackward0>)
size_num_loss 6.5
loss: tensor([5.9406], grad_fn=<AddBackward0>)
2
epoch:  2 ; loss:  5.940621376037598 ; pred:  tensor([0.0426, 0.0307, 0.0254, 0.9012], grad_fn=<SoftmaxBackward0>)
num_high 58 len(mask) 67
mask_without_small tensor([0.8358, 0.8255, 0.8109, 0.5838, 0.8040, 0.6194, 0.6670, 0.6044, 0.6387,
        0.8294, 0.6529, 0.6126, 0.6397, 0.6463, 0.6380, 0.8069, 0.8292, 0.6621,
        0.6488, 0.6923, 0.6385, 0.8155, 0.8080, 0.8301, 0.8205, 0.8209, 0.8013,
        0.8219, 0.6593, 0.6706, 0.6585, 0.8097, 0.6134, 0.6340, 0.6597, 0.8310,
        0.6842, 0.6516, 0.7895, 0.6378, 0.6064, 0.8134, 0.6336, 0.6447, 0.6178,
        0.8406, 0.6196, 0.6492, 0.6324, 0.6424, 0.7851, 0.8006, 0.7942, 0.6631,
        0.6267, 0.8205, 0.6751, 0.6806, 0.6701, 0.7934, 0.8015, 0.6431, 0.5796,
        0.6388, 0.8557, 0.7940, 0.6654], grad_fn=<IndexBackward0>)
pred_loss tensor([1.0771], grad_fn=<MulBackward0>)
size_loss tensor(-8.7840, grad_fn=<MulBackward0>)
size_num_loss 5.800000000000001
loss: tensor([3.4435], grad_fn=<AddBackward0>)
3
epoch:  3 ; loss:  3.443502902984619 ; pred:  tensor([0.0436, 0.0321, 0.0264, 0.8979], grad_fn=<SoftmaxBackward0>)
num_high 45 len(mask) 67
mask_without_small tensor([0.8481, 0.8389, 0.8257, 0.5598, 0.8192, 0.5957, 0.6445, 0.5806, 0.6152,
        0.8423, 0.6298, 0.5888, 0.6162, 0.6230, 0.6146, 0.8219, 0.8422, 0.6393,
        0.6255, 0.6724, 0.6150, 0.8299, 0.8230, 0.8430, 0.8344, 0.8348, 0.8164,
        0.8356, 0.6364, 0.6484, 0.6356, 0.8246, 0.5896, 0.6104, 0.6368, 0.8438,
        0.6633, 0.6285, 0.8046, 0.6143, 0.5828, 0.8280, 0.6101, 0.6213, 0.5941,
        0.8529, 0.5961, 0.6259, 0.6089, 0.6190, 0.8008, 0.8161, 0.8098, 0.6410,
        0.6030, 0.8344, 0.6532, 0.6595, 0.6480, 0.8083, 0.8168, 0.6197, 0.5556,
        0.6154, 0.8668, 0.8095, 0.6429], grad_fn=<IndexBackward0>)
pred_loss tensor([1.1226], grad_fn=<MulBackward0>)
size_loss tensor(-10.5759, grad_fn=<MulBackward0>)
size_num_loss 4.5
loss: tensor([0.3419], grad_fn=<AddBackward0>)
4
epoch:  4 ; loss:  0.3419151306152344 ; pred:  tensor([0.0449, 0.0337, 0.0276, 0.8938], grad_fn=<SoftmaxBackward0>)
num_high 33 len(mask) 67
mask_without_small tensor([0.8591, 0.8509, 0.8394, 0.5358, 0.8333, 0.5716, 0.6212, 0.5565, 0.5912,
        0.8539, 0.6060, 0.5647, 0.5922, 0.5991, 0.5905, 0.8359, 0.8538, 0.6157,
        0.6016, 0.6511, 0.5910, 0.8431, 0.8370, 0.8545, 0.8470, 0.8473, 0.8307,
        0.8480, 0.6128, 0.6252, 0.6119, 0.8384, 0.5655, 0.5864, 0.6131, 0.8552,
        0.6412, 0.6046, 0.8192, 0.5903, 0.5589, 0.8414, 0.5860, 0.5974, 0.5700,
        0.8640, 0.5724, 0.6021, 0.5852, 0.5951, 0.8159, 0.8306, 0.8246, 0.6180,
        0.5790, 0.8470, 0.6303, 0.6373, 0.6251, 0.8228, 0.8313, 0.5957, 0.5316,
        0.5915, 0.8767, 0.8242, 0.6196], grad_fn=<IndexBackward0>)
pred_loss tensor([1.1771], grad_fn=<MulBackward0>)
size_loss tensor(-12.3555, grad_fn=<MulBackward0>)
size_num_loss 3.3000000000000003
loss: tensor([-2.6448], grad_fn=<AddBackward0>)
5
epoch:  5 ; loss:  -2.6447930335998535 ; pred:  tensor([0.0465, 0.0355, 0.0291, 0.8890], grad_fn=<SoftmaxBackward0>)
num_high 28 len(mask) 67
mask_without_small tensor([0.8687, 0.8616, 0.8518, 0.5118, 0.8464, 0.5473, 0.5971, 0.5323, 0.5668,
        0.8642, 0.5816, 0.5404, 0.5677, 0.5746, 0.5661, 0.8488, 0.8641, 0.5915,
        0.5772, 0.6288, 0.5665, 0.8550, 0.8497, 0.8647, 0.8583, 0.8585, 0.8439,
        0.8592, 0.5885, 0.6013, 0.5876, 0.8510, 0.5412, 0.5619, 0.5888, 0.8653,
        0.6182, 0.5802, 0.8330, 0.5659, 0.5350, 0.8534, 0.5615, 0.5729, 0.5456,
        0.8740, 0.5487, 0.5778, 0.5612, 0.5707, 0.8302, 0.8442, 0.8384, 0.5942,
        0.5546, 0.8584, 0.6066, 0.6141, 0.6013, 0.8364, 0.8447, 0.5713, 0.5076,
        0.5674, 0.8856, 0.8380, 0.5955], grad_fn=<IndexBackward0>)
pred_loss tensor([1.2406], grad_fn=<MulBackward0>)
size_loss tensor(-14.1081, grad_fn=<MulBackward0>)
size_num_loss 2.8000000000000003
loss: tensor([-4.9015], grad_fn=<AddBackward0>)
6
epoch:  6 ; loss:  -4.901543617248535 ; pred:  tensor([0.0483, 0.0376, 0.0307, 0.8833], grad_fn=<SoftmaxBackward0>)
num_high 27 len(mask) 67
mask_without_small tensor([0.8770, 0.8709, 0.8628, 0.8581, 0.5228, 0.5725, 0.5080, 0.5420, 0.8731,
        0.5567, 0.5160, 0.5430, 0.5498, 0.5413, 0.8603, 0.8730, 0.5667, 0.5523,
        0.6055, 0.5417, 0.8655, 0.8611, 0.8736, 0.8682, 0.8684, 0.8559, 0.8689,
        0.5637, 0.5768, 0.5628, 0.8622, 0.5168, 0.5372, 0.5640, 0.8741, 0.5943,
        0.5554, 0.8460, 0.5411, 0.5112, 0.8640, 0.5366, 0.5480, 0.5211, 0.8830,
        0.5250, 0.5532, 0.5373, 0.5461, 0.8437, 0.8567, 0.8514, 0.5699, 0.5300,
        0.8684, 0.5822, 0.5901, 0.5769, 0.8491, 0.8571, 0.5465, 0.5432, 0.8935,
        0.8508, 0.5709], grad_fn=<IndexBackward0>)
pred_loss tensor([1.3132], grad_fn=<MulBackward0>)
size_loss tensor(-1570.4175, grad_fn=<MulBackward0>)
size_num_loss 2.7
loss: tensor([-1565.3917], grad_fn=<AddBackward0>)
7
epoch:  7 ; loss:  -1565.3917236328125 ; pred:  tensor([0.0505, 0.0400, 0.0326, 0.8769], grad_fn=<SoftmaxBackward0>)
num_high 27 len(mask) 67
mask_without_small tensor([0.8824, 0.8765, 0.8687, 0.8642, 0.5097, 0.5597, 0.5290, 0.8787, 0.5439,
        0.5030, 0.5300, 0.5369, 0.5284, 0.8662, 0.8786, 0.5538, 0.5395, 0.5931,
        0.5288, 0.8712, 0.8670, 0.8791, 0.8738, 0.8741, 0.8620, 0.8746, 0.5509,
        0.5641, 0.5500, 0.8680, 0.5037, 0.5242, 0.5513, 0.8796, 0.5818, 0.5425,
        0.8525, 0.5283, 0.8699, 0.5236, 0.5351, 0.5080, 0.8882, 0.5123, 0.5406,
        0.5246, 0.5334, 0.8503, 0.8629, 0.8577, 0.5575, 0.5170, 0.8740, 0.5696,
        0.5776, 0.5643, 0.8555, 0.8632, 0.5336, 0.5306, 0.8983, 0.8572, 0.5583],
       grad_fn=<IndexBackward0>)
pred_loss tensor([1.3527], grad_fn=<MulBackward0>)
size_loss tensor(-1657.0713, grad_fn=<MulBackward0>)
size_num_loss 2.7
loss: tensor([-1652.0225], grad_fn=<AddBackward0>)
8
epoch:  8 ; loss:  -1652.0224609375 ; pred:  tensor([0.0516, 0.0413, 0.0337, 0.8735], grad_fn=<SoftmaxBackward0>)
num_high 27 len(mask) 67
mask_without_small tensor([0.8891, 0.8835, 0.8761, 0.8717, 0.5432, 0.5123, 0.8856, 0.5272, 0.5132,
        0.5202, 0.5116, 0.8737, 0.8855, 0.5371, 0.5227, 0.5768, 0.5120, 0.8785,
        0.8744, 0.8860, 0.8810, 0.8812, 0.8696, 0.8817, 0.5342, 0.5475, 0.5333,
        0.8754, 0.5074, 0.5346, 0.8864, 0.5654, 0.5258, 0.8606, 0.5115, 0.8772,
        0.5067, 0.5183, 0.8947, 0.5240, 0.5080, 0.5168, 0.8586, 0.8706, 0.8656,
        0.5411, 0.5002, 0.8812, 0.5531, 0.5612, 0.5479, 0.8634, 0.8709, 0.5169,
        0.5140, 0.9043, 0.8651, 0.5417], grad_fn=<IndexBackward0>)
pred_loss tensor([1.4054], grad_fn=<MulBackward0>)
size_loss tensor(-1762.4199, grad_fn=<MulBackward0>)
size_num_loss 2.7
loss: tensor([-1757.3516], grad_fn=<AddBackward0>)
9
epoch:  9 ; loss:  -1757.3515625 ; pred:  tensor([0.0530, 0.0431, 0.0350, 0.8689], grad_fn=<SoftmaxBackward0>)
num_high 27 len(mask) 67
mask_without_small tensor([0.8964, 0.8911, 0.8841, 0.8800, 0.5240, 0.8931, 0.5080, 0.5009, 0.8819,
        0.8930, 0.5179, 0.5035, 0.5581, 0.8864, 0.8826, 0.8934, 0.8887, 0.8889,
        0.8780, 0.8894, 0.5151, 0.5285, 0.5142, 0.8835, 0.5154, 0.8939, 0.5465,
        0.5066, 0.8695, 0.8851, 0.9017, 0.5048, 0.8677, 0.8790, 0.8743, 0.5221,
        0.8889, 0.5341, 0.5423, 0.5289, 0.8722, 0.8792, 0.9108, 0.8738, 0.5226],
       grad_fn=<IndexBackward0>)
pred_loss tensor([1.4658], grad_fn=<MulBackward0>)
size_loss tensor(-1805.5061, grad_fn=<MulBackward0>)
size_num_loss 2.7
loss: tensor([-1800.4510], grad_fn=<AddBackward0>)
10
epoch:  10 ; loss:  -1800.4510498046875 ; pred:  tensor([0.0547, 0.0451, 0.0366, 0.8637], grad_fn=<SoftmaxBackward0>)
num_high 27 len(mask) 67
mask_without_small tensor([0.9039, 0.8989, 0.8923, 0.8885, 0.5039, 0.9007, 0.8902, 0.9006, 0.5386,
        0.8945, 0.8909, 0.9011, 0.8967, 0.8968, 0.8866, 0.8973, 0.5084, 0.8917,
        0.9015, 0.5268, 0.8786, 0.8933, 0.9089, 0.8769, 0.8876, 0.8831, 0.5020,
        0.8968, 0.5141, 0.5225, 0.5088, 0.8811, 0.8877, 0.9173, 0.8826, 0.5025],
       grad_fn=<IndexBackward0>)
pred_loss tensor([1.5225], grad_fn=<MulBackward0>)
size_loss tensor(-1669.0339, grad_fn=<MulBackward0>)
size_num_loss 2.7
loss: tensor([-1663.9747], grad_fn=<AddBackward0>)
11
epoch:  11 ; loss:  -1663.9747314453125 ; pred:  tensor([0.0562, 0.0470, 0.0380, 0.8588], grad_fn=<SoftmaxBackward0>)
num_high 27 len(mask) 67
mask_without_small tensor([0.9111, 0.9065, 0.9004, 0.8968, 0.9082, 0.8984, 0.9081, 0.5191, 0.9024,
        0.8990, 0.9085, 0.9044, 0.9046, 0.8950, 0.9050, 0.8998, 0.9089, 0.5070,
        0.8875, 0.9012, 0.9158, 0.8859, 0.8959, 0.8917, 0.9046, 0.5026, 0.8899,
        0.8961, 0.9237, 0.8913], grad_fn=<IndexBackward0>)
pred_loss tensor([1.5746], grad_fn=<MulBackward0>)
size_loss tensor(-1198.9072, grad_fn=<MulBackward0>)
size_num_loss 2.7
loss: tensor([-1193.8326], grad_fn=<AddBackward0>)
12
epoch:  12 ; loss:  -1193.8326416015625 ; pred:  tensor([0.0576, 0.0488, 0.0394, 0.8543], grad_fn=<SoftmaxBackward0>)
num_high 27 len(mask) 67
mask_without_small tensor([0.9180, 0.9137, 0.9079, 0.9045, 0.9153, 0.9060, 0.9152, 0.5003, 0.9098,
        0.9066, 0.9156, 0.9117, 0.9119, 0.9028, 0.9122, 0.9074, 0.9159, 0.8957,
        0.9087, 0.9224, 0.8942, 0.9037, 0.8997, 0.9119, 0.8979, 0.9038, 0.9298,
        0.8993], grad_fn=<IndexBackward0>)
pred_loss tensor([1.6211], grad_fn=<MulBackward0>)
size_loss tensor(-776.5504, grad_fn=<MulBackward0>)
size_num_loss 2.7
loss: tensor([-771.4457], grad_fn=<AddBackward0>)
13
epoch:  13 ; loss:  -771.4456787109375 ; pred:  tensor([0.0588, 0.0504, 0.0405, 0.8504], grad_fn=<SoftmaxBackward0>)
num_high 27 len(mask) 67
mask_without_small tensor([0.9244, 0.9202, 0.9146, 0.9113, 0.9217, 0.9128, 0.9217, 0.9164, 0.9134,
        0.9220, 0.9183, 0.9185, 0.9097, 0.9188, 0.9141, 0.9224, 0.9026, 0.9154,
        0.9285, 0.9012, 0.9106, 0.9067, 0.9185, 0.9049, 0.9107, 0.9355, 0.9062],
       grad_fn=<IndexBackward0>)
pred_loss tensor([1.6640], grad_fn=<MulBackward0>)
size_loss tensor(-79.2687, grad_fn=<MulBackward0>)
size_num_loss 2.7
loss: tensor([-74.1321], grad_fn=<AddBackward0>)
14
epoch:  14 ; loss:  -74.13208770751953 ; pred:  tensor([0.0598, 0.0518, 0.0416, 0.8467], grad_fn=<SoftmaxBackward0>)
num_high 27 len(mask) 67
mask_without_small tensor([0.9306, 0.9266, 0.9200, 0.9154, 0.9281, 0.9175, 0.9280, 0.9223, 0.9183,
        0.9284, 0.9245, 0.9247, 0.9130, 0.9251, 0.9193, 0.9287, 0.9030, 0.9211,
        0.9344, 0.9011, 0.9144, 0.9087, 0.9247, 0.9061, 0.9145, 0.9406, 0.9080],
       grad_fn=<IndexBackward0>)
pred_loss tensor([1.7198], grad_fn=<MulBackward0>)
size_loss tensor(-96.2177, grad_fn=<MulBackward0>)
size_num_loss 2.7
loss: tensor([-91.0305], grad_fn=<AddBackward0>)
15
epoch:  15 ; loss:  -91.0305404663086 ; pred:  tensor([0.0612, 0.0538, 0.0430, 0.8420], grad_fn=<SoftmaxBackward0>)
num_high 27 len(mask) 67
mask_without_small tensor([0.9366, 0.9326, 0.9245, 0.9176, 0.9342, 0.9209, 0.9341, 0.9277, 0.9220,
        0.9345, 0.9304, 0.9306, 0.9140, 0.9310, 0.9236, 0.9348, 0.9005, 0.9260,
        0.9401, 0.8981, 0.9162, 0.9079, 0.9306, 0.9044, 0.9163, 0.9456, 0.9070],
       grad_fn=<IndexBackward0>)
pred_loss tensor([1.7796], grad_fn=<MulBackward0>)
size_loss tensor(-124.2693, grad_fn=<MulBackward0>)
size_num_loss 2.7
loss: tensor([-119.0270], grad_fn=<AddBackward0>)
16
epoch:  16 ; loss:  -119.02698516845703 ; pred:  tensor([0.0627, 0.0559, 0.0444, 0.8370], grad_fn=<SoftmaxBackward0>)
num_high 27 len(mask) 67
mask_without_small tensor([0.9423, 0.9384, 0.9285, 0.9185, 0.9400, 0.9232, 0.9399, 0.9327, 0.9250,
        0.9403, 0.9360, 0.9362, 0.9134, 0.9367, 0.9272, 0.9406, 0.8959, 0.9305,
        0.9455, 0.8931, 0.9165, 0.9051, 0.9362, 0.9007, 0.9166, 0.9504, 0.9040],
       grad_fn=<IndexBackward0>)
pred_loss tensor([1.8417], grad_fn=<MulBackward0>)
size_loss tensor(-159.7756, grad_fn=<MulBackward0>)
size_num_loss 2.7
loss: tensor([-154.4754], grad_fn=<AddBackward0>)
17
epoch:  17 ; loss:  -154.4754180908203 ; pred:  tensor([0.0642, 0.0581, 0.0458, 0.8318], grad_fn=<SoftmaxBackward0>)
num_high 27 len(mask) 67
mask_without_small tensor([0.9476, 0.9438, 0.9322, 0.9182, 0.9454, 0.9249, 0.9453, 0.9375, 0.9273,
        0.9457, 0.9412, 0.9415, 0.9112, 0.9420, 0.9304, 0.9460, 0.8898, 0.9348,
        0.9504, 0.8866, 0.9155, 0.9007, 0.9415, 0.8954, 0.9156, 0.9548, 0.8993],
       grad_fn=<IndexBackward0>)
pred_loss tensor([1.9058], grad_fn=<MulBackward0>)
size_loss tensor(-200.8902, grad_fn=<MulBackward0>)
size_num_loss 2.7
loss: tensor([-195.5299], grad_fn=<AddBackward0>)
18
epoch:  18 ; loss:  -195.5299072265625 ; pred:  tensor([0.0658, 0.0604, 0.0473, 0.8265], grad_fn=<SoftmaxBackward0>)
num_high 27 len(mask) 67
mask_without_small tensor([0.9524, 0.9489, 0.9357, 0.9169, 0.9504, 0.9261, 0.9503, 0.9421, 0.9293,
        0.9506, 0.9462, 0.9465, 0.9078, 0.9470, 0.9334, 0.9509, 0.8822, 0.9389,
        0.9550, 0.8785, 0.9133, 0.8949, 0.9465, 0.8886, 0.9136, 0.9589, 0.8932],
       grad_fn=<IndexBackward0>)
pred_loss tensor([1.9720], grad_fn=<MulBackward0>)
size_loss tensor(-246.7382, grad_fn=<MulBackward0>)
size_num_loss 2.7
loss: tensor([-241.3151], grad_fn=<AddBackward0>)
19
epoch:  19 ; loss:  -241.3151397705078 ; pred:  tensor([0.0673, 0.0629, 0.0488, 0.8210], grad_fn=<SoftmaxBackward0>)
Finished Training
sel masked ver tensor(indices=tensor([[6912, 6915, 6916, 6996, 5917, 5466, 5470, 5935, 6912,
                        6915, 6956, 6996, 6996, 6996, 7064, 5429, 5470, 5935,
                        5752, 5752, 5752, 5752, 5505, 5505, 5505, 5752, 5752],
                       [5752, 5752, 5752, 5752, 5505, 5935, 5935, 5470, 5470,
                        5470, 5470, 5388, 5429, 5455, 5466, 5752, 5752, 5752,
                        6912, 7044, 7064, 7859, 6915, 7044, 7064, 5935, 5230]]),
       values=tensor([0.9524, 0.9489, 0.9357, 0.9169, 0.9504, 0.9261, 0.9503,
                      0.9421, 0.9293, 0.9506, 0.9462, 0.9465, 0.9078, 0.9470,
                      0.9334, 0.9509, 0.8822, 0.9389, 0.9550, 0.8785, 0.9133,
                      0.8949, 0.9465, 0.8886, 0.9136, 0.9589, 0.8932]),
       size=(7065, 7860), nnz=27, layout=torch.sparse_coo,
       grad_fn=<SparseCooTensorWithDimsAndTensorsBackward0>)
Counter({'isAbout': 8, 'publication': 4, 'author': 4, 'publishes': 3, 'isWorkedOnBy': 2, 'dealtWithIn': 2, 'type': 1, 'worksAtProject': 1, 'carriedOutBy': 1, 'member': 1})
dict index: {}
node_idx 5752
 node original label [3]
 node predicted label explain 3
 node prediction probability explain tensor([0.0673, 0.0629, 0.0488, 0.8210], grad_fn=<SoftmaxBackward0>)
 node predicted label full 3 most important relations  {'isWorkedOnBy': 2, 'dealtWithIn': 2, 'publishes': 3, 'type': 1, 'worksAtProject': 1, 'publication': 4, 'carriedOutBy': 1, 'isAbout': 8, 'member': 1, 'author': 4, 'label': 3, 'node_idx': '5752'}
 final masks and lenght tensor(indices=tensor([[ 23482,  23485,  23486,  23526,  23566,  23614,  23634,
                         24429,  24430,  39057,  39075,  46930,  46930,  63383,
                         63424,  63461,  63465,  88602, 146780, 146780, 146780,
                        146780, 147757, 147760, 147801, 147841, 147841, 147841,
                        147841, 147889, 147909, 147909, 147909, 148705, 154518,
                        154559, 154585, 154596, 154600, 179490, 179902, 179920,
                        196307, 229447, 237732, 254302, 254302, 254302, 254302,
                        254302, 254302, 254302, 254302, 254302, 262340, 262340,
                        262340, 262340, 262340, 262340, 262340, 262340, 262340,
                        304012, 304012, 328867, 328867],
                       [  5752,   5752,   5752,   5752,   5752,   5752,   5752,
                          5752,   5752,   5505,   5505,   5917,   5935,   5935,
                          5935,   5935,   5935,      0,   5388,   5429,   5466,
                          5470,   5470,   5470,   5470,   5388,   5429,   5455,
                          5470,   5470,   5429,   5466,   5470,   5470,   5752,
                          5752,   5752,   5752,   5752,   5752,   5752,   5752,
                          1279,     77,   5559,   6912,   6915,   6916,   6956,
                          6996,   7044,   7064,   7859,   7860,   6912,   6915,
                          6916,   6956,   6996,   7044,   7064,   7859,   7860,
                          5917,   5935,   5230,   5231]]),
       values=tensor([0.9524, 0.9489, 0.9357, 0.3557, 0.9169, 0.3961, 0.3893,
                      0.4165, 0.3881, 0.9504, 0.3845, 0.3897, 0.3891, 0.3775,
                      0.3875, 0.9261, 0.9503, 0.3940, 0.3801, 0.4103, 0.3879,
                      0.9421, 0.9293, 0.9506, 0.9462, 0.9465, 0.9078, 0.9470,
                      0.3916, 0.3940, 0.3907, 0.9334, 0.3904, 0.3835, 0.3920,
                      0.9509, 0.4070, 0.3831, 0.8822, 0.3879, 0.4226, 0.9389,
                      0.3819, 0.3937, 0.3943, 0.9550, 0.4017, 0.3824, 0.3863,
                      0.3942, 0.8785, 0.9133, 0.8949, 0.3881, 0.3767, 0.9465,
                      0.3999, 0.4025, 0.3948, 0.8886, 0.9136, 0.3928, 0.3519,
                      0.3921, 0.9589, 0.8932, 0.3881]),
       size=(753935, 8285), nnz=67, layout=torch.sparse_coo,
       grad_fn=<SparseCooTensorWithDimsAndTensorsBackward0>) 27
 ---------------------------------------------------------------
r_exp.py:102: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  hor_graph, ver_graph = torch.tensor(sub_hor_graph, dtype=torch.float), torch.tensor(sub_ver_graph, dtype=torch.float)
wandb: WARNING Calling wandb.login() after wandb.init() has no effect.
node label: 3
14
num_high 31 len(mask) 32
mask_without_small tensor([0.8148, 0.7977, 0.7730, 0.6162, 0.7631, 0.6663, 0.7289, 0.6454, 0.6925,
        0.8041, 0.7113, 0.6568, 0.6938, 0.7027, 0.6916, 0.7669, 0.8039, 0.7231,
        0.7059, 0.7521, 0.6922, 0.7807, 0.7686, 0.8054, 0.7891, 0.7899, 0.7600,
        0.7914, 0.7195, 0.7331, 0.7185, 0.7712], grad_fn=<IndexBackward0>)
pred_loss tensor([4.6748], grad_fn=<MulBackward0>)
size_loss tensor(-5.2526, grad_fn=<MulBackward0>)
size_num_loss 3.1
loss: tensor([5.4530], grad_fn=<AddBackward0>)
0
epoch:  0 ; loss:  5.4529619216918945 ; pred:  tensor([0.1309, 0.1300, 0.1125, 0.6266], grad_fn=<SoftmaxBackward0>)
num_high 30 len(mask) 32
mask_without_small tensor([0.8294, 0.8133, 0.7900, 0.5923, 0.7807, 0.6437, 0.7087, 0.6222, 0.6708,
        0.8194, 0.6904, 0.6339, 0.6722, 0.6814, 0.6699, 0.7843, 0.8191, 0.7027,
        0.6847, 0.7703, 0.6705, 0.7973, 0.7859, 0.8206, 0.8053, 0.8060, 0.7778,
        0.8075, 0.6989, 0.7522, 0.6979, 0.7883], grad_fn=<IndexBackward0>)
pred_loss tensor([4.3758], grad_fn=<MulBackward0>)
size_loss tensor(-6.9977, grad_fn=<MulBackward0>)
size_num_loss 3.0
loss: tensor([3.3007], grad_fn=<AddBackward0>)
1
epoch:  1 ; loss:  3.3007094860076904 ; pred:  tensor([0.1251, 0.1227, 0.1066, 0.6456], grad_fn=<SoftmaxBackward0>)
num_high 28 len(mask) 32
mask_without_small tensor([0.8430, 0.8279, 0.8062, 0.5681, 0.7972, 0.6204, 0.6880, 0.5984, 0.6484,
        0.8336, 0.6686, 0.6104, 0.6497, 0.6593, 0.6474, 0.8007, 0.8334, 0.6814,
        0.6627, 0.7860, 0.6480, 0.8130, 0.8022, 0.8347, 0.8205, 0.8211, 0.7946,
        0.8225, 0.6777, 0.7704, 0.6786, 0.8046], grad_fn=<IndexBackward0>)
pred_loss tensor([4.1009], grad_fn=<MulBackward0>)
size_loss tensor(-8.8019, grad_fn=<MulBackward0>)
size_num_loss 2.8000000000000003
loss: tensor([1.0099], grad_fn=<AddBackward0>)
2
epoch:  2 ; loss:  1.0098953247070312 ; pred:  tensor([0.1195, 0.1158, 0.1011, 0.6636], grad_fn=<SoftmaxBackward0>)
num_high 28 len(mask) 32
mask_without_small tensor([0.8554, 0.8415, 0.8213, 0.5436, 0.8129, 0.5966, 0.6664, 0.5743, 0.6252,
        0.8467, 0.6460, 0.5865, 0.6266, 0.6364, 0.6242, 0.8162, 0.8465, 0.6592,
        0.6400, 0.8015, 0.6248, 0.8276, 0.8176, 0.8478, 0.8346, 0.8352, 0.8105,
        0.8364, 0.6556, 0.7877, 0.6580, 0.8198], grad_fn=<IndexBackward0>)
pred_loss tensor([3.8529], grad_fn=<MulBackward0>)
size_loss tensor(-10.6287, grad_fn=<MulBackward0>)
size_num_loss 2.8000000000000003
loss: tensor([-1.0803], grad_fn=<AddBackward0>)
3
epoch:  3 ; loss:  -1.0803241729736328 ; pred:  tensor([0.1143, 0.1095, 0.0960, 0.6803], grad_fn=<SoftmaxBackward0>)
num_high 22 len(mask) 32
mask_without_small tensor([0.8666, 0.8539, 0.8355, 0.5190, 0.8277, 0.5724, 0.6440, 0.5498, 0.6014,
        0.8587, 0.6227, 0.5621, 0.6028, 0.6128, 0.6003, 0.8308, 0.8585, 0.6363,
        0.6165, 0.8164, 0.6010, 0.8412, 0.8320, 0.8598, 0.8476, 0.8482, 0.8254,
        0.8494, 0.6328, 0.8040, 0.6363, 0.8340], grad_fn=<IndexBackward0>)
pred_loss tensor([3.6316], grad_fn=<MulBackward0>)
size_loss tensor(-12.4569, grad_fn=<MulBackward0>)
size_num_loss 2.2
loss: tensor([-3.7485], grad_fn=<AddBackward0>)
4
epoch:  4 ; loss:  -3.748518705368042 ; pred:  tensor([0.1096, 0.1037, 0.0912, 0.6955], grad_fn=<SoftmaxBackward0>)
num_high 17 len(mask) 32
mask_without_small tensor([0.8768, 0.8652, 0.8487, 0.8415, 0.5478, 0.6208, 0.5252, 0.5770, 0.8696,
        0.5987, 0.5375, 0.5784, 0.5886, 0.5759, 0.8444, 0.8694, 0.6126, 0.5923,
        0.8307, 0.5766, 0.8537, 0.8454, 0.8707, 0.8597, 0.8601, 0.8393, 0.8612,
        0.6091, 0.8193, 0.6136, 0.8472], grad_fn=<IndexBackward0>)
pred_loss tensor([3.4343], grad_fn=<MulBackward0>)
size_loss tensor(-1387.3174, grad_fn=<MulBackward0>)
size_num_loss 1.7000000000000002
loss: tensor([-1381.4171], grad_fn=<AddBackward0>)
5
epoch:  5 ; loss:  -1381.4171142578125 ; pred:  tensor([0.1053, 0.0984, 0.0869, 0.7093], grad_fn=<SoftmaxBackward0>)
num_high 17 len(mask) 32
mask_without_small tensor([0.8825, 0.8714, 0.8554, 0.8485, 0.5344, 0.6081, 0.5117, 0.5637, 0.8756,
        0.5857, 0.5240, 0.5652, 0.5755, 0.5627, 0.8513, 0.8754, 0.5996, 0.5793,
        0.8380, 0.5634, 0.8606, 0.8524, 0.8768, 0.8662, 0.8666, 0.8465, 0.8676,
        0.5963, 0.8275, 0.6010, 0.8541], grad_fn=<IndexBackward0>)
pred_loss tensor([3.3312], grad_fn=<MulBackward0>)
size_loss tensor(-1486.0503, grad_fn=<MulBackward0>)
size_num_loss 1.7000000000000002
loss: tensor([-1480.2584], grad_fn=<AddBackward0>)
6
epoch:  6 ; loss:  -1480.2584228515625 ; pred:  tensor([0.1030, 0.0957, 0.0847, 0.7167], grad_fn=<SoftmaxBackward0>)
num_high 17 len(mask) 32
mask_without_small tensor([0.8895, 0.8789, 0.8637, 0.8571, 0.5171, 0.5915, 0.5466, 0.8829, 0.5689,
        0.5067, 0.5482, 0.5585, 0.5457, 0.8598, 0.8827, 0.5829, 0.5623, 0.8471,
        0.5463, 0.8689, 0.8609, 0.8841, 0.8741, 0.8744, 0.8552, 0.8753, 0.5797,
        0.8374, 0.5845, 0.8626], grad_fn=<IndexBackward0>)
pred_loss tensor([3.2104], grad_fn=<MulBackward0>)
size_loss tensor(-1579.9724, grad_fn=<MulBackward0>)
size_num_loss 1.7000000000000002
loss: tensor([-1574.3136], grad_fn=<AddBackward0>)
7
epoch:  7 ; loss:  -1574.3135986328125 ; pred:  tensor([0.1003, 0.0924, 0.0820, 0.7254], grad_fn=<SoftmaxBackward0>)
num_high 17 len(mask) 32
mask_without_small tensor([0.8969, 0.8870, 0.8726, 0.8664, 0.5724, 0.5270, 0.8907, 0.5496, 0.5287,
        0.5391, 0.5261, 0.8689, 0.8906, 0.5636, 0.5429, 0.8569, 0.5268, 0.8776,
        0.8700, 0.8919, 0.8825, 0.8827, 0.8647, 0.8836, 0.5605, 0.8480, 0.5654,
        0.8716], grad_fn=<IndexBackward0>)
pred_loss tensor([3.0868], grad_fn=<MulBackward0>)
size_loss tensor(-1652.7782, grad_fn=<MulBackward0>)
size_num_loss 1.7000000000000002
loss: tensor([-1647.2612], grad_fn=<AddBackward0>)
8
epoch:  8 ; loss:  -1647.26123046875 ; pred:  tensor([0.0974, 0.0889, 0.0792, 0.7344], grad_fn=<SoftmaxBackward0>)
num_high 17 len(mask) 32
mask_without_small tensor([0.9044, 0.8951, 0.8817, 0.8758, 0.5516, 0.5058, 0.8986, 0.5284, 0.5074,
        0.5179, 0.5049, 0.8782, 0.8985, 0.5426, 0.5218, 0.8669, 0.5055, 0.8865,
        0.8792, 0.8997, 0.8909, 0.8912, 0.8742, 0.8920, 0.5395, 0.8587, 0.5445,
        0.8807], grad_fn=<IndexBackward0>)
pred_loss tensor([2.9657], grad_fn=<MulBackward0>)
size_loss tensor(-1800.7517, grad_fn=<MulBackward0>)
size_num_loss 1.7000000000000002
loss: tensor([-1795.3652], grad_fn=<AddBackward0>)
9
epoch:  9 ; loss:  -1795.365234375 ; pred:  tensor([0.0946, 0.0855, 0.0765, 0.7434], grad_fn=<SoftmaxBackward0>)
num_high 17 len(mask) 32
mask_without_small tensor([0.9117, 0.9031, 0.8906, 0.8851, 0.5293, 0.9063, 0.5060, 0.8873, 0.9062,
        0.5202, 0.8768, 0.8951, 0.8883, 0.9074, 0.8992, 0.8994, 0.8836, 0.9002,
        0.5171, 0.8692, 0.5222, 0.8897], grad_fn=<IndexBackward0>)
pred_loss tensor([2.8506], grad_fn=<MulBackward0>)
size_loss tensor(-1612.7562, grad_fn=<MulBackward0>)
size_num_loss 1.7000000000000002
loss: tensor([-1607.5247], grad_fn=<AddBackward0>)
10
epoch:  10 ; loss:  -1607.524658203125 ; pred:  tensor([0.0919, 0.0822, 0.0740, 0.7520], grad_fn=<SoftmaxBackward0>)
num_high 17 len(mask) 32
mask_without_small tensor([0.9187, 0.9107, 0.8990, 0.8939, 0.5070, 0.9137, 0.8959, 0.9135, 0.8861,
        0.9033, 0.8969, 0.9147, 0.9071, 0.9072, 0.8925, 0.9080, 0.8792, 0.8982],
       grad_fn=<IndexBackward0>)
pred_loss tensor([2.7461], grad_fn=<MulBackward0>)
size_loss tensor(-937.6890, grad_fn=<MulBackward0>)
size_num_loss 1.7000000000000002
loss: tensor([-932.5915], grad_fn=<AddBackward0>)
11
epoch:  11 ; loss:  -932.5914916992188 ; pred:  tensor([0.0894, 0.0792, 0.0715, 0.7599], grad_fn=<SoftmaxBackward0>)
num_high 17 len(mask) 32
mask_without_small tensor([0.9251, 0.9176, 0.9066, 0.9017, 0.9204, 0.9036, 0.9203, 0.8942, 0.9107,
        0.9046, 0.9214, 0.9143, 0.9144, 0.9004, 0.9151, 0.8875, 0.9059],
       grad_fn=<IndexBackward0>)
pred_loss tensor([2.6564], grad_fn=<MulBackward0>)
size_loss tensor(-103.1739, grad_fn=<MulBackward0>)
size_num_loss 1.7000000000000002
loss: tensor([-98.1798], grad_fn=<AddBackward0>)
12
epoch:  12 ; loss:  -98.17980194091797 ; pred:  tensor([0.0873, 0.0765, 0.0695, 0.7667], grad_fn=<SoftmaxBackward0>)
num_high 17 len(mask) 32
mask_without_small tensor([0.9313, 0.9243, 0.9121, 0.9055, 0.9270, 0.9082, 0.9269, 0.8952, 0.9172,
        0.9095, 0.9279, 0.9210, 0.9211, 0.9037, 0.9218, 0.8868, 0.9112],
       grad_fn=<IndexBackward0>)
pred_loss tensor([2.6093], grad_fn=<MulBackward0>)
size_loss tensor(-123.2984, grad_fn=<MulBackward0>)
size_num_loss 1.7000000000000002
loss: tensor([-118.3586], grad_fn=<AddBackward0>)
13
epoch:  13 ; loss:  -118.35858917236328 ; pred:  tensor([0.0861, 0.0750, 0.0685, 0.7703], grad_fn=<SoftmaxBackward0>)
num_high 17 len(mask) 32
mask_without_small tensor([0.9372, 0.9307, 0.9162, 0.9068, 0.9333, 0.9107, 0.9332, 0.8931, 0.9230,
        0.9126, 0.9341, 0.9274, 0.9275, 0.9044, 0.9282, 0.8829, 0.9150],
       grad_fn=<IndexBackward0>)
pred_loss tensor([2.5803], grad_fn=<MulBackward0>)
size_loss tensor(-154.4658, grad_fn=<MulBackward0>)
size_num_loss 1.7000000000000002
loss: tensor([-149.5612], grad_fn=<AddBackward0>)
14
epoch:  14 ; loss:  -149.56121826171875 ; pred:  tensor([0.0854, 0.0740, 0.0679, 0.7726], grad_fn=<SoftmaxBackward0>)
num_high 17 len(mask) 32
mask_without_small tensor([0.9428, 0.9367, 0.9195, 0.9063, 0.9392, 0.9116, 0.9391, 0.8888, 0.9284,
        0.9143, 0.9400, 0.9333, 0.9334, 0.9030, 0.9342, 0.8769, 0.9178],
       grad_fn=<IndexBackward0>)
pred_loss tensor([2.5639], grad_fn=<MulBackward0>)
size_loss tensor(-192.6183, grad_fn=<MulBackward0>)
size_num_loss 1.7000000000000002
loss: tensor([-187.7356], grad_fn=<AddBackward0>)
15
epoch:  15 ; loss:  -187.73556518554688 ; pred:  tensor([0.0850, 0.0734, 0.0677, 0.7738], grad_fn=<SoftmaxBackward0>)
num_high 17 len(mask) 32
mask_without_small tensor([0.9480, 0.9423, 0.9221, 0.9043, 0.9446, 0.9114, 0.9445, 0.8828, 0.9334,
        0.9151, 0.9454, 0.9389, 0.9390, 0.9001, 0.9398, 0.8692, 0.9198],
       grad_fn=<IndexBackward0>)
pred_loss tensor([2.5580], grad_fn=<MulBackward0>)
size_loss tensor(-235.9829, grad_fn=<MulBackward0>)
size_num_loss 1.7000000000000002
loss: tensor([-231.1111], grad_fn=<AddBackward0>)
16
epoch:  16 ; loss:  -231.11105346679688 ; pred:  tensor([0.0849, 0.0730, 0.0678, 0.7743], grad_fn=<SoftmaxBackward0>)
num_high 17 len(mask) 32
mask_without_small tensor([0.9527, 0.9474, 0.9243, 0.9010, 0.9496, 0.9102, 0.9495, 0.8753, 0.9382,
        0.9151, 0.9503, 0.9441, 0.9442, 0.8958, 0.9450, 0.8600, 0.9213],
       grad_fn=<IndexBackward0>)
pred_loss tensor([2.5614], grad_fn=<MulBackward0>)
size_loss tensor(-283.7536, grad_fn=<MulBackward0>)
size_num_loss 1.7000000000000002
loss: tensor([-278.8828], grad_fn=<AddBackward0>)
17
epoch:  17 ; loss:  -278.88275146484375 ; pred:  tensor([0.0850, 0.0729, 0.0681, 0.7740], grad_fn=<SoftmaxBackward0>)
num_high 17 len(mask) 32
mask_without_small tensor([0.9569, 0.9521, 0.9261, 0.8965, 0.9542, 0.9080, 0.9541, 0.8663, 0.9427,
        0.9143, 0.9548, 0.9488, 0.9490, 0.8901, 0.9497, 0.8493, 0.9224],
       grad_fn=<IndexBackward0>)
pred_loss tensor([2.5737], grad_fn=<MulBackward0>)
size_loss tensor(-335.6286, grad_fn=<MulBackward0>)
size_num_loss 1.7000000000000002
loss: tensor([-330.7492], grad_fn=<AddBackward0>)
18
epoch:  18 ; loss:  -330.7491760253906 ; pred:  tensor([0.0853, 0.0731, 0.0686, 0.7731], grad_fn=<SoftmaxBackward0>)
num_high 17 len(mask) 32
mask_without_small tensor([0.9608, 0.9564, 0.9278, 0.8907, 0.9583, 0.9049, 0.9582, 0.8559, 0.9470,
        0.9129, 0.9589, 0.9532, 0.9533, 0.8831, 0.9541, 0.8371, 0.9231],
       grad_fn=<IndexBackward0>)
pred_loss tensor([2.5948], grad_fn=<MulBackward0>)
size_loss tensor(-391.6028, grad_fn=<MulBackward0>)
size_num_loss 1.7000000000000002
loss: tensor([-386.7055], grad_fn=<AddBackward0>)
19
epoch:  19 ; loss:  -386.70550537109375 ; pred:  tensor([0.0858, 0.0734, 0.0693, 0.7715], grad_fn=<SoftmaxBackward0>)
Finished Training
sel masked ver tensor(indices=tensor([[6949, 7139, 7140, 5935, 6949, 5505, 5935, 5795, 5795,
                        5795, 5795, 5795, 5505, 5505, 5505, 5795, 5795],
                       [5795, 5795, 5795, 5505, 5935, 5795, 5795, 5590, 6949,
                        7139, 7140, 7142, 6949, 7139, 7140, 5935, 5230]]),
       values=tensor([0.9608, 0.9564, 0.9278, 0.8907, 0.9583, 0.9049, 0.9582,
                      0.8559, 0.9470, 0.9129, 0.9589, 0.9532, 0.9533, 0.8831,
                      0.9541, 0.8371, 0.9231]),
       size=(7141, 7143), nnz=17, layout=torch.sparse_coo,
       grad_fn=<SparseCooTensorWithDimsAndTensorsBackward0>)
Counter({'publication': 4, 'publishes': 3, 'author': 3, 'member': 2, 'type': 1, 'photo': 1, 'worksAtProject': 1, 'carriedOutBy': 1, 'hasProject': 1})
dict index: {}
node_idx 5795
 node original label [3]
 node predicted label explain 3
 node prediction probability explain tensor([0.0858, 0.0734, 0.0693, 0.7715], grad_fn=<SoftmaxBackward0>)
 node predicted label full 3 most important relations  {'member': 2, 'publishes': 3, 'type': 1, 'photo': 1, 'worksAtProject': 1, 'publication': 4, 'carriedOutBy': 1, 'author': 3, 'hasProject': 1, 'label': 3, 'node_idx': '5795'}
 final masks and lenght tensor(indices=tensor([[ 23519,  23709,  23710,  23712,  39075,  46930,  63383,
                         63425,  88645, 114654, 146780, 146780, 147794, 154518,
                        154560, 179490, 179920, 196350, 229490, 237775, 246200,
                        254345, 254345, 254345, 254345, 262340, 262340, 262340,
                        262340, 304055, 328910, 328910],
                       [  5795,   5795,   5795,   5795,   5505,   5935,   5935,
                          5935,     98,   5935,   5388,   5430,   5430,   5795,
                          5795,   5795,   5795,   2670,     46,   5590,   6949,
                          6949,   7139,   7140,   7142,   6949,   7139,   7140,
                          7142,   5935,   5217,   5230]]),
       values=tensor([0.9608, 0.9564, 0.9278, 0.3516, 0.8907, 0.3805, 0.3965,
                      0.3884, 0.3617, 0.9583, 0.3693, 0.3708, 0.3635, 0.3733,
                      0.3612, 0.9049, 0.9582, 0.3831, 0.3769, 0.8559, 0.3618,
                      0.9470, 0.9129, 0.9589, 0.9532, 0.9533, 0.8831, 0.9541,
                      0.3805, 0.8371, 0.3859, 0.9231]),
       size=(753935, 8285), nnz=32, layout=torch.sparse_coo,
       grad_fn=<SparseCooTensorWithDimsAndTensorsBackward0>) 17
 ---------------------------------------------------------------
r_exp.py:102: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  hor_graph, ver_graph = torch.tensor(sub_hor_graph, dtype=torch.float), torch.tensor(sub_ver_graph, dtype=torch.float)
wandb: WARNING Calling wandb.login() after wandb.init() has no effect.
node label: 3
10
num_high 17 len(mask) 17
mask_without_small tensor([0.8404, 0.7202, 0.6962, 0.7597, 0.6770, 0.7974, 0.7815, 0.8287, 0.7544,
        0.8092, 0.7702, 0.8112, 0.7152, 0.7339, 0.7138, 0.7850, 0.7097],
       grad_fn=<IndexBackward0>)
pred_loss tensor([13.2760], grad_fn=<MulBackward0>)
size_loss tensor(-4.9201, grad_fn=<MulBackward0>)
size_num_loss 1.7000000000000002
loss: tensor([11.8922], grad_fn=<AddBackward0>)
0
epoch:  0 ; loss:  11.892181396484375 ; pred:  tensor([0.3074, 0.2531, 0.1743, 0.2651], grad_fn=<SoftmaxBackward0>)
num_high 17 len(mask) 17
mask_without_small tensor([0.8533, 0.6996, 0.6747, 0.7409, 0.6547, 0.8130, 0.7981, 0.8424, 0.7354,
        0.8241, 0.7874, 0.8260, 0.6944, 0.7139, 0.6929, 0.8014, 0.6886],
       grad_fn=<IndexBackward0>)
pred_loss tensor([13.4138], grad_fn=<MulBackward0>)
size_loss tensor(-6.5839, grad_fn=<MulBackward0>)
size_num_loss 1.7000000000000002
loss: tensor([10.3592], grad_fn=<AddBackward0>)
1
epoch:  1 ; loss:  10.359190940856934 ; pred:  tensor([0.3075, 0.2534, 0.1776, 0.2615], grad_fn=<SoftmaxBackward0>)
num_high 17 len(mask) 17
mask_without_small tensor([0.8653, 0.6781, 0.6524, 0.7233, 0.6318, 0.8278, 0.8137, 0.8552, 0.7163,
        0.8381, 0.8024, 0.8399, 0.6727, 0.6932, 0.6712, 0.8168, 0.6668],
       grad_fn=<IndexBackward0>)
pred_loss tensor([13.5552], grad_fn=<MulBackward0>)
size_loss tensor(-8.3047, grad_fn=<MulBackward0>)
size_num_loss 1.7000000000000002
loss: tensor([8.7706], grad_fn=<AddBackward0>)
2
epoch:  2 ; loss:  8.770605087280273 ; pred:  tensor([0.3074, 0.2537, 0.1811, 0.2578], grad_fn=<SoftmaxBackward0>)
num_high 16 len(mask) 17
mask_without_small tensor([0.8762, 0.6558, 0.6294, 0.7044, 0.6083, 0.8416, 0.8283, 0.8669, 0.6962,
        0.8512, 0.8169, 0.8528, 0.6503, 0.6717, 0.6487, 0.8314, 0.6442],
       grad_fn=<IndexBackward0>)
pred_loss tensor([13.6999], grad_fn=<MulBackward0>)
size_loss tensor(-10.0526, grad_fn=<MulBackward0>)
size_num_loss 1.6
loss: tensor([7.0561], grad_fn=<AddBackward0>)
3
epoch:  3 ; loss:  7.056118011474609 ; pred:  tensor([0.3072, 0.2541, 0.1846, 0.2541], grad_fn=<SoftmaxBackward0>)
num_high 14 len(mask) 17
mask_without_small tensor([0.8862, 0.6328, 0.6058, 0.6842, 0.5844, 0.8544, 0.8421, 0.8776, 0.6752,
        0.8632, 0.8310, 0.8646, 0.6271, 0.6493, 0.6254, 0.8450, 0.6208],
       grad_fn=<IndexBackward0>)
pred_loss tensor([13.8473], grad_fn=<MulBackward0>)
size_loss tensor(-11.8107, grad_fn=<MulBackward0>)
size_num_loss 1.4000000000000001
loss: tensor([5.2319], grad_fn=<AddBackward0>)
4
epoch:  4 ; loss:  5.2319183349609375 ; pred:  tensor([0.3069, 0.2544, 0.1883, 0.2504], grad_fn=<SoftmaxBackward0>)
num_high 11 len(mask) 17
mask_without_small tensor([0.8953, 0.6091, 0.5817, 0.6629, 0.5601, 0.8663, 0.8550, 0.8874, 0.6532,
        0.8743, 0.8443, 0.8755, 0.6033, 0.6261, 0.6015, 0.8576, 0.5969],
       grad_fn=<IndexBackward0>)
pred_loss tensor([13.9966], grad_fn=<MulBackward0>)
size_loss tensor(-13.5683, grad_fn=<MulBackward0>)
size_num_loss 1.1
loss: tensor([3.3079], grad_fn=<AddBackward0>)
5
epoch:  5 ; loss:  3.307929039001465 ; pred:  tensor([0.3065, 0.2547, 0.1921, 0.2467], grad_fn=<SoftmaxBackward0>)
num_high 10 len(mask) 17
mask_without_small tensor([0.9034, 0.5848, 0.5572, 0.6406, 0.5355, 0.8773, 0.8670, 0.8963, 0.6302,
        0.8845, 0.8569, 0.8853, 0.5789, 0.6021, 0.5771, 0.8694, 0.5725],
       grad_fn=<IndexBackward0>)
pred_loss tensor([14.1468], grad_fn=<MulBackward0>)
size_loss tensor(-15.3175, grad_fn=<MulBackward0>)
size_num_loss 1.0
loss: tensor([1.5912], grad_fn=<AddBackward0>)
6
epoch:  6 ; loss:  1.591212272644043 ; pred:  tensor([0.3059, 0.2550, 0.1960, 0.2430], grad_fn=<SoftmaxBackward0>)
num_high 8 len(mask) 17
mask_without_small tensor([0.9108, 0.5600, 0.5324, 0.6174, 0.5108, 0.8874, 0.8782, 0.9044, 0.6064,
        0.8938, 0.8687, 0.8942, 0.5541, 0.5775, 0.5521, 0.8802, 0.5477],
       grad_fn=<IndexBackward0>)
pred_loss tensor([14.2969], grad_fn=<MulBackward0>)
size_loss tensor(-17.0518, grad_fn=<MulBackward0>)
size_num_loss 0.8
loss: tensor([-0.2126], grad_fn=<AddBackward0>)
7
epoch:  7 ; loss:  -0.21255922317504883 ; pred:  tensor([0.3053, 0.2553, 0.2000, 0.2394], grad_fn=<SoftmaxBackward0>)
num_high 8 len(mask) 17
mask_without_small tensor([0.9174, 0.5348, 0.5075, 0.5933, 0.8966, 0.8884, 0.9117, 0.5819, 0.9022,
        0.8797, 0.9022, 0.5290, 0.5523, 0.5269, 0.8901, 0.5226],
       grad_fn=<IndexBackward0>)
pred_loss tensor([14.4460], grad_fn=<MulBackward0>)
size_loss tensor(-1846.5343, grad_fn=<MulBackward0>)
size_num_loss 0.8
loss: tensor([-1830.6543], grad_fn=<AddBackward0>)
8
epoch:  8 ; loss:  -1830.654296875 ; pred:  tensor([0.3045, 0.2557, 0.2040, 0.2358], grad_fn=<SoftmaxBackward0>)
num_high 8 len(mask) 17
mask_without_small tensor([0.9213, 0.5221, 0.5810, 0.9012, 0.8933, 0.9157, 0.5696, 0.9067, 0.8850,
        0.9066, 0.5162, 0.5399, 0.5146, 0.8951, 0.5100],
       grad_fn=<IndexBackward0>)
pred_loss tensor([14.5003], grad_fn=<MulBackward0>)
size_loss tensor(-1905.6298, grad_fn=<MulBackward0>)
size_num_loss 0.8
loss: tensor([-1889.7052], grad_fn=<AddBackward0>)
9
epoch:  9 ; loss:  -1889.7052001953125 ; pred:  tensor([0.3039, 0.2558, 0.2058, 0.2346], grad_fn=<SoftmaxBackward0>)
num_high 8 len(mask) 17
mask_without_small tensor([0.9260, 0.5056, 0.5649, 0.9070, 0.8995, 0.9207, 0.5533, 0.9122, 0.8915,
        0.9120, 0.5235, 0.9012], grad_fn=<IndexBackward0>)
pred_loss tensor([14.5872], grad_fn=<MulBackward0>)
size_loss tensor(-1838.9637, grad_fn=<MulBackward0>)
size_num_loss 0.8
loss: tensor([-1822.9734], grad_fn=<AddBackward0>)
10
epoch:  10 ; loss:  -1822.973388671875 ; pred:  tensor([0.3032, 0.2559, 0.2083, 0.2325], grad_fn=<SoftmaxBackward0>)
num_high 8 len(mask) 17
mask_without_small tensor([0.9310, 0.5469, 0.9132, 0.9061, 0.9261, 0.5352, 0.9181, 0.8986, 0.9179,
        0.5052, 0.9078], grad_fn=<IndexBackward0>)
pred_loss tensor([14.6689], grad_fn=<MulBackward0>)
size_loss tensor(-1806.7999, grad_fn=<MulBackward0>)
size_num_loss 0.8
loss: tensor([-1790.7395], grad_fn=<AddBackward0>)
11
epoch:  11 ; loss:  -1790.739501953125 ; pred:  tensor([0.3025, 0.2562, 0.2106, 0.2306], grad_fn=<SoftmaxBackward0>)
num_high 8 len(mask) 17
mask_without_small tensor([0.9362, 0.5272, 0.9195, 0.9129, 0.9316, 0.5152, 0.9241, 0.9059, 0.9239,
        0.9145], grad_fn=<IndexBackward0>)
pred_loss tensor([14.7457], grad_fn=<MulBackward0>)
size_loss tensor(-1688.4688, grad_fn=<MulBackward0>)
size_num_loss 0.8
loss: tensor([-1672.3440], grad_fn=<AddBackward0>)
12
epoch:  12 ; loss:  -1672.343994140625 ; pred:  tensor([0.3018, 0.2566, 0.2128, 0.2289], grad_fn=<SoftmaxBackward0>)
num_high 8 len(mask) 17
mask_without_small tensor([0.9412, 0.5062, 0.9257, 0.9195, 0.9369, 0.9299, 0.9129, 0.9297, 0.9210],
       grad_fn=<IndexBackward0>)
pred_loss tensor([14.8129], grad_fn=<MulBackward0>)
size_loss tensor(-1405.8403, grad_fn=<MulBackward0>)
size_num_loss 0.8
loss: tensor([-1389.6614], grad_fn=<AddBackward0>)
13
epoch:  13 ; loss:  -1389.661376953125 ; pred:  tensor([0.3011, 0.2569, 0.2147, 0.2273], grad_fn=<SoftmaxBackward0>)
num_high 8 len(mask) 17
mask_without_small tensor([0.9459, 0.9315, 0.9257, 0.9420, 0.9354, 0.9194, 0.9352, 0.9271],
       grad_fn=<IndexBackward0>)
pred_loss tensor([14.8716], grad_fn=<MulBackward0>)
size_loss tensor(-87.2504, grad_fn=<MulBackward0>)
size_num_loss 0.8
loss: tensor([-71.0255], grad_fn=<AddBackward0>)
14
epoch:  14 ; loss:  -71.02545166015625 ; pred:  tensor([0.3004, 0.2572, 0.2164, 0.2260], grad_fn=<SoftmaxBackward0>)
num_high 8 len(mask) 17
mask_without_small tensor([0.9504, 0.9360, 0.9281, 0.9469, 0.9406, 0.9198, 0.9403, 0.9301],
       grad_fn=<IndexBackward0>)
pred_loss tensor([14.9270], grad_fn=<MulBackward0>)
size_loss tensor(-101.4302, grad_fn=<MulBackward0>)
size_num_loss 0.8
loss: tensor([-85.1560], grad_fn=<AddBackward0>)
15
epoch:  15 ; loss:  -85.1559829711914 ; pred:  tensor([0.2998, 0.2575, 0.2180, 0.2248], grad_fn=<SoftmaxBackward0>)
num_high 8 len(mask) 17
mask_without_small tensor([0.9547, 0.9397, 0.9285, 0.9516, 0.9454, 0.9177, 0.9451, 0.9313],
       grad_fn=<IndexBackward0>)
pred_loss tensor([14.9782], grad_fn=<MulBackward0>)
size_loss tensor(-125.7600, grad_fn=<MulBackward0>)
size_num_loss 0.8
loss: tensor([-109.4397], grad_fn=<AddBackward0>)
16
epoch:  16 ; loss:  -109.439697265625 ; pred:  tensor([0.2991, 0.2578, 0.2195, 0.2236], grad_fn=<SoftmaxBackward0>)
num_high 8 len(mask) 17
mask_without_small tensor([0.9588, 0.9429, 0.9272, 0.9560, 0.9500, 0.9139, 0.9496, 0.9311],
       grad_fn=<IndexBackward0>)
pred_loss tensor([15.0255], grad_fn=<MulBackward0>)
size_loss tensor(-156.8888, grad_fn=<MulBackward0>)
size_num_loss 0.8
loss: tensor([-140.5258], grad_fn=<AddBackward0>)
17
epoch:  17 ; loss:  -140.5258331298828 ; pred:  tensor([0.2984, 0.2581, 0.2209, 0.2226], grad_fn=<SoftmaxBackward0>)
num_high 8 len(mask) 17
mask_without_small tensor([0.9626, 0.9459, 0.9248, 0.9601, 0.9543, 0.9087, 0.9539, 0.9298],
       grad_fn=<IndexBackward0>)
pred_loss tensor([15.0690], grad_fn=<MulBackward0>)
size_loss tensor(-193.2416, grad_fn=<MulBackward0>)
size_num_loss 0.8
loss: tensor([-176.8391], grad_fn=<AddBackward0>)
18
epoch:  18 ; loss:  -176.8390655517578 ; pred:  tensor([0.2978, 0.2584, 0.2222, 0.2216], grad_fn=<SoftmaxBackward0>)
num_high 8 len(mask) 17
mask_without_small tensor([0.9661, 0.9487, 0.9212, 0.9639, 0.9583, 0.9022, 0.9579, 0.9275],
       grad_fn=<IndexBackward0>)
pred_loss tensor([15.1092], grad_fn=<MulBackward0>)
size_loss tensor(-234.1009, grad_fn=<MulBackward0>)
size_num_loss 0.8
loss: tensor([-217.6617], grad_fn=<AddBackward0>)
19
epoch:  19 ; loss:  -217.6616973876953 ; pred:  tensor([0.2972, 0.2586, 0.2234, 0.2207], grad_fn=<SoftmaxBackward0>)
Finished Training
sel masked ver tensor(indices=tensor([[5935, 5935, 5935, 5388, 5505, 5935, 5753, 5753],
                       [5505, 5388, 5409, 5753, 5753, 5753, 1223, 5230]]),
       values=tensor([0.9661, 0.9487, 0.9212, 0.9639, 0.9583, 0.9022, 0.9579,
                      0.9275]),
       size=(5936, 5754), nnz=8, layout=torch.sparse_coo,
       grad_fn=<SparseCooTensorWithDimsAndTensorsBackward0>)
Counter({'member': 2, 'isAbout': 2, 'isWorkedOnBy': 1, 'name': 1, 'type': 1, 'carriedOutBy': 1})
dict index: {}
node_idx 5753
 node original label [3]
 node predicted label explain 0
 node prediction probability explain tensor([0.2972, 0.2586, 0.2234, 0.2207], grad_fn=<SoftmaxBackward0>)
 node predicted label full 3 most important relations  {'isWorkedOnBy': 1, 'member': 2, 'name': 1, 'type': 1, 'isAbout': 2, 'carriedOutBy': 1, 'label': 3, 'node_idx': '5753'}
 final masks and lenght tensor(indices=tensor([[ 39075,  46930,  63383,  63404,  88603, 146780, 146780,
                        154518, 154539, 179490, 179920, 196308, 229448, 237733,
                        304013, 328868, 328868],
                       [  5505,   5935,   5935,   5935,      0,   5388,   5409,
                          5753,   5753,   5753,   5753,   1223,     87,   5560,
                          5935,   5230,   5231]]),
       values=tensor([0.9661, 0.3924, 0.4207, 0.4050, 0.3133, 0.9487, 0.9212,
                      0.9639, 0.4045, 0.9583, 0.9022, 0.9579, 0.4084, 0.3909,
                      0.4101, 0.9275, 0.4036]),
       size=(753935, 8285), nnz=17, layout=torch.sparse_coo,
       grad_fn=<SparseCooTensorWithDimsAndTensorsBackward0>) 8
 ---------------------------------------------------------------
r_exp.py:102: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  hor_graph, ver_graph = torch.tensor(sub_hor_graph, dtype=torch.float), torch.tensor(sub_ver_graph, dtype=torch.float)
wandb: WARNING Calling wandb.login() after wandb.init() has no effect.
node label: 3
6
num_high 6 len(mask) 7
mask_without_small tensor([0.7649, 0.7444, 0.7550, 0.7546, 0.5986, 0.7110, 0.8985],
       grad_fn=<IndexBackward0>)
pred_loss tensor([18.8256], grad_fn=<MulBackward0>)
size_loss tensor(-8.8219, grad_fn=<MulBackward0>)
size_num_loss 0.6000000000000001
loss: tensor([11.6736], grad_fn=<AddBackward0>)
0
epoch:  0 ; loss:  11.67361068725586 ; pred:  tensor([0.3423, 0.3037, 0.2018, 0.1522], grad_fn=<SoftmaxBackward0>)
num_high 6 len(mask) 7
mask_without_small tensor([0.7465, 0.7249, 0.7360, 0.7356, 0.5744, 0.6901, 0.9072],
       grad_fn=<IndexBackward0>)
pred_loss tensor([18.6974], grad_fn=<MulBackward0>)
size_loss tensor(-9.7965, grad_fn=<MulBackward0>)
size_num_loss 0.6000000000000001
loss: tensor([10.5723], grad_fn=<AddBackward0>)
1
epoch:  1 ; loss:  10.572305679321289 ; pred:  tensor([0.3411, 0.3022, 0.2026, 0.1542], grad_fn=<SoftmaxBackward0>)
num_high 6 len(mask) 7
mask_without_small tensor([0.7275, 0.7048, 0.7162, 0.7207, 0.5498, 0.6683, 0.9153],
       grad_fn=<IndexBackward0>)
pred_loss tensor([18.5704], grad_fn=<MulBackward0>)
size_loss tensor(-10.7926, grad_fn=<MulBackward0>)
size_num_loss 0.6000000000000001
loss: tensor([9.4488], grad_fn=<AddBackward0>)
2
epoch:  2 ; loss:  9.448847770690918 ; pred:  tensor([0.3399, 0.3011, 0.2029, 0.1561], grad_fn=<SoftmaxBackward0>)
num_high 6 len(mask) 7
mask_without_small tensor([0.7078, 0.6840, 0.6957, 0.7032, 0.5250, 0.6457, 0.9227],
       grad_fn=<IndexBackward0>)
pred_loss tensor([18.4378], grad_fn=<MulBackward0>)
size_loss tensor(-11.7955, grad_fn=<MulBackward0>)
size_num_loss 0.6000000000000001
loss: tensor([8.3114], grad_fn=<AddBackward0>)
3
epoch:  3 ; loss:  8.311405181884766 ; pred:  tensor([0.3386, 0.2998, 0.2034, 0.1582], grad_fn=<SoftmaxBackward0>)
num_high 5 len(mask) 7
mask_without_small tensor([0.6872, 0.6624, 0.6742, 0.6839, 0.5002, 0.6223, 0.9295],
       grad_fn=<IndexBackward0>)
pred_loss tensor([18.2994], grad_fn=<MulBackward0>)
size_loss tensor(-12.8009, grad_fn=<MulBackward0>)
size_num_loss 0.5
loss: tensor([7.0640], grad_fn=<AddBackward0>)
4
epoch:  4 ; loss:  7.063970565795898 ; pred:  tensor([0.3373, 0.2982, 0.2041, 0.1604], grad_fn=<SoftmaxBackward0>)
num_high 5 len(mask) 7
mask_without_small tensor([0.6657, 0.6399, 0.6520, 0.6633, 0.5983, 0.9357],
       grad_fn=<IndexBackward0>)
pred_loss tensor([18.1554], grad_fn=<MulBackward0>)
size_loss tensor(-1216.3478, grad_fn=<MulBackward0>)
size_num_loss 0.5
loss: tensor([-1197.0537], grad_fn=<AddBackward0>)
5
epoch:  5 ; loss:  -1197.0537109375 ; pred:  tensor([0.3359, 0.2965, 0.2049, 0.1627], grad_fn=<SoftmaxBackward0>)
num_high 5 len(mask) 7
mask_without_small tensor([0.6538, 0.6277, 0.6398, 0.6515, 0.5855, 0.9388],
       grad_fn=<IndexBackward0>)
pred_loss tensor([18.0769], grad_fn=<MulBackward0>)
size_loss tensor(-1278.4163, grad_fn=<MulBackward0>)
size_num_loss 0.5
loss: tensor([-1259.1980], grad_fn=<AddBackward0>)
6
epoch:  6 ; loss:  -1259.197998046875 ; pred:  tensor([0.3349, 0.2958, 0.2053, 0.1640], grad_fn=<SoftmaxBackward0>)
num_high 4 len(mask) 7
mask_without_small tensor([0.6381, 0.6116, 0.6239, 0.6359, 0.5688, 0.9427],
       grad_fn=<IndexBackward0>)
pred_loss tensor([17.9725], grad_fn=<MulBackward0>)
size_loss tensor(-1358.7802, grad_fn=<MulBackward0>)
size_num_loss 0.4
loss: tensor([-1339.7633], grad_fn=<AddBackward0>)
7
epoch:  7 ; loss:  -1339.7633056640625 ; pred:  tensor([0.3338, 0.2945, 0.2059, 0.1658], grad_fn=<SoftmaxBackward0>)
num_high 1 len(mask) 7
mask_without_small tensor([0.6198, 0.5929, 0.6054, 0.6176, 0.5496, 0.9468],
       grad_fn=<IndexBackward0>)
pred_loss tensor([17.8511], grad_fn=<MulBackward0>)
size_loss tensor(-1450.2998, grad_fn=<MulBackward0>)
size_num_loss 0.1
loss: tensor([-1431.7020], grad_fn=<AddBackward0>)
8
epoch:  8 ; loss:  -1431.7020263671875 ; pred:  tensor([0.3327, 0.2929, 0.2066, 0.1678], grad_fn=<SoftmaxBackward0>)
num_high 1 len(mask) 7
mask_without_small tensor([0.5996, 0.5724, 0.5850, 0.5975, 0.5287, 0.9508],
       grad_fn=<IndexBackward0>)
pred_loss tensor([17.7171], grad_fn=<MulBackward0>)
size_loss tensor(-1549.3571, grad_fn=<MulBackward0>)
size_num_loss 0.1
loss: tensor([-1530.8915], grad_fn=<AddBackward0>)
9
epoch:  9 ; loss:  -1530.8914794921875 ; pred:  tensor([0.3316, 0.2910, 0.2074, 0.1700], grad_fn=<SoftmaxBackward0>)
num_high 1 len(mask) 7
mask_without_small tensor([0.5779, 0.5505, 0.5631, 0.5758, 0.5065, 0.9548],
       grad_fn=<IndexBackward0>)
pred_loss tensor([17.5733], grad_fn=<MulBackward0>)
size_loss tensor(-1653.6853, grad_fn=<MulBackward0>)
size_num_loss 0.1
loss: tensor([-1635.3629], grad_fn=<AddBackward0>)
10
epoch:  10 ; loss:  -1635.3629150390625 ; pred:  tensor([0.3305, 0.2888, 0.2082, 0.1725], grad_fn=<SoftmaxBackward0>)
num_high 1 len(mask) 7
mask_without_small tensor([0.5549, 0.5274, 0.5401, 0.5528, 0.9585], grad_fn=<IndexBackward0>)
pred_loss tensor([17.4217], grad_fn=<MulBackward0>)
size_loss tensor(-1857.9218, grad_fn=<MulBackward0>)
size_num_loss 0.1
loss: tensor([-1839.7563], grad_fn=<AddBackward0>)
11
epoch:  11 ; loss:  -1839.75634765625 ; pred:  tensor([0.3294, 0.2865, 0.2090, 0.1751], grad_fn=<SoftmaxBackward0>)
num_high 1 len(mask) 7
mask_without_small tensor([0.5313, 0.5034, 0.5162, 0.5292, 0.9621], grad_fn=<IndexBackward0>)
pred_loss tensor([17.2634], grad_fn=<MulBackward0>)
size_loss tensor(-1980.0619, grad_fn=<MulBackward0>)
size_num_loss 0.1
loss: tensor([-1962.0565], grad_fn=<AddBackward0>)
12
epoch:  12 ; loss:  -1962.0565185546875 ; pred:  tensor([0.3282, 0.2841, 0.2097, 0.1779], grad_fn=<SoftmaxBackward0>)
num_high 1 len(mask) 7
mask_without_small tensor([0.5068, 0.5046, 0.9653], grad_fn=<IndexBackward0>)
pred_loss tensor([17.0978], grad_fn=<MulBackward0>)
size_loss tensor(-2653.7969, grad_fn=<MulBackward0>)
size_num_loss 0.1
loss: tensor([17.0978], grad_fn=<MulBackward0>)
13
epoch:  13 ; loss:  17.097782135009766 ; pred:  tensor([0.3271, 0.2815, 0.2104, 0.1809], grad_fn=<SoftmaxBackward0>)
num_high 1 len(mask) 7
mask_without_small tensor([0.9681], grad_fn=<IndexBackward0>)
pred_loss tensor([16.9487], grad_fn=<MulBackward0>)
size_loss tensor(nan, grad_fn=<MulBackward0>)
size_num_loss 0.1
loss: tensor([16.9487], grad_fn=<MulBackward0>)
14
epoch:  14 ; loss:  16.948734283447266 ; pred:  tensor([0.3261, 0.2792, 0.2110, 0.1836], grad_fn=<SoftmaxBackward0>)
num_high 1 len(mask) 7
mask_without_small tensor([0.9703], grad_fn=<IndexBackward0>)
pred_loss tensor([16.8148], grad_fn=<MulBackward0>)
size_loss tensor(nan, grad_fn=<MulBackward0>)
size_num_loss 0.1
loss: tensor([16.8148], grad_fn=<MulBackward0>)
15
epoch:  15 ; loss:  16.81479263305664 ; pred:  tensor([0.3252, 0.2771, 0.2116, 0.1861], grad_fn=<SoftmaxBackward0>)
num_high 1 len(mask) 7
mask_without_small tensor([0.9723], grad_fn=<IndexBackward0>)
pred_loss tensor([16.6945], grad_fn=<MulBackward0>)
size_loss tensor(nan, grad_fn=<MulBackward0>)
size_num_loss 0.1
loss: tensor([16.6945], grad_fn=<MulBackward0>)
16
epoch:  16 ; loss:  16.69454002380371 ; pred:  tensor([0.3243, 0.2752, 0.2121, 0.1883], grad_fn=<SoftmaxBackward0>)
num_high 1 len(mask) 7
mask_without_small tensor([0.9739], grad_fn=<IndexBackward0>)
pred_loss tensor([16.5866], grad_fn=<MulBackward0>)
size_loss tensor(nan, grad_fn=<MulBackward0>)
size_num_loss 0.1
loss: tensor([16.5866], grad_fn=<MulBackward0>)
17
epoch:  17 ; loss:  16.586612701416016 ; pred:  tensor([0.3236, 0.2735, 0.2125, 0.1904], grad_fn=<SoftmaxBackward0>)
num_high 1 len(mask) 7
mask_without_small tensor([0.9753], grad_fn=<IndexBackward0>)
pred_loss tensor([16.4897], grad_fn=<MulBackward0>)
size_loss tensor(nan, grad_fn=<MulBackward0>)
size_num_loss 0.1
loss: tensor([16.4897], grad_fn=<MulBackward0>)
18
epoch:  18 ; loss:  16.489744186401367 ; pred:  tensor([0.3229, 0.2720, 0.2129, 0.1922], grad_fn=<SoftmaxBackward0>)
num_high 1 len(mask) 7
mask_without_small tensor([0.9766], grad_fn=<IndexBackward0>)
pred_loss tensor([16.4028], grad_fn=<MulBackward0>)
size_loss tensor(nan, grad_fn=<MulBackward0>)
size_num_loss 0.1
loss: tensor([16.4028], grad_fn=<MulBackward0>)
19
epoch:  19 ; loss:  16.402769088745117 ; pred:  tensor([0.3222, 0.2706, 0.2132, 0.1939], grad_fn=<SoftmaxBackward0>)
Finished Training
sel masked ver tensor(indices=tensor([[5798, 5798, 5798],
                       [   0,    0, 5231]]),
       values=tensor([1.6082, 1.6007, 0.9766]),
       size=(5799, 5232), nnz=3, layout=torch.sparse_coo,
       grad_fn=<SparseCooTensorWithDimsAndTensorsBackward0>)
Counter({'fax': 1, 'phone': 1, 'type': 1})
dict index: {}
node_idx 5798
 node original label [3]
 node predicted label explain 0
 node prediction probability explain tensor([0.3434, 0.3050, 0.1910, 0.1607], grad_fn=<SoftmaxBackward0>)
 node predicted label full 0 most important relations  {'fax': 1, 'phone': 1, 'type': 1, 'label': 3, 'node_idx': '5798'}
 final masks and lenght tensor(indices=tensor([[ 88648, 179490, 196353, 229493, 237778, 328913, 328913],
                       [     0,   5798,   3422,      0,   5592,   5230,   5231]]),
       values=tensor([1.6082, 0.3748, 0.3867, 1.6007, 0.3299, 0.3672, 0.9766]),
       size=(753935, 8285), nnz=7, layout=torch.sparse_coo,
       grad_fn=<SparseCooTensorWithDimsAndTensorsBackward0>) 3
 ---------------------------------------------------------------
r_exp.py:102: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  hor_graph, ver_graph = torch.tensor(sub_hor_graph, dtype=torch.float), torch.tensor(sub_ver_graph, dtype=torch.float)
wandb: WARNING Calling wandb.login() after wandb.init() has no effect.
node label: 3
31
num_high 78 len(mask) 78
mask_without_small tensor([0.7873, 0.7752, 0.7585, 0.6599, 0.7519, 0.6905, 0.7297, 0.6777, 0.7067,
        0.7797, 0.7185, 0.6847, 0.7075, 0.7131, 0.7062, 0.7544, 0.7795, 0.7260,
        0.7151, 0.7447, 0.7065, 0.7636, 0.7555, 0.7806, 0.7694, 0.7699, 0.7498,
        0.7710, 0.7237, 0.7324, 0.7231, 0.7573, 0.6853, 0.7028, 0.7240, 0.7816,
        0.7410, 0.7175, 0.7406, 0.7060, 0.6793, 0.7612, 0.7025, 0.7117, 0.6891,
        0.7925, 0.6905, 0.7154, 0.7013, 0.7098, 0.7335, 0.7473, 0.7154, 0.7669,
        0.7047, 0.7073, 0.6847, 0.7322, 0.7291, 0.7518, 0.7280, 0.7851, 0.7416,
        0.7811, 0.7314, 0.7203, 0.6868, 0.7122, 0.7476, 0.7473, 0.6824, 0.7737,
        0.7538, 0.7156, 0.6968, 0.7496, 0.6735, 0.7042],
       grad_fn=<IndexBackward0>)
pred_loss tensor([0.7691], grad_fn=<MulBackward0>)
size_loss tensor(-3.2431, grad_fn=<MulBackward0>)
size_num_loss 7.800000000000001
loss: tensor([11.5993], grad_fn=<AddBackward0>)
0
epoch:  0 ; loss:  11.599281311035156 ; pred:  tensor([0.0434, 0.0052, 0.0255, 0.9260], grad_fn=<SoftmaxBackward0>)
num_high 78 len(mask) 78
mask_without_small tensor([0.8035, 0.7922, 0.7763, 0.6371, 0.7701, 0.6687, 0.7095, 0.6554, 0.6856,
        0.7964, 0.6979, 0.6627, 0.6864, 0.6922, 0.6850, 0.7724, 0.7962, 0.7057,
        0.6943, 0.7252, 0.6854, 0.7812, 0.7735, 0.7972, 0.7866, 0.7871, 0.7681,
        0.7881, 0.7033, 0.7123, 0.7026, 0.7752, 0.6634, 0.6815, 0.7035, 0.7982,
        0.7213, 0.6968, 0.7209, 0.6848, 0.6571, 0.7789, 0.6812, 0.6908, 0.6673,
        0.8084, 0.6687, 0.6946, 0.6800, 0.6888, 0.7526, 0.7657, 0.6946, 0.7843,
        0.6834, 0.6861, 0.6627, 0.7513, 0.7089, 0.7700, 0.7077, 0.8014, 0.7220,
        0.7977, 0.7113, 0.6997, 0.6649, 0.6913, 0.7660, 0.7657, 0.6603, 0.7908,
        0.7719, 0.6948, 0.6752, 0.7679, 0.6512, 0.6830],
       grad_fn=<IndexBackward0>)
pred_loss tensor([0.7987], grad_fn=<MulBackward0>)
size_loss tensor(-4.9337, grad_fn=<MulBackward0>)
size_num_loss 7.800000000000001
loss: tensor([9.8926], grad_fn=<AddBackward0>)
1
epoch:  1 ; loss:  9.892595291137695 ; pred:  tensor([0.0442, 0.0056, 0.0270, 0.9232], grad_fn=<SoftmaxBackward0>)
num_high 77 len(mask) 78
mask_without_small tensor([0.8186, 0.8080, 0.7932, 0.6139, 0.7866, 0.6462, 0.6888, 0.6326, 0.6636,
        0.8120, 0.6764, 0.6400, 0.6645, 0.6705, 0.6630, 0.7893, 0.8118, 0.6846,
        0.6727, 0.7089, 0.6634, 0.7978, 0.7904, 0.8127, 0.8029, 0.8033, 0.7841,
        0.8043, 0.6820, 0.6918, 0.6813, 0.7921, 0.6407, 0.6594, 0.6823, 0.8136,
        0.7029, 0.6752, 0.7023, 0.6628, 0.6344, 0.7957, 0.6591, 0.6690, 0.6448,
        0.8230, 0.6462, 0.6730, 0.6578, 0.6670, 0.7671, 0.7829, 0.6730, 0.8007,
        0.6614, 0.6642, 0.6401, 0.7651, 0.6883, 0.7871, 0.6878, 0.8167, 0.7036,
        0.8132, 0.6911, 0.6783, 0.6424, 0.6695, 0.7826, 0.7822, 0.6376, 0.8067,
        0.7889, 0.6732, 0.6530, 0.7841, 0.6283, 0.6609],
       grad_fn=<IndexBackward0>)
pred_loss tensor([0.8389], grad_fn=<MulBackward0>)
size_loss tensor(-6.6969, grad_fn=<MulBackward0>)
size_num_loss 7.7
loss: tensor([8.0160], grad_fn=<AddBackward0>)
2
epoch:  2 ; loss:  8.016045570373535 ; pred:  tensor([0.0455, 0.0060, 0.0289, 0.9195], grad_fn=<SoftmaxBackward0>)
num_high 68 len(mask) 78
mask_without_small tensor([0.8323, 0.8227, 0.8091, 0.5903, 0.8025, 0.6232, 0.6671, 0.6093, 0.6409,
        0.8262, 0.6540, 0.6169, 0.6418, 0.6479, 0.6403, 0.8053, 0.8261, 0.6627,
        0.6502, 0.6902, 0.6407, 0.8134, 0.8064, 0.8269, 0.8180, 0.8184, 0.7998,
        0.8193, 0.6600, 0.6705, 0.6592, 0.8080, 0.6176, 0.6365, 0.6603, 0.8277,
        0.6830, 0.6529, 0.6823, 0.6401, 0.6111, 0.8115, 0.6362, 0.6464, 0.6217,
        0.8361, 0.6232, 0.6505, 0.6350, 0.6443, 0.7824, 0.7992, 0.6506, 0.8161,
        0.6387, 0.6415, 0.6170, 0.7802, 0.6668, 0.8034, 0.6668, 0.8307, 0.6838,
        0.8274, 0.6699, 0.6560, 0.6192, 0.6470, 0.7987, 0.7983, 0.6144, 0.8216,
        0.8051, 0.6508, 0.6300, 0.7999, 0.6050, 0.6381],
       grad_fn=<IndexBackward0>)
pred_loss tensor([0.8835], grad_fn=<MulBackward0>)
size_loss tensor(-8.4847, grad_fn=<MulBackward0>)
size_num_loss 6.800000000000001
loss: tensor([5.3118], grad_fn=<AddBackward0>)
3
epoch:  3 ; loss:  5.311777114868164 ; pred:  tensor([0.0470, 0.0065, 0.0310, 0.9154], grad_fn=<SoftmaxBackward0>)
num_high 53 len(mask) 78
mask_without_small tensor([0.8446, 0.8360, 0.8240, 0.5665, 0.8177, 0.5996, 0.6446, 0.5856, 0.6175,
        0.8392, 0.6309, 0.5932, 0.6184, 0.6247, 0.6169, 0.8204, 0.8391, 0.6399,
        0.6270, 0.6701, 0.6173, 0.8278, 0.8215, 0.8398, 0.8319, 0.8322, 0.8149,
        0.8330, 0.6371, 0.6481, 0.6363, 0.8230, 0.5939, 0.6131, 0.6374, 0.8405,
        0.6619, 0.6297, 0.6611, 0.6167, 0.5874, 0.8261, 0.6128, 0.6231, 0.5981,
        0.8474, 0.5997, 0.6274, 0.6117, 0.6210, 0.7978, 0.8148, 0.6274, 0.8304,
        0.6154, 0.6182, 0.5936, 0.7954, 0.6444, 0.8187, 0.6447, 0.8435, 0.6627,
        0.8403, 0.6477, 0.6330, 0.5956, 0.6237, 0.8141, 0.8137, 0.5908, 0.8352,
        0.8203, 0.6276, 0.6066, 0.8150, 0.5815, 0.6148],
       grad_fn=<IndexBackward0>)
pred_loss tensor([0.9339], grad_fn=<MulBackward0>)
size_loss tensor(-10.2703, grad_fn=<MulBackward0>)
size_num_loss 5.300000000000001
loss: tensor([2.0080], grad_fn=<AddBackward0>)
4
epoch:  4 ; loss:  2.0079588890075684 ; pred:  tensor([0.0486, 0.0072, 0.0334, 0.9108], grad_fn=<SoftmaxBackward0>)
num_high 36 len(mask) 78
mask_without_small tensor([0.8554, 0.8479, 0.8376, 0.5427, 0.8319, 0.5756, 0.6212, 0.5617, 0.5936,
        0.8507, 0.6072, 0.5693, 0.5945, 0.6008, 0.5929, 0.8344, 0.8506, 0.6163,
        0.6032, 0.6486, 0.5934, 0.8409, 0.8354, 0.8512, 0.8444, 0.8447, 0.8291,
        0.8454, 0.6134, 0.6250, 0.6126, 0.8368, 0.5700, 0.5891, 0.6138, 0.8519,
        0.6397, 0.6059, 0.6389, 0.5928, 0.5635, 0.8395, 0.5888, 0.5992, 0.5741,
        0.8571, 0.5758, 0.6035, 0.5879, 0.5972, 0.8128, 0.8293, 0.6037, 0.8436,
        0.5916, 0.5944, 0.5699, 0.8104, 0.6212, 0.8330, 0.6216, 0.8550, 0.6406,
        0.8518, 0.6246, 0.6092, 0.5716, 0.5998, 0.8285, 0.8282, 0.5669, 0.8475,
        0.8345, 0.6038, 0.5826, 0.8293, 0.5579, 0.5909],
       grad_fn=<IndexBackward0>)
pred_loss tensor([0.9906], grad_fn=<MulBackward0>)
size_loss tensor(-12.0355, grad_fn=<MulBackward0>)
size_num_loss 3.6
loss: tensor([-1.4764], grad_fn=<AddBackward0>)
5
epoch:  5 ; loss:  -1.4764456748962402 ; pred:  tensor([0.0504, 0.0079, 0.0361, 0.9057], grad_fn=<SoftmaxBackward0>)
num_high 31 len(mask) 78
mask_without_small tensor([0.8648, 0.8584, 0.8499, 0.5189, 0.8448, 0.5514, 0.5972, 0.5376, 0.5692,
        0.8607, 0.5828, 0.5451, 0.5701, 0.5764, 0.5684, 0.8471, 0.8606, 0.5921,
        0.5788, 0.6261, 0.5690, 0.8526, 0.8480, 0.8612, 0.8555, 0.8557, 0.8421,
        0.8562, 0.5892, 0.6011, 0.5884, 0.8491, 0.5458, 0.5648, 0.5895, 0.8617,
        0.6166, 0.5816, 0.6157, 0.5684, 0.5394, 0.8514, 0.5645, 0.5749, 0.5499,
        0.8648, 0.5517, 0.5792, 0.5638, 0.5729, 0.8271, 0.8429, 0.5795, 0.8556,
        0.5675, 0.5702, 0.5461, 0.8247, 0.5972, 0.8463, 0.5979, 0.8652, 0.6175,
        0.8619, 0.6008, 0.5849, 0.5474, 0.5755, 0.8419, 0.8417, 0.5429, 0.8586,
        0.8475, 0.5794, 0.5584, 0.8424, 0.5344, 0.5666],
       grad_fn=<IndexBackward0>)
pred_loss tensor([1.0541], grad_fn=<MulBackward0>)
size_loss tensor(-13.7656, grad_fn=<MulBackward0>)
size_num_loss 3.1
loss: tensor([-3.7260], grad_fn=<AddBackward0>)
6
epoch:  6 ; loss:  -3.726006031036377 ; pred:  tensor([0.0523, 0.0087, 0.0390, 0.9000], grad_fn=<SoftmaxBackward0>)
num_high 30 len(mask) 78
mask_without_small tensor([0.8727, 0.8673, 0.8605, 0.8562, 0.5270, 0.5725, 0.5135, 0.5445, 0.8693,
        0.5580, 0.5208, 0.5454, 0.5517, 0.5435, 0.8582, 0.8692, 0.5674, 0.5540,
        0.6027, 0.5443, 0.8627, 0.8590, 0.8696, 0.8649, 0.8651, 0.8538, 0.8656,
        0.5644, 0.5765, 0.5636, 0.8599, 0.5215, 0.5402, 0.5648, 0.8701, 0.5926,
        0.5568, 0.5917, 0.5437, 0.5152, 0.8617, 0.5399, 0.5501, 0.5255, 0.8707,
        0.5275, 0.5544, 0.5397, 0.5483, 0.8406, 0.8553, 0.5551, 0.8665, 0.5434,
        0.5459, 0.5225, 0.8384, 0.5727, 0.8585, 0.5735, 0.8742, 0.5936, 0.8705,
        0.5763, 0.5601, 0.5231, 0.5507, 0.8541, 0.8541, 0.5188, 0.8683, 0.8593,
        0.5546, 0.5341, 0.8542, 0.5110, 0.5420], grad_fn=<IndexBackward0>)
pred_loss tensor([1.1244], grad_fn=<MulBackward0>)
size_loss tensor(-1541.8613, grad_fn=<MulBackward0>)
size_num_loss 3.0
loss: tensor([-1536.6427], grad_fn=<AddBackward0>)
7
epoch:  7 ; loss:  -1536.6427001953125 ; pred:  tensor([0.0545, 0.0096, 0.0423, 0.8936], grad_fn=<SoftmaxBackward0>)
num_high 30 len(mask) 78
mask_without_small tensor([0.8783, 0.8730, 0.8664, 0.8622, 0.5139, 0.5597, 0.5004, 0.5315, 0.8749,
        0.5451, 0.5077, 0.5324, 0.5387, 0.5303, 0.8642, 0.8748, 0.5546, 0.5411,
        0.5901, 0.5313, 0.8686, 0.8649, 0.8753, 0.8707, 0.8709, 0.8600, 0.8713,
        0.5516, 0.5638, 0.5508, 0.8659, 0.5084, 0.5272, 0.5519, 0.8757, 0.5800,
        0.5439, 0.5791, 0.5307, 0.5021, 0.8676, 0.5268, 0.5372, 0.5124, 0.8763,
        0.5146, 0.5415, 0.5269, 0.5356, 0.8474, 0.8615, 0.5425, 0.8723, 0.5306,
        0.5332, 0.5095, 0.8452, 0.5601, 0.8646, 0.5610, 0.8797, 0.5810, 0.8761,
        0.5637, 0.5473, 0.5100, 0.5378, 0.8603, 0.8603, 0.5057, 0.8740, 0.8654,
        0.5417, 0.5211, 0.8603, 0.5291], grad_fn=<IndexBackward0>)
pred_loss tensor([1.1640], grad_fn=<MulBackward0>)
size_loss tensor(-1632.8817, grad_fn=<MulBackward0>)
size_num_loss 3.0
loss: tensor([-1627.6354], grad_fn=<AddBackward0>)
8
epoch:  8 ; loss:  -1627.6353759765625 ; pred:  tensor([0.0556, 0.0101, 0.0441, 0.8901], grad_fn=<SoftmaxBackward0>)
num_high 30 len(mask) 78
mask_without_small tensor([0.8852, 0.8802, 0.8739, 0.8699, 0.5431, 0.5147, 0.8820, 0.5284, 0.5156,
        0.5219, 0.5134, 0.8718, 0.8819, 0.5379, 0.5243, 0.5738, 0.5145, 0.8759,
        0.8725, 0.8823, 0.8780, 0.8782, 0.8677, 0.8786, 0.5349, 0.5472, 0.5341,
        0.8734, 0.5103, 0.5352, 0.8828, 0.5636, 0.5271, 0.5626, 0.5139, 0.8750,
        0.5100, 0.5204, 0.8833, 0.5247, 0.5102, 0.5189, 0.8558, 0.8693, 0.5260,
        0.8795, 0.5139, 0.5166, 0.8537, 0.5436, 0.8722, 0.5446, 0.8866, 0.5645,
        0.8832, 0.5471, 0.5306, 0.5211, 0.8681, 0.8681, 0.8811, 0.8729, 0.5250,
        0.5043, 0.8681, 0.5123], grad_fn=<IndexBackward0>)
pred_loss tensor([1.2158], grad_fn=<MulBackward0>)
size_loss tensor(-1740.9232, grad_fn=<MulBackward0>)
size_num_loss 3.0
loss: tensor([-1735.6842], grad_fn=<AddBackward0>)
9
epoch:  9 ; loss:  -1735.6842041015625 ; pred:  tensor([0.0571, 0.0108, 0.0465, 0.8855], grad_fn=<SoftmaxBackward0>)
num_high 30 len(mask) 78
mask_without_small tensor([0.8927, 0.8880, 0.8821, 0.8783, 0.5241, 0.8897, 0.5092, 0.5028, 0.8801,
        0.8896, 0.5188, 0.5052, 0.5551, 0.8840, 0.8807, 0.8900, 0.8859, 0.8861,
        0.8762, 0.8865, 0.5158, 0.5282, 0.5149, 0.8815, 0.5161, 0.8904, 0.5448,
        0.5080, 0.5438, 0.8831, 0.5012, 0.8909, 0.5056, 0.8650, 0.8777, 0.5069,
        0.8874, 0.8630, 0.5246, 0.8804, 0.5256, 0.8941, 0.5457, 0.8908, 0.5282,
        0.5114, 0.5019, 0.8765, 0.8766, 0.8889, 0.8811, 0.5058, 0.8765],
       grad_fn=<IndexBackward0>)
pred_loss tensor([1.2733], grad_fn=<MulBackward0>)
size_loss tensor(-1824.1383, grad_fn=<MulBackward0>)
size_num_loss 3.0
loss: tensor([-1818.9160], grad_fn=<AddBackward0>)
10
epoch:  10 ; loss:  -1818.916015625 ; pred:  tensor([0.0587, 0.0117, 0.0492, 0.8804], grad_fn=<SoftmaxBackward0>)
num_high 30 len(mask) 78
mask_without_small tensor([0.9004, 0.8960, 0.8904, 0.8868, 0.5039, 0.8975, 0.8885, 0.8975, 0.5356,
        0.8922, 0.8891, 0.8979, 0.8940, 0.8942, 0.8849, 0.8945, 0.5081, 0.8899,
        0.8982, 0.5250, 0.5240, 0.8914, 0.8987, 0.8744, 0.8863, 0.8954, 0.8725,
        0.5045, 0.8889, 0.5056, 0.9017, 0.5259, 0.8986, 0.5081, 0.8852, 0.8853,
        0.8968, 0.8895, 0.8852], grad_fn=<IndexBackward0>)
pred_loss tensor([1.3269], grad_fn=<MulBackward0>)
size_loss tensor(-1606.0219, grad_fn=<MulBackward0>)
size_num_loss 3.0
loss: tensor([-1600.8234], grad_fn=<AddBackward0>)
11
epoch:  11 ; loss:  -1600.8233642578125 ; pred:  tensor([0.0601, 0.0125, 0.0517, 0.8757], grad_fn=<SoftmaxBackward0>)
num_high 30 len(mask) 78
mask_without_small tensor([0.9079, 0.9038, 0.8986, 0.8952, 0.9053, 0.8968, 0.9052, 0.5170, 0.9002,
        0.8974, 0.9056, 0.9020, 0.9021, 0.8934, 0.9024, 0.8981, 0.9059, 0.5061,
        0.5051, 0.8995, 0.9063, 0.8835, 0.8948, 0.9033, 0.8818, 0.8972, 0.9091,
        0.5071, 0.9063, 0.8937, 0.8937, 0.9046, 0.8978, 0.8937],
       grad_fn=<IndexBackward0>)
pred_loss tensor([1.3731], grad_fn=<MulBackward0>)
size_loss tensor(-1279.3259, grad_fn=<MulBackward0>)
size_num_loss 3.0
loss: tensor([-1274.1125], grad_fn=<AddBackward0>)
12
epoch:  12 ; loss:  -1274.112548828125 ; pred:  tensor([0.0613, 0.0132, 0.0538, 0.8717], grad_fn=<SoftmaxBackward0>)
num_high 30 len(mask) 78
mask_without_small tensor([0.9150, 0.9112, 0.9062, 0.9031, 0.9125, 0.9046, 0.9125, 0.9078, 0.9051,
        0.9128, 0.9094, 0.9096, 0.9013, 0.9099, 0.9058, 0.9131, 0.9071, 0.9135,
        0.8920, 0.9027, 0.9107, 0.8903, 0.9049, 0.9162, 0.9135, 0.9017, 0.9017,
        0.9119, 0.9055, 0.9016], grad_fn=<IndexBackward0>)
pred_loss tensor([1.4130], grad_fn=<MulBackward0>)
size_loss tensor(-61.9456, grad_fn=<MulBackward0>)
size_num_loss 3.0
loss: tensor([-56.7181], grad_fn=<AddBackward0>)
13
epoch:  13 ; loss:  -56.71814727783203 ; pred:  tensor([0.0622, 0.0138, 0.0558, 0.8682], grad_fn=<SoftmaxBackward0>)
num_high 30 len(mask) 78
mask_without_small tensor([0.9219, 0.9183, 0.9123, 0.9072, 0.9197, 0.9097, 0.9196, 0.9145, 0.9105,
        0.9199, 0.9165, 0.9167, 0.9044, 0.9170, 0.9116, 0.9202, 0.9135, 0.9206,
        0.8911, 0.9066, 0.9179, 0.8890, 0.9103, 0.9229, 0.9206, 0.9050, 0.9050,
        0.9191, 0.9111, 0.9049], grad_fn=<IndexBackward0>)
pred_loss tensor([1.4734], grad_fn=<MulBackward0>)
size_loss tensor(-83.8784, grad_fn=<MulBackward0>)
size_num_loss 3.0
loss: tensor([-78.5955], grad_fn=<AddBackward0>)
14
epoch:  14 ; loss:  -78.59552764892578 ; pred:  tensor([0.0639, 0.0147, 0.0584, 0.8630], grad_fn=<SoftmaxBackward0>)
num_high 30 len(mask) 78
mask_without_small tensor([0.9286, 0.9252, 0.9173, 0.9091, 0.9266, 0.9131, 0.9265, 0.9206, 0.9145,
        0.9268, 0.9232, 0.9234, 0.9046, 0.9238, 0.9162, 0.9271, 0.9192, 0.9274,
        0.8872, 0.9081, 0.9248, 0.8846, 0.9141, 0.9294, 0.9274, 0.9055, 0.9055,
        0.9260, 0.9155, 0.9054], grad_fn=<IndexBackward0>)
pred_loss tensor([1.5395], grad_fn=<MulBackward0>)
size_loss tensor(-115.9690, grad_fn=<MulBackward0>)
size_num_loss 3.0
loss: tensor([-110.6247], grad_fn=<AddBackward0>)
15
epoch:  15 ; loss:  -110.62467956542969 ; pred:  tensor([0.0659, 0.0157, 0.0611, 0.8573], grad_fn=<SoftmaxBackward0>)
num_high 30 len(mask) 78
mask_without_small tensor([0.9349, 0.9317, 0.9217, 0.9090, 0.9330, 0.9152, 0.9330, 0.9263, 0.9173,
        0.9333, 0.9295, 0.9297, 0.9027, 0.9302, 0.9201, 0.9335, 0.9245, 0.9338,
        0.8812, 0.9076, 0.9313, 0.8784, 0.9169, 0.9356, 0.9338, 0.9039, 0.9040,
        0.9325, 0.9190, 0.9038], grad_fn=<IndexBackward0>)
pred_loss tensor([1.6094], grad_fn=<MulBackward0>)
size_loss tensor(-154.7151, grad_fn=<MulBackward0>)
size_num_loss 3.0
loss: tensor([-149.3053], grad_fn=<AddBackward0>)
16
epoch:  16 ; loss:  -149.30528259277344 ; pred:  tensor([0.0680, 0.0167, 0.0640, 0.8513], grad_fn=<SoftmaxBackward0>)
num_high 30 len(mask) 78
mask_without_small tensor([0.9407, 0.9378, 0.9256, 0.9075, 0.9391, 0.9162, 0.9390, 0.9318, 0.9194,
        0.9393, 0.9355, 0.9357, 0.8991, 0.9362, 0.9234, 0.9395, 0.9294, 0.9398,
        0.8737, 0.9056, 0.9373, 0.8705, 0.9187, 0.9414, 0.9398, 0.9006, 0.9008,
        0.9386, 0.9218, 0.9004], grad_fn=<IndexBackward0>)
pred_loss tensor([1.6824], grad_fn=<MulBackward0>)
size_loss tensor(-198.6061, grad_fn=<MulBackward0>)
size_num_loss 3.0
loss: tensor([-193.1274], grad_fn=<AddBackward0>)
17
epoch:  17 ; loss:  -193.12738037109375 ; pred:  tensor([0.0701, 0.0178, 0.0670, 0.8452], grad_fn=<SoftmaxBackward0>)
num_high 30 len(mask) 78
mask_without_small tensor([0.9461, 0.9434, 0.9293, 0.9046, 0.9446, 0.9164, 0.9446, 0.9369, 0.9209,
        0.9448, 0.9411, 0.9413, 0.8940, 0.9418, 0.9264, 0.9450, 0.9341, 0.9453,
        0.8646, 0.9021, 0.9430, 0.8611, 0.9199, 0.9467, 0.9453, 0.8958, 0.8961,
        0.9442, 0.9242, 0.8956], grad_fn=<IndexBackward0>)
pred_loss tensor([1.7587], grad_fn=<MulBackward0>)
size_loss tensor(-246.9165, grad_fn=<MulBackward0>)
size_num_loss 3.0
loss: tensor([-241.3652], grad_fn=<AddBackward0>)
18
epoch:  18 ; loss:  -241.36517333984375 ; pred:  tensor([0.0724, 0.0189, 0.0700, 0.8387], grad_fn=<SoftmaxBackward0>)
num_high 30 len(mask) 78
mask_without_small tensor([0.9510, 0.9486, 0.9328, 0.9005, 0.9497, 0.9159, 0.9497, 0.9418, 0.9218,
        0.9499, 0.9462, 0.9465, 0.8874, 0.9470, 0.9291, 0.9501, 0.9386, 0.9503,
        0.8541, 0.8973, 0.9481, 0.8502, 0.9206, 0.9515, 0.9503, 0.8897, 0.8899,
        0.9493, 0.9263, 0.8894], grad_fn=<IndexBackward0>)
pred_loss tensor([1.8387], grad_fn=<MulBackward0>)
size_loss tensor(-299.3587, grad_fn=<MulBackward0>)
size_num_loss 3.0
loss: tensor([-293.7309], grad_fn=<AddBackward0>)
19
epoch:  19 ; loss:  -293.73089599609375 ; pred:  tensor([0.0748, 0.0201, 0.0731, 0.8320], grad_fn=<SoftmaxBackward0>)
Finished Training
sel masked ver tensor(indices=tensor([[6912, 6915, 6956, 7044, 7860, 5854, 6912, 6915, 7064,
                        7064, 7367, 7367, 7367, 7368, 7368, 7890, 5414, 5466,
                        5854, 5854, 5854, 5854, 5854, 5854, 5854, 5854, 5505,
                        5505, 5505, 5505, 5505, 5505],
                       [5854, 5854, 5854, 5854, 5854,    0, 5470, 5470, 5466,
                        5470, 5388, 5466, 5470, 5388, 5466, 5388, 5854, 5854,
                        2111,    0, 6956, 6996, 7064, 7860, 7888, 7891, 6915,
                        7367, 7368, 7860, 7867, 7891]]),
       values=tensor([0.9510, 0.9486, 0.9328, 0.9005, 0.9497, 1.5510, 0.9159,
                      0.9497, 0.9418, 0.9218, 0.9499, 0.9462, 0.9465, 0.8874,
                      0.9470, 0.9291, 0.9501, 0.9386, 0.9503, 1.6069, 0.8541,
                      0.8973, 0.9481, 0.8502, 0.9206, 0.9515, 0.9503, 0.8897,
                      0.8899, 0.9493, 0.9263, 0.8894]),
       size=(7891, 7892), nnz=32, layout=torch.sparse_coo,
       grad_fn=<SparseCooTensorWithDimsAndTensorsBackward0>)
Counter({'isAbout': 10, 'publishes': 6, 'publication': 6, 'author': 5, 'isWorkedOnBy': 2, 'fax': 1, 'phone': 1, 'name': 1})
dict index: {}
node_idx 5854
 node original label [3]
 node predicted label explain 3
 node prediction probability explain tensor([0.0925, 0.0264, 0.0758, 0.8053], grad_fn=<SoftmaxBackward0>)
 node predicted label full 3 most important relations  {'isWorkedOnBy': 2, 'publishes': 6, 'fax': 1, 'phone': 1, 'name': 1, 'publication': 6, 'isAbout': 10, 'author': 5, 'label': 3, 'node_idx': '5854'}
 final masks and lenght tensor(indices=tensor([[ 23482,  23485,  23526,  23566,  23614,  23634,  23937,
                         23938,  24429,  24430,  24437,  24458,  24460,  24461,
                         88704, 147757, 147760, 147801, 147841, 147841, 147889,
                        147909, 147909, 148212, 148212, 148212, 148213, 148213,
                        148213, 148705, 148733, 148735, 148736, 154518, 154539,
                        154544, 154549, 154554, 154558, 154560, 154576, 154596,
                        154600, 154607, 179490, 196409, 229549, 237834, 254404,
                        254404, 254404, 254404, 254404, 254404, 254404, 254404,
                        254404, 254404, 254404, 254404, 254404, 254404, 262340,
                        262340, 262340, 262340, 262340, 262340, 262340, 262340,
                        262340, 262340, 262340, 262340, 262340, 262340, 328969,
                        328969],
                       [  5854,   5854,   5854,   5854,   5854,   5854,   5854,
                          5854,   5854,   5854,   5854,   5854,   5854,   5854,
                             0,   5470,   5470,   5470,   5388,   5470,   5470,
                          5466,   5470,   5388,   5466,   5470,   5388,   5466,
                          5470,   5470,   5388,   5388,   5388,   5854,   5854,
                          5854,   5854,   5854,   5854,   5854,   5854,   5854,
                          5854,   5854,   5854,   2111,      0,   5635,   6912,
                          6915,   6956,   6996,   7044,   7064,   7367,   7368,
                          7859,   7860,   7867,   7888,   7890,   7891,   6912,
                          6915,   6956,   6996,   7044,   7064,   7367,   7368,
                          7859,   7860,   7867,   7888,   7890,   7891,   5223,
                          5230]]),
       values=tensor([0.9510, 0.9486, 0.9328, 0.3639, 0.9005, 0.4000, 0.3939,
                      0.3870, 0.3908, 0.9497, 0.3854, 0.3940, 0.3917, 0.3789,
                      1.5510, 0.9159, 0.9497, 0.3951, 0.3813, 0.4166, 0.3906,
                      0.9418, 0.9218, 0.9499, 0.9462, 0.9465, 0.8874, 0.9470,
                      0.3920, 0.3983, 0.3911, 0.9291, 0.3947, 0.3865, 0.3923,
                      0.9501, 0.4052, 0.3841, 0.4041, 0.3900, 0.3886, 0.9386,
                      0.3862, 0.3774, 0.3986, 0.9503, 1.6069, 0.3818, 0.3881,
                      0.3962, 0.8541, 0.8973, 0.3844, 0.9481, 0.3918, 0.3944,
                      0.3979, 0.8502, 0.3949, 0.9206, 0.3961, 0.9515, 0.4062,
                      0.9503, 0.3985, 0.3876, 0.3961, 0.3782, 0.8897, 0.8899,
                      0.3926, 0.9493, 0.9263, 0.3821, 0.3814, 0.8894, 0.4223,
                      0.3890]),
       size=(753935, 8285), nnz=78, layout=torch.sparse_coo,
       grad_fn=<SparseCooTensorWithDimsAndTensorsBackward0>) 32
 ---------------------------------------------------------------