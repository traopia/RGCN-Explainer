{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RGCN \n",
    "In this notebook we want to make an easy implementation of the RGCN model from scratch - namely using only Pytorch (no pyg or dgl or whatever)\n",
    "\n",
    "The first task will probably be defining a class to define a RGCN layer. \n",
    "\n",
    "In order to do that we will have to remember what we need!\n",
    "\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "import kgbench as kg\n",
    "import fire, sys\n",
    "\n",
    "from kgbench import load, tic, toc, d\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "\n",
    "#\n",
    "from torch_geometric.utils import to_networkx\n",
    "import networkx as nx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded data aifb (0.1832s).\n",
      "Number of entities: 8285\n",
      "Number of classes: 4\n",
      "Types of relations: 45\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[4938,   39, 8181],\n",
       "        [4938,   42, 8175],\n",
       "        [4938,   44, 5292],\n",
       "        ...,\n",
       "        [7628,   37,  350],\n",
       "        [7628,   39, 5220],\n",
       "        [8052,   39, 8178]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = kg.load('aifb', torch=True) \n",
    "print(f'Number of entities: {data.num_entities}') #data.i2e\n",
    "print(f'Number of classes: {data.num_classes}')\n",
    "print(f'Types of relations: {data.num_relations}') #data.i2r\n",
    "data.triples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#one adjacency \n",
    "def edge_index_oneadj(data):\n",
    "    edge_index = torch.stack((data.triples[:, 0], data.triples[:, 2]),dim=0)\n",
    "    return edge_index\n",
    "\n",
    "\n",
    "def edge_index_reladj(data):\n",
    "    data.edge_index = torch.stack((data.triples[:, 0], data.triples[:, 2]), dim=0)\n",
    "    data.edge_type = data.triples[:, 1]\n",
    "    return data.edge_index, data.edge_type\n",
    "\n",
    "\n",
    "def edge_index_dic(data):# Create an empty dictionary\n",
    "    data.edge_index = {}\n",
    "\n",
    "    # Iterate over some keys and values and append the values to the dictionary\n",
    "    for i in np.unique(data.triples[:,1]):\n",
    "        # Create some key and value (e.g. index and data)\n",
    "        key = i\n",
    "        value = [(s.item(), o.item()) for s, p, o in data.triples[torch.eq(data.triples[:, 1], i)]]\n",
    "        value = torch.Tensor(value).t().long()\n",
    "        #value = torch.Tensor(value).long()\n",
    "        # Check if the key is already in the dictionary\n",
    "        if key in data.edge_index:\n",
    "            # If the key is already in the dictionary, append the value to the existing list\n",
    "            data.edge_index[key].append(value)\n",
    "        else:\n",
    "            # If the key is not yet in the dictionary, create a new list with the value\n",
    "            data.edge_index[key] = value\n",
    "    return data.edge_index   \n",
    "\n",
    "def get_adjacency(data):\n",
    "    data.edge_index = edge_index_oneadj(data)\n",
    "    adj = torch.zeros(data.num_entities, data.num_entities)\n",
    "    for edge in data.edge_index.t():\n",
    "        adj[edge[0]][edge[1]] = 1\n",
    "        adj[edge[1]][edge[0]] = 1\n",
    "\n",
    "    return adj\n",
    "\n",
    "\n",
    "\n",
    "def get_adjacencies_dict(data):\n",
    "    data.edge_index = edge_index_dic(data)\n",
    "    adj = torch.zeros(data.num_entities, data.num_entities)\n",
    "    for i in np.unique(data.triples[:,1]):\n",
    "        for edge in data.edge_index[i].t():\n",
    "            adj[edge[0]][edge[1]] = i+1\n",
    "            adj[edge[1]][edge[0]] = i+1\n",
    "\n",
    "    \n",
    "    return adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_adjacencies_dict(data)\n",
    "# get_adjacency(data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nieghborhood\n",
    "We define functions to extract neighborhood for the sake of visualization of node connectivity (and because it will be somehow useful then)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ww/33zq_rh50tx94n81lb4thx0w0000gn/T/ipykernel_72369/4292683132.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  adj = torch.tensor(adj, dtype=torch.float)\n"
     ]
    }
   ],
   "source": [
    "def neighborhoods(adj, n_hops):\n",
    "    \"\"\"Returns the n_hops degree adjacency matrix adj.\"\"\"\n",
    "    adj = torch.tensor(adj, dtype=torch.float)\n",
    "    hop_adj = power_adj = adj\n",
    "    for i in range(n_hops - 1):\n",
    "        power_adj = power_adj @ adj\n",
    "        prev_hop_adj = hop_adj\n",
    "        hop_adj = hop_adj + power_adj\n",
    "        #print(type(hop_adj))\n",
    "        hop_adj = (hop_adj > 0).float()\n",
    "        #print(hop_adj)\n",
    "    return hop_adj.cpu().numpy().astype(int)\n",
    "\n",
    "def extract_neighborhood(node_idx,adj,n_hops):\n",
    "    \"\"\"Returns the neighborhood of a given ndoe.\"\"\"\n",
    "    neighbors_adj_row = neighborhoods(adj,n_hops)[node_idx, :] #take row of the node in the new adj matrix\n",
    "    # index of the query node in the new adj\n",
    "    node_idx_new = sum(neighbors_adj_row[:node_idx]) #sum of all the nodes before the query node (since they are 1 or 0) - it becomes count of nodes before the query node\n",
    "    neighbors = np.nonzero(neighbors_adj_row)[0] #return the indices of the nodes that are connected to the query node (and thus are non zero)\n",
    "    sub_adj = adj[neighbors][:, neighbors]\n",
    "    return node_idx_new, neighbors , sub_adj\n",
    "\n",
    "#node_idx_new, neighbors, sub_adj = extract_neighborhood(5757,get_adjacency(data),1)\n",
    "node_idx_new, neighbors, sub_adj = extract_neighborhood(5757,get_adjacencies_dict(data),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_adjacencies_dict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MICRO DATASET\n",
    "adj_dict = get_adjacencies_dict(data)\n",
    "#adj_dict = sub_adj\n",
    "\n",
    "G = nx.from_pandas_adjacency(pd.DataFrame(adj_dict.detach().numpy()))\n",
    "\n",
    "colors = [adj_dict.detach().numpy()[u][v] for u, v in G.edges()]\n",
    "\n",
    "nx.draw(G, with_labels=True, edge_color=colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AIFB\n",
    "adj_dict = sub_adj\n",
    "\n",
    "G = nx.from_pandas_adjacency(pd.DataFrame(adj_dict.detach().numpy()))\n",
    "\n",
    "colors = [adj_dict.detach().numpy()[u][v] for u, v in G.edges()]\n",
    "\n",
    "nx.draw(G, with_labels=True, edge_color=colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_dict"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation\n",
    "- add self-loops and inverse relation to the KG\n",
    "- adjacency matrix for each relation type\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enrich(triples : torch.Tensor, n : int, r: int):\n",
    "    \"\"\"\n",
    "    Enriches the given triples with self-loops and inverse relations.\n",
    "\n",
    "    \"\"\"\n",
    "    cuda = triples.is_cuda\n",
    "\n",
    "    inverses = torch.cat([\n",
    "        triples[:, 2:],\n",
    "        triples[:, 1:2] + r,\n",
    "        triples[:, :1]\n",
    "    ], dim=1)\n",
    "\n",
    "    selfloops = torch.cat([\n",
    "        torch.arange(n, dtype=torch.long,  device=d(cuda))[:, None],\n",
    "        torch.full((n, 1), fill_value=2*r),\n",
    "        torch.arange(n, dtype=torch.long, device=d(cuda))[:, None],\n",
    "    ], dim=1)\n",
    "\n",
    "    return torch.cat([triples, inverses, selfloops], dim=0)\n",
    "\n",
    "a = enrich(data.triples, data.num_entities, data.num_relations)\n",
    "len(a)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ADJ\n",
    "The whole idea of creating the adjacency matrix for multiple relation in here is based on the idea of having an adjacency matrix with size :(num_nodes * num_nodes,num_nodes * num_nodes)  where in each portion of the adjacency matrix a different relation is defined.\n",
    "\n",
    "So for instance in the first 'quadrant' nxn we have the relations of relation type 1, in the second 'quadrant' relation type 2 and so on - where to move between quadrants we 'jump' of num_nodes rows and columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here we print the adjacency matrix with original triples and where the different relation types are not taken into account\n",
    "#original adjacency - no enriched triples\n",
    "n = data.num_entities\n",
    "adj_matrix = np.zeros((n, n), dtype=int)\n",
    "\n",
    "# Fill the adjacency matrix with the edges\n",
    "for edge in edge_index_oneadj(data).t():\n",
    "#for edge in hor_ind:\n",
    "    adj_matrix[edge[0]][edge[1]] = 1\n",
    "    #adj_matrix[edge[1]][edge[0]] = 1\n",
    "\n",
    "print(adj_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adj(triples, num_nodes, num_rels, cuda=False, vertical=True):\n",
    "    \"\"\"\n",
    "     Computes a sparse adjacency matrix for the given graph (the adjacency matrices of all\n",
    "     relations are stacked vertically).\n",
    "\n",
    "     :param edges: List representing the triples\n",
    "     :param i2r: list of relations\n",
    "     :param i2n: list of nodes\n",
    "     :return: sparse tensor\n",
    "    \"\"\"\n",
    "    r, n = num_rels, num_nodes\n",
    "    size = (r * n, n) if vertical else (n, r * n)\n",
    "\n",
    "    from_indices = []\n",
    "    upto_indices = []\n",
    "\n",
    "    for s, p, o in triples:\n",
    "\n",
    "        offset = p.item() * n\n",
    "        print(offset)\n",
    "\n",
    "        if vertical:\n",
    "            s = offset + s.item()\n",
    "        else:\n",
    "            o = offset + o.item()\n",
    "\n",
    "        from_indices.append(s)\n",
    "        upto_indices.append(o)\n",
    "        \n",
    "\n",
    "    indices = torch.tensor([from_indices, upto_indices], dtype=torch.long, device=d(cuda))\n",
    "\n",
    "\n",
    "    assert indices.size(1) == len(triples)\n",
    "    assert indices[0, :].max() < size[0], f'{indices[0, :].max()}, {size}, {r}'\n",
    "    assert indices[1, :].max() < size[1], f'{indices[1, :].max()}, {size}, {r}'\n",
    "\n",
    "    return indices.t(), size\n",
    "triples = enrich(data.triples,data.num_entities, data.num_relations)\n",
    "indices, size = adj(triples, data.num_entities, triples.shape[0], cuda=False, vertical=False)\n",
    "\n",
    "#indices, size = adj(data.triples, data.num_entities, data.num_relations, cuda=False, vertical=True)\n",
    "# print(indices)\n",
    "# print(data.triples)\n",
    "\n",
    "def get_adjacency(indices):\n",
    "    #data.edge_index = edge_index_oneadj(data)\n",
    "    adj = torch.zeros(size[0]*4, size[0]*4)\n",
    "    for edge in indices.t():\n",
    "        print(edge)\n",
    "        adj[edge[0]][edge[1]] = 1\n",
    "        adj[edge[1]][edge[0]] = 1\n",
    "        #adj[edge[0]][edge[0]] = 1\n",
    "\n",
    "    return adj\n",
    "\n",
    "adj_matrix = get_adjacency(indices)\n",
    "print(adj_matrix)\n",
    "size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neighborhoods(adj, n_hops):\n",
    "    \"\"\"Returns the n_hops degree adjacency matrix adj.\"\"\"\n",
    "    adj = torch.tensor(adj, dtype=torch.float)\n",
    "    hop_adj = power_adj = adj\n",
    "    for i in range(n_hops - 1):\n",
    "        power_adj = power_adj @ adj\n",
    "        prev_hop_adj = hop_adj\n",
    "        hop_adj = hop_adj + power_adj\n",
    "        #print(type(hop_adj))\n",
    "        hop_adj = (hop_adj > 0).float()\n",
    "        #print(hop_adj)\n",
    "    return hop_adj.cpu().numpy().astype(int)\n",
    "\n",
    "def extract_neighborhood(node_idx,adj,n_hops):\n",
    "    \"\"\"Returns the neighborhood of a given ndoe.\"\"\"\n",
    "    neighbors_adj_row = neighborhoods(adj,n_hops)[node_idx, :] #take row of the node in the new adj matrix\n",
    "    # index of the query node in the new adj\n",
    "    node_idx_new = sum(neighbors_adj_row[:node_idx]) #sum of all the nodes before the query node (since they are 1 or 0) - it becomes count of nodes before the query node\n",
    "    neighbors = np.nonzero(neighbors_adj_row)[0] \n",
    "    for i in range(len(neighbors)):\n",
    "        if neighbors[i] > data.num_entities:\n",
    "            neighbors[i] = neighbors[i] - data.num_entities#return the indices of the nodes that are connected to the query node (and thus are non zero)\n",
    "    sub_adj = adj[neighbors][:, neighbors]\n",
    "    return node_idx_new, neighbors , sub_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a = get_adjacency(data)\n",
    "print(neighborhoods(adj_matrix,2))\n",
    "# print(neighborhoods(a,1))\n",
    "extract_neighborhood(1,adj_matrix,2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we show the adjacency matrix of the original triples where the different relation types are taken into account\n",
    "print('original triples:', data.triples)\n",
    "\n",
    "\n",
    "n = data.num_entities*2\n",
    "hor_graph = np.zeros((n, n), dtype=int)\n",
    "ver_graph =  np.zeros((n, n), dtype=int)\n",
    "hor_ind, hor_size = adj(data.triples, data.num_entities, data.num_relations, cuda=False, vertical=False)\n",
    "ver_ind, ver_size = adj(data.triples, data.num_entities, data.num_relations, cuda=False, vertical=True)\n",
    "\n",
    "for edge in hor_ind:\n",
    "    hor_graph[edge[0]][edge[1]] = 1\n",
    "    #adj_matrix[edge[1]][edge[0]] = 1\n",
    "    \n",
    "for edge in ver_ind:\n",
    "    ver_graph[edge[0]][edge[1]] = 1\n",
    "#print(adj_matrix)\n",
    "#print(hor_ind)\n",
    "print('relation 0 adj matrix: \\n',hor_graph[0:5][:, 0:5])\n",
    "print('relation 1 adj matrix: \\n',hor_graph[0:5][:,5:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_sparse(indices, values, size, row=True):\n",
    "    \"\"\"\n",
    "    Sum the rows or columns of a sparse matrix, and redistribute the\n",
    "    results back to the non-sparse row/column entries\n",
    "\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    ST = torch.cuda.sparse.FloatTensor if indices.is_cuda else torch.sparse.FloatTensor\n",
    "\n",
    "    assert len(indices.size()) == 2\n",
    "\n",
    "    k, r = indices.size()\n",
    "\n",
    "    if not row:\n",
    "        # transpose the matrix\n",
    "        indices = torch.cat([indices[:, 1:2], indices[:, 0:1]], dim=1)\n",
    "        size = size[1], size[0]\n",
    "\n",
    "    ones = torch.ones((size[1], 1), device=d(indices))\n",
    "\n",
    "    smatrix = ST(indices.t(), values, size=size)\n",
    "    sums = torch.mm(smatrix, ones) # row/column sums\n",
    "\n",
    "    sums = sums[indices[:, 0]]\n",
    "\n",
    "    assert sums.size() == (k, 1)\n",
    "\n",
    "    return sums.view(k)\n",
    "\n",
    "#sum_sparse(indices, ,size, row=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the RGCN class module"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#triples = enrich(data.triples, data.num_entities, data.num_relations)\n",
    "triples = data.triples\n",
    "hor_ind, hor_size = adj(triples, data.num_entities, 2*data.num_relations+1, cuda=False, vertical=False)\n",
    "ver_ind, ver_size = adj(triples, data.num_entities, 2*data.num_relations+1, cuda=False, vertical=True)\n",
    "_, rn = hor_size #horizontally stacked adjacency matrix size\n",
    "r = rn // data.num_entities \n",
    "print('r:', r)\n",
    "print('rn:', rn)\n",
    "\n",
    "vals = torch.ones(ver_ind.size(0), dtype=torch.float) #number of edges\n",
    "vals = vals / sum_sparse(ver_ind, vals, ver_size)\n",
    "print('vals is', vals)\n",
    "\n",
    "hor_graph = torch.sparse.FloatTensor(indices=hor_ind.t(), values=vals, size=hor_size)\n",
    "print('hor graph is: ', hor_graph)\n",
    "\n",
    "ver_graph = torch.sparse.FloatTensor(indices=ver_ind.t(), values=vals, size=ver_size)\n",
    "print('ver graph is: ', ver_graph)\n",
    "# weights1 = nn.Parameter(torch.FloatTensor(r, data.num_entities, 16))\n",
    "# print('weights1 is: ', weights1, 'shape is: ', weights1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n, r = data.num_entities, data.num_relations\n",
    "ind,size = adj(enrich(data.triples,n,r), n, 2*r+1, vertical=True) #size is r*n where r is 5\n",
    "vals = torch.ones(ind.size(0), dtype=torch.float) #number of edges\n",
    "vals = vals / sum_sparse(ind, vals, size)\n",
    "ind.size(1)\n",
    "enrich(data.triples,n,r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = kg.load('micro', torch=True) \n",
    "print(f'Number of entities: {data.num_entities}') #data.i2e\n",
    "print(f'Number of classes: {data.num_classes}')\n",
    "print(f'Types of relations: {data.num_relations}') #data.i2r\n",
    "data.triples\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RGCN\n",
    "- Horizontal indexes are: edge indexes when edge connections stacked horizontally\n",
    "- Vertical indexes: // vertically\n",
    "- hor graph is the horizontal adjacency "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IDEA\n",
    "Use the stacked adjacecny as it is - but find a way to index back each node to its original - it should just be a matter of dividing by the offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idxt, clst = data.training[:, 0], data.training[:, 1]\n",
    "idxw, clsw = data.withheld[:, 0], data.withheld[:, 1]\n",
    "idxt, clst = idxt.long(), clst.long()\n",
    "idxw, clsw = idxw.long(), clsw.long()\n",
    "data.training.data\n",
    "data.withheld\n",
    "data.num_classes #4 Possible values for y\n",
    "data.training.data.shape\n",
    "data.withheld\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RGCN(nn.Module):\n",
    "    \"\"\"\n",
    "    Classic RGCN\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, triples, n, r, numcls, emb=16, bases=None):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.emb = emb\n",
    "        self.bases = bases\n",
    "        self.numcls = numcls\n",
    "\n",
    "        self.triples = enrich(triples, n, r)\n",
    "\n",
    "        # horizontally and vertically stacked versions of the adjacency graph\n",
    "        hor_ind, hor_size = adj(self.triples, n, 2*r+1, vertical=False)\n",
    "        ver_ind, ver_size = adj(self.triples, n, 2*r+1, vertical=True)\n",
    "        #number of relations is 2*r+1 because we added the inverse and self loop\n",
    "\n",
    "        _, rn = hor_size #horizontally stacked adjacency matrix size\n",
    "        r = rn // n #number of relations enriched divided by number of nodes\n",
    "\n",
    "        vals = torch.ones(ver_ind.size(0), dtype=torch.float) #number of enriched triples\n",
    "        vals = vals / sum_sparse(ver_ind, vals, ver_size) #normalize the values by the number of edges\n",
    "\n",
    "        hor_graph = torch.sparse.FloatTensor(indices=hor_ind.t(), values=vals, size=hor_size) #size: n,r, emb\n",
    "        \n",
    "        \n",
    "        self.register_buffer('hor_graph', hor_graph)\n",
    "\n",
    "        ver_graph = torch.sparse.FloatTensor(indices=ver_ind.t(), values=vals, size=ver_size)\n",
    "        self.register_buffer('ver_graph', ver_graph)\n",
    "\n",
    "        # layer 1 weights\n",
    "        if bases is None:\n",
    "            self.weights1 = nn.Parameter(torch.FloatTensor(r, n, emb))\n",
    "            nn.init.xavier_uniform_(self.weights1, gain=nn.init.calculate_gain('relu'))\n",
    "\n",
    "            self.bases1 = None\n",
    "        else:\n",
    "            self.comps1 = nn.Parameter(torch.FloatTensor(r, bases))\n",
    "            nn.init.xavier_uniform_(self.comps1, gain=nn.init.calculate_gain('relu'))\n",
    "\n",
    "            self.bases1 = nn.Parameter(torch.FloatTensor(bases, n, emb))\n",
    "            nn.init.xavier_uniform_(self.bases1, gain=nn.init.calculate_gain('relu'))\n",
    "\n",
    "        # layer 2 weights\n",
    "        if bases is None:\n",
    "\n",
    "            self.weights2 = nn.Parameter(torch.FloatTensor(r, emb, numcls) )\n",
    "            nn.init.xavier_uniform_(self.weights2, gain=nn.init.calculate_gain('relu'))\n",
    "\n",
    "            self.bases2 = None\n",
    "        else:\n",
    "            self.comps2 = nn.Parameter(torch.FloatTensor(r, bases))\n",
    "            nn.init.xavier_uniform_(self.comps2, gain=nn.init.calculate_gain('relu'))\n",
    "\n",
    "            self.bases2 = nn.Parameter(torch.FloatTensor(bases, emb, numcls))\n",
    "            nn.init.xavier_uniform_(self.bases2, gain=nn.init.calculate_gain('relu'))\n",
    "\n",
    "        self.bias1 = nn.Parameter(torch.FloatTensor(emb).zero_())\n",
    "        self.bias2 = nn.Parameter(torch.FloatTensor(numcls).zero_())\n",
    "\n",
    "    def forward2(self, hor_graph, ver_graph):\n",
    "\n",
    "\n",
    "        ## Layer 1\n",
    "\n",
    "        n, rn = hor_graph.size() #horizontally stacked adjacency matrix size\n",
    "        r = rn // n\n",
    "        e = self.emb\n",
    "        b, c = self.bases, self.numcls\n",
    "\n",
    "        if self.bases1 is not None:\n",
    "            # weights = torch.einsum('rb, bij -> rij', self.comps1, self.bases1)\n",
    "            weights = torch.mm(self.comps1, self.bases1.view(b, n*e)).view(r, n, e)\n",
    "        else:\n",
    "            weights = self.weights1\n",
    "\n",
    "        assert weights.size() == (r, n, e) #r relations, n nodes, e embedding size\n",
    "\n",
    "        # Apply weights and sum over relations\n",
    "        #hidden layer\n",
    "        h = torch.mm(hor_graph, weights.view(r*n, e))  #matmul with horizontally stacked adjacency matrix and initialized weights\n",
    "        assert h.size() == (n, e)\n",
    "\n",
    "        h = F.relu(h + self.bias1) #apply non linearity and add bias\n",
    "\n",
    "        ## Layer 2\n",
    "\n",
    "        # Multiply adjacencies by hidden\n",
    "        h = torch.mm(ver_graph, h) # sparse mm\n",
    "        h = h.view(r, n, e) # new dim for the relations\n",
    "\n",
    "        if self.bases2 is not None:\n",
    "            # weights = torch.einsum('rb, bij -> rij', self.comps2, self.bases2)\n",
    "            weights = torch.mm(self.comps2, self.bases2.view(b, e * c)).view(r, e, c)\n",
    "        else:\n",
    "            weights = self.weights2\n",
    "\n",
    "        # Apply weights, sum over relations\n",
    "        # h = torch.einsum('rhc, rnh -> nc', weights, h)\n",
    "        h = torch.bmm(h, weights).sum(dim=0)\n",
    "\n",
    "        assert h.size() == (n, c)\n",
    "\n",
    "        return h + self.bias2 # -- softmax is applied in the loss\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    def forward(self):\n",
    "\n",
    "\n",
    "        ## Layer 1\n",
    "\n",
    "        n, rn = self.hor_graph.size() #horizontally stacked adjacency matrix size\n",
    "        r = rn // n\n",
    "        e = self.emb\n",
    "        b, c = self.bases, self.numcls\n",
    "\n",
    "        if self.bases1 is not None:\n",
    "            # weights = torch.einsum('rb, bij -> rij', self.comps1, self.bases1)\n",
    "            weights = torch.mm(self.comps1, self.bases1.view(b, n*e)).view(r, n, e)\n",
    "        else:\n",
    "            weights = self.weights1\n",
    "\n",
    "        assert weights.size() == (r, n, e) #r relations, n nodes, e embedding size\n",
    "\n",
    "        # Apply weights and sum over relations\n",
    "        #hidden layer\n",
    "        h = torch.mm(self.hor_graph, weights.view(r*n, e))  #matmul with horizontally stacked adjacency matrix and initialized weights\n",
    "        assert h.size() == (n, e)\n",
    "\n",
    "        h = F.relu(h + self.bias1) #apply non linearity and add bias\n",
    "\n",
    "        ## Layer 2\n",
    "\n",
    "        # Multiply adjacencies by hidden\n",
    "        h = torch.mm(self.ver_graph, h) # sparse mm\n",
    "        h = h.view(r, n, e) # new dim for the relations\n",
    "\n",
    "        if self.bases2 is not None:\n",
    "            # weights = torch.einsum('rb, bij -> rij', self.comps2, self.bases2)\n",
    "            weights = torch.mm(self.comps2, self.bases2.view(b, e * c)).view(r, e, c)\n",
    "        else:\n",
    "            weights = self.weights2\n",
    "\n",
    "        # Apply weights, sum over relations\n",
    "        # h = torch.einsum('rhc, rnh -> nc', weights, h)\n",
    "        h = torch.bmm(h, weights).sum(dim=0)\n",
    "\n",
    "        assert h.size() == (n, c)\n",
    "\n",
    "        return h + self.bias2 # -- softmax is applied in the loss\n",
    "\n",
    "    def penalty(self, p=2):\n",
    "        \"\"\"\n",
    "        L2 penalty on the weights\n",
    "        \"\"\"\n",
    "        assert p==2\n",
    "\n",
    "        if self.bases is None:\n",
    "            return self.weights1.pow(2).sum()\n",
    "\n",
    "        return self.comps1.pow(p).sum() + self.bases1.pow(p).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def go(name='aifb', lr=0.01, wd=0.0, l2=0.0, epochs=50, prune=False, optimizer='adam', final=True,  emb=16, bases=None, printnorms=True):\n",
    "\n",
    "    include_val = name in ('aifb','mutag','bgs','am')\n",
    "    # -- For these datasets, the validation is added to the training for the final eval.\n",
    "\n",
    "    data = load(name, torch=True, prune_dist=2 if prune else None, final=final, include_val=include_val)\n",
    "\n",
    "    print(f'{data.triples.size(0)} triples')\n",
    "    print(f'{data.num_entities} entities')\n",
    "    print(f'{data.num_relations} relations')\n",
    "\n",
    "    tic()\n",
    "    rgcn = RGCN(data.triples, n=data.num_entities, r=data.num_relations, numcls=data.num_classes, emb=emb, bases=bases)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        print('Using cuda.')\n",
    "        rgcn.cuda()\n",
    "\n",
    "        data.training = data.training.cuda()\n",
    "        data.withheld = data.withheld.cuda()\n",
    "\n",
    "    print(f'construct: {toc():.5}s')\n",
    "\n",
    "    if optimizer == 'adam':\n",
    "        opt = torch.optim.Adam(lr=lr, weight_decay=wd, params=rgcn.parameters())\n",
    "    elif optimizer == 'adamw':\n",
    "        opt = torch.optim.AdamW(lr=lr, weight_decay=wd, params=rgcn.parameters())\n",
    "    else:\n",
    "        raise Exception(f'Optimizer {optimizer} not known')\n",
    "\n",
    "    for e in range(epochs):\n",
    "        tic()\n",
    "        opt.zero_grad()\n",
    "        out = rgcn()\n",
    "        print('out',out)\n",
    "\n",
    "        idxt, clst = data.training[:, 0], data.training[:, 1]\n",
    "        idxw, clsw = data.withheld[:, 0], data.withheld[:, 1]\n",
    "        idxt, clst = idxt.long(), clst.long()\n",
    "        idxw, clsw = idxw.long(), clsw.long()\n",
    "        out_train = out[idxt, :]\n",
    "        loss = F.cross_entropy(out_train, clst, reduction='mean')\n",
    "        if l2 != 0.0:\n",
    "            loss = loss + l2 * rgcn.penalty()\n",
    "\n",
    "        # compute performance metrics\n",
    "        with torch.no_grad():\n",
    "            training_acc = (out[idxt, :].argmax(dim=1) == clst).sum().item() / idxt.size(0)\n",
    "            withheld_acc = (out[idxw, :].argmax(dim=1) == clsw).sum().item() / idxw.size(0)\n",
    "            #torch.save(out[idxw, :].argmax(dim=1) , 'aifb_chk/prediction_aifb')\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        if printnorms is not None:\n",
    "            # Print relation norms layer 1\n",
    "            nr = data.num_relations\n",
    "            weights = rgcn.weights1 if bases is None else rgcn.comps1\n",
    "\n",
    "            ctr = Counter()\n",
    "\n",
    "            for r in range(nr):\n",
    "\n",
    "                ctr[data.i2r[r]] = weights[r].norm()\n",
    "                ctr['inv_'+ data.i2r[r]] = weights[r+nr].norm()\n",
    "\n",
    "            print('relations with largest weight norms in layer 1.')\n",
    "            for rel, w in ctr.most_common(printnorms):\n",
    "                print(f'     norm {w:.4} for {rel} ')\n",
    "\n",
    "            weights = rgcn.weights2 if bases is None else rgcn.comps2\n",
    "\n",
    "            ctr = Counter()\n",
    "            for r in range(nr):\n",
    "\n",
    "                ctr[data.i2r[r]] = weights[r].norm()\n",
    "                ctr['inv_'+ data.i2r[r]] = weights[r+nr].norm()\n",
    "\n",
    "            print('relations with largest weight norms in layer 2.')\n",
    "            for rel, w in ctr.most_common(printnorms):\n",
    "                print(f'     norm {w:.4} for {rel} ')\n",
    "        torch.save(out[idxw, :], 'aifb_chk/prediction_aifb')\n",
    "        torch.save(rgcn,'model_aifb')\n",
    "        print(f'epoch {e:02}: loss {loss:.2}, train acc {training_acc:.2}, \\t withheld acc {withheld_acc:.2} \\t ({toc():.5}s)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('arguments ', ' '.join(sys.argv))\n",
    "fire.Fire(go)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('model_aifb')\n",
    "pred = model.forward2(hor_graph, ver_graph)\n",
    "pred.argmax(dim=1).shape\n",
    "#data.training[:,1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SO\n",
    "So instead of adjacency matrix and x - we could use for the forward hor_graph /ver_graph - the problem is how to define the neighborhood subgraph???\n",
    "\n",
    "ALSO the problem is that we cant just go to a matrix from the edges (sparse tensor)- it woukld be too big and computationally expensive!!\n",
    "\n",
    "Have to find another way to get the neighborsss\n",
    "\n",
    "\n",
    "through indexes maybeeee\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_adjacency(hor_graph):\n",
    "    adj = torch.zeros(hor_graph.size(1),hor_graph.size(1))\n",
    "    for edge in hor_graph.coalesce().indices().t():\n",
    "        adj[edge[0]][edge[1]] = 1\n",
    "    return adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_adjacency(hor_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.withheld[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "753935/8285 #this is the number of relations 45*2 +1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in data.withheld[:,0]:\n",
    "    if i in sparse_tensor.indices().t()[:,0]:\n",
    "        print(sparse_tensor.indices().t()[:,0][i])\n",
    "\n",
    "# data.withheld[:,0]\n",
    "# sparse_tensor.indices().t()[:,0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ChatGPT implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_sparse import SparseTensor\n",
    "\n",
    "class RGCNLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features, num_rels):\n",
    "        super(RGCNLayer, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.num_rels = num_rels\n",
    "        self.weight = nn.Parameter(torch.Tensor(self.num_rels, self.in_features, self.out_features))\n",
    "        nn.init.xavier_uniform_(self.weight, gain=nn.init.calculate_gain('relu'))\n",
    "\n",
    "    def forward(self, x, edge_index, edge_type):\n",
    "        # x = data.triples\n",
    "        # edge_index = data.edge_index.t()\n",
    "        # edge_type = data.edge_type\n",
    "        num_nodes = x.size(0)\n",
    "        num_edges = edge_index.size(1)\n",
    "        rel_weights = self.weight[edge_type].view(num_edges, self.in_features, self.out_features)\n",
    "        rel_adj_list = [[] for _ in range(self.num_rels)]\n",
    "        for i in range(num_edges):\n",
    "            src = edge_index[0][i].item()\n",
    "            dst = edge_index[1][i].item()\n",
    "            rel = edge_type[i].item()\n",
    "            rel_adj_list[rel].append((src, dst))\n",
    "        h = x.unsqueeze(1)\n",
    "        rel_messages = []\n",
    "        for r in range(self.num_rels):\n",
    "            if len(rel_adj_list[r]) == 0:\n",
    "                continue\n",
    "            rel_adj = torch.LongTensor(rel_adj_list[r]).t()\n",
    "            adj_t = SparseTensor(row=rel_adj[1], col=rel_adj[0], sparse_sizes=(num_nodes, num_nodes))\n",
    "            rel_messages.append(adj_t.matmul(h).squeeze())\n",
    "        rel_messages = torch.cat(rel_messages, dim=1)\n",
    "        output = torch.bmm(rel_messages.unsqueeze(1), rel_weights).squeeze()\n",
    "        return F.relu(output)\n",
    "\n",
    "class RGCN(nn.Module):\n",
    "    def __init__(self, num_entities, num_relations, hidden_dim, num_classes):\n",
    "        super(RGCN, self).__init__()\n",
    "        self.num_entities = num_entities\n",
    "        self.num_relations = num_relations\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_classes = num_classes\n",
    "        self.embedding = nn.Embedding(self.num_entities, self.hidden_dim)\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(RGCNLayer(self.hidden_dim, self.hidden_dim, self.num_relations))\n",
    "        self.layers.append(RGCNLayer(self.hidden_dim, self.num_classes, self.num_relations))\n",
    "\n",
    "    def forward(self, x, edge_index, edge_type):\n",
    "        x = self.embedding(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, edge_index, edge_type)\n",
    "        return F.softmax(x, dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #data.triples[[data.training[:,0]]]\n",
    "# data.triples[[data.training[:,0].detach().numpy()]]\n",
    "# #data.training[:,0].detach().numpy()\n",
    "# data.edge_index.t()[[data.training[:,0].detach().numpy()]].detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = RGCN(data.num_entities, data.num_relations, 16, data.num_classes).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "def train():\n",
    "      model.train()\n",
    "      optimizer.zero_grad()  # Clear gradients.\n",
    "      #out = model(data.triples[[data.training[:,0].detach().numpy()]].detach().numpy(), data.edge_index.t()[[data.training[:,0].detach().numpy()]].detach().numpy(), data.edge_type()[[data.training[:,0].detach().numpy()]].detach().numpy())  # Perform a single forward pass.\n",
    "      out = model(data.triples, data.edge_index,data.edge_type)\n",
    "      loss = criterion(out[data.training[:,0]], data.training[:,1])  # Compute the loss solely based on the training nodes.\n",
    "      loss.backward()  # Derive gradients.\n",
    "      optimizer.step()  # Update parameters based on gradients.\n",
    "      return loss\n",
    "\n",
    "# def test():\n",
    "#       model.eval()\n",
    "#       out = model(data.x, get_adjacency(data))\n",
    "#       pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "#       test_correct = pred[data.test_mask] == data.y[data.test_mask]  # Check against ground-truth labels.\n",
    "#       test_acc = int(test_correct.sum()) / int(data.test_mask.sum())  # Derive ratio of correct predictions.\n",
    "#       torch.save(out, 'cora_chk/prediction_cora')\n",
    "#       return test_acc\n",
    "\n",
    "\n",
    "for epoch in tqdm(range(1, 401)):\n",
    "    loss = train()\n",
    "\n",
    "# test_acc = test()\n",
    "# print(f'Test Accuracy: {test_acc:.4f}') \n",
    "\n",
    "\n",
    "model.eval()\n",
    "torch.save(model, 'rel')\n",
    "\n",
    "# pred, adj = model(data.x, get_adjacency(data))\n",
    "# pred_label = pred.argmax(dim=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c30f2af5f468e7f5b45bcc30fca5f4886c90d54777aed916ed5f6294dfb24bf2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
