{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "import kgbench as kg\n",
    "import fire, sys\n",
    "import math\n",
    "\n",
    "from kgbench import load, tic, toc, d\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "\n",
    "#\n",
    "from torch_geometric.utils import to_networkx\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.3367,  0.1288,  0.2345,  0.2303],\n",
      "        [-1.1229, -0.1863,  2.2082, -0.6380],\n",
      "        [ 0.4617,  0.2674,  0.5349,  0.8094]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.3367,  0.1288,  0.2345,  0.2303],\n",
      "        [-1.1229, -0.1863,  2.2082, -0.6380],\n",
      "        [ 0.4617,  0.2674,  0.5349,  0.8094]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "def adj_feat_grad(node_idx, pred_label_node, model , adj):\n",
    "    \"\"\"\n",
    "    Compute the gradient of the prediction w.r.t. the adjacency matrix\n",
    "    and the node features.\n",
    "    \"\"\"\n",
    "    model.zero_grad()\n",
    "    adj.requires_grad = True\n",
    "    print(adj)\n",
    "    if adj.grad is not None:\n",
    "        adj.grad.zero_() # zero out the gradient\n",
    "\n",
    "\n",
    "    adj = adj\n",
    "    ypred, _ = model( adj)\n",
    "\n",
    "    logit = nn.Softmax(dim=0)(ypred[ node_idx, :])\n",
    "    logit = logit[pred_label_node]\n",
    "    loss = -torch.log(logit)\n",
    "    loss.backward()\n",
    "    return adj.grad\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded data aifb (0.177s).\n",
      "Number of entities: 8285\n",
      "Number of classes: 4\n",
      "Types of relations: 45\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: [5678,\n",
       "  5724,\n",
       "  5699,\n",
       "  5688,\n",
       "  5702,\n",
       "  5714,\n",
       "  5708,\n",
       "  5843,\n",
       "  5873,\n",
       "  5697,\n",
       "  5783,\n",
       "  5701,\n",
       "  5845,\n",
       "  5778],\n",
       " 1: [5731, 5905, 5808, 5785],\n",
       " 2: [5757, 5797, 5900, 5677, 5791, 5811, 5831, 5839, 5755, 5844, 5861],\n",
       " 3: [5857, 5752, 5795, 5753, 5798, 5854]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = kg.load('aifb', torch=True) \n",
    "print(f'Number of entities: {data.num_entities}') #data.i2e\n",
    "print(f'Number of classes: {data.num_classes}')\n",
    "print(f'Types of relations: {data.num_relations}') #data.i2r\n",
    "\n",
    "\n",
    "d = {key.item(): data.withheld[:, 0][data.withheld[:, 1] == key].tolist() for key in torch.unique(data.withheld[:, 1])}\n",
    "# for i in d[0]:\n",
    "#     print(data.i2e[i])\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[k for k, v in d.items() if 5757 in v]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[58], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfor\u001b[39;00m i,j \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(data\u001b[39m.\u001b[39mtriples,\u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(data\u001b[39m.\u001b[39mtriples))):\n\u001b[0;32m----> 2\u001b[0m     \u001b[39mif\u001b[39;00m i[:,\u001b[39m0\u001b[39;49m][j]\u001b[39m==\u001b[39m\u001b[39m5757\u001b[39m:\n\u001b[1;32m      3\u001b[0m         \u001b[39mprint\u001b[39m(i[:,\u001b[39m0\u001b[39m])\n\u001b[1;32m      4\u001b[0m         \u001b[39mprint\u001b[39m(i[:,\u001b[39m1\u001b[39m])\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for tensor of dimension 1"
     ]
    }
   ],
   "source": [
    "for i,j in zip(data.triples,range(len(data.triples))):\n",
    "    if i[:,0][j]==5757:\n",
    "        print(i[:,0])\n",
    "        print(i[:,1])\n",
    "        print(i[:,2])\n",
    "    #print(data.i2e[i[0]], data.i2r[i[1]], data.i2e[i[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29043"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data.triples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#group 1\n",
    "high = [Counter({'isWorkedOnBy': 11, 'author': 11, 'isAbout': 8, 'member': 2, 'dealtWithIn': 1, 'fax': 1, 'phone': 1, 'name': 1, 'homepage': 1, '1999': 1, 'photo': 1, 'projectInfo': 1, 'hasProject': 1}), Counter({'fax': 1, 'phone': 1, 'name': 1, '1999': 1, 'photo': 1, 'publication': 1, 'author': 1}), Counter({'isAbout': 4, 'author': 3, 'member': 2, 'hasProject': 2, 'isWorkedOnBy': 1, 'dealtWithIn': 1, 'carriesOut': 1, 'fax': 1, 'phone': 1, 'name': 1, 'worksAtProject': 1, 'photo': 1, 'homepage': 1, 'carriedOutBy': 1}), Counter({'author': 7, 'isWorkedOnBy': 5, 'dealtWithIn': 3, 'hasProject': 3, 'member': 2, 'carriesOut': 1, 'phone': 1, 'fax': 1, 'name': 1, 'photo': 1, 'worksAtProject': 1, 'carriedOutBy': 1, 'isAbout': 1})]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#group 1\n",
    "high1 = [Counter({'isWorkedOnBy': 11, 'author': 11, 'isAbout': 8, 'member': 2, 'dealtWithIn': 1, 'fax': 1, 'phone': 1, 'name': 1, 'homepage': 1, '1999': 1, 'photo': 1, 'projectInfo': 1, 'hasProject': 1}), Counter({'fax': 1, 'phone': 1, 'name': 1, '1999': 1, 'photo': 1, 'publication': 1}), Counter({'isAbout': 4, 'author': 3, 'member': 2, 'hasProject': 2, 'isWorkedOnBy': 1, 'dealtWithIn': 1, 'carriesOut': 1, 'fax': 1, 'phone': 1, 'name': 1, 'worksAtProject': 1, 'photo': 1, 'homepage': 1, 'carriedOutBy': 1}), Counter({'author': 7, 'isWorkedOnBy': 5, 'dealtWithIn': 3, 'hasProject': 3, 'member': 2, 'carriesOut': 1, 'phone': 1, 'fax': 1, 'name': 1, 'photo': 1, 'worksAtProject': 1, 'carriedOutBy': 1, 'isAbout': 1})]\n",
    "low = [Counter({'publication': 11, 'isAbout': 7, 'worksAtProject': 2}), Counter({'author': 1}), Counter({'publishes': 3, 'publication': 3, '1999': 2, 'projectInfo': 2}), Counter({'isAbout': 8, 'publishes': 7, 'publication': 7, 'projectInfo': 3, '1999': 2})]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'Forschungsgruppen': 19, 'Personen': 19, 'Projekte': 15, 'Publikationen': 2})\n"
     ]
    }
   ],
   "source": [
    "list_entities = [5502, 5502, 5937, 5937, 5939, 5939, 5939, 5939, 7393,\n",
    "                        7393, 5939, 5939, 5939, 5939, 5939, 5939, 5939, 5939,\n",
    "                        5939, 5678, 5678, 5678, 5678, 5678, 5678, 5678, 5678,\n",
    "                        5678, 5678, 5678, 5678, 5678, 5678, 5678, 5678, 5678,\n",
    "                        5502, 5502, 5502, 5502, 5502, 5502, 5502, 5502, 5502,\n",
    "                        5502, 5502, 5502, 5502, 5502, 5502, 5502, 5502, 5678,\n",
    "                        5678]\n",
    "l = []\n",
    "for i in list_entities:\n",
    "    \n",
    "    l.append(data.i2e[i][0].split('/')[3])\n",
    "print(Counter(l))\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEPS\n",
    "1. edge index\n",
    "2. Neighborhood\n",
    "3. Extract neighborhood\n",
    "4. Match to triples\n",
    "5. Match to classes\n",
    "6. Preprocess data: enrich, sum sparse, adj \n",
    "6. Get hor ver edges (adjacency for rels)\n",
    "7. Edge mask\n",
    "8. Masked hor ver edges\n",
    "9. Train and save model and predictions\n",
    "10. Loss function\n",
    "11. Explain class - namely put all together\n",
    "12. Forward and optimize for the masked adjacency\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#edge index\n",
    "def edge_index_oneadj(triples):\n",
    "    edge_index = torch.stack((triples[:, 0], triples[:, 2]),dim=0)\n",
    "    return edge_index\n",
    "\n",
    "#Neighborhood\n",
    "#Extract neighborhood\n",
    "\n",
    "def find_n_hop_neighbors(edge_index, n, node=None):\n",
    "    \"\"\" \n",
    "    edge_index \n",
    "    n = num hops\n",
    "    node = node_idx\n",
    "    \"\"\"\n",
    "    # create dictionary of node neighborhoods\n",
    "    neighborhoods = {}\n",
    "    for i in range(edge_index.max().item() + 1):\n",
    "        neighborhoods[i] = set()\n",
    "\n",
    "    # find 1-hop neighbors and corresponding edges\n",
    "    edges = []\n",
    "    for j in range(edge_index.shape[1]):\n",
    "        src, dst = edge_index[0, j].item(), edge_index[1, j].item()\n",
    "        neighborhoods[src].add(dst)\n",
    "        neighborhoods[dst].add(src)\n",
    "        edges.append((src, dst))\n",
    "\n",
    "    # find n-hop neighbors for the specified node or all nodes\n",
    "\n",
    "    for k in range(2, n+1):\n",
    "        new_neighbors = set()\n",
    "        for neighbor in neighborhoods[node]:\n",
    "            new_neighbors.update(neighborhoods[neighbor])\n",
    "        neighborhoods[node].update(new_neighbors)\n",
    "    sub_edges = []\n",
    "    for edge in edges:\n",
    "        src, dst = edge\n",
    "        if src in neighborhoods[node] and dst in neighborhoods[node] or src == node or dst == node:\n",
    "            sub_edges.append(edge)\n",
    "            \n",
    "    sub_edges_tensor = torch.tensor([sub_edges[i] for i in range(len(sub_edges))]).t()        \n",
    "\n",
    "    #return {node: sub_edges}, {node: neighborhoods[node]}, sub_edges_tensor\n",
    "    return sub_edges, neighborhoods[node], sub_edges_tensor\n",
    "\n",
    "\n",
    "#match triples \n",
    "def match_to_triples(tensor1, tensor2):\n",
    "    \"\"\"\n",
    "    tensor1: sub_edge tensor: edges of the neighborhood - transpose!!\n",
    "    tensor2: data.triples: all edges\n",
    "    \"\"\"\n",
    "    matching = []\n",
    "    for i,i2 in zip(tensor1[:,0],tensor1[:,1]):\n",
    "        for j,j1,j2, index in zip(tensor2[:,0],tensor2[:,1],  tensor2[:,2], range(len(tensor2[:,0]))):\n",
    "            if i == j and i2 == j2:\n",
    "                matching.append(tensor2[index])\n",
    "\n",
    "    result = torch.stack(matching)\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "#PREPROCESS\n",
    "def enrich(triples : torch.Tensor, n : int, r: int):\n",
    "    \"\"\"\n",
    "    Enriches the given triples with self-loops and inverse relations.\n",
    "\n",
    "    \"\"\"\n",
    "    cuda = triples.is_cuda\n",
    "\n",
    "    inverses = torch.cat([\n",
    "        triples[:, 2:],\n",
    "        triples[:, 1:2] + r,\n",
    "        triples[:, :1]\n",
    "    ], dim=1)\n",
    "\n",
    "    selfloops = torch.cat([\n",
    "        torch.arange(n, dtype=torch.long,  device=d(cuda))[:, None],\n",
    "        torch.full((n, 1), fill_value=2*r),\n",
    "        torch.arange(n, dtype=torch.long, device=d(cuda))[:, None],\n",
    "    ], dim=1)\n",
    "\n",
    "    return torch.cat([triples, inverses, selfloops], dim=0)\n",
    "\n",
    "def sum_sparse(indices, values, size, row=True):\n",
    "    \"\"\"\n",
    "    Sum the rows or columns of a sparse matrix, and redistribute the\n",
    "    results back to the non-sparse row/column entries\n",
    "\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    ST = torch.cuda.sparse.FloatTensor if indices.is_cuda else torch.sparse.FloatTensor\n",
    "\n",
    "    assert len(indices.size()) == 2\n",
    "\n",
    "    k, r = indices.size()\n",
    "\n",
    "    if not row:\n",
    "        # transpose the matrix\n",
    "        indices = torch.cat([indices[:, 1:2], indices[:, 0:1]], dim=1)\n",
    "        size = size[1], size[0]\n",
    "\n",
    "    ones = torch.ones((size[1], 1), device=d(indices))\n",
    "\n",
    "    smatrix = ST(indices.t(), values, size=size)\n",
    "    sums = torch.mm(smatrix, ones) # row/column sums\n",
    "\n",
    "    sums = sums[indices[:, 0]]\n",
    "\n",
    "    assert sums.size() == (k, 1)\n",
    "\n",
    "    return sums.view(k)\n",
    "\n",
    "def adj(triples, num_nodes, num_rels, cuda=False, vertical=True):\n",
    "    \"\"\"\n",
    "     Computes a sparse adjacency matrix for the given graph (the adjacency matrices of all\n",
    "     relations are stacked vertically).\n",
    "\n",
    "     :param edges: List representing the triples\n",
    "     :param i2r: list of relations\n",
    "     :param i2n: list of nodes\n",
    "     :return: sparse tensor\n",
    "    \"\"\"\n",
    "    r, n = num_rels, num_nodes\n",
    "    size = (r * n, n) if vertical else (n, r * n)\n",
    "\n",
    "    from_indices = []\n",
    "    upto_indices = []\n",
    "\n",
    "    for s, p, o in triples:\n",
    "\n",
    "        offset = p.item() * n\n",
    "        \n",
    "\n",
    "        if vertical:\n",
    "            s = offset + s.item()\n",
    "        else:\n",
    "            o = offset + o.item()\n",
    "\n",
    "        from_indices.append(s)\n",
    "        upto_indices.append(o)\n",
    "        \n",
    "\n",
    "    indices = torch.tensor([from_indices, upto_indices], dtype=torch.long, device=d(cuda))\n",
    "\n",
    "\n",
    "    assert indices.size(1) == len(triples)\n",
    "    assert indices[0, :].max() < size[0], f'{indices[0, :].max()}, {size}, {r}'\n",
    "    assert indices[1, :].max() < size[1], f'{indices[1, :].max()}, {size}, {r}'\n",
    "\n",
    "    return indices.t(), size\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Get adjacency matrix: in this context this is hor / ver graph\n",
    "def hor_ver_graph(triples, n, r):\n",
    "    #triples = enrich(triples_small, n, r)\n",
    "\n",
    "    hor_ind, hor_size = adj(triples, n, 2*r+1, vertical=False)\n",
    "    ver_ind, ver_size = adj(triples, n, 2*r+1, vertical=True)\n",
    "    #number of relations is 2*r+1 because we added the inverse and self loop\n",
    "\n",
    "    _, rn = hor_size #horizontally stacked adjacency matrix size\n",
    "    print(hor_size)\n",
    "    r = rn // n #number of relations enriched divided by number of nodes\n",
    "\n",
    "    vals = torch.ones(ver_ind.size(0), dtype=torch.float) #number of enriched triples\n",
    "    vals = vals / sum_sparse(ver_ind, vals, ver_size) #normalize the values by the number of edges\n",
    "\n",
    "    hor_graph = torch.sparse.FloatTensor(indices=hor_ind.t(), values=vals, size=hor_size) #size: n,r, emb\n",
    "\n",
    "\n",
    "    ver_graph = torch.sparse.FloatTensor(indices=ver_ind.t(), values=vals, size=ver_size)\n",
    "\n",
    "    return hor_graph, ver_graph\n",
    "\n",
    "\n",
    "#Edge mask\n",
    "\n",
    "def construct_edge_mask( num_nodes, init_strategy=\"normal\", const_val=1.0):\n",
    "    \"\"\"\n",
    "    Construct edge mask\n",
    "    input;\n",
    "        num_nodes: number of nodes in the neighborhood\n",
    "        init_strategy: initialization strategy for the mask\n",
    "        const_val: constant value for the mask\n",
    "    output:\n",
    "        mask: edge mask    \n",
    "    \"\"\"\n",
    "    mask = nn.Parameter(torch.FloatTensor(num_nodes))  #initialize the mask\n",
    "    if init_strategy == \"normal\":\n",
    "        std = nn.init.calculate_gain(\"relu\") * math.sqrt(\n",
    "            2.0 / (num_nodes + num_nodes)\n",
    "        )\n",
    "        with torch.no_grad():\n",
    "            mask.normal_(1.0, std)\n",
    "    elif init_strategy == \"const\":\n",
    "        nn.init.constant_(mask, const_val)\n",
    "    return torch.tensor(mask)\n",
    "\n",
    "#Masked 'adjacency' - masked hor vergraph\n",
    "\n",
    "def _masked_adj(mask,adj, diag_mask, graph):\n",
    "    \"\"\" Masked adjacency matrix \n",
    "    input: edge_mask, sub_adj, diag_mask\n",
    "    output: masked_adj\n",
    "    \"\"\"\n",
    "    sym_mask = mask\n",
    "    sym_mask = torch.sigmoid(mask)\n",
    "    \n",
    "    sym_mask = (sym_mask + sym_mask.t()) / 2\n",
    "    adj = torch.tensor(adj)\n",
    "    masked_adj = adj * sym_mask\n",
    "\n",
    "    #return masked_adj #* diag_mask\n",
    "    return torch.sparse.FloatTensor(indices=graph.coalesce().indices(), values= masked_adj, size=graph.coalesce().size())\n",
    "\n",
    "\n",
    "\n",
    "#match tro classes\n",
    "def match_to_classes(tensor1, tensor2):\n",
    "    \"\"\"\n",
    "    tensor1: sub graph indices\n",
    "    tensor2: data.y labelsss\n",
    "    \"\"\"\n",
    "    matching = []\n",
    "    for i in (tensor1[:,0]):\n",
    "        for j, index in zip(tensor2[:,0],range(len(tensor2[:,0]))):\n",
    "            if i == j:\n",
    "                matching.append(tensor2[index])\n",
    "    return matching"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RGCN MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RGCN(nn.Module):\n",
    "    \"\"\"\n",
    "    Classic RGCN\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, triples, n, r, numcls, emb=16, bases=None):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.emb = emb\n",
    "        self.bases = bases\n",
    "        self.numcls = numcls\n",
    "\n",
    "        self.triples = enrich(triples, n, r)\n",
    "\n",
    "        # horizontally and vertically stacked versions of the adjacency graph\n",
    "        hor_ind, hor_size = adj(self.triples, n, 2*r+1, vertical=False)\n",
    "        ver_ind, ver_size = adj(self.triples, n, 2*r+1, vertical=True)\n",
    "        #number of relations is 2*r+1 because we added the inverse and self loop\n",
    "\n",
    "        _, rn = hor_size #horizontally stacked adjacency matrix size\n",
    "        r = rn // n #number of relations enriched divided by number of nodes\n",
    "\n",
    "        vals = torch.ones(ver_ind.size(0), dtype=torch.float) #number of enriched triples\n",
    "        vals = vals / sum_sparse(ver_ind, vals, ver_size) #normalize the values by the number of edges\n",
    "\n",
    "        hor_graph = torch.sparse.FloatTensor(indices=hor_ind.t(), values=vals, size=hor_size) #size: n,r, emb\n",
    "        \n",
    "        \n",
    "        self.register_buffer('hor_graph', hor_graph)\n",
    "\n",
    "        ver_graph = torch.sparse.FloatTensor(indices=ver_ind.t(), values=vals, size=ver_size)\n",
    "        self.register_buffer('ver_graph', ver_graph)\n",
    "\n",
    "        # layer 1 weights\n",
    "        if bases is None:\n",
    "            self.weights1 = nn.Parameter(torch.FloatTensor(r, n, emb))\n",
    "            nn.init.xavier_uniform_(self.weights1, gain=nn.init.calculate_gain('relu'))\n",
    "\n",
    "            self.bases1 = None\n",
    "        else:\n",
    "            self.comps1 = nn.Parameter(torch.FloatTensor(r, bases))\n",
    "            nn.init.xavier_uniform_(self.comps1, gain=nn.init.calculate_gain('relu'))\n",
    "\n",
    "            self.bases1 = nn.Parameter(torch.FloatTensor(bases, n, emb))\n",
    "            nn.init.xavier_uniform_(self.bases1, gain=nn.init.calculate_gain('relu'))\n",
    "\n",
    "        # layer 2 weights\n",
    "        if bases is None:\n",
    "\n",
    "            self.weights2 = nn.Parameter(torch.FloatTensor(r, emb, numcls) )\n",
    "            nn.init.xavier_uniform_(self.weights2, gain=nn.init.calculate_gain('relu'))\n",
    "\n",
    "            self.bases2 = None\n",
    "        else:\n",
    "            self.comps2 = nn.Parameter(torch.FloatTensor(r, bases))\n",
    "            nn.init.xavier_uniform_(self.comps2, gain=nn.init.calculate_gain('relu'))\n",
    "\n",
    "            self.bases2 = nn.Parameter(torch.FloatTensor(bases, emb, numcls))\n",
    "            nn.init.xavier_uniform_(self.bases2, gain=nn.init.calculate_gain('relu'))\n",
    "\n",
    "        self.bias1 = nn.Parameter(torch.FloatTensor(emb).zero_())\n",
    "        self.bias2 = nn.Parameter(torch.FloatTensor(numcls).zero_())\n",
    "\n",
    "    def forward2(self, hor_graph, ver_graph):\n",
    "\n",
    "\n",
    "        ## Layer 1\n",
    "\n",
    "        n, rn = hor_graph.size() #horizontally stacked adjacency matrix size\n",
    "        r = rn // n\n",
    "        e = self.emb\n",
    "        b, c = self.bases, self.numcls\n",
    "\n",
    "        if self.bases1 is not None:\n",
    "            # weights = torch.einsum('rb, bij -> rij', self.comps1, self.bases1)\n",
    "            weights = torch.mm(self.comps1, self.bases1.view(b, n*e)).view(r, n, e)\n",
    "        else:\n",
    "            weights = self.weights1\n",
    "\n",
    "        assert weights.size() == (r, n, e) #r relations, n nodes, e embedding size\n",
    "\n",
    "        # Apply weights and sum over relations\n",
    "        #hidden layer\n",
    "        h = torch.mm(hor_graph, weights.view(r*n, e))  #matmul with horizontally stacked adjacency matrix and initialized weights\n",
    "        assert h.size() == (n, e)\n",
    "\n",
    "        h = F.relu(h + self.bias1) #apply non linearity and add bias\n",
    "\n",
    "        ## Layer 2\n",
    "\n",
    "        # Multiply adjacencies by hidden\n",
    "        h = torch.mm(ver_graph, h) # sparse mm\n",
    "        h = h.view(r, n, e) # new dim for the relations\n",
    "\n",
    "        if self.bases2 is not None:\n",
    "            # weights = torch.einsum('rb, bij -> rij', self.comps2, self.bases2)\n",
    "            weights = torch.mm(self.comps2, self.bases2.view(b, e * c)).view(r, e, c)\n",
    "        else:\n",
    "            weights = self.weights2\n",
    "\n",
    "        # Apply weights, sum over relations\n",
    "        # h = torch.einsum('rhc, rnh -> nc', weights, h)\n",
    "        h = torch.bmm(h, weights).sum(dim=0)\n",
    "\n",
    "        assert h.size() == (n, c)\n",
    "\n",
    "        return h + self.bias2 # -- softmax is applied in the loss\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    def forward(self):\n",
    "\n",
    "\n",
    "        ## Layer 1\n",
    "\n",
    "        n, rn = self.hor_graph.size() #horizontally stacked adjacency matrix size\n",
    "        r = rn // n\n",
    "        e = self.emb\n",
    "        b, c = self.bases, self.numcls\n",
    "\n",
    "        if self.bases1 is not None:\n",
    "            # weights = torch.einsum('rb, bij -> rij', self.comps1, self.bases1)\n",
    "            weights = torch.mm(self.comps1, self.bases1.view(b, n*e)).view(r, n, e)\n",
    "        else:\n",
    "            weights = self.weights1\n",
    "\n",
    "        assert weights.size() == (r, n, e) #r relations, n nodes, e embedding size\n",
    "\n",
    "        # Apply weights and sum over relations\n",
    "        #hidden layer\n",
    "        h = torch.mm(self.hor_graph, weights.view(r*n, e))  #matmul with horizontally stacked adjacency matrix and initialized weights\n",
    "        assert h.size() == (n, e)\n",
    "\n",
    "        h = F.relu(h + self.bias1) #apply non linearity and add bias\n",
    "\n",
    "        ## Layer 2\n",
    "\n",
    "        # Multiply adjacencies by hidden\n",
    "        h = torch.mm(self.ver_graph, h) # sparse mm\n",
    "        h = h.view(r, n, e) # new dim for the relations\n",
    "\n",
    "        if self.bases2 is not None:\n",
    "            # weights = torch.einsum('rb, bij -> rij', self.comps2, self.bases2)\n",
    "            weights = torch.mm(self.comps2, self.bases2.view(b, e * c)).view(r, e, c)\n",
    "        else:\n",
    "            weights = self.weights2\n",
    "\n",
    "        # Apply weights, sum over relations\n",
    "        # h = torch.einsum('rhc, rnh -> nc', weights, h)\n",
    "        h = torch.bmm(h, weights).sum(dim=0)\n",
    "\n",
    "        assert h.size() == (n, c)\n",
    "\n",
    "        return h + self.bias2 # -- softmax is applied in the loss\n",
    "\n",
    "    def penalty(self, p=2):\n",
    "        \"\"\"\n",
    "        L2 penalty on the weights\n",
    "        \"\"\"\n",
    "        assert p==2\n",
    "\n",
    "        if self.bases is None:\n",
    "            return self.weights1.pow(2).sum()\n",
    "\n",
    "        return self.comps1.pow(p).sum() + self.bases1.pow(p).sum()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXPLAINER CLASS \n",
    "where we combine all of the previously defined functions for the sake of explanation training loop"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loss function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FUNCTIONS THAT WE USE IN THE EXPLAIN.PY FILE\n",
    "\n",
    "def loss_fc(edge_mask,  pred, pred_label,label, node_idx, epoch, print=False):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        pred: y_e :  prediction made by current model\n",
    "        pred_label: y_hat : the label predicted by the original model.\n",
    "    \"\"\"\n",
    "    \n",
    "    #PRED LOSS\n",
    "    pred_label_node = pred_label[node_idx] #pred label is the prediction made by the original model\n",
    "    gt_label_node = label[node_idx]\n",
    "\n",
    "    logit = pred[gt_label_node] #pred is the prediction made by the current model\n",
    "\n",
    "    pred_loss = -torch.log(logit) #this is basically taking the cross entropy loss\n",
    "\n",
    "    # MASK SIZE EDGE LOSS\n",
    "    \n",
    "    mask = edge_mask\n",
    "    mask = torch.sigmoid(mask)\n",
    "\n",
    "    size_loss = 0.005 * torch.sum(mask)\n",
    "\n",
    "    \n",
    "    #MASK SIZE FEATURE LOSS\n",
    "    # feat_mask = (torch.sigmoid(feat_mask))\n",
    "    # feat_size_loss = 1.0 * torch.mean(feat_mask)\n",
    "\n",
    "    # EDGE MASK ENTROPY LOSS\n",
    "    mask_ent = -mask * torch.log(mask) - (1 - mask) * torch.log(1 - mask)\n",
    "    mask_ent_loss = 1.0 * torch.mean(mask_ent)\n",
    "    \n",
    "    # FEATURE MASK ENTROPY LOSS\n",
    "    # feat_mask_ent = - feat_mask * torch.log(feat_mask) - (1 - feat_mask) * torch.log(1 - feat_mask)\n",
    "\n",
    "    # feat_mask_ent_loss = 0.1  * torch.mean(feat_mask_ent)\n",
    "\n",
    "    # LAPLACIAN LOSS\n",
    "    # D = torch.diag(torch.sum(masked_adj, 0))\n",
    "    # m_adj = masked_adj \n",
    "    # L = D - m_adj\n",
    "\n",
    "    # pred_label_t = torch.tensor(pred_label, dtype=torch.float)\n",
    "\n",
    "\n",
    "    # lap_loss = ( 1.0\n",
    "    #     * (pred_label_t @ L @ pred_label_t)\n",
    "    #     / torch.Tensor(adj).numel())\n",
    "\n",
    "\n",
    "    loss = pred_loss + size_loss  + mask_ent_loss # + feat_size_loss + lap_loss\n",
    "    if print== True:\n",
    "        print(\"optimization/size_loss\", size_loss, epoch)\n",
    "        #print(\"optimization/feat_size_loss\", feat_size_loss, epoch)\n",
    "        print(\"optimization/mask_ent_loss\", mask_ent_loss, epoch)\n",
    "        print(\n",
    "            \"optimization/feat_mask_ent_loss\", mask_ent_loss, epoch\n",
    "        )\n",
    "\n",
    "        print(\"optimization/pred_loss\", pred_loss, epoch)\n",
    "        #print(\"optimization/lap_loss\", lap_loss, epoch)\n",
    "        print(\"optimization/overall_loss\", loss, epoch)\n",
    "    return loss\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#SEE how the loss function is used in the explain.py file\n",
    "# pred_label = torch.load('aifb_chk/prediction_aifb') #load prediction of original model\n",
    "# pred_label = torch.argmax(pred_label[], dim=1) #get the prediction of the query node\n",
    "# adj = torch.load('cora_chk/adj_cora') #load the adjacency matrix of the original model\n",
    "# model = torch.load('cora_chk/model_cora') #load the original model\n",
    "\n",
    "# #We are using the original model to get the prediction of the query node\n",
    "# pred, adj = model.forward(torch.Tensor(subdata), torch.Tensor(sub_adj))\n",
    "# node_pred = pred[node_idx_new, :] #get the prediction of the query node\n",
    "# pred = nn.Softmax(dim=0)(node_pred) #apply softmax to it\n",
    "# loss_fc( edge_mask, feat_mask, masked_adj,adj, pred, pred_label,data.y, node_idx, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rgcn_explainer_utils import encode_classes, d_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded data aifb (0.1914s).\n",
      "Number of entities: 8285\n",
      "Number of classes: 4\n",
      "Types of relations: 45\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'AllgMA': 0,\n",
       " 'AtanasKiryakov': 1,\n",
       " 'BIK': 2,\n",
       " 'CoM': 3,\n",
       " 'EffAlg': 4,\n",
       " 'Forschungsgebiete': 5,\n",
       " 'Forschungsgruppen': 6,\n",
       " 'Kooperationen': 7,\n",
       " 'Ontoagents': 8,\n",
       " 'Personen': 9,\n",
       " 'Projekte': 10,\n",
       " 'Publications': 11,\n",
       " 'Publikationen': 12,\n",
       " 'Users': 13,\n",
       " 'WBS': 14,\n",
       " 'about': 15,\n",
       " 'ath': 16,\n",
       " 'auslese': 17,\n",
       " 'blaschke': 18,\n",
       " 'bpem': 19,\n",
       " 'cms': 20,\n",
       " 'docs': 21,\n",
       " 'eOrganisation': 22,\n",
       " 'english': 23,\n",
       " 'fbe': 24,\n",
       " 'fmricci': 25,\n",
       " 'home': 26,\n",
       " 'ia': 27,\n",
       " 'kangal': 28,\n",
       " 'mitarbeiter': 29,\n",
       " 'optrek': 30,\n",
       " 'padlr': 31,\n",
       " 'people': 32,\n",
       " 'personen': 33,\n",
       " 'projects': 34,\n",
       " 'projekte': 35,\n",
       " 'prost': 36,\n",
       " 'seanb': 37,\n",
       " 'semanticportal': 38,\n",
       " 'sites': 39,\n",
       " 'uli': 40,\n",
       " 'users': 41,\n",
       " 'valencia': 42,\n",
       " 'who': 43,\n",
       " 'wiki': 44,\n",
       " 'wim': 45,\n",
       " 'ze': 46}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = kg.load('aifb', torch=True) \n",
    "print(f'Number of entities: {data.num_entities}') #data.i2e\n",
    "print(f'Number of classes: {data.num_classes}')\n",
    "print(f'Types of relations: {data.num_relations}') #data.i2r\n",
    "\n",
    "\n",
    "\n",
    "def get_relations(data):\n",
    "    \"\"\" \n",
    "    This function is used to get the relations from the data.i2r dictionary and store it in a dictionary with the relation id as the key and the relation name as the value.\n",
    "    \"\"\"\n",
    "\n",
    "    all_relations = []\n",
    "    for i in range(data.num_relations):\n",
    "        if '#' in str(data.i2r[i]).split('/')[3]:\n",
    "            all_relations.append(str(data.i2r[i]).split('/')[3].split('#')[1])\n",
    "        else:\n",
    "            all_relations.append(str(data.i2r[i]).split('/')[3])\n",
    "    dict = {}\n",
    "    for i in range(len(all_relations)):\n",
    "        dict[i] = [all_relations[i]]\n",
    "        dict[i].append(data.i2r[i])\n",
    "\n",
    "    data.i2rel = dict\n",
    "    return data.i2rel\n",
    "get_relations(data)\n",
    "data.entities = np.append(data.triples[:,0].detach().numpy(),(data.triples[:,2].detach().numpy()))\n",
    "get_relations(data)\n",
    "d_classes(data)\n",
    "#data.i2rel\n",
    "#data.triples.shape\n",
    "\n",
    "# data.withheld[:, 1]\n",
    "# idxw, clsw = data.withheld[:, 0], data.withheld[:, 1]\n",
    "# idxw, clsw = idxw.long(), clsw.long()\n",
    "# d = list(data.e2i.keys())\n",
    "#there is correspondance index and label person \n",
    "#print(dir(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: ['abstract', 'http://swrc.ontoware.org/ontology#abstract'],\n",
       " 1: ['address', 'http://swrc.ontoware.org/ontology#address'],\n",
       " 2: ['author', 'http://swrc.ontoware.org/ontology#author'],\n",
       " 3: ['booktitle', 'http://swrc.ontoware.org/ontology#booktitle'],\n",
       " 4: ['carriedOutBy', 'http://swrc.ontoware.org/ontology#carriedOutBy'],\n",
       " 5: ['carriesOut', 'http://swrc.ontoware.org/ontology#carriesOut'],\n",
       " 6: ['chapter', 'http://swrc.ontoware.org/ontology#chapter'],\n",
       " 7: ['dealtWithIn', 'http://swrc.ontoware.org/ontology#dealtWithIn'],\n",
       " 8: ['edition', 'http://swrc.ontoware.org/ontology#edition'],\n",
       " 9: ['editor', 'http://swrc.ontoware.org/ontology#editor'],\n",
       " 10: ['fax', 'http://swrc.ontoware.org/ontology#fax'],\n",
       " 11: ['financedBy', 'http://swrc.ontoware.org/ontology#financedBy'],\n",
       " 12: ['finances', 'http://swrc.ontoware.org/ontology#finances'],\n",
       " 13: ['hasProject', 'http://swrc.ontoware.org/ontology#hasProject'],\n",
       " 14: ['head', 'http://swrc.ontoware.org/ontology#head'],\n",
       " 15: ['homepage', 'http://swrc.ontoware.org/ontology#homepage'],\n",
       " 16: ['howpublished', 'http://swrc.ontoware.org/ontology#howpublished'],\n",
       " 17: ['isAbout', 'http://swrc.ontoware.org/ontology#isAbout'],\n",
       " 18: ['isWorkedOnBy', 'http://swrc.ontoware.org/ontology#isWorkedOnBy'],\n",
       " 19: ['isbn', 'http://swrc.ontoware.org/ontology#isbn'],\n",
       " 20: ['journal', 'http://swrc.ontoware.org/ontology#journal'],\n",
       " 21: ['member', 'http://swrc.ontoware.org/ontology#member'],\n",
       " 22: ['month', 'http://swrc.ontoware.org/ontology#month'],\n",
       " 23: ['name', 'http://swrc.ontoware.org/ontology#name'],\n",
       " 24: ['note', 'http://swrc.ontoware.org/ontology#note'],\n",
       " 25: ['number', 'http://swrc.ontoware.org/ontology#number'],\n",
       " 26: ['pages', 'http://swrc.ontoware.org/ontology#pages'],\n",
       " 27: ['phone', 'http://swrc.ontoware.org/ontology#phone'],\n",
       " 28: ['photo', 'http://swrc.ontoware.org/ontology#photo'],\n",
       " 29: ['projectInfo', 'http://swrc.ontoware.org/ontology#projectInfo'],\n",
       " 30: ['publication', 'http://swrc.ontoware.org/ontology#publication'],\n",
       " 31: ['publishes', 'http://swrc.ontoware.org/ontology#publishes'],\n",
       " 32: ['series', 'http://swrc.ontoware.org/ontology#series'],\n",
       " 33: ['title', 'http://swrc.ontoware.org/ontology#title'],\n",
       " 34: ['type', 'http://swrc.ontoware.org/ontology#type'],\n",
       " 35: ['volume', 'http://swrc.ontoware.org/ontology#volume'],\n",
       " 36: ['worksAtProject', 'http://swrc.ontoware.org/ontology#worksAtProject'],\n",
       " 37: ['year', 'http://swrc.ontoware.org/ontology#year'],\n",
       " 38: ['WBS', 'http://www.aifb.uni-karlsruhe.de/WBS/dvr/owltools/merge/type'],\n",
       " 39: ['1999', 'http://www.w3.org/1999/02/22-rdf-syntax-ns#type'],\n",
       " 40: ['2000', 'http://www.w3.org/2000/01/rdf-schema#range'],\n",
       " 41: ['2000', 'http://www.w3.org/2000/01/rdf-schema#subClassOf'],\n",
       " 42: ['2002', 'http://www.w3.org/2002/07/owl#allValuesFrom'],\n",
       " 43: ['2002', 'http://www.w3.org/2002/07/owl#inverseOf'],\n",
       " 44: ['2002', 'http://www.w3.org/2002/07/owl#onProperty']}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.i2rel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.i2e[5757]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_label = torch.load('aifb_chk/prediction_aifb')\n",
    "pred_label[1]\n",
    "label = data.withheld[:, 1]\n",
    "\n",
    "gt_label_node = label[1]\n",
    "\n",
    "\n",
    "# logit = ypred[gt_label_node]\n",
    "# -torch.log(logit) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Explain(nn.Module):\n",
    "    def __init__(self, model, data, node_idx, n_hops):\n",
    "        super(Explain, self).__init__()\n",
    "        #Those are the parameters of the original data and model\n",
    "        self.model = model\n",
    "        self.data = data\n",
    "        self.n = data.num_entities\n",
    "        self.r = data.num_relations\n",
    "    \n",
    "        #self.triples = enrich(data.triples, self.n, self.r)\n",
    "        self.triples = data.triples\n",
    "        self.node_idx = node_idx\n",
    "        self.n_hops = n_hops\n",
    "        #self.adj = get_adjacency(data)\n",
    "        self.edge_index = edge_index_oneadj(self.triples)\n",
    "        #print(self.edge_index)\n",
    "        #self.label =  np.concatenate((np.array(data.training), np.array(data.withheld)), axis=0)\n",
    "        self.label = self.data.withheld[:, 1]\n",
    "        #self.feat = torch.Tensor(data.x)\n",
    "        #self.feat_dim = data.num_features\n",
    "        self.epoch = 1\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "        self.hor_graph, self.ver_graph = hor_ver_graph(self.triples, self.n, self.r)\n",
    "        #print(self.hor_graph)\n",
    "        self.pred_label = torch.load('aifb_chk/prediction_aifb')\n",
    "\n",
    "        #self.node_idx_new, self.sub_adj, self.sub_feat, self.sub_label, self.neighbors = extract_neighborhood(self.node_idx, self.adj, self.feat, self.label, self.n_hops)\n",
    "        self.sub_edges, self.neighbors, self.sub_edges_tensor = find_n_hop_neighbors(self.edge_index, n=self.n_hops, node=self.node_idx)\n",
    "        #print('neighbors', self.neighbors)\n",
    "\n",
    "        self.sub_triples = match_to_triples(self.sub_edges_tensor.t(),self.triples)\n",
    "        #print('sub_triples', self.sub_triples)\n",
    "        self.sub_hor_graph, self.sub_ver_graph = hor_ver_graph(self.sub_triples, self.n, self.r)\n",
    "        self.num_nodes = self.sub_hor_graph.coalesce().values().shape[0] \n",
    "        \n",
    "        #self.diag_mask = construct_diag_mask(self.neighbors)\n",
    "        #self.subdata = torch.Tensor(data.subgraph(torch.tensor(self.neighbors)).x)\n",
    "        self.edge_mask = construct_edge_mask(self.num_nodes)\n",
    "        #print('edge_mask', self.edge_mask)\n",
    "        #self.feat_mask = construct_feat_mask(self.feat_dim, init_strategy=\"normal\")\n",
    "\n",
    "\n",
    "\n",
    "    def _masked_adj(self, sub_graph):\n",
    "        \"\"\" Masked adjacency matrix \n",
    "        input: edge_mask, sub_adj, diag_mask\n",
    "        output: masked_adj\n",
    "        \"\"\"\n",
    "        adj = sub_graph.coalesce().values()\n",
    "        indices = sub_graph.coalesce().indices()\n",
    "        size = sub_graph.coalesce().size()\n",
    "        sym_mask = self.edge_mask\n",
    "        sym_mask = torch.sigmoid(self.edge_mask)\n",
    "        \n",
    "        sym_mask = (sym_mask + sym_mask.t()) / 2\n",
    "        adj = torch.tensor(adj)\n",
    "        masked_adj = adj * sym_mask\n",
    "\n",
    "        #return masked_adj #* diag_mask\n",
    "        return torch.sparse.FloatTensor(indices=indices, values= masked_adj, size=size )\n",
    "    \n",
    "    def softmax(self, pred):\n",
    "        \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "        x_index = pred[self.node_idx].detach().numpy()\n",
    "        e_x = np.exp(x_index - np.max(x_index))\n",
    "        return e_x / e_x.sum(axis=0),  np.argmax(e_x / e_x.sum(axis=0))\n",
    "    \n",
    "    def new_index(self):\n",
    "        idxw, clsw = self.data.withheld[:, 0], self.data.withheld[:, 1]\n",
    "        idxw, clsw = idxw.long(), clsw.long()\n",
    "        idxw_list = list(idxw)\n",
    "        self.new_node_idx = idxw_list.index(self.node_idx)\n",
    "        return self.new_node_idx \n",
    "        \n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            ypred: prediction of the query node made by the current model (on the subgraph)\n",
    "\n",
    "        \"\"\"\n",
    "        masked_hor = self._masked_adj(self.sub_hor_graph)\n",
    "        masked_ver = self._masked_adj(self.sub_ver_graph)\n",
    "\n",
    "\n",
    "        # self.masked_adj = self._masked_adj()\n",
    "        # feat_mask = (torch.sigmoid(self.feat_mask))\n",
    "        # x = self.sub_feat * feat_mask\n",
    "        #ypred, adj_att = model(self.subdata, masked_adj)\n",
    "        #ypred, adj_att = self.model(x, self.masked_adj)\n",
    "        ypred = self.model.forward2(masked_hor, masked_ver)\n",
    " \n",
    "\n",
    "        #_, res = self.softmax(ypred)\n",
    "        \n",
    "        node_pred = ypred[self.node_idx, :]\n",
    "        res = nn.Softmax(dim=0)(node_pred)\n",
    "  \n",
    "        return res, masked_hor, masked_ver\n",
    "    \n",
    "    def criterion(self, epoch):\n",
    "        \"\"\"\n",
    "        Computes the loss of the current model\n",
    "        \"\"\"\n",
    "        #prediction of explanation model\n",
    "        pred, masked_hor, masked_ver = self.forward()\n",
    "\n",
    "        #prediction of original model\n",
    "        #pred_label = self.pred_label[self.new_node_idx]\n",
    "\n",
    "        self.new_node_idx = self.new_index()\n",
    "        loss_val = loss_fc(self.edge_mask,  pred, self.pred_label,self.label, self.new_node_idx, epoch, print=False)\n",
    "\n",
    "        return loss_val \n",
    "    \n",
    "    # def mask_density(self):\n",
    "    #     \"\"\"\n",
    "    #     Computes the density of the edge mask\n",
    "    #     \"\"\"\n",
    "    #     mask_sum = torch.sum(self.masked_adj)\n",
    "    #     adj_sum = torch.sum(self.adj)\n",
    "    #     return mask_sum / adj_sum\n",
    "    \n",
    "    def return_stuff(self):\n",
    "        #pred_label = torch.argmax(self.pred_label[self.neighbors], dim=1)\n",
    "        return self.neighbors,self.n_hops, self.node_idx\n",
    "    \n",
    "    def adj_feat_grad(self, node_idx, pred_label_node):\n",
    "        \"\"\"\n",
    "        Compute the gradient of the prediction w.r.t. the adjacency matrix\n",
    "        and the node features.\n",
    "        \"\"\"\n",
    "        self.model.zero_grad()\n",
    "        self.masked_ver.requires_grad = True\n",
    "        self.masked_hor.requires_grad = True\n",
    "        \n",
    "        if self.masked_hor.grad is not None:\n",
    "            print('self.adj.grad', self.masked_hor.grad)\n",
    "            self.masked_hor.grad.zero_() # zero out the gradient\n",
    "            self.masked_ver.gradzero_() # zero out the gradient\n",
    "        else:    \n",
    "            masked_hor, masked_ver = self.masekd_hor, self.masked_ver\n",
    "        ypred, _ = self.model(masked_hor, masked_ver)\n",
    "\n",
    "        logit = nn.Softmax(dim=0)(ypred[node_idx, :])\n",
    "        logit = logit[pred_label_node]\n",
    "        loss = -torch.log(logit)\n",
    "        loss.backward()\n",
    "        return self.masked_hor.grad, self.masked_ver.grad\n",
    "    \n",
    "    def log_adj_grad(self, node_idx, pred_label, epoch, label=None):\n",
    "\n",
    "\n",
    "        predicted_label = pred_label[node_idx]\n",
    "        # adj_grad = torch.abs(self.adj_feat_grad(node_idx, predicted_label)[0])[self.graph_idx]\n",
    "        masked_hor_grad, masekd_ver_grad = self.adj_feat_grad(node_idx, predicted_label)\n",
    "    \n",
    "        masked_hor_grad = torch.abs(masked_hor_grad)\n",
    "        x_grad = x_grad[node_idx][:, np.newaxis]\n",
    "            # x_grad = torch.sum(x_grad[self.graph_idx], 0, keepdim=True).t()\n",
    "        masked_hor_grad = (masked_hor_grad + masked_hor_grad.t()) / 2\n",
    "        masked_hor_grad = (masked_hor_grad * self.masked_hor).squeeze()\n",
    "    \n",
    "        # masked_adj = self.masked_adj[0].cpu().detach().numpy()\n",
    "\n",
    "        # # only for graph mode since many node neighborhoods for syn tasks are relatively large for\n",
    "        # # visualization\n",
    "        \n",
    "        # adj_grad = adj_grad.detach().numpy()\n",
    "    \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'dict' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m explainer \u001b[39m=\u001b[39m Explain(model \u001b[39m=\u001b[39;49m torch\u001b[39m.\u001b[39;49mload(\u001b[39m'\u001b[39;49m\u001b[39maifb_chk/model_aifb\u001b[39;49m\u001b[39m'\u001b[39;49m), data \u001b[39m=\u001b[39;49m data, node_idx \u001b[39m=\u001b[39;49m \u001b[39m5757\u001b[39;49m, n_hops \u001b[39m=\u001b[39;49m \u001b[39m1\u001b[39;49m)\n\u001b[1;32m      2\u001b[0m optimizer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mAdam(explainer\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39m\u001b[39m0.01\u001b[39m)\n",
      "Cell \u001b[0;32mIn[29], line 27\u001b[0m, in \u001b[0;36mExplain.__init__\u001b[0;34m(self, model, data, node_idx, n_hops)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[39m#self.feat = torch.Tensor(data.x)\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[39m#self.feat_dim = data.num_features\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepoch \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m---> 27\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhor_graph, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mver_graph \u001b[39m=\u001b[39m hor_ver_graph(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtriples, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mr)\n\u001b[1;32m     28\u001b[0m \u001b[39m#print(self.hor_graph)\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpred_label \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mload(\u001b[39m'\u001b[39m\u001b[39maifb_chk/prediction_aifb\u001b[39m\u001b[39m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[24], line 164\u001b[0m, in \u001b[0;36mhor_ver_graph\u001b[0;34m(triples, n, r)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mhor_ver_graph\u001b[39m(triples, n, r):\n\u001b[1;32m    162\u001b[0m     \u001b[39m#triples = enrich(triples_small, n, r)\u001b[39;00m\n\u001b[0;32m--> 164\u001b[0m     hor_ind, hor_size \u001b[39m=\u001b[39m adj(triples, n, \u001b[39m2\u001b[39;49m\u001b[39m*\u001b[39;49mr\u001b[39m+\u001b[39;49m\u001b[39m1\u001b[39;49m, vertical\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m    165\u001b[0m     ver_ind, ver_size \u001b[39m=\u001b[39m adj(triples, n, \u001b[39m2\u001b[39m\u001b[39m*\u001b[39mr\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m, vertical\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    166\u001b[0m     \u001b[39m#number of relations is 2*r+1 because we added the inverse and self loop\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[24], line 146\u001b[0m, in \u001b[0;36madj\u001b[0;34m(triples, num_nodes, num_rels, cuda, vertical)\u001b[0m\n\u001b[1;32m    142\u001b[0m     from_indices\u001b[39m.\u001b[39mappend(s)\n\u001b[1;32m    143\u001b[0m     upto_indices\u001b[39m.\u001b[39mappend(o)\n\u001b[0;32m--> 146\u001b[0m indices \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor([from_indices, upto_indices], dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mlong, device\u001b[39m=\u001b[39md(cuda))\n\u001b[1;32m    149\u001b[0m \u001b[39massert\u001b[39;00m indices\u001b[39m.\u001b[39msize(\u001b[39m1\u001b[39m) \u001b[39m==\u001b[39m \u001b[39mlen\u001b[39m(triples)\n\u001b[1;32m    150\u001b[0m \u001b[39massert\u001b[39;00m indices[\u001b[39m0\u001b[39m, :]\u001b[39m.\u001b[39mmax() \u001b[39m<\u001b[39m size[\u001b[39m0\u001b[39m], \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mindices[\u001b[39m0\u001b[39m, :]\u001b[39m.\u001b[39mmax()\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m{\u001b[39;00msize\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m{\u001b[39;00mr\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'dict' object is not callable"
     ]
    }
   ],
   "source": [
    "explainer = Explain(model = torch.load('aifb_chk/model_aifb'), data = data, node_idx = 5757, n_hops = 1)\n",
    "optimizer = torch.optim.Adam(explainer.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer.train()\n",
    "for epoch in range(100):\n",
    "    explainer.zero_grad()\n",
    "    optimizer.zero_grad()\n",
    "    ypred, masked_hor, masked_ver = explainer.forward()\n",
    "    loss = explainer.criterion(epoch)\n",
    "    neighbors,n_hops, node_idx = explainer.return_stuff()\n",
    "    #pred_label,neighbors, , num_hops = explainer.return_stuff()\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    #mask_density = explainer.mask_density()\n",
    "    # single_subgraph_label = sub_label.squeeze()\n",
    "    # if epoch % 25 == 0:\n",
    "    # #     explainer.log_mask(epoch)\n",
    "    # #     explainer.log_masked_adj(\n",
    "    # #         node_idx_new, epoch, label=single_subgraph_label\n",
    "    # #     )\n",
    "\n",
    "\n",
    "    #     explainer.log_adj_grad(\n",
    "    #         node_idx_new, pred_label, epoch, label=single_subgraph_label)\n",
    "\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "\n",
    "        print(\n",
    "        \"epoch: \",\n",
    "        epoch,\n",
    "        \"; loss: \",\n",
    "        loss.item(),\n",
    "        # \"; mask density: \",\n",
    "        # mask_density.item(),\n",
    "        \"; pred: \",\n",
    "        ypred,\n",
    "        # \"; labels equal: \",\n",
    "        # torch.argmax(ypred) == original_label== pred_label,\n",
    "\n",
    "    )\n",
    "\n",
    "\n",
    "# adj_atts = torch.sigmoid(adj_atts).squeeze()\n",
    "# masked_adj = adj_atts * sub_adj.squeeze()\n",
    "# masked_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_ver.coalesce().values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_nodes = masked_ver.coalesce().indices().detach().numpy()\n",
    "\n",
    "d = list(data.e2i.keys())\n",
    "values_indices_nodes = [d[i] for i in indices_nodes[1]]\n",
    "values_indices_nodes\n",
    "\n",
    "np.transpose(indices_nodes)\n",
    "new_ver = indices_nodes[0]%data.num_entities\n",
    "indices_nodes[0]/data.num_entities #we indeed have 91 relations so that makes sense that max is around 90.5 sth relation\n",
    "new_index = np.transpose(np.stack((new_ver, indices_nodes[1])))\n",
    "new_index\n",
    "G = nx.from_edgelist(new_index)\n",
    "list(masked_ver.coalesce().values().detach().numpy())\n",
    "G.number_of_edges()\n",
    "values_indices_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_nodes = masked_ver.coalesce().indices().detach().numpy()\n",
    "\n",
    "d = list(data.e2i.keys())\n",
    "values_indices_nodes = [d[i] for i in indices_nodes[1]]\n",
    "values_indices_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_index_classes(data, masked_ver):\n",
    "    indices_nodes = masked_ver.coalesce().indices().detach().numpy()\n",
    "\n",
    "    d = list(data.e2i.keys())\n",
    "    values_indices_nodes = [d[i] for i in indices_nodes[1]]\n",
    "    dict = {}\n",
    "    for i in range(len(values_indices_nodes)):\n",
    "        try:\n",
    "            dict[values_indices_nodes[i][0]] = str(values_indices_nodes[i][1]).split('/')[3]\n",
    "            \n",
    "        except IndexError :\n",
    "            dict[values_indices_nodes[i][0]] = str(values_indices_nodes[i][1])\n",
    "    return dict        \n",
    "  \n",
    "dict_index = dict_index_classes(data,masked_ver)\n",
    "dict_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_triples = match_to_triples(new_index, data.triples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_triples_semantics(data,  masked_ver, sub_triples):\n",
    "    \"\"\" \n",
    "    Function to get a dictionary where the keys are the triples in indexes and values their semantic values\n",
    "    Input:\n",
    "        - Data\n",
    "        - Sub Triples\n",
    "        - Dict index\n",
    "        \n",
    "    \"\"\"\n",
    "    dict_index = dict_index_classes(data,masked_ver)\n",
    "    indices_nodes = masked_ver.coalesce().indices().detach().numpy()\n",
    "    new_ver = indices_nodes[0]%data.num_entities\n",
    "    new_index = np.transpose(np.stack((new_ver, indices_nodes[1])))\n",
    "    #sub_triples = match_to_triples(new_index, data.triples)\n",
    "    dict = {}\n",
    "    all_p = []\n",
    "    for i in range(len(sub_triples)):\n",
    "\n",
    "        s = dict_index[int(sub_triples[:,0][i])]\n",
    "        p = data.i2rel[int(sub_triples[:,1][i])][0]\n",
    "        o = dict_index[int(sub_triples[:,2][i])]\n",
    "        all_p.append(p)\n",
    "        dict[sub_triples[i]] = s + ' ' + p + ' ' + o \n",
    "    print(set(all_p))    \n",
    "    return dict \n",
    "\n",
    "sem_triples = dict_triples_semantics(data, masked_ver, sub_triples)\n",
    "sem_triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_relations = []\n",
    "for i in range(data.num_relations):\n",
    "    if '#' in str(data.i2r[i]).split('/')[3]:\n",
    "        all_relations.append(str(data.i2r[i]).split('/')[3].split('#')[1])\n",
    "    else:\n",
    "        all_relations.append(str(data.i2r[i]).split('/')[3])\n",
    "\n",
    "all_relations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relations(data):\n",
    "\n",
    "    all_relations = []\n",
    "    for i in range(data.num_relations):\n",
    "        if '#' in str(data.i2r[i]).split('/')[3]:\n",
    "            all_relations.append(str(data.i2r[i]).split('/')[3].split('#')[1])\n",
    "        else:\n",
    "            all_relations.append(str(data.i2r[i]).split('/')[3])\n",
    "    dict = {}\n",
    "    for i in range(len(all_relations)):\n",
    "        dict[i] = [all_relations[i]]\n",
    "        dict[i].append(data.i2r[i])\n",
    "\n",
    "    data.i2rel = dict\n",
    "    return data.i2rel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict_index_classes(data,masked_ver)\n",
    "# sub_triples\n",
    "# #dict_index[int(sub_triples[:,0])]\n",
    "# sub_triples[:,0][0]\n",
    "# #dict_index[int(sub_triples[:,0][0])]\n",
    "# dict_index\n",
    "# len(sub_triples)\n",
    "# len(neighbors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_tensor = masked_ver\n",
    "nonzero_indices = sparse_tensor.coalesce().indices()[:, sparse_tensor.coalesce().values() > 0.01]\n",
    "nonzero_indices[0] = nonzero_indices[0]%data.num_entities\n",
    "nonzero_values = sparse_tensor.coalesce().values()[sparse_tensor.coalesce().values() > 0.01]\n",
    "nonzero_values\n",
    "sel_masked_ver = torch.sparse_coo_tensor(nonzero_indices, nonzero_values)\n",
    "sel_masked_ver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = masked_ver.coalesce().values()\n",
    "b = sel_masked_ver.coalesce().values()\n",
    "print(a, '\\n', b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sub_sparse_tensor(sparse_tensor, threshold):\n",
    "    nonzero_indices = sparse_tensor.coalesce().indices()[:, sparse_tensor.coalesce().values() > threshold]\n",
    "    nonzero_indices[0] = nonzero_indices[0]%data.num_entities\n",
    "    nonzero_values = sparse_tensor.coalesce().values()[sparse_tensor.coalesce().values() > threshold]\n",
    "    sel_masked_ver = torch.sparse_coo_tensor(nonzero_indices, nonzero_values)\n",
    "    return sel_masked_ver\n",
    "sub_sparse_tensor(masked_ver, 0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def sub_sparse_tensor(sparse_tensor, threshold, data, low_threshold=False):\n",
    "    if low_threshold:\n",
    "        nonzero_indices = sparse_tensor.coalesce().indices()[:, sparse_tensor.coalesce().values() < threshold]\n",
    "        nonzero_indices[0] = nonzero_indices[0]%data.num_entities\n",
    "        nonzero_values = sparse_tensor.coalesce().values()[sparse_tensor.coalesce().values() < threshold]\n",
    "        sel_masked_ver = torch.sparse_coo_tensor(nonzero_indices, nonzero_values)\n",
    "    else:\n",
    "        nonzero_indices = sparse_tensor.coalesce().indices()[:, sparse_tensor.coalesce().values() > threshold]\n",
    "        nonzero_indices[0] = nonzero_indices[0]%data.num_entities\n",
    "        nonzero_values = sparse_tensor.coalesce().values()[sparse_tensor.coalesce().values() > threshold]\n",
    "        sel_masked_ver = torch.sparse_coo_tensor(nonzero_indices, nonzero_values)    \n",
    "    return sel_masked_ver\n",
    "\n",
    "\n",
    "def encode_classes(dict_index):\n",
    "    d = []\n",
    "    for k,v in dict_index.items():\n",
    "        d.append(v)\n",
    "    a = np.unique(d)\n",
    "    dict = {}\n",
    "    for i,j in zip(a, range(len(a))):\n",
    "        dict[i] = j\n",
    "        \n",
    "    return dict\n",
    "\n",
    "\n",
    "def encode_dict(dict_index):\n",
    "    encoded_dict = {}\n",
    "    dict = encode_classes(dict_index)\n",
    "    for k,v in dict_index.items():\n",
    "        for k1,v1 in dict.items():\n",
    "            if v==k1:\n",
    "                encoded_dict[k] = v1\n",
    "                #encoded_dict[k].append(v)\n",
    "    return encoded_dict\n",
    "\n",
    "\n",
    "\n",
    "def visualize(node_idx, n_hop, data, masked_ver,threshold, result_weights=False, low_threshold=False ):\n",
    "    \"\"\" \n",
    "    Visualize important nodes for node idx prediction\n",
    "    \"\"\"\n",
    "    dict_index = dict_index_classes(data,masked_ver)\n",
    "    \n",
    "    \n",
    "    #select only nodes with a certain threshold\n",
    "    sel_masked_ver = sub_sparse_tensor(masked_ver, threshold,data, low_threshold)\n",
    "\n",
    "    indices_nodes = sel_masked_ver.coalesce().indices().detach().numpy()\n",
    "    new_index = np.transpose(np.stack((indices_nodes[0], indices_nodes[1]))) #original edge indexes\n",
    "\n",
    "    \n",
    "    \n",
    "    G = nx.Graph()\n",
    "    if result_weights:\n",
    "        values = sel_masked_ver.coalesce().values().detach().numpy()\n",
    "        for s,p,o in zip(indices_nodes[0],values , indices_nodes[1]):\n",
    "            G.add_edge(int(s), int(o), weight=np.round(p, 2))\n",
    "\n",
    "    else:\n",
    "        #get triples to get relations \n",
    "        \n",
    "        triples_matched = match_to_triples(np.array(new_index), data.triples)\n",
    "        for s,p,o in triples_matched:\n",
    "            G.add_edge(int(s), int(o), weight=int(p))\n",
    "\n",
    "    edges,weights = zip(*nx.get_edge_attributes(G,'weight').items())\n",
    "\n",
    "\n",
    "    pos = nx.circular_layout(G)\n",
    "\n",
    "    ordered_dict = {}\n",
    "    for item in list(G.nodes):\n",
    "        if item in ordered_dict:\n",
    "            ordered_dict[item].append(dict_index[item])\n",
    "        else:\n",
    "            ordered_dict[item] =  dict_index[item]\n",
    "\n",
    "    dict_index = ordered_dict\n",
    "    \n",
    "\n",
    "    labeldict = {}\n",
    "    for node in G.nodes:\n",
    "        labeldict[int(node)] = int(node)  \n",
    "    # color_list = list(encode_dict(dict_index).values())\n",
    "    # print(color_list)\n",
    "\n",
    "    dict = {}\n",
    "    for k,v in dict_index.items():\n",
    "        for k1,v1 in data.entities_classes.items():\n",
    "            if v==k1: \n",
    "\n",
    "                dict[k] = v1\n",
    "            else:\n",
    "                if k not in dict:\n",
    "                    dict[k] = 0\n",
    "                \n",
    "\n",
    "    color_list = list(dict.values())\n",
    "    print(color_list)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    # color_dict = encode_classes(dict_index)\n",
    "    # print(color_dict)\n",
    "\n",
    "    \n",
    "    if result_weights:\n",
    "        \n",
    "        nx.draw(G, pos,labels = labeldict,  edgelist=edges, edge_color=weights, node_color =  color_list, cmap=\"Set2\",edge_cmap=plt.cm.Reds, font_size=7)\n",
    "        nx.draw_networkx_edge_labels( G, pos,edge_labels=nx.get_edge_attributes(G,'weight'),font_size=8,font_color='red')\n",
    "        sm = plt.cm.ScalarMappable(cmap=plt.cm.Reds, norm=plt.Normalize(vmin=0, vmax=1))\n",
    "        sm.set_array(weights)\n",
    "        cbar = plt.colorbar(sm)\n",
    "        cbar.ax.set_title('Weight')\n",
    "        plt.title(\"Node {}'s {}-hop neighborhood important nodes\".format(node_idx, n_hop))\n",
    "        #plt.legend(data.entities_classes.values())\n",
    "        plt.show()\n",
    "    else:\n",
    "        rel = nx.get_edge_attributes(G,'weight')\n",
    "        for k,v in rel.items():\n",
    "            rel[k] = data.i2rel[v][0]\n",
    "        print(rel)\n",
    "        nx.draw(G, pos,labels = labeldict,  edgelist=edges, edge_color=weights,node_color =  color_list, cmap=\"Set2\",font_size=7)\n",
    "        nx.draw_networkx_edge_labels( G, pos,edge_labels=rel,font_size=5,font_color='red')\n",
    "        # legend_colors = []\n",
    "        # for k,v in set(color_dict.items()):\n",
    "        #     legend_colors.append(plt.Line2D([0], [0], marker='o', color='w', label=k,\n",
    "        #                         markerfacecolor=v, markersize=10))\n",
    "\n",
    "        # # display the legend\n",
    "        # plt.legend(handles= legend_colors)\n",
    "        res = Counter(rel.values())\n",
    "        plt.show()\n",
    "        return res\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = []\n",
    "for i in sub_triples[:,1]:\n",
    "    l.append(data.i2rel[int(i)][0])\n",
    "print(Counter(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0984, 0.2280, 0.4754, 0.1983], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mver_5797 = torch.load('aifb_chk/masked_ver5797')\n",
    "mhor_5797 = torch.load('aifb_chk/masked_hor5797')\n",
    "\n",
    "model = torch.load('aifb_chk/model_aifb')\n",
    "\n",
    "ypred = model.forward2(mhor_5797, mver_5797)\n",
    "node_pred = ypred[5797, :]\n",
    "res = nn.Softmax(dim=0)(node_pred)\n",
    "print(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([753935, 8285])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mver_5797.indices = mver_5797.coalesce().indices()[0]%data.num_entities\n",
    "mver_5797.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.2278, 0.3259, 0.2451, 0.2012], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mver_5757 = torch.load('aifb_chk/masked_ver5757')\n",
    "mhor_5757 = torch.load('aifb_chk/masked_hor5757')\n",
    "\n",
    "model = torch.load('aifb_chk/model_aifb')\n",
    "\n",
    "\n",
    "ypred = model.forward2(mhor_5757, mver_5757)\n",
    "node_pred = ypred[5757, :]\n",
    "res = nn.Softmax(dim=0)(node_pred)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hor, ver = hor_ver_graph(data.triples, data.num_entities, data.num_relations)\n",
    "triples = enrich(data.triples,data.num_entities, data.num_relations)\n",
    "hor, ver = hor_ver_graph(triples, data.num_entities, data.num_relations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8285, 753935)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1235, -0.0913, -0.0046, -0.0849],\n",
       "        [ 0.1235, -0.0913, -0.0046, -0.0849],\n",
       "        [ 0.1235, -0.0913, -0.0046, -0.0849],\n",
       "        ...,\n",
       "        [ 0.1235, -0.0913, -0.0046, -0.0849],\n",
       "        [ 0.1235, -0.0913, -0.0046, -0.0849],\n",
       "        [ 0.1235, -0.0913, -0.0046, -0.0849]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hor, ver = hor_ver_graph(data.triples, data.num_entities, data.num_relations)\n",
    "model = torch.load('aifb_chk/model_aifb')\n",
    "\n",
    "ypred = model.forward2(hor, ver)\n",
    "ypred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = sub_sparse_tensor(masked_ver, 0, data, low_threshold=False)\n",
    "a = match_to_triples(m.coalesce().indices().detach().numpy().T, data.triples)\n",
    "l = []\n",
    "for i in sub_triples[:,1]:\n",
    "    l.append(data.i2rel[int(i)][0])\n",
    "print(Counter(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "rel = visualize(node_idx, 1, data, masked_ver, 0.5, result_weights= False, low_threshold=True)\n",
    "rel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_dict(dict_index):\n",
    "    encoded_dict = {}\n",
    "    dict = encode_classes(dict_index)\n",
    "    for k,v in dict_index.items():\n",
    "        for k1,v1 in dict.items():\n",
    "            if v==k1:\n",
    "                encoded_dict[k] = v1\n",
    "                #encoded_dict[k].append(v)\n",
    "    return encoded_dict\n",
    "\n",
    "encode_dict(dict_index)\n",
    "encode_classes(dict_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_classes(dict_index):\n",
    "    d = []\n",
    "    for k,v in dict_index.items():\n",
    "        d.append(v)\n",
    "    a = np.unique(d)\n",
    "    dict = {}\n",
    "    for i,j in zip(a, range(len(a))):\n",
    "        dict[i] = j\n",
    "        \n",
    "    return dict\n",
    "\n",
    "dict_index = dict_index_classes(data,masked_ver)\n",
    "a = encode_classes(dict_index)\n",
    "dict_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict = {}\n",
    "for k,v in dict_index.items():\n",
    "    for k1,v1 in data.entities_classes.items():\n",
    "        if v==k1: \n",
    "\n",
    "            dict[k] = v1\n",
    "        else:\n",
    "            if k not in dict:\n",
    "                dict[k] = 100\n",
    "           \n",
    "\n",
    "#color_list = list(dict.values())\n",
    "dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Data' object has no attribute 'entities'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 27\u001b[0m\n\u001b[1;32m     24\u001b[0m     data\u001b[39m.\u001b[39mentities_classes \u001b[39m=\u001b[39m d\n\u001b[1;32m     25\u001b[0m     \u001b[39mreturn\u001b[39;00m d      \n\u001b[0;32m---> 27\u001b[0m d_classes(data)\n",
      "Cell \u001b[0;32mIn[20], line 5\u001b[0m, in \u001b[0;36md_classes\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39md_classes\u001b[39m(data):\n\u001b[1;32m      2\u001b[0m     \u001b[39m\"\"\" \u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m    Get classes of nodes (select only the alphanum - not literals)\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m     indices_nodes \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39;49mentities\n\u001b[1;32m      6\u001b[0m     d \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(data\u001b[39m.\u001b[39me2i\u001b[39m.\u001b[39mkeys())\n\u001b[1;32m      7\u001b[0m     values_indices_nodes \u001b[39m=\u001b[39m [d[i] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m indices_nodes]\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Data' object has no attribute 'entities'"
     ]
    }
   ],
   "source": [
    "def d_classes(data):\n",
    "    \"\"\" \n",
    "    Get classes of nodes (select only the alphanum - not literals)\n",
    "    \"\"\"\n",
    "    indices_nodes = data.entities\n",
    "    d = list(data.e2i.keys())\n",
    "    values_indices_nodes = [d[i] for i in indices_nodes]\n",
    "    dict = {}\n",
    "    for i in range(len(values_indices_nodes)):\n",
    "        try:\n",
    "            dict[values_indices_nodes[i][0]] = str(values_indices_nodes[i]).split('/')[3]\n",
    "            \n",
    "        except IndexError :\n",
    "            dict[values_indices_nodes[i][0]] = str(values_indices_nodes[i])\n",
    "\n",
    "    a = encode_classes(dict)   \n",
    "    d = {}\n",
    "\n",
    "    c = 0\n",
    "    for k in a.keys():\n",
    "        if k.isalpha():\n",
    "            d[k] = c\n",
    "            c+=1\n",
    "    data.entities_classes = d\n",
    "    return d      \n",
    "\n",
    "d_classes(data)\n",
    "# dict_index = d_classes(data)\n",
    "# a = encode_classes(dict_index)\n",
    "\n",
    "# d = {}\n",
    "\n",
    "# c = 0\n",
    "# for k in a.keys():\n",
    "#     if k.isalpha():\n",
    "#         d[k] = c\n",
    "#         c+=1\n",
    "# dict       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_classes(dict_index):\n",
    "    d = []\n",
    "    for k,v in dict_index.items():\n",
    "        d.append(v)\n",
    "    a = np.unique(d)\n",
    "    dict = {}\n",
    "    for i,j in zip(a, range(len(a))):\n",
    "        dict[i] = j\n",
    "        \n",
    "    return dict\n",
    "\n",
    "\n",
    "def encode_dict(dict_index):\n",
    "    encoded_dict = {}\n",
    "    dict = encode_classes(dict_index)\n",
    "    for k,v in dict_index.items():\n",
    "        for k1,v1 in dict.items():\n",
    "            if v==k1:\n",
    "                encoded_dict[k] = v1\n",
    "    return encoded_dict\n",
    "\n",
    "encode_dict(dict_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize the result\n",
    "def visualize_data(node_idx, data, num_hops):\n",
    "    \"\"\"Visualizes the n-hop neighborhood of a given node.\"\"\"\n",
    "    edge_index = edge_index_oneadj(data.triples)\n",
    "    sub_edges, neighborhoods, sub_edges_tensor = find_n_hop_neighbors(edge_index, num_hops, node_idx)\n",
    "\n",
    "    G = nx.from_edgelist(sub_edges)\n",
    "    \n",
    "    #create dict of index - node: to visualize index of the node\n",
    "    labeldict = {}\n",
    "    for node in G.nodes:\n",
    "        labeldict[node] = node \n",
    "    print(G.nodes)\n",
    "    print(G.number_of_edges)\n",
    "    # dict_index = dict_index_classes(data,masked_ver)\n",
    "    # #order dict index according to G nodes in networkx\n",
    "    # ordered_dict = {}\n",
    "    # for item in list(G.nodes):\n",
    "    #     ordered_dict[item] = dict_index[item]\n",
    "\n",
    "    # dict_index = ordered_dict\n",
    "    \n",
    "\n",
    "    # #get inverse of dict to allow mapping of different 'classes' of nodes to different nodes\n",
    "    # inv_map = {v: k for k, v in dict_index.items()}\n",
    "    # print(inv_map) #use inv:map to get a legend of node colors for later \n",
    "    # color_list = list(dict_index.values())\n",
    "    \n",
    "    # #make a list out of it \n",
    "    # for i in range(len(color_list)):\n",
    "    #     if color_list[i] in inv_map:\n",
    "    #         color_list[i] = inv_map[color_list[i]]   \n",
    "\n",
    "            \n",
    "    #edge colors reflect the masked ver values - more important relations have darker color\n",
    "    #to check why we have relations than expexted :')\n",
    "\n",
    "    #can get it through original triples \n",
    "    triples_matched = match_to_triples(np.array(sub_edges), data.triples)\n",
    "    print(triples_matched)\n",
    "    edge_colors = triples_matched[:,1].detach().numpy()\n",
    "    print(edge_colors)\n",
    "    colormap = plt.cm.ScalarMappable(norm=plt.Normalize(vmin=min(edge_colors), vmax=max(edge_colors)), cmap = \"rainbow\")\n",
    "\n",
    "    # Create a color map for each value in the list\n",
    "    colors = [colormap.to_rgba(val) for val in edge_colors]\n",
    "    #cmap=cm.rainbow(np.array(edge_colors))\n",
    "\n",
    "\n",
    "    # draw graph with edge colors\n",
    "    plt.figure()  \n",
    "    plt.title(\"Node {}'s {}-hop neighborhood\".format(node_idx, num_hops))\n",
    "    pos = nx.circular_layout(G)\n",
    "    nx.draw(G, pos=pos, with_labels=True, edge_color = colors, edge_cmap=plt.cm.Reds,labels = labeldict, cmap=\"Set2\" )\n",
    "\n",
    "\n",
    "\n",
    "    plt.show()    \n",
    "\n",
    "visualize_data(5797, data, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "a = torch.load('aifb_chk/masked_ver1')\n",
    "b = torch.load('aifb_chk/masked_ver2')\n",
    "a, b = a.coalesce().values(), b.coalesce().values()\n",
    "s = a.shape[0]\n",
    "# print(s*s-np.count_nonzero(a==b))\n",
    "# np.count_nonzero(b)\n",
    "a,b = torch.tensor(a), torch.tensor(b)\n",
    "\n",
    "res = torch.abs(a-b) <= 0.1\n",
    "res.float().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = torch.abs(a-b) <= 0.1\n",
    "s*s- res.float().sum()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c30f2af5f468e7f5b45bcc30fca5f4886c90d54777aed916ed5f6294dfb24bf2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
