{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Caskroom/miniconda/base/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "import kgbench as kg\n",
    "import fire, sys\n",
    "import math\n",
    "\n",
    "from kgbench import load, tic, toc, d\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "\n",
    "#\n",
    "from torch_geometric.utils import to_networkx\n",
    "import networkx as nx"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEPS\n",
    "1. edge index\n",
    "2. Neighborhood\n",
    "3. Extract neighborhood\n",
    "4. Match to triples\n",
    "5. Match to classes\n",
    "6. Preprocess data: enrich, sum sparse, adj \n",
    "6. Get hor ver edges (adjacency for rels)\n",
    "7. Edge mask\n",
    "8. Masked hor ver edges\n",
    "9. Train and save model and predictions\n",
    "10. Loss function\n",
    "11. Explain class - namely put all together\n",
    "12. Forward and optimize for the masked adjacency\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#edge index\n",
    "def edge_index_oneadj(triples):\n",
    "    edge_index = torch.stack((triples[:, 0], triples[:, 2]),dim=0)\n",
    "    return edge_index\n",
    "\n",
    "#Neighborhood\n",
    "#Extract neighborhood\n",
    "\n",
    "def find_n_hop_neighbors(edge_index, n, node=None):\n",
    "    # create dictionary of node neighborhoods\n",
    "    neighborhoods = {}\n",
    "    for i in range(edge_index.max().item() + 1):\n",
    "        neighborhoods[i] = set()\n",
    "\n",
    "    # find 1-hop neighbors and corresponding edges\n",
    "    edges = []\n",
    "    for j in range(edge_index.shape[1]):\n",
    "        src, dst = edge_index[0, j].item(), edge_index[1, j].item()\n",
    "        neighborhoods[src].add(dst)\n",
    "        neighborhoods[dst].add(src)\n",
    "        edges.append((src, dst))\n",
    "\n",
    "    # find n-hop neighbors for the specified node or all nodes\n",
    "\n",
    "    for k in range(2, n+1):\n",
    "        new_neighbors = set()\n",
    "        for neighbor in neighborhoods[node]:\n",
    "            new_neighbors.update(neighborhoods[neighbor])\n",
    "        neighborhoods[node].update(new_neighbors)\n",
    "    sub_edges = []\n",
    "    for edge in edges:\n",
    "        src, dst = edge\n",
    "        if src in neighborhoods[node] and dst in neighborhoods[node] or src == node or dst == node:\n",
    "            sub_edges.append(edge)\n",
    "            \n",
    "    sub_edges_tensor = torch.tensor([sub_edges[i] for i in range(len(sub_edges))]).t()        \n",
    "\n",
    "    #return {node: sub_edges}, {node: neighborhoods[node]}, sub_edges_tensor\n",
    "    return sub_edges, neighborhoods[node], sub_edges_tensor\n",
    "\n",
    "\n",
    "#match triples \n",
    "def match_to_triples(tensor1, tensor2):\n",
    "    \"\"\"\n",
    "    tensor1: sub_edge tensor: edges of the neighborhood - transpose!!\n",
    "    tensor2: data.triples: all edges\n",
    "    \"\"\"\n",
    "    matching = []\n",
    "    for i,i2 in zip(tensor1[:,0],tensor1[:,1]):\n",
    "        for j,j1,j2, index in zip(tensor2[:,0],tensor2[:,1],  tensor2[:,2], range(len(tensor2[:,0]))):\n",
    "            if i == j and i2 == j2:\n",
    "                matching.append(tensor2[index])\n",
    "\n",
    "    result = torch.stack(matching)\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "#PREPROCESS\n",
    "def enrich(triples : torch.Tensor, n : int, r: int):\n",
    "    \"\"\"\n",
    "    Enriches the given triples with self-loops and inverse relations.\n",
    "\n",
    "    \"\"\"\n",
    "    cuda = triples.is_cuda\n",
    "\n",
    "    inverses = torch.cat([\n",
    "        triples[:, 2:],\n",
    "        triples[:, 1:2] + r,\n",
    "        triples[:, :1]\n",
    "    ], dim=1)\n",
    "\n",
    "    selfloops = torch.cat([\n",
    "        torch.arange(n, dtype=torch.long,  device=d(cuda))[:, None],\n",
    "        torch.full((n, 1), fill_value=2*r),\n",
    "        torch.arange(n, dtype=torch.long, device=d(cuda))[:, None],\n",
    "    ], dim=1)\n",
    "\n",
    "    return torch.cat([triples, inverses, selfloops], dim=0)\n",
    "\n",
    "def sum_sparse(indices, values, size, row=True):\n",
    "    \"\"\"\n",
    "    Sum the rows or columns of a sparse matrix, and redistribute the\n",
    "    results back to the non-sparse row/column entries\n",
    "\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    ST = torch.cuda.sparse.FloatTensor if indices.is_cuda else torch.sparse.FloatTensor\n",
    "\n",
    "    assert len(indices.size()) == 2\n",
    "\n",
    "    k, r = indices.size()\n",
    "\n",
    "    if not row:\n",
    "        # transpose the matrix\n",
    "        indices = torch.cat([indices[:, 1:2], indices[:, 0:1]], dim=1)\n",
    "        size = size[1], size[0]\n",
    "\n",
    "    ones = torch.ones((size[1], 1), device=d(indices))\n",
    "\n",
    "    smatrix = ST(indices.t(), values, size=size)\n",
    "    sums = torch.mm(smatrix, ones) # row/column sums\n",
    "\n",
    "    sums = sums[indices[:, 0]]\n",
    "\n",
    "    assert sums.size() == (k, 1)\n",
    "\n",
    "    return sums.view(k)\n",
    "\n",
    "def adj(triples, num_nodes, num_rels, cuda=False, vertical=True):\n",
    "    \"\"\"\n",
    "     Computes a sparse adjacency matrix for the given graph (the adjacency matrices of all\n",
    "     relations are stacked vertically).\n",
    "\n",
    "     :param edges: List representing the triples\n",
    "     :param i2r: list of relations\n",
    "     :param i2n: list of nodes\n",
    "     :return: sparse tensor\n",
    "    \"\"\"\n",
    "    r, n = num_rels, num_nodes\n",
    "    size = (r * n, n) if vertical else (n, r * n)\n",
    "\n",
    "    from_indices = []\n",
    "    upto_indices = []\n",
    "\n",
    "    for s, p, o in triples:\n",
    "\n",
    "        offset = p.item() * n\n",
    "        \n",
    "\n",
    "        if vertical:\n",
    "            s = offset + s.item()\n",
    "        else:\n",
    "            o = offset + o.item()\n",
    "\n",
    "        from_indices.append(s)\n",
    "        upto_indices.append(o)\n",
    "        \n",
    "\n",
    "    indices = torch.tensor([from_indices, upto_indices], dtype=torch.long, device=d(cuda))\n",
    "\n",
    "\n",
    "    assert indices.size(1) == len(triples)\n",
    "    assert indices[0, :].max() < size[0], f'{indices[0, :].max()}, {size}, {r}'\n",
    "    assert indices[1, :].max() < size[1], f'{indices[1, :].max()}, {size}, {r}'\n",
    "\n",
    "    return indices.t(), size\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Get adjacency matrix: in this context this is hor / ver graph\n",
    "def hor_ver_graph(triples, n, r):\n",
    "    #triples = enrich(triples_small, n, r)\n",
    "\n",
    "    hor_ind, hor_size = adj(triples, n, 2*r+1, vertical=False)\n",
    "    ver_ind, ver_size = adj(triples, n, 2*r+1, vertical=True)\n",
    "    #number of relations is 2*r+1 because we added the inverse and self loop\n",
    "\n",
    "    _, rn = hor_size #horizontally stacked adjacency matrix size\n",
    "    print(hor_size)\n",
    "    r = rn // n #number of relations enriched divided by number of nodes\n",
    "\n",
    "    vals = torch.ones(ver_ind.size(0), dtype=torch.float) #number of enriched triples\n",
    "    vals = vals / sum_sparse(ver_ind, vals, ver_size) #normalize the values by the number of edges\n",
    "\n",
    "    hor_graph = torch.sparse.FloatTensor(indices=hor_ind.t(), values=vals, size=hor_size) #size: n,r, emb\n",
    "\n",
    "\n",
    "    ver_graph = torch.sparse.FloatTensor(indices=ver_ind.t(), values=vals, size=ver_size)\n",
    "\n",
    "    return hor_graph, ver_graph\n",
    "\n",
    "\n",
    "#Edge mask\n",
    "\n",
    "def construct_edge_mask( num_nodes, init_strategy=\"normal\", const_val=1.0):\n",
    "    \"\"\"\n",
    "    Construct edge mask\n",
    "    input;\n",
    "        num_nodes: number of nodes in the neighborhood\n",
    "        init_strategy: initialization strategy for the mask\n",
    "        const_val: constant value for the mask\n",
    "    output:\n",
    "        mask: edge mask    \n",
    "    \"\"\"\n",
    "    mask = nn.Parameter(torch.FloatTensor(num_nodes))  #initialize the mask\n",
    "    if init_strategy == \"normal\":\n",
    "        std = nn.init.calculate_gain(\"relu\") * math.sqrt(\n",
    "            2.0 / (num_nodes + num_nodes)\n",
    "        )\n",
    "        with torch.no_grad():\n",
    "            mask.normal_(1.0, std)\n",
    "    elif init_strategy == \"const\":\n",
    "        nn.init.constant_(mask, const_val)\n",
    "    return torch.tensor(mask)\n",
    "\n",
    "#Masked 'adjacency' - masked hor vergraph\n",
    "\n",
    "def _masked_adj(mask,adj, diag_mask, graph):\n",
    "    \"\"\" Masked adjacency matrix \n",
    "    input: edge_mask, sub_adj, diag_mask\n",
    "    output: masked_adj\n",
    "    \"\"\"\n",
    "    sym_mask = mask\n",
    "    sym_mask = torch.sigmoid(mask)\n",
    "    \n",
    "    sym_mask = (sym_mask + sym_mask.t()) / 2\n",
    "    adj = torch.tensor(adj)\n",
    "    masked_adj = adj * sym_mask\n",
    "\n",
    "    #return masked_adj #* diag_mask\n",
    "    return torch.sparse.FloatTensor(indices=graph.coalesce().indices(), values= masked_adj, size=graph.coalesce().size())\n",
    "\n",
    "\n",
    "\n",
    "#match tro classes\n",
    "def match_to_classes(tensor1, tensor2):\n",
    "    \"\"\"\n",
    "    tensor1: sub graph indices\n",
    "    tensor2: data.y labelsss\n",
    "    \"\"\"\n",
    "    matching = []\n",
    "    for i in (tensor1[:,0]):\n",
    "        for j, index in zip(tensor2[:,0],range(len(tensor2[:,0]))):\n",
    "            if i == j:\n",
    "                matching.append(tensor2[index])\n",
    "    return matching"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RGCN MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RGCN(nn.Module):\n",
    "    \"\"\"\n",
    "    Classic RGCN\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, triples, n, r, numcls, emb=16, bases=None):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.emb = emb\n",
    "        self.bases = bases\n",
    "        self.numcls = numcls\n",
    "\n",
    "        self.triples = enrich(triples, n, r)\n",
    "\n",
    "        # horizontally and vertically stacked versions of the adjacency graph\n",
    "        hor_ind, hor_size = adj(self.triples, n, 2*r+1, vertical=False)\n",
    "        ver_ind, ver_size = adj(self.triples, n, 2*r+1, vertical=True)\n",
    "        #number of relations is 2*r+1 because we added the inverse and self loop\n",
    "\n",
    "        _, rn = hor_size #horizontally stacked adjacency matrix size\n",
    "        r = rn // n #number of relations enriched divided by number of nodes\n",
    "\n",
    "        vals = torch.ones(ver_ind.size(0), dtype=torch.float) #number of enriched triples\n",
    "        vals = vals / sum_sparse(ver_ind, vals, ver_size) #normalize the values by the number of edges\n",
    "\n",
    "        hor_graph = torch.sparse.FloatTensor(indices=hor_ind.t(), values=vals, size=hor_size) #size: n,r, emb\n",
    "        \n",
    "        \n",
    "        self.register_buffer('hor_graph', hor_graph)\n",
    "\n",
    "        ver_graph = torch.sparse.FloatTensor(indices=ver_ind.t(), values=vals, size=ver_size)\n",
    "        self.register_buffer('ver_graph', ver_graph)\n",
    "\n",
    "        # layer 1 weights\n",
    "        if bases is None:\n",
    "            self.weights1 = nn.Parameter(torch.FloatTensor(r, n, emb))\n",
    "            nn.init.xavier_uniform_(self.weights1, gain=nn.init.calculate_gain('relu'))\n",
    "\n",
    "            self.bases1 = None\n",
    "        else:\n",
    "            self.comps1 = nn.Parameter(torch.FloatTensor(r, bases))\n",
    "            nn.init.xavier_uniform_(self.comps1, gain=nn.init.calculate_gain('relu'))\n",
    "\n",
    "            self.bases1 = nn.Parameter(torch.FloatTensor(bases, n, emb))\n",
    "            nn.init.xavier_uniform_(self.bases1, gain=nn.init.calculate_gain('relu'))\n",
    "\n",
    "        # layer 2 weights\n",
    "        if bases is None:\n",
    "\n",
    "            self.weights2 = nn.Parameter(torch.FloatTensor(r, emb, numcls) )\n",
    "            nn.init.xavier_uniform_(self.weights2, gain=nn.init.calculate_gain('relu'))\n",
    "\n",
    "            self.bases2 = None\n",
    "        else:\n",
    "            self.comps2 = nn.Parameter(torch.FloatTensor(r, bases))\n",
    "            nn.init.xavier_uniform_(self.comps2, gain=nn.init.calculate_gain('relu'))\n",
    "\n",
    "            self.bases2 = nn.Parameter(torch.FloatTensor(bases, emb, numcls))\n",
    "            nn.init.xavier_uniform_(self.bases2, gain=nn.init.calculate_gain('relu'))\n",
    "\n",
    "        self.bias1 = nn.Parameter(torch.FloatTensor(emb).zero_())\n",
    "        self.bias2 = nn.Parameter(torch.FloatTensor(numcls).zero_())\n",
    "\n",
    "    def forward2(self, hor_graph, ver_graph):\n",
    "\n",
    "\n",
    "        ## Layer 1\n",
    "\n",
    "        n, rn = hor_graph.size() #horizontally stacked adjacency matrix size\n",
    "        r = rn // n\n",
    "        e = self.emb\n",
    "        b, c = self.bases, self.numcls\n",
    "\n",
    "        if self.bases1 is not None:\n",
    "            # weights = torch.einsum('rb, bij -> rij', self.comps1, self.bases1)\n",
    "            weights = torch.mm(self.comps1, self.bases1.view(b, n*e)).view(r, n, e)\n",
    "        else:\n",
    "            weights = self.weights1\n",
    "\n",
    "        assert weights.size() == (r, n, e) #r relations, n nodes, e embedding size\n",
    "\n",
    "        # Apply weights and sum over relations\n",
    "        #hidden layer\n",
    "        h = torch.mm(hor_graph, weights.view(r*n, e))  #matmul with horizontally stacked adjacency matrix and initialized weights\n",
    "        assert h.size() == (n, e)\n",
    "\n",
    "        h = F.relu(h + self.bias1) #apply non linearity and add bias\n",
    "\n",
    "        ## Layer 2\n",
    "\n",
    "        # Multiply adjacencies by hidden\n",
    "        h = torch.mm(ver_graph, h) # sparse mm\n",
    "        h = h.view(r, n, e) # new dim for the relations\n",
    "\n",
    "        if self.bases2 is not None:\n",
    "            # weights = torch.einsum('rb, bij -> rij', self.comps2, self.bases2)\n",
    "            weights = torch.mm(self.comps2, self.bases2.view(b, e * c)).view(r, e, c)\n",
    "        else:\n",
    "            weights = self.weights2\n",
    "\n",
    "        # Apply weights, sum over relations\n",
    "        # h = torch.einsum('rhc, rnh -> nc', weights, h)\n",
    "        h = torch.bmm(h, weights).sum(dim=0)\n",
    "\n",
    "        assert h.size() == (n, c)\n",
    "\n",
    "        return h + self.bias2 # -- softmax is applied in the loss\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    def forward(self):\n",
    "\n",
    "\n",
    "        ## Layer 1\n",
    "\n",
    "        n, rn = self.hor_graph.size() #horizontally stacked adjacency matrix size\n",
    "        r = rn // n\n",
    "        e = self.emb\n",
    "        b, c = self.bases, self.numcls\n",
    "\n",
    "        if self.bases1 is not None:\n",
    "            # weights = torch.einsum('rb, bij -> rij', self.comps1, self.bases1)\n",
    "            weights = torch.mm(self.comps1, self.bases1.view(b, n*e)).view(r, n, e)\n",
    "        else:\n",
    "            weights = self.weights1\n",
    "\n",
    "        assert weights.size() == (r, n, e) #r relations, n nodes, e embedding size\n",
    "\n",
    "        # Apply weights and sum over relations\n",
    "        #hidden layer\n",
    "        h = torch.mm(self.hor_graph, weights.view(r*n, e))  #matmul with horizontally stacked adjacency matrix and initialized weights\n",
    "        assert h.size() == (n, e)\n",
    "\n",
    "        h = F.relu(h + self.bias1) #apply non linearity and add bias\n",
    "\n",
    "        ## Layer 2\n",
    "\n",
    "        # Multiply adjacencies by hidden\n",
    "        h = torch.mm(self.ver_graph, h) # sparse mm\n",
    "        h = h.view(r, n, e) # new dim for the relations\n",
    "\n",
    "        if self.bases2 is not None:\n",
    "            # weights = torch.einsum('rb, bij -> rij', self.comps2, self.bases2)\n",
    "            weights = torch.mm(self.comps2, self.bases2.view(b, e * c)).view(r, e, c)\n",
    "        else:\n",
    "            weights = self.weights2\n",
    "\n",
    "        # Apply weights, sum over relations\n",
    "        # h = torch.einsum('rhc, rnh -> nc', weights, h)\n",
    "        h = torch.bmm(h, weights).sum(dim=0)\n",
    "\n",
    "        assert h.size() == (n, c)\n",
    "\n",
    "        return h + self.bias2 # -- softmax is applied in the loss\n",
    "\n",
    "    def penalty(self, p=2):\n",
    "        \"\"\"\n",
    "        L2 penalty on the weights\n",
    "        \"\"\"\n",
    "        assert p==2\n",
    "\n",
    "        if self.bases is None:\n",
    "            return self.weights1.pow(2).sum()\n",
    "\n",
    "        return self.comps1.pow(p).sum() + self.bases1.pow(p).sum()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXPLAINER CLASS \n",
    "where we combine all of the previously defined functions for the sake of explanation training loop"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loss function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.6931)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_label = torch.load('aifb_chk/prediction_aifb')\n",
    "pred_label[1]\n",
    "label = data.withheld[:, 1]\n",
    "\n",
    "gt_label_node = label[1]\n",
    "\n",
    "\n",
    "logit = ypred[gt_label_node]\n",
    "-torch.log(logit) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FUNCTIONS THAT WE USE IN THE EXPLAIN.PY FILE\n",
    "\n",
    "def loss_fc(edge_mask,  pred, pred_label,label, node_idx, epoch, print=False):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        pred: y_e :  prediction made by current model\n",
    "        pred_label: y_hat : the label predicted by the original model.\n",
    "    \"\"\"\n",
    "    \n",
    "    #PRED LOSS\n",
    "    pred_label_node = pred_label[node_idx] #pred label is the prediction made by the original model\n",
    "    gt_label_node = label[node_idx]\n",
    "\n",
    "    logit = pred[gt_label_node] #pred is the prediction made by the current model\n",
    "\n",
    "    pred_loss = -torch.log(logit) #this is basically taking the cross entropy loss\n",
    "\n",
    "    # MASK SIZE EDGE LOSS\n",
    "    \n",
    "    mask = edge_mask\n",
    "    mask = torch.sigmoid(mask)\n",
    "\n",
    "    size_loss = 0.005 * torch.sum(mask)\n",
    "\n",
    "    \n",
    "    #MASK SIZE FEATURE LOSS\n",
    "    # feat_mask = (torch.sigmoid(feat_mask))\n",
    "    # feat_size_loss = 1.0 * torch.mean(feat_mask)\n",
    "\n",
    "    # EDGE MASK ENTROPY LOSS\n",
    "    mask_ent = -mask * torch.log(mask) - (1 - mask) * torch.log(1 - mask)\n",
    "    mask_ent_loss = 1.0 * torch.mean(mask_ent)\n",
    "    \n",
    "    # FEATURE MASK ENTROPY LOSS\n",
    "    # feat_mask_ent = - feat_mask * torch.log(feat_mask) - (1 - feat_mask) * torch.log(1 - feat_mask)\n",
    "\n",
    "    # feat_mask_ent_loss = 0.1  * torch.mean(feat_mask_ent)\n",
    "\n",
    "    # LAPLACIAN LOSS\n",
    "    # D = torch.diag(torch.sum(masked_adj, 0))\n",
    "    # m_adj = masked_adj \n",
    "    # L = D - m_adj\n",
    "\n",
    "    # pred_label_t = torch.tensor(pred_label, dtype=torch.float)\n",
    "\n",
    "\n",
    "    # lap_loss = ( 1.0\n",
    "    #     * (pred_label_t @ L @ pred_label_t)\n",
    "    #     / torch.Tensor(adj).numel())\n",
    "\n",
    "\n",
    "    loss = pred_loss + size_loss  + mask_ent_loss # + feat_size_loss + lap_loss\n",
    "    if print== True:\n",
    "        print(\"optimization/size_loss\", size_loss, epoch)\n",
    "        #print(\"optimization/feat_size_loss\", feat_size_loss, epoch)\n",
    "        print(\"optimization/mask_ent_loss\", mask_ent_loss, epoch)\n",
    "        print(\n",
    "            \"optimization/feat_mask_ent_loss\", mask_ent_loss, epoch\n",
    "        )\n",
    "\n",
    "        print(\"optimization/pred_loss\", pred_loss, epoch)\n",
    "        #print(\"optimization/lap_loss\", lap_loss, epoch)\n",
    "        print(\"optimization/overall_loss\", loss, epoch)\n",
    "    return loss\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#SEE how the loss function is used in the explain.py file\n",
    "# pred_label = torch.load('aifb_chk/prediction_aifb') #load prediction of original model\n",
    "# pred_label = torch.argmax(pred_label[], dim=1) #get the prediction of the query node\n",
    "# adj = torch.load('cora_chk/adj_cora') #load the adjacency matrix of the original model\n",
    "# model = torch.load('cora_chk/model_cora') #load the original model\n",
    "\n",
    "# #We are using the original model to get the prediction of the query node\n",
    "# pred, adj = model.forward(torch.Tensor(subdata), torch.Tensor(sub_adj))\n",
    "# node_pred = pred[node_idx_new, :] #get the prediction of the query node\n",
    "# pred = nn.Softmax(dim=0)(node_pred) #apply softmax to it\n",
    "# loss_fc( edge_mask, feat_mask, masked_adj,adj, pred, pred_label,data.y, node_idx, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded data aifb (0.1884s).\n",
      "Number of entities: 8285\n",
      "Number of classes: 4\n",
      "Types of relations: 45\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([2, 2, 0, 2, 2, 1, 0, 2, 0, 3, 3, 0, 0, 0, 1, 3, 2, 0, 0, 0, 0, 3, 2, 2,\n",
       "        0, 2, 1, 2, 3, 0, 0, 2, 0, 3, 1])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = kg.load('aifb', torch=True) \n",
    "print(f'Number of entities: {data.num_entities}') #data.i2e\n",
    "print(f'Number of classes: {data.num_classes}')\n",
    "print(f'Types of relations: {data.num_relations}') #data.i2r\n",
    "data.triples.shape\n",
    "\n",
    "data.withheld[:, 1]\n",
    "idxw, clsw = data.withheld[:, 0], data.withheld[:, 1]\n",
    "idxw, clsw = idxw.long(), clsw.long()\n",
    "\n",
    "#(out[idxw, :].argmax(dim=1) == clsw).sum().item() / idxw.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Explain(nn.Module):\n",
    "    def __init__(self, model, data, node_idx, n_hops):\n",
    "        super(Explain, self).__init__()\n",
    "        #Those are the parameters of the original data and model\n",
    "        self.model = model\n",
    "        self.data = data\n",
    "        self.n = data.num_entities\n",
    "        self.r = data.num_relations\n",
    "        self.triples = enrich(data.triples, self.n, self.r)\n",
    "        self.node_idx = node_idx\n",
    "        self.n_hops = n_hops\n",
    "        #self.adj = get_adjacency(data)\n",
    "        self.edge_index = edge_index_oneadj(self.triples)\n",
    "        #print(self.edge_index)\n",
    "        #self.label =  np.concatenate((np.array(data.training), np.array(data.withheld)), axis=0)\n",
    "        self.label = self.data.withheld[:, 1]\n",
    "        #self.feat = torch.Tensor(data.x)\n",
    "        #self.feat_dim = data.num_features\n",
    "        self.epoch = 1\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "        self.hor_graph, self.ver_graph = hor_ver_graph(self.triples, self.n, self.r)\n",
    "        #print(self.hor_graph)\n",
    "        self.pred_label = torch.load('aifb_chk/prediction_aifb')\n",
    "\n",
    "        #self.node_idx_new, self.sub_adj, self.sub_feat, self.sub_label, self.neighbors = extract_neighborhood(self.node_idx, self.adj, self.feat, self.label, self.n_hops)\n",
    "        self.sub_edges, self.neighbors, self.sub_edges_tensor = find_n_hop_neighbors(self.edge_index, n=self.n_hops, node=self.node_idx)\n",
    "        #print('neighbors', self.neighbors)\n",
    "\n",
    "        self.sub_triples = match_to_triples(self.sub_edges_tensor.t(),self.triples)\n",
    "        #print('sub_triples', self.sub_triples)\n",
    "        self.sub_hor_graph, self.sub_ver_graph = hor_ver_graph(self.sub_triples, self.n, self.r)\n",
    "        self.num_nodes = self.sub_hor_graph.coalesce().values().shape[0] \n",
    "        \n",
    "        #self.diag_mask = construct_diag_mask(self.neighbors)\n",
    "        #self.subdata = torch.Tensor(data.subgraph(torch.tensor(self.neighbors)).x)\n",
    "        self.edge_mask = construct_edge_mask(self.num_nodes)\n",
    "        #print('edge_mask', self.edge_mask)\n",
    "        #self.feat_mask = construct_feat_mask(self.feat_dim, init_strategy=\"normal\")\n",
    "\n",
    "\n",
    "\n",
    "    def _masked_adj(self, sub_graph):\n",
    "        \"\"\" Masked adjacency matrix \n",
    "        input: edge_mask, sub_adj, diag_mask\n",
    "        output: masked_adj\n",
    "        \"\"\"\n",
    "        adj = sub_graph.coalesce().values()\n",
    "        indices = sub_graph.coalesce().indices()\n",
    "        size = sub_graph.coalesce().size()\n",
    "        sym_mask = self.edge_mask\n",
    "        sym_mask = torch.sigmoid(self.edge_mask)\n",
    "        \n",
    "        sym_mask = (sym_mask + sym_mask.t()) / 2\n",
    "        adj = torch.tensor(adj)\n",
    "        masked_adj = adj * sym_mask\n",
    "\n",
    "        #return masked_adj #* diag_mask\n",
    "        return torch.sparse.FloatTensor(indices=indices, values= masked_adj, size=size )\n",
    "    \n",
    "    def softmax(self, pred):\n",
    "        \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "        x_index = pred[self.node_idx].detach().numpy()\n",
    "        e_x = np.exp(x_index - np.max(x_index))\n",
    "        return e_x / e_x.sum(axis=0),  np.argmax(e_x / e_x.sum(axis=0))\n",
    "    \n",
    "    def new_index(self):\n",
    "        idxw, clsw = self.data.withheld[:, 0], self.data.withheld[:, 1]\n",
    "        idxw, clsw = idxw.long(), clsw.long()\n",
    "        idxw_list = list(idxw)\n",
    "        self.new_node_idx = idxw_list.index(self.node_idx)\n",
    "        return self.new_node_idx \n",
    "        \n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            ypred: prediction of the query node made by the current model (on the subgraph)\n",
    "\n",
    "        \"\"\"\n",
    "        masked_hor = self._masked_adj(self.sub_hor_graph)\n",
    "        masked_ver = self._masked_adj(self.sub_ver_graph)\n",
    "\n",
    "\n",
    "        # self.masked_adj = self._masked_adj()\n",
    "        # feat_mask = (torch.sigmoid(self.feat_mask))\n",
    "        # x = self.sub_feat * feat_mask\n",
    "        #ypred, adj_att = model(self.subdata, masked_adj)\n",
    "        #ypred, adj_att = self.model(x, self.masked_adj)\n",
    "        ypred = self.model.forward2(masked_hor, masked_ver)\n",
    " \n",
    "\n",
    "        #_, res = self.softmax(ypred)\n",
    "        \n",
    "        node_pred = ypred[self.node_idx, :]\n",
    "        res = nn.Softmax(dim=0)(node_pred)\n",
    "  \n",
    "        return res, masked_hor, masked_ver\n",
    "    \n",
    "    def criterion(self, epoch):\n",
    "        \"\"\"\n",
    "        Computes the loss of the current model\n",
    "        \"\"\"\n",
    "        #prediction of explanation model\n",
    "        pred, masked_hor, masked_ver = self.forward()\n",
    "\n",
    "        #prediction of original model\n",
    "        #pred_label = self.pred_label[self.new_node_idx]\n",
    "\n",
    "        self.new_node_idx = self.new_index()\n",
    "        loss_val = loss_fc(self.edge_mask,  pred, self.pred_label,self.label, self.new_node_idx, epoch, print=False)\n",
    "\n",
    "        return loss_val \n",
    "    \n",
    "    # def mask_density(self):\n",
    "    #     \"\"\"\n",
    "    #     Computes the density of the edge mask\n",
    "    #     \"\"\"\n",
    "    #     mask_sum = torch.sum(self.masked_adj)\n",
    "    #     adj_sum = torch.sum(self.adj)\n",
    "    #     return mask_sum / adj_sum\n",
    "    \n",
    "    # def return_stuff(self):\n",
    "    #     pred_label = torch.argmax(self.pred_label[self.neighbors], dim=1)\n",
    "    #     return pred_label[self.node_idx], self.label[self.node_idx], self.neighbors, self.sub_label, self.sub_feat, self.n_hops\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4938, 4938, 4938,  ..., 8282, 8283, 8284],\n",
      "        [8181, 8175, 5292,  ..., 8282, 8283, 8284]])\n",
      "(8285, 753935)\n",
      "tensor(indices=tensor([[  4938,   4938,   4938,  ...,   8282,   8283,   8284],\n",
      "                       [331296, 356145, 369832,  ..., 753932, 753933, 753934]]),\n",
      "       values=tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
      "       size=(8285, 753935), nnz=66371, layout=torch.sparse_coo)\n",
      "neighbors {6976, 7905, 7973, 5757, 6920, 7933, 1002, 908, 6860, 5230, 7857, 2227, 7731, 6874, 7837}\n",
      "sub_triples tensor([[5757,   10,  908],\n",
      "        [5757,   23, 2227],\n",
      "        [5757,   27, 1002],\n",
      "        [5757,   30, 6860],\n",
      "        [5757,   47, 6860],\n",
      "        [5757,   30, 6874],\n",
      "        [5757,   47, 6874],\n",
      "        [5757,   30, 6920],\n",
      "        [5757,   47, 6920],\n",
      "        [5757,   30, 6976],\n",
      "        [5757,   47, 6976],\n",
      "        [5757,   30, 7731],\n",
      "        [5757,   47, 7731],\n",
      "        [5757,   30, 7837],\n",
      "        [5757,   47, 7837],\n",
      "        [5757,   30, 7857],\n",
      "        [5757,   47, 7857],\n",
      "        [5757,   30, 7905],\n",
      "        [5757,   47, 7905],\n",
      "        [5757,   30, 7933],\n",
      "        [5757,   47, 7933],\n",
      "        [5757,   30, 7973],\n",
      "        [5757,   47, 7973],\n",
      "        [5757,   39, 5230],\n",
      "        [6860,    2, 5757],\n",
      "        [6860,   75, 5757],\n",
      "        [6874,    2, 5757],\n",
      "        [6874,   75, 5757],\n",
      "        [6920,    2, 5757],\n",
      "        [6920,   75, 5757],\n",
      "        [6976,    2, 5757],\n",
      "        [6976,   75, 5757],\n",
      "        [7731,    2, 5757],\n",
      "        [7731,   75, 5757],\n",
      "        [7837,    2, 5757],\n",
      "        [7837,   75, 5757],\n",
      "        [7857,    2, 5757],\n",
      "        [7857,   75, 5757],\n",
      "        [7905,    2, 5757],\n",
      "        [7905,   75, 5757],\n",
      "        [7933,    2, 5757],\n",
      "        [7933,   75, 5757],\n",
      "        [7973,    2, 5757],\n",
      "        [7973,   75, 5757],\n",
      "        [ 908,   55, 5757],\n",
      "        [2227,   68, 5757],\n",
      "        [1002,   72, 5757],\n",
      "        [6860,    2, 5757],\n",
      "        [6860,   75, 5757],\n",
      "        [6874,    2, 5757],\n",
      "        [6874,   75, 5757],\n",
      "        [6920,    2, 5757],\n",
      "        [6920,   75, 5757],\n",
      "        [6976,    2, 5757],\n",
      "        [6976,   75, 5757],\n",
      "        [7731,    2, 5757],\n",
      "        [7731,   75, 5757],\n",
      "        [7837,    2, 5757],\n",
      "        [7837,   75, 5757],\n",
      "        [7857,    2, 5757],\n",
      "        [7857,   75, 5757],\n",
      "        [7905,    2, 5757],\n",
      "        [7905,   75, 5757],\n",
      "        [7933,    2, 5757],\n",
      "        [7933,   75, 5757],\n",
      "        [7973,    2, 5757],\n",
      "        [7973,   75, 5757],\n",
      "        [5230,   84, 5757],\n",
      "        [5757,   30, 6860],\n",
      "        [5757,   47, 6860],\n",
      "        [5757,   30, 6874],\n",
      "        [5757,   47, 6874],\n",
      "        [5757,   30, 6920],\n",
      "        [5757,   47, 6920],\n",
      "        [5757,   30, 6976],\n",
      "        [5757,   47, 6976],\n",
      "        [5757,   30, 7731],\n",
      "        [5757,   47, 7731],\n",
      "        [5757,   30, 7837],\n",
      "        [5757,   47, 7837],\n",
      "        [5757,   30, 7857],\n",
      "        [5757,   47, 7857],\n",
      "        [5757,   30, 7905],\n",
      "        [5757,   47, 7905],\n",
      "        [5757,   30, 7933],\n",
      "        [5757,   47, 7933],\n",
      "        [5757,   30, 7973],\n",
      "        [5757,   47, 7973],\n",
      "        [ 908,   90,  908],\n",
      "        [1002,   90, 1002],\n",
      "        [2227,   90, 2227],\n",
      "        [5230,   90, 5230],\n",
      "        [5757,   90, 5757],\n",
      "        [6860,   90, 6860],\n",
      "        [6874,   90, 6874],\n",
      "        [6920,   90, 6920],\n",
      "        [6976,   90, 6976],\n",
      "        [7731,   90, 7731],\n",
      "        [7837,   90, 7837],\n",
      "        [7857,   90, 7857],\n",
      "        [7905,   90, 7905],\n",
      "        [7933,   90, 7933],\n",
      "        [7973,   90, 7973]])\n",
      "(8285, 753935)\n",
      "edge_mask tensor([0.9866, 1.2486, 1.0426, 1.0787, 0.9204, 0.5305, 1.0048, 1.2535, 0.7823,\n",
      "        1.0498, 0.9658, 1.1377, 0.9737, 0.8490, 1.1654, 1.2102, 0.8107, 0.9344,\n",
      "        1.2047, 1.0316, 0.7246, 0.9445, 0.7385, 0.9273, 0.9726, 0.7513, 0.9587,\n",
      "        1.4153, 0.9266, 0.9508, 1.0020, 0.8082, 0.6500, 0.9966, 1.0682, 1.2982,\n",
      "        0.6226, 0.7904, 0.7868, 0.8709, 0.8070, 1.2944, 1.0225, 0.9736, 0.9703,\n",
      "        1.1924, 1.0986, 1.3060, 1.2074, 1.1465, 0.9776, 1.0661, 0.6569, 0.9975,\n",
      "        1.1261, 1.1089, 0.9415, 1.3633, 1.1563, 1.1474, 0.8457, 0.9884, 0.9772])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ww/33zq_rh50tx94n81lb4thx0w0000gn/T/ipykernel_32166/3664967514.py:199: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(mask)\n"
     ]
    }
   ],
   "source": [
    "explainer = Explain(model = torch.load('aifb_chk/model_aifb'), data = data, node_idx = 5757, n_hops = 1)\n",
    "optimizer = torch.optim.Adam(explainer.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ww/33zq_rh50tx94n81lb4thx0w0000gn/T/ipykernel_32166/2967616077.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  adj = torch.tensor(adj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0 ; loss:  0.8559755682945251 ; pred:  tensor([0.0112, 0.0229, 0.9557, 0.0103], grad_fn=<SoftmaxBackward0>)\n",
      "epoch:  10 ; loss:  0.8107907772064209 ; pred:  tensor([3.4579e-05, 8.2919e-05, 9.9984e-01, 3.9904e-05],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "epoch:  20 ; loss:  0.8106486797332764 ; pred:  tensor([3.1855e-06, 8.2809e-06, 9.9998e-01, 3.9041e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "epoch:  30 ; loss:  0.8106390237808228 ; pred:  tensor([1.1708e-06, 3.1338e-06, 9.9999e-01, 1.4652e-06],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "epoch:  40 ; loss:  0.8106371164321899 ; pred:  tensor([7.7378e-07, 2.0937e-06, 1.0000e+00, 9.7576e-07],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "epoch:  50 ; loss:  0.8106365203857422 ; pred:  tensor([6.5210e-07, 1.7710e-06, 1.0000e+00, 8.2448e-07],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "epoch:  60 ; loss:  0.8106362819671631 ; pred:  tensor([6.0580e-07, 1.6469e-06, 1.0000e+00, 7.6654e-07],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "epoch:  70 ; loss:  0.8106361627578735 ; pred:  tensor([5.8458e-07, 1.5890e-06, 1.0000e+00, 7.3974e-07],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "epoch:  80 ; loss:  0.8106361627578735 ; pred:  tensor([5.7223e-07, 1.5546e-06, 1.0000e+00, 7.2395e-07],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "epoch:  90 ; loss:  0.8106361627578735 ; pred:  tensor([5.6295e-07, 1.5283e-06, 1.0000e+00, 7.1197e-07],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "explainer.train()\n",
    "for epoch in range(100):\n",
    "    explainer.zero_grad()\n",
    "    optimizer.zero_grad()\n",
    "    ypred, masked_hor, masked_ver = explainer.forward()\n",
    "    loss = explainer.criterion(epoch)\n",
    "    #pred_label, original_label, neighbors, sub_label, sub_feat, num_hops = explainer.return_stuff()\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    #mask_density = explainer.mask_density()\n",
    "\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "\n",
    "        print(\n",
    "        \"epoch: \",\n",
    "        epoch,\n",
    "        \"; loss: \",\n",
    "        loss.item(),\n",
    "        # \"; mask density: \",\n",
    "        # mask_density.item(),\n",
    "        \"; pred: \",\n",
    "        ypred,\n",
    "        # \"; labels equal: \",\n",
    "        # torch.argmax(ypred) == original_label== pred_label,\n",
    "\n",
    "    )\n",
    "\n",
    "\n",
    "# adj_atts = torch.sigmoid(adj_atts).squeeze()\n",
    "# masked_adj = adj_atts * sub_adj.squeeze()\n",
    "# masked_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(indices=tensor([[ 23430,  23444,  23490,  23546,  24301,  24407,  24427,\n",
       "                         24475,  24503,  24543,  88607, 196312, 229452, 254307,\n",
       "                        254307, 254307, 254307, 254307, 254307, 254307, 254307,\n",
       "                        254307, 254307, 328872, 395152, 395152, 395152, 395152,\n",
       "                        395152, 395152, 395152, 395152, 395152, 395152, 456583,\n",
       "                        565607, 597522, 628235, 628249, 628295, 628351, 629106,\n",
       "                        629212, 629232, 629280, 629308, 629348, 701170, 746558,\n",
       "                        746652, 747877, 750880, 751407, 752510, 752524, 752570,\n",
       "                        752626, 753381, 753487, 753507, 753555, 753583, 753623],\n",
       "                       [  5757,   5757,   5757,   5757,   5757,   5757,   5757,\n",
       "                          5757,   5757,   5757,    908,   2227,   1002,   6860,\n",
       "                          6874,   6920,   6976,   7731,   7837,   7857,   7905,\n",
       "                          7933,   7973,   5230,   6860,   6874,   6920,   6976,\n",
       "                          7731,   7837,   7857,   7905,   7933,   7973,   5757,\n",
       "                          5757,   5757,   5757,   5757,   5757,   5757,   5757,\n",
       "                          5757,   5757,   5757,   5757,   5757,   5757,    908,\n",
       "                          1002,   2227,   5230,   5757,   6860,   6874,   6920,\n",
       "                          6976,   7731,   7837,   7857,   7905,   7933,   7973]]),\n",
       "       values=tensor([0.7284, 0.7771, 0.7394, 0.7463, 0.7151, 0.6296, 0.7320,\n",
       "                      0.7779, 0.6862, 0.7407, 0.7243, 0.7573, 0.7259, 0.0700,\n",
       "                      0.0762, 0.0770, 0.0692, 0.0718, 0.0769, 0.0737, 0.0674,\n",
       "                      0.0720, 0.0677, 0.7165, 0.0726, 0.0679, 0.0723, 0.0805,\n",
       "                      0.0716, 0.0721, 0.0731, 0.0692, 0.0657, 0.0730, 0.7442,\n",
       "                      0.7855, 0.6508, 0.6879, 0.6871, 0.7049, 0.6915, 0.7849,\n",
       "                      0.7355, 0.7258, 0.7252, 0.7672, 0.7500, 0.7868, 0.7698,\n",
       "                      0.7589, 0.7266, 0.7439, 0.6586, 0.7306, 0.7551, 0.7519,\n",
       "                      0.7194, 0.7963, 0.7607, 0.7590, 0.6997, 0.7288, 0.7265]),\n",
       "       size=(753935, 8285), nnz=63, layout=torch.sparse_coo)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_ver"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
